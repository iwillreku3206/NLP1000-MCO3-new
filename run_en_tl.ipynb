{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iajq1W8ipjyK"
      },
      "source": [
        "### Unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 560
        },
        "id": "QmUBVEnvCDJv",
        "outputId": "21af24cf-3a25-4f65-9445-ef1f24fcdd19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "W1117 14:23:16.057000 2756 site-packages\\torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\rek\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\unsloth_zoo\\gradient_checkpointing.py:348: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:35.)\n",
            "  GPU_BUFFERS = tuple([torch.empty(2*256*2048, dtype = dtype, device = f\"{DEVICE_TYPE_TORCH}:{i}\") for i in range(n_gpus)])\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth 2025.11.3: Fast Qwen3 patching. Transformers: 4.56.2.\n",
            "   \\\\   /|    NVIDIA GeForce RTX 2060 SUPER. Num GPUs = 1. Max memory: 8.0 GB. Platform: Windows.\n",
            "O^O/ \\_/ \\    Torch: 2.9.0+cu128. CUDA: 7.5. CUDA Toolkit: 12.8. Triton: 3.5.1\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.33.post1. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth 2025.11.3 patched 36 layers with 36 QKV layers, 36 O layers and 36 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "from datasets import Dataset\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"trained-en-tl\",\n",
        "    max_seq_length = 512,   # Context length - can be longer, but uses more memory\n",
        "    load_in_4bit = True,     # 4bit uses much less memory\n",
        "    load_in_8bit = False,    # A bit more accurate, uses 2x memory\n",
        "    full_finetuning = False, # We have full finetuning now!\n",
        "    # token = \"hf_...\",      # use one if using gated models\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "kR3gIAX-SM2q"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Narito kami na makapagkasundo ngayon.\n"
          ]
        }
      ],
      "source": [
        "from transformers import TextIteratorStreamer\n",
        "\n",
        "def translate(sentence: str):\n",
        "    messages = [\n",
        "        {\"role\" : \"user\", \"content\" : f\"Translate English to Tagalog: {sentence}\"}\n",
        "    ]\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize = False,\n",
        "        add_generation_prompt = True, # Must add for generation\n",
        "        enable_thinking = False, # Disable thinking \n",
        "    )\n",
        "    streamer = TextIteratorStreamer(tokenizer, skip_prompt = True, skip_special_tokens = True)\n",
        "    _ = model.generate(\n",
        "        **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
        "        max_new_tokens = 256, # Increase for longer outputs!\n",
        "        temperature = 0.7, top_p = 0.8, top_k = 20, # For non thinking\n",
        "        streamer = streamer\n",
        "    )\n",
        "\n",
        "    output = \"\"\n",
        "    for new_text in streamer:\n",
        "        output += new_text\n",
        "    \n",
        "    return output\n",
        "\n",
        "for sentence in [\n",
        "    'We will win today'\n",
        "]:\n",
        "    print(translate(sentence))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
