{
  "best_global_step": 1600,
  "best_metric": 7.04296875,
  "best_model_checkpoint": "training-nllb-tgl-to-bicol-working\\checkpoint-1600",
  "epoch": 0.47800433191425795,
  "eval_steps": 200,
  "global_step": 1600,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.00029875270744641124,
      "grad_norm": 0.10821348428726196,
      "learning_rate": 0.0005,
      "loss": 10.5107,
      "step": 1
    },
    {
      "epoch": 0.0005975054148928225,
      "grad_norm": 0.11005502939224243,
      "learning_rate": 0.0004999253285543608,
      "loss": 11.3281,
      "step": 2
    },
    {
      "epoch": 0.0008962581223392337,
      "grad_norm": 0.17715798318386078,
      "learning_rate": 0.0004998506571087216,
      "loss": 10.7266,
      "step": 3
    },
    {
      "epoch": 0.001195010829785645,
      "grad_norm": 0.18851852416992188,
      "learning_rate": 0.0004997759856630824,
      "loss": 10.7266,
      "step": 4
    },
    {
      "epoch": 0.001493763537232056,
      "grad_norm": 0.27457132935523987,
      "learning_rate": 0.0004997013142174433,
      "loss": 11.1172,
      "step": 5
    },
    {
      "epoch": 0.0017925162446784674,
      "grad_norm": 0.32356637716293335,
      "learning_rate": 0.0004996266427718041,
      "loss": 11.127,
      "step": 6
    },
    {
      "epoch": 0.002091268952124879,
      "grad_norm": 0.38698747754096985,
      "learning_rate": 0.0004995519713261649,
      "loss": 11.0762,
      "step": 7
    },
    {
      "epoch": 0.00239002165957129,
      "grad_norm": 0.4688027501106262,
      "learning_rate": 0.0004994772998805257,
      "loss": 11.0762,
      "step": 8
    },
    {
      "epoch": 0.002688774367017701,
      "grad_norm": 0.5016260147094727,
      "learning_rate": 0.0004994026284348865,
      "loss": 10.5703,
      "step": 9
    },
    {
      "epoch": 0.002987527074464112,
      "grad_norm": 0.6383640170097351,
      "learning_rate": 0.0004993279569892473,
      "loss": 10.9414,
      "step": 10
    },
    {
      "epoch": 0.0032862797819105238,
      "grad_norm": 0.6936139464378357,
      "learning_rate": 0.0004992532855436081,
      "loss": 11.3438,
      "step": 11
    },
    {
      "epoch": 0.003585032489356935,
      "grad_norm": 0.6707635521888733,
      "learning_rate": 0.000499178614097969,
      "loss": 10.9121,
      "step": 12
    },
    {
      "epoch": 0.003883785196803346,
      "grad_norm": 0.6898233890533447,
      "learning_rate": 0.0004991039426523298,
      "loss": 10.5566,
      "step": 13
    },
    {
      "epoch": 0.004182537904249758,
      "grad_norm": 0.6574143767356873,
      "learning_rate": 0.0004990292712066906,
      "loss": 11.2637,
      "step": 14
    },
    {
      "epoch": 0.004481290611696168,
      "grad_norm": 0.5881863832473755,
      "learning_rate": 0.0004989545997610514,
      "loss": 10.5469,
      "step": 15
    },
    {
      "epoch": 0.00478004331914258,
      "grad_norm": 0.5066705942153931,
      "learning_rate": 0.0004988799283154122,
      "loss": 10.5879,
      "step": 16
    },
    {
      "epoch": 0.0050787960265889906,
      "grad_norm": 0.5587491989135742,
      "learning_rate": 0.000498805256869773,
      "loss": 11.0312,
      "step": 17
    },
    {
      "epoch": 0.005377548734035402,
      "grad_norm": 0.4914754033088684,
      "learning_rate": 0.0004987305854241338,
      "loss": 10.9629,
      "step": 18
    },
    {
      "epoch": 0.005676301441481814,
      "grad_norm": 0.47820284962654114,
      "learning_rate": 0.0004986559139784946,
      "loss": 10.7637,
      "step": 19
    },
    {
      "epoch": 0.005975054148928224,
      "grad_norm": 0.6641180515289307,
      "learning_rate": 0.0004985812425328555,
      "loss": 11.5117,
      "step": 20
    },
    {
      "epoch": 0.006273806856374636,
      "grad_norm": 0.4952411949634552,
      "learning_rate": 0.0004985065710872163,
      "loss": 10.7676,
      "step": 21
    },
    {
      "epoch": 0.0065725595638210475,
      "grad_norm": 0.436328262090683,
      "learning_rate": 0.0004984318996415771,
      "loss": 9.9941,
      "step": 22
    },
    {
      "epoch": 0.006871312271267458,
      "grad_norm": 0.5016461610794067,
      "learning_rate": 0.0004983572281959379,
      "loss": 10.2441,
      "step": 23
    },
    {
      "epoch": 0.00717006497871387,
      "grad_norm": 0.48784565925598145,
      "learning_rate": 0.0004982825567502987,
      "loss": 10.7227,
      "step": 24
    },
    {
      "epoch": 0.0074688176861602805,
      "grad_norm": 0.46044033765792847,
      "learning_rate": 0.0004982078853046595,
      "loss": 9.875,
      "step": 25
    },
    {
      "epoch": 0.007767570393606692,
      "grad_norm": 0.4846183657646179,
      "learning_rate": 0.0004981332138590203,
      "loss": 10.5723,
      "step": 26
    },
    {
      "epoch": 0.008066323101053104,
      "grad_norm": 0.45062729716300964,
      "learning_rate": 0.0004980585424133811,
      "loss": 9.9922,
      "step": 27
    },
    {
      "epoch": 0.008365075808499515,
      "grad_norm": 0.5014210343360901,
      "learning_rate": 0.000497983870967742,
      "loss": 10.6836,
      "step": 28
    },
    {
      "epoch": 0.008663828515945925,
      "grad_norm": 0.5104053616523743,
      "learning_rate": 0.0004979091995221028,
      "loss": 10.3477,
      "step": 29
    },
    {
      "epoch": 0.008962581223392337,
      "grad_norm": 0.4748744070529938,
      "learning_rate": 0.0004978345280764636,
      "loss": 9.6602,
      "step": 30
    },
    {
      "epoch": 0.009261333930838748,
      "grad_norm": 0.49339559674263,
      "learning_rate": 0.0004977598566308244,
      "loss": 10.7129,
      "step": 31
    },
    {
      "epoch": 0.00956008663828516,
      "grad_norm": 0.39628931879997253,
      "learning_rate": 0.0004976851851851852,
      "loss": 9.627,
      "step": 32
    },
    {
      "epoch": 0.009858839345731571,
      "grad_norm": 0.42897528409957886,
      "learning_rate": 0.000497610513739546,
      "loss": 10.4668,
      "step": 33
    },
    {
      "epoch": 0.010157592053177981,
      "grad_norm": 0.4145178198814392,
      "learning_rate": 0.0004975358422939068,
      "loss": 10.3262,
      "step": 34
    },
    {
      "epoch": 0.010456344760624393,
      "grad_norm": 0.4588963985443115,
      "learning_rate": 0.0004974611708482676,
      "loss": 11.2031,
      "step": 35
    },
    {
      "epoch": 0.010755097468070804,
      "grad_norm": 0.4078393578529358,
      "learning_rate": 0.0004973864994026285,
      "loss": 9.9121,
      "step": 36
    },
    {
      "epoch": 0.011053850175517216,
      "grad_norm": 0.36339741945266724,
      "learning_rate": 0.0004973118279569893,
      "loss": 9.2246,
      "step": 37
    },
    {
      "epoch": 0.011352602882963627,
      "grad_norm": 0.40116608142852783,
      "learning_rate": 0.0004972371565113501,
      "loss": 9.7773,
      "step": 38
    },
    {
      "epoch": 0.011651355590410037,
      "grad_norm": 0.379043847322464,
      "learning_rate": 0.0004971624850657109,
      "loss": 9.5439,
      "step": 39
    },
    {
      "epoch": 0.011950108297856449,
      "grad_norm": 0.38495877385139465,
      "learning_rate": 0.0004970878136200717,
      "loss": 9.5215,
      "step": 40
    },
    {
      "epoch": 0.01224886100530286,
      "grad_norm": 0.41120296716690063,
      "learning_rate": 0.0004970131421744325,
      "loss": 10.4277,
      "step": 41
    },
    {
      "epoch": 0.012547613712749272,
      "grad_norm": 0.40164628624916077,
      "learning_rate": 0.0004969384707287933,
      "loss": 10.0625,
      "step": 42
    },
    {
      "epoch": 0.012846366420195683,
      "grad_norm": 0.4104005694389343,
      "learning_rate": 0.0004968637992831542,
      "loss": 9.9902,
      "step": 43
    },
    {
      "epoch": 0.013145119127642095,
      "grad_norm": 0.4140368103981018,
      "learning_rate": 0.000496789127837515,
      "loss": 9.8848,
      "step": 44
    },
    {
      "epoch": 0.013443871835088505,
      "grad_norm": 0.3986252248287201,
      "learning_rate": 0.0004967144563918758,
      "loss": 9.3008,
      "step": 45
    },
    {
      "epoch": 0.013742624542534916,
      "grad_norm": 0.40926143527030945,
      "learning_rate": 0.0004966397849462366,
      "loss": 9.6934,
      "step": 46
    },
    {
      "epoch": 0.014041377249981328,
      "grad_norm": 0.39645010232925415,
      "learning_rate": 0.0004965651135005974,
      "loss": 8.3613,
      "step": 47
    },
    {
      "epoch": 0.01434012995742774,
      "grad_norm": 0.4267260730266571,
      "learning_rate": 0.0004964904420549582,
      "loss": 9.5488,
      "step": 48
    },
    {
      "epoch": 0.014638882664874151,
      "grad_norm": 0.3833109736442566,
      "learning_rate": 0.000496415770609319,
      "loss": 8.9131,
      "step": 49
    },
    {
      "epoch": 0.014937635372320561,
      "grad_norm": 0.38322240114212036,
      "learning_rate": 0.0004963410991636798,
      "loss": 9.1816,
      "step": 50
    },
    {
      "epoch": 0.015236388079766973,
      "grad_norm": 0.41379764676094055,
      "learning_rate": 0.0004962664277180407,
      "loss": 9.498,
      "step": 51
    },
    {
      "epoch": 0.015535140787213384,
      "grad_norm": 0.36599066853523254,
      "learning_rate": 0.0004961917562724015,
      "loss": 8.71,
      "step": 52
    },
    {
      "epoch": 0.015833893494659794,
      "grad_norm": 0.3643648028373718,
      "learning_rate": 0.0004961170848267623,
      "loss": 8.5176,
      "step": 53
    },
    {
      "epoch": 0.016132646202106207,
      "grad_norm": 0.361419677734375,
      "learning_rate": 0.0004960424133811231,
      "loss": 8.8027,
      "step": 54
    },
    {
      "epoch": 0.016431398909552617,
      "grad_norm": 0.3777402341365814,
      "learning_rate": 0.0004959677419354839,
      "loss": 8.957,
      "step": 55
    },
    {
      "epoch": 0.01673015161699903,
      "grad_norm": 0.3822852373123169,
      "learning_rate": 0.0004958930704898447,
      "loss": 8.3691,
      "step": 56
    },
    {
      "epoch": 0.01702890432444544,
      "grad_norm": 0.3792678415775299,
      "learning_rate": 0.0004958183990442055,
      "loss": 9.1738,
      "step": 57
    },
    {
      "epoch": 0.01732765703189185,
      "grad_norm": 0.37306538224220276,
      "learning_rate": 0.0004957437275985663,
      "loss": 8.4873,
      "step": 58
    },
    {
      "epoch": 0.017626409739338263,
      "grad_norm": 0.38344499468803406,
      "learning_rate": 0.0004956690561529272,
      "loss": 9.1914,
      "step": 59
    },
    {
      "epoch": 0.017925162446784673,
      "grad_norm": 0.38386160135269165,
      "learning_rate": 0.000495594384707288,
      "loss": 8.8926,
      "step": 60
    },
    {
      "epoch": 0.018223915154231086,
      "grad_norm": 0.348595529794693,
      "learning_rate": 0.0004955197132616488,
      "loss": 7.8936,
      "step": 61
    },
    {
      "epoch": 0.018522667861677496,
      "grad_norm": 0.403987854719162,
      "learning_rate": 0.0004954450418160096,
      "loss": 8.7227,
      "step": 62
    },
    {
      "epoch": 0.018821420569123906,
      "grad_norm": 0.3449745774269104,
      "learning_rate": 0.0004953703703703704,
      "loss": 8.5898,
      "step": 63
    },
    {
      "epoch": 0.01912017327657032,
      "grad_norm": 0.35804587602615356,
      "learning_rate": 0.0004952956989247312,
      "loss": 8.5215,
      "step": 64
    },
    {
      "epoch": 0.01941892598401673,
      "grad_norm": 0.34505829215049744,
      "learning_rate": 0.0004952210274790919,
      "loss": 8.1533,
      "step": 65
    },
    {
      "epoch": 0.019717678691463143,
      "grad_norm": 0.34219563007354736,
      "learning_rate": 0.0004951463560334528,
      "loss": 7.9258,
      "step": 66
    },
    {
      "epoch": 0.020016431398909552,
      "grad_norm": 0.33780527114868164,
      "learning_rate": 0.0004950716845878137,
      "loss": 8.4648,
      "step": 67
    },
    {
      "epoch": 0.020315184106355962,
      "grad_norm": 0.3311116397380829,
      "learning_rate": 0.0004949970131421745,
      "loss": 8.3604,
      "step": 68
    },
    {
      "epoch": 0.020613936813802376,
      "grad_norm": 0.3329460918903351,
      "learning_rate": 0.0004949223416965353,
      "loss": 8.0234,
      "step": 69
    },
    {
      "epoch": 0.020912689521248785,
      "grad_norm": 0.3190329968929291,
      "learning_rate": 0.0004948476702508961,
      "loss": 8.4531,
      "step": 70
    },
    {
      "epoch": 0.0212114422286952,
      "grad_norm": 0.33799904584884644,
      "learning_rate": 0.0004947729988052569,
      "loss": 8.2656,
      "step": 71
    },
    {
      "epoch": 0.02151019493614161,
      "grad_norm": 0.31499767303466797,
      "learning_rate": 0.0004946983273596177,
      "loss": 8.4141,
      "step": 72
    },
    {
      "epoch": 0.02180894764358802,
      "grad_norm": 0.31318432092666626,
      "learning_rate": 0.0004946236559139785,
      "loss": 8.0254,
      "step": 73
    },
    {
      "epoch": 0.02210770035103443,
      "grad_norm": 0.3035445213317871,
      "learning_rate": 0.0004945489844683392,
      "loss": 7.7998,
      "step": 74
    },
    {
      "epoch": 0.02240645305848084,
      "grad_norm": 0.3642057776451111,
      "learning_rate": 0.0004944743130227002,
      "loss": 7.5146,
      "step": 75
    },
    {
      "epoch": 0.022705205765927255,
      "grad_norm": 0.39204663038253784,
      "learning_rate": 0.000494399641577061,
      "loss": 7.9434,
      "step": 76
    },
    {
      "epoch": 0.023003958473373665,
      "grad_norm": 0.3150036334991455,
      "learning_rate": 0.0004943249701314218,
      "loss": 7.7666,
      "step": 77
    },
    {
      "epoch": 0.023302711180820074,
      "grad_norm": 0.3041803240776062,
      "learning_rate": 0.0004942502986857826,
      "loss": 8.1553,
      "step": 78
    },
    {
      "epoch": 0.023601463888266488,
      "grad_norm": 0.32640504837036133,
      "learning_rate": 0.0004941756272401434,
      "loss": 8.2227,
      "step": 79
    },
    {
      "epoch": 0.023900216595712898,
      "grad_norm": 0.3017626404762268,
      "learning_rate": 0.0004941009557945042,
      "loss": 8.0449,
      "step": 80
    },
    {
      "epoch": 0.02419896930315931,
      "grad_norm": 0.2896050810813904,
      "learning_rate": 0.0004940262843488649,
      "loss": 7.8027,
      "step": 81
    },
    {
      "epoch": 0.02449772201060572,
      "grad_norm": 0.2856549918651581,
      "learning_rate": 0.0004939516129032259,
      "loss": 7.5742,
      "step": 82
    },
    {
      "epoch": 0.024796474718052134,
      "grad_norm": 0.2921745479106903,
      "learning_rate": 0.0004938769414575866,
      "loss": 7.7959,
      "step": 83
    },
    {
      "epoch": 0.025095227425498544,
      "grad_norm": 0.2688756585121155,
      "learning_rate": 0.0004938022700119475,
      "loss": 8.083,
      "step": 84
    },
    {
      "epoch": 0.025393980132944954,
      "grad_norm": 0.275928258895874,
      "learning_rate": 0.0004937275985663083,
      "loss": 7.583,
      "step": 85
    },
    {
      "epoch": 0.025692732840391367,
      "grad_norm": 0.272701621055603,
      "learning_rate": 0.0004936529271206691,
      "loss": 7.8018,
      "step": 86
    },
    {
      "epoch": 0.025991485547837777,
      "grad_norm": 0.2676122784614563,
      "learning_rate": 0.0004935782556750299,
      "loss": 8.0166,
      "step": 87
    },
    {
      "epoch": 0.02629023825528419,
      "grad_norm": 0.3009990453720093,
      "learning_rate": 0.0004935035842293907,
      "loss": 7.5195,
      "step": 88
    },
    {
      "epoch": 0.0265889909627306,
      "grad_norm": 0.32099971175193787,
      "learning_rate": 0.0004934289127837515,
      "loss": 7.2861,
      "step": 89
    },
    {
      "epoch": 0.02688774367017701,
      "grad_norm": 0.2675405740737915,
      "learning_rate": 0.0004933542413381122,
      "loss": 7.667,
      "step": 90
    },
    {
      "epoch": 0.027186496377623423,
      "grad_norm": 0.3011398911476135,
      "learning_rate": 0.0004932795698924732,
      "loss": 7.2764,
      "step": 91
    },
    {
      "epoch": 0.027485249085069833,
      "grad_norm": 0.24202702939510345,
      "learning_rate": 0.0004932048984468339,
      "loss": 7.7969,
      "step": 92
    },
    {
      "epoch": 0.027784001792516246,
      "grad_norm": 0.2650386393070221,
      "learning_rate": 0.0004931302270011948,
      "loss": 7.8994,
      "step": 93
    },
    {
      "epoch": 0.028082754499962656,
      "grad_norm": 0.2493761032819748,
      "learning_rate": 0.0004930555555555556,
      "loss": 7.7256,
      "step": 94
    },
    {
      "epoch": 0.028381507207409066,
      "grad_norm": 0.24684636294841766,
      "learning_rate": 0.0004929808841099164,
      "loss": 7.6885,
      "step": 95
    },
    {
      "epoch": 0.02868025991485548,
      "grad_norm": 0.32382333278656006,
      "learning_rate": 0.0004929062126642772,
      "loss": 7.3018,
      "step": 96
    },
    {
      "epoch": 0.02897901262230189,
      "grad_norm": 0.2478044331073761,
      "learning_rate": 0.0004928315412186379,
      "loss": 8.1084,
      "step": 97
    },
    {
      "epoch": 0.029277765329748302,
      "grad_norm": 0.28959137201309204,
      "learning_rate": 0.0004927568697729989,
      "loss": 7.8066,
      "step": 98
    },
    {
      "epoch": 0.029576518037194712,
      "grad_norm": 0.26750457286834717,
      "learning_rate": 0.0004926821983273596,
      "loss": 7.3613,
      "step": 99
    },
    {
      "epoch": 0.029875270744641122,
      "grad_norm": 0.24644550681114197,
      "learning_rate": 0.0004926075268817205,
      "loss": 7.8154,
      "step": 100
    },
    {
      "epoch": 0.030174023452087535,
      "grad_norm": 0.21514536440372467,
      "learning_rate": 0.0004925328554360812,
      "loss": 8.0107,
      "step": 101
    },
    {
      "epoch": 0.030472776159533945,
      "grad_norm": 0.24907834827899933,
      "learning_rate": 0.0004924581839904421,
      "loss": 7.8574,
      "step": 102
    },
    {
      "epoch": 0.03077152886698036,
      "grad_norm": 0.2640770375728607,
      "learning_rate": 0.0004923835125448029,
      "loss": 7.7686,
      "step": 103
    },
    {
      "epoch": 0.031070281574426768,
      "grad_norm": 0.3616735637187958,
      "learning_rate": 0.0004923088410991637,
      "loss": 7.6465,
      "step": 104
    },
    {
      "epoch": 0.03136903428187318,
      "grad_norm": 0.47297847270965576,
      "learning_rate": 0.0004922341696535245,
      "loss": 7.2412,
      "step": 105
    },
    {
      "epoch": 0.03166778698931959,
      "grad_norm": 0.2724394202232361,
      "learning_rate": 0.0004921594982078853,
      "loss": 7.6572,
      "step": 106
    },
    {
      "epoch": 0.031966539696766,
      "grad_norm": 0.23566237092018127,
      "learning_rate": 0.0004920848267622462,
      "loss": 7.7812,
      "step": 107
    },
    {
      "epoch": 0.032265292404212415,
      "grad_norm": 0.24848216772079468,
      "learning_rate": 0.0004920101553166069,
      "loss": 7.4004,
      "step": 108
    },
    {
      "epoch": 0.03256404511165883,
      "grad_norm": 0.3075593113899231,
      "learning_rate": 0.0004919354838709678,
      "loss": 6.8564,
      "step": 109
    },
    {
      "epoch": 0.032862797819105234,
      "grad_norm": 0.3262488543987274,
      "learning_rate": 0.0004918608124253285,
      "loss": 7.0518,
      "step": 110
    },
    {
      "epoch": 0.03316155052655165,
      "grad_norm": 0.2236112356185913,
      "learning_rate": 0.0004917861409796894,
      "loss": 7.7959,
      "step": 111
    },
    {
      "epoch": 0.03346030323399806,
      "grad_norm": 0.2831442952156067,
      "learning_rate": 0.0004917114695340502,
      "loss": 7.1348,
      "step": 112
    },
    {
      "epoch": 0.03375905594144447,
      "grad_norm": 0.24397167563438416,
      "learning_rate": 0.0004916367980884109,
      "loss": 7.8721,
      "step": 113
    },
    {
      "epoch": 0.03405780864889088,
      "grad_norm": 0.2545168697834015,
      "learning_rate": 0.0004915621266427719,
      "loss": 7.4492,
      "step": 114
    },
    {
      "epoch": 0.034356561356337294,
      "grad_norm": 0.22379270195960999,
      "learning_rate": 0.0004914874551971326,
      "loss": 7.6738,
      "step": 115
    },
    {
      "epoch": 0.0346553140637837,
      "grad_norm": 0.36304497718811035,
      "learning_rate": 0.0004914127837514935,
      "loss": 6.917,
      "step": 116
    },
    {
      "epoch": 0.03495406677123011,
      "grad_norm": 0.24335503578186035,
      "learning_rate": 0.0004913381123058542,
      "loss": 7.46,
      "step": 117
    },
    {
      "epoch": 0.03525281947867653,
      "grad_norm": 0.19890791177749634,
      "learning_rate": 0.0004912634408602151,
      "loss": 7.8145,
      "step": 118
    },
    {
      "epoch": 0.03555157218612294,
      "grad_norm": 0.2813917398452759,
      "learning_rate": 0.0004911887694145758,
      "loss": 7.0918,
      "step": 119
    },
    {
      "epoch": 0.035850324893569346,
      "grad_norm": 0.2551155686378479,
      "learning_rate": 0.0004911140979689367,
      "loss": 7.415,
      "step": 120
    },
    {
      "epoch": 0.03614907760101576,
      "grad_norm": 0.21404586732387543,
      "learning_rate": 0.0004910394265232976,
      "loss": 7.7285,
      "step": 121
    },
    {
      "epoch": 0.03644783030846217,
      "grad_norm": 0.2606382369995117,
      "learning_rate": 0.0004909647550776583,
      "loss": 7.4502,
      "step": 122
    },
    {
      "epoch": 0.03674658301590858,
      "grad_norm": 0.27905747294425964,
      "learning_rate": 0.0004908900836320192,
      "loss": 7.3135,
      "step": 123
    },
    {
      "epoch": 0.03704533572335499,
      "grad_norm": 0.27660632133483887,
      "learning_rate": 0.0004908154121863799,
      "loss": 7.1143,
      "step": 124
    },
    {
      "epoch": 0.037344088430801406,
      "grad_norm": 0.26993387937545776,
      "learning_rate": 0.0004907407407407408,
      "loss": 7.5371,
      "step": 125
    },
    {
      "epoch": 0.03764284113824781,
      "grad_norm": 0.20496590435504913,
      "learning_rate": 0.0004906660692951015,
      "loss": 7.7764,
      "step": 126
    },
    {
      "epoch": 0.037941593845694226,
      "grad_norm": 0.26359736919403076,
      "learning_rate": 0.0004905913978494624,
      "loss": 7.5264,
      "step": 127
    },
    {
      "epoch": 0.03824034655314064,
      "grad_norm": 0.2587951421737671,
      "learning_rate": 0.0004905167264038231,
      "loss": 7.4697,
      "step": 128
    },
    {
      "epoch": 0.03853909926058705,
      "grad_norm": 0.27789872884750366,
      "learning_rate": 0.000490442054958184,
      "loss": 7.0439,
      "step": 129
    },
    {
      "epoch": 0.03883785196803346,
      "grad_norm": 0.24561715126037598,
      "learning_rate": 0.0004903673835125449,
      "loss": 7.5801,
      "step": 130
    },
    {
      "epoch": 0.03913660467547987,
      "grad_norm": 0.29134175181388855,
      "learning_rate": 0.0004902927120669056,
      "loss": 7.2666,
      "step": 131
    },
    {
      "epoch": 0.039435357382926285,
      "grad_norm": 0.29421618580818176,
      "learning_rate": 0.0004902180406212665,
      "loss": 7.6748,
      "step": 132
    },
    {
      "epoch": 0.03973411009037269,
      "grad_norm": 0.30324870347976685,
      "learning_rate": 0.0004901433691756272,
      "loss": 7.2764,
      "step": 133
    },
    {
      "epoch": 0.040032862797819105,
      "grad_norm": 0.24822299182415009,
      "learning_rate": 0.0004900686977299881,
      "loss": 7.5156,
      "step": 134
    },
    {
      "epoch": 0.04033161550526552,
      "grad_norm": 0.3090725243091583,
      "learning_rate": 0.0004899940262843488,
      "loss": 7.4834,
      "step": 135
    },
    {
      "epoch": 0.040630368212711925,
      "grad_norm": 0.2992507517337799,
      "learning_rate": 0.0004899193548387097,
      "loss": 7.3252,
      "step": 136
    },
    {
      "epoch": 0.04092912092015834,
      "grad_norm": 0.3073965907096863,
      "learning_rate": 0.0004898446833930705,
      "loss": 7.2656,
      "step": 137
    },
    {
      "epoch": 0.04122787362760475,
      "grad_norm": 0.32228711247444153,
      "learning_rate": 0.0004897700119474313,
      "loss": 6.9688,
      "step": 138
    },
    {
      "epoch": 0.041526626335051164,
      "grad_norm": 0.290147066116333,
      "learning_rate": 0.0004896953405017922,
      "loss": 7.46,
      "step": 139
    },
    {
      "epoch": 0.04182537904249757,
      "grad_norm": 0.3048872947692871,
      "learning_rate": 0.0004896206690561529,
      "loss": 7.1982,
      "step": 140
    },
    {
      "epoch": 0.042124131749943984,
      "grad_norm": 0.2502696216106415,
      "learning_rate": 0.0004895459976105138,
      "loss": 7.7822,
      "step": 141
    },
    {
      "epoch": 0.0424228844573904,
      "grad_norm": 0.277056485414505,
      "learning_rate": 0.0004894713261648745,
      "loss": 6.9238,
      "step": 142
    },
    {
      "epoch": 0.042721637164836804,
      "grad_norm": 0.3534789979457855,
      "learning_rate": 0.0004893966547192354,
      "loss": 7.1211,
      "step": 143
    },
    {
      "epoch": 0.04302038987228322,
      "grad_norm": 0.2861802875995636,
      "learning_rate": 0.0004893219832735961,
      "loss": 7.2949,
      "step": 144
    },
    {
      "epoch": 0.04331914257972963,
      "grad_norm": 0.2737436890602112,
      "learning_rate": 0.000489247311827957,
      "loss": 7.168,
      "step": 145
    },
    {
      "epoch": 0.04361789528717604,
      "grad_norm": 0.3162184953689575,
      "learning_rate": 0.0004891726403823178,
      "loss": 7.1924,
      "step": 146
    },
    {
      "epoch": 0.04391664799462245,
      "grad_norm": 0.2845972776412964,
      "learning_rate": 0.0004890979689366786,
      "loss": 7.0381,
      "step": 147
    },
    {
      "epoch": 0.04421540070206886,
      "grad_norm": 0.35005950927734375,
      "learning_rate": 0.0004890232974910394,
      "loss": 7.3398,
      "step": 148
    },
    {
      "epoch": 0.04451415340951528,
      "grad_norm": 0.25128173828125,
      "learning_rate": 0.0004889486260454002,
      "loss": 7.3232,
      "step": 149
    },
    {
      "epoch": 0.04481290611696168,
      "grad_norm": 0.2506413757801056,
      "learning_rate": 0.0004888739545997611,
      "loss": 7.5654,
      "step": 150
    },
    {
      "epoch": 0.045111658824408096,
      "grad_norm": 0.3365420699119568,
      "learning_rate": 0.0004887992831541218,
      "loss": 7.0918,
      "step": 151
    },
    {
      "epoch": 0.04541041153185451,
      "grad_norm": 0.30113887786865234,
      "learning_rate": 0.0004887246117084828,
      "loss": 7.0449,
      "step": 152
    },
    {
      "epoch": 0.045709164239300916,
      "grad_norm": 0.31116804480552673,
      "learning_rate": 0.0004886499402628435,
      "loss": 7.4502,
      "step": 153
    },
    {
      "epoch": 0.04600791694674733,
      "grad_norm": 0.30247560143470764,
      "learning_rate": 0.0004885752688172043,
      "loss": 7.1523,
      "step": 154
    },
    {
      "epoch": 0.04630666965419374,
      "grad_norm": 0.21981866657733917,
      "learning_rate": 0.0004885005973715651,
      "loss": 7.5488,
      "step": 155
    },
    {
      "epoch": 0.04660542236164015,
      "grad_norm": 0.3004038631916046,
      "learning_rate": 0.0004884259259259259,
      "loss": 7.5127,
      "step": 156
    },
    {
      "epoch": 0.04690417506908656,
      "grad_norm": 0.25513529777526855,
      "learning_rate": 0.0004883512544802867,
      "loss": 7.4385,
      "step": 157
    },
    {
      "epoch": 0.047202927776532976,
      "grad_norm": 0.33559417724609375,
      "learning_rate": 0.0004882765830346476,
      "loss": 7.4717,
      "step": 158
    },
    {
      "epoch": 0.04750168048397939,
      "grad_norm": 0.26063433289527893,
      "learning_rate": 0.0004882019115890084,
      "loss": 7.6895,
      "step": 159
    },
    {
      "epoch": 0.047800433191425795,
      "grad_norm": 0.39380547404289246,
      "learning_rate": 0.0004881272401433692,
      "loss": 7.2266,
      "step": 160
    },
    {
      "epoch": 0.04809918589887221,
      "grad_norm": 0.33631765842437744,
      "learning_rate": 0.00048805256869773,
      "loss": 6.8701,
      "step": 161
    },
    {
      "epoch": 0.04839793860631862,
      "grad_norm": 0.2894502878189087,
      "learning_rate": 0.0004879778972520908,
      "loss": 7.4658,
      "step": 162
    },
    {
      "epoch": 0.04869669131376503,
      "grad_norm": 0.3700374364852905,
      "learning_rate": 0.00048790322580645164,
      "loss": 6.8213,
      "step": 163
    },
    {
      "epoch": 0.04899544402121144,
      "grad_norm": 0.2870594263076782,
      "learning_rate": 0.0004878285543608124,
      "loss": 7.2793,
      "step": 164
    },
    {
      "epoch": 0.049294196728657855,
      "grad_norm": 0.2825789153575897,
      "learning_rate": 0.00048775388291517327,
      "loss": 7.5088,
      "step": 165
    },
    {
      "epoch": 0.04959294943610427,
      "grad_norm": 0.24907280504703522,
      "learning_rate": 0.000487679211469534,
      "loss": 7.5146,
      "step": 166
    },
    {
      "epoch": 0.049891702143550674,
      "grad_norm": 0.257586807012558,
      "learning_rate": 0.0004876045400238949,
      "loss": 7.5244,
      "step": 167
    },
    {
      "epoch": 0.05019045485099709,
      "grad_norm": 0.2747199535369873,
      "learning_rate": 0.0004875298685782557,
      "loss": 7.7803,
      "step": 168
    },
    {
      "epoch": 0.0504892075584435,
      "grad_norm": 0.2723439633846283,
      "learning_rate": 0.00048745519713261647,
      "loss": 7.4814,
      "step": 169
    },
    {
      "epoch": 0.05078796026588991,
      "grad_norm": 0.3231061100959778,
      "learning_rate": 0.00048738052568697733,
      "loss": 7.7725,
      "step": 170
    },
    {
      "epoch": 0.05108671297333632,
      "grad_norm": 0.25988444685935974,
      "learning_rate": 0.0004873058542413381,
      "loss": 7.7178,
      "step": 171
    },
    {
      "epoch": 0.051385465680782734,
      "grad_norm": 0.3010958433151245,
      "learning_rate": 0.00048723118279569896,
      "loss": 7.2129,
      "step": 172
    },
    {
      "epoch": 0.05168421838822914,
      "grad_norm": 0.292570561170578,
      "learning_rate": 0.0004871565113500597,
      "loss": 7.3984,
      "step": 173
    },
    {
      "epoch": 0.051982971095675554,
      "grad_norm": 0.2666943669319153,
      "learning_rate": 0.0004870818399044206,
      "loss": 7.2725,
      "step": 174
    },
    {
      "epoch": 0.05228172380312197,
      "grad_norm": 0.3564080595970154,
      "learning_rate": 0.00048700716845878134,
      "loss": 7.4648,
      "step": 175
    },
    {
      "epoch": 0.05258047651056838,
      "grad_norm": 0.3185379207134247,
      "learning_rate": 0.0004869324970131422,
      "loss": 7.1289,
      "step": 176
    },
    {
      "epoch": 0.05287922921801479,
      "grad_norm": 0.2607346773147583,
      "learning_rate": 0.000486857825567503,
      "loss": 7.3965,
      "step": 177
    },
    {
      "epoch": 0.0531779819254612,
      "grad_norm": 0.3033246695995331,
      "learning_rate": 0.0004867831541218638,
      "loss": 7.2402,
      "step": 178
    },
    {
      "epoch": 0.05347673463290761,
      "grad_norm": 0.29828473925590515,
      "learning_rate": 0.00048670848267622465,
      "loss": 7.3066,
      "step": 179
    },
    {
      "epoch": 0.05377548734035402,
      "grad_norm": 0.46538135409355164,
      "learning_rate": 0.0004866338112305854,
      "loss": 7.1484,
      "step": 180
    },
    {
      "epoch": 0.05407424004780043,
      "grad_norm": 0.34644702076911926,
      "learning_rate": 0.0004865591397849463,
      "loss": 7.0264,
      "step": 181
    },
    {
      "epoch": 0.054372992755246846,
      "grad_norm": 0.2750479578971863,
      "learning_rate": 0.00048648446833930703,
      "loss": 7.6006,
      "step": 182
    },
    {
      "epoch": 0.05467174546269325,
      "grad_norm": 0.39400234818458557,
      "learning_rate": 0.0004864097968936679,
      "loss": 6.7461,
      "step": 183
    },
    {
      "epoch": 0.054970498170139666,
      "grad_norm": 0.2518077492713928,
      "learning_rate": 0.00048633512544802866,
      "loss": 7.4766,
      "step": 184
    },
    {
      "epoch": 0.05526925087758608,
      "grad_norm": 0.31082457304000854,
      "learning_rate": 0.00048626045400238947,
      "loss": 7.292,
      "step": 185
    },
    {
      "epoch": 0.05556800358503249,
      "grad_norm": 0.24337168037891388,
      "learning_rate": 0.00048618578255675034,
      "loss": 7.6064,
      "step": 186
    },
    {
      "epoch": 0.0558667562924789,
      "grad_norm": 0.26724016666412354,
      "learning_rate": 0.0004861111111111111,
      "loss": 7.6484,
      "step": 187
    },
    {
      "epoch": 0.05616550899992531,
      "grad_norm": 0.33405354619026184,
      "learning_rate": 0.00048603643966547196,
      "loss": 7.5039,
      "step": 188
    },
    {
      "epoch": 0.056464261707371725,
      "grad_norm": 0.3647632300853729,
      "learning_rate": 0.0004859617682198327,
      "loss": 7.1309,
      "step": 189
    },
    {
      "epoch": 0.05676301441481813,
      "grad_norm": 0.3169994652271271,
      "learning_rate": 0.0004858870967741936,
      "loss": 7.7275,
      "step": 190
    },
    {
      "epoch": 0.057061767122264545,
      "grad_norm": 0.3262353837490082,
      "learning_rate": 0.00048581242532855435,
      "loss": 7.2393,
      "step": 191
    },
    {
      "epoch": 0.05736051982971096,
      "grad_norm": 0.28487446904182434,
      "learning_rate": 0.0004857377538829152,
      "loss": 7.4795,
      "step": 192
    },
    {
      "epoch": 0.057659272537157365,
      "grad_norm": 0.2790040373802185,
      "learning_rate": 0.000485663082437276,
      "loss": 7.5664,
      "step": 193
    },
    {
      "epoch": 0.05795802524460378,
      "grad_norm": 0.4375808537006378,
      "learning_rate": 0.0004855884109916368,
      "loss": 7.2256,
      "step": 194
    },
    {
      "epoch": 0.05825677795205019,
      "grad_norm": 0.2864578664302826,
      "learning_rate": 0.00048551373954599765,
      "loss": 7.1836,
      "step": 195
    },
    {
      "epoch": 0.058555530659496605,
      "grad_norm": 0.36548492312431335,
      "learning_rate": 0.0004854390681003584,
      "loss": 7.1211,
      "step": 196
    },
    {
      "epoch": 0.05885428336694301,
      "grad_norm": 0.4068611264228821,
      "learning_rate": 0.0004853643966547193,
      "loss": 7.3398,
      "step": 197
    },
    {
      "epoch": 0.059153036074389424,
      "grad_norm": 0.34425196051597595,
      "learning_rate": 0.00048528972520908004,
      "loss": 6.7324,
      "step": 198
    },
    {
      "epoch": 0.05945178878183584,
      "grad_norm": 0.3324277102947235,
      "learning_rate": 0.0004852150537634409,
      "loss": 7.2988,
      "step": 199
    },
    {
      "epoch": 0.059750541489282244,
      "grad_norm": 0.3078002631664276,
      "learning_rate": 0.00048514038231780166,
      "loss": 7.2334,
      "step": 200
    },
    {
      "epoch": 0.059750541489282244,
      "eval_bleu": 0.03907982849907875,
      "eval_loss": 7.1328125,
      "eval_runtime": 2607.2509,
      "eval_samples_per_second": 0.54,
      "eval_steps_per_second": 0.034,
      "step": 200
    },
    {
      "epoch": 0.06004929419672866,
      "grad_norm": 0.3123011589050293,
      "learning_rate": 0.0004850657108721625,
      "loss": 7.2529,
      "step": 201
    },
    {
      "epoch": 0.06034804690417507,
      "grad_norm": 0.2943539619445801,
      "learning_rate": 0.0004849910394265233,
      "loss": 7.2393,
      "step": 202
    },
    {
      "epoch": 0.06064679961162148,
      "grad_norm": 0.2779425084590912,
      "learning_rate": 0.0004849163679808841,
      "loss": 7.3496,
      "step": 203
    },
    {
      "epoch": 0.06094555231906789,
      "grad_norm": 0.29054632782936096,
      "learning_rate": 0.00048484169653524497,
      "loss": 7.4219,
      "step": 204
    },
    {
      "epoch": 0.061244305026514304,
      "grad_norm": 0.26427578926086426,
      "learning_rate": 0.00048476702508960573,
      "loss": 7.4111,
      "step": 205
    },
    {
      "epoch": 0.06154305773396072,
      "grad_norm": 0.3190177083015442,
      "learning_rate": 0.0004846923536439666,
      "loss": 7.2773,
      "step": 206
    },
    {
      "epoch": 0.06184181044140712,
      "grad_norm": 0.28507933020591736,
      "learning_rate": 0.00048461768219832735,
      "loss": 7.21,
      "step": 207
    },
    {
      "epoch": 0.062140563148853536,
      "grad_norm": 0.28055888414382935,
      "learning_rate": 0.0004845430107526882,
      "loss": 7.5439,
      "step": 208
    },
    {
      "epoch": 0.06243931585629995,
      "grad_norm": 0.305084228515625,
      "learning_rate": 0.000484468339307049,
      "loss": 7.2793,
      "step": 209
    },
    {
      "epoch": 0.06273806856374636,
      "grad_norm": 0.3277842700481415,
      "learning_rate": 0.0004843936678614098,
      "loss": 7.7324,
      "step": 210
    },
    {
      "epoch": 0.06303682127119277,
      "grad_norm": 0.2979891300201416,
      "learning_rate": 0.0004843189964157706,
      "loss": 7.0898,
      "step": 211
    },
    {
      "epoch": 0.06333557397863918,
      "grad_norm": 0.31598925590515137,
      "learning_rate": 0.0004842443249701314,
      "loss": 7.291,
      "step": 212
    },
    {
      "epoch": 0.0636343266860856,
      "grad_norm": 0.2918887734413147,
      "learning_rate": 0.0004841696535244923,
      "loss": 7.4854,
      "step": 213
    },
    {
      "epoch": 0.063933079393532,
      "grad_norm": 0.30401235818862915,
      "learning_rate": 0.00048409498207885304,
      "loss": 7.2002,
      "step": 214
    },
    {
      "epoch": 0.06423183210097841,
      "grad_norm": 0.3682520389556885,
      "learning_rate": 0.0004840203106332139,
      "loss": 7.2324,
      "step": 215
    },
    {
      "epoch": 0.06453058480842483,
      "grad_norm": 0.3752498924732208,
      "learning_rate": 0.00048394563918757467,
      "loss": 7.0205,
      "step": 216
    },
    {
      "epoch": 0.06482933751587124,
      "grad_norm": 0.3327663838863373,
      "learning_rate": 0.0004838709677419355,
      "loss": 7.4141,
      "step": 217
    },
    {
      "epoch": 0.06512809022331766,
      "grad_norm": 0.38402241468429565,
      "learning_rate": 0.0004837962962962963,
      "loss": 6.9395,
      "step": 218
    },
    {
      "epoch": 0.06542684293076406,
      "grad_norm": 0.271368145942688,
      "learning_rate": 0.0004837216248506571,
      "loss": 7.5869,
      "step": 219
    },
    {
      "epoch": 0.06572559563821047,
      "grad_norm": 0.25743579864501953,
      "learning_rate": 0.0004836469534050179,
      "loss": 7.5557,
      "step": 220
    },
    {
      "epoch": 0.06602434834565689,
      "grad_norm": 0.28932467103004456,
      "learning_rate": 0.00048357228195937873,
      "loss": 7.6787,
      "step": 221
    },
    {
      "epoch": 0.0663231010531033,
      "grad_norm": 0.33489927649497986,
      "learning_rate": 0.0004834976105137396,
      "loss": 7.4746,
      "step": 222
    },
    {
      "epoch": 0.0666218537605497,
      "grad_norm": 0.3144931197166443,
      "learning_rate": 0.00048342293906810036,
      "loss": 7.459,
      "step": 223
    },
    {
      "epoch": 0.06692060646799612,
      "grad_norm": 0.3177618980407715,
      "learning_rate": 0.00048334826762246123,
      "loss": 7.585,
      "step": 224
    },
    {
      "epoch": 0.06721935917544253,
      "grad_norm": 0.40274351835250854,
      "learning_rate": 0.000483273596176822,
      "loss": 7.2705,
      "step": 225
    },
    {
      "epoch": 0.06751811188288893,
      "grad_norm": 0.3625737130641937,
      "learning_rate": 0.0004831989247311828,
      "loss": 7.0977,
      "step": 226
    },
    {
      "epoch": 0.06781686459033535,
      "grad_norm": 0.3747721016407013,
      "learning_rate": 0.0004831242532855436,
      "loss": 7.3496,
      "step": 227
    },
    {
      "epoch": 0.06811561729778176,
      "grad_norm": 0.30746516585350037,
      "learning_rate": 0.0004830495818399044,
      "loss": 7.4443,
      "step": 228
    },
    {
      "epoch": 0.06841437000522817,
      "grad_norm": 0.35161441564559937,
      "learning_rate": 0.00048297491039426524,
      "loss": 6.709,
      "step": 229
    },
    {
      "epoch": 0.06871312271267459,
      "grad_norm": 0.3419186770915985,
      "learning_rate": 0.00048290023894862605,
      "loss": 7.2734,
      "step": 230
    },
    {
      "epoch": 0.069011875420121,
      "grad_norm": 0.32175663113594055,
      "learning_rate": 0.0004828255675029869,
      "loss": 7.5088,
      "step": 231
    },
    {
      "epoch": 0.0693106281275674,
      "grad_norm": 0.310066819190979,
      "learning_rate": 0.0004827508960573477,
      "loss": 7.2734,
      "step": 232
    },
    {
      "epoch": 0.06960938083501382,
      "grad_norm": 0.38308191299438477,
      "learning_rate": 0.0004826762246117085,
      "loss": 7.2852,
      "step": 233
    },
    {
      "epoch": 0.06990813354246023,
      "grad_norm": 0.33334752917289734,
      "learning_rate": 0.0004826015531660693,
      "loss": 7.2979,
      "step": 234
    },
    {
      "epoch": 0.07020688624990663,
      "grad_norm": 0.3677629828453064,
      "learning_rate": 0.0004825268817204301,
      "loss": 6.959,
      "step": 235
    },
    {
      "epoch": 0.07050563895735305,
      "grad_norm": 0.6028388142585754,
      "learning_rate": 0.00048245221027479093,
      "loss": 6.6934,
      "step": 236
    },
    {
      "epoch": 0.07080439166479946,
      "grad_norm": 0.30435067415237427,
      "learning_rate": 0.00048237753882915174,
      "loss": 7.4219,
      "step": 237
    },
    {
      "epoch": 0.07110314437224588,
      "grad_norm": 0.3424874246120453,
      "learning_rate": 0.00048230286738351255,
      "loss": 6.9922,
      "step": 238
    },
    {
      "epoch": 0.07140189707969229,
      "grad_norm": 0.38234105706214905,
      "learning_rate": 0.00048222819593787337,
      "loss": 7.5273,
      "step": 239
    },
    {
      "epoch": 0.07170064978713869,
      "grad_norm": 0.34200963377952576,
      "learning_rate": 0.0004821535244922341,
      "loss": 7.0391,
      "step": 240
    },
    {
      "epoch": 0.07199940249458511,
      "grad_norm": 0.38949450850486755,
      "learning_rate": 0.000482078853046595,
      "loss": 7.6494,
      "step": 241
    },
    {
      "epoch": 0.07229815520203152,
      "grad_norm": 0.3774808943271637,
      "learning_rate": 0.0004820041816009558,
      "loss": 6.9707,
      "step": 242
    },
    {
      "epoch": 0.07259690790947793,
      "grad_norm": 0.3394872844219208,
      "learning_rate": 0.0004819295101553166,
      "loss": 7.375,
      "step": 243
    },
    {
      "epoch": 0.07289566061692435,
      "grad_norm": 0.2433769851922989,
      "learning_rate": 0.00048185483870967743,
      "loss": 7.415,
      "step": 244
    },
    {
      "epoch": 0.07319441332437075,
      "grad_norm": 0.5029091238975525,
      "learning_rate": 0.00048178016726403824,
      "loss": 6.7168,
      "step": 245
    },
    {
      "epoch": 0.07349316603181716,
      "grad_norm": 0.2827988564968109,
      "learning_rate": 0.00048170549581839906,
      "loss": 7.499,
      "step": 246
    },
    {
      "epoch": 0.07379191873926358,
      "grad_norm": 0.3065880537033081,
      "learning_rate": 0.00048163082437275987,
      "loss": 7.5312,
      "step": 247
    },
    {
      "epoch": 0.07409067144670999,
      "grad_norm": 0.2757122218608856,
      "learning_rate": 0.0004815561529271207,
      "loss": 7.4023,
      "step": 248
    },
    {
      "epoch": 0.07438942415415639,
      "grad_norm": 0.35145482420921326,
      "learning_rate": 0.00048148148148148144,
      "loss": 7.1299,
      "step": 249
    },
    {
      "epoch": 0.07468817686160281,
      "grad_norm": 0.3465346097946167,
      "learning_rate": 0.0004814068100358423,
      "loss": 7.1553,
      "step": 250
    },
    {
      "epoch": 0.07498692956904922,
      "grad_norm": 0.32320737838745117,
      "learning_rate": 0.0004813321385902031,
      "loss": 7.1562,
      "step": 251
    },
    {
      "epoch": 0.07528568227649562,
      "grad_norm": 0.2975021004676819,
      "learning_rate": 0.00048125746714456393,
      "loss": 7.1943,
      "step": 252
    },
    {
      "epoch": 0.07558443498394204,
      "grad_norm": 0.30543196201324463,
      "learning_rate": 0.00048118279569892475,
      "loss": 7.6523,
      "step": 253
    },
    {
      "epoch": 0.07588318769138845,
      "grad_norm": 0.31832584738731384,
      "learning_rate": 0.00048110812425328556,
      "loss": 7.7295,
      "step": 254
    },
    {
      "epoch": 0.07618194039883486,
      "grad_norm": 0.45462581515312195,
      "learning_rate": 0.00048103345280764637,
      "loss": 6.6504,
      "step": 255
    },
    {
      "epoch": 0.07648069310628128,
      "grad_norm": 0.29367244243621826,
      "learning_rate": 0.00048095878136200713,
      "loss": 7.5576,
      "step": 256
    },
    {
      "epoch": 0.07677944581372768,
      "grad_norm": 0.3785807490348816,
      "learning_rate": 0.000480884109916368,
      "loss": 6.9893,
      "step": 257
    },
    {
      "epoch": 0.0770781985211741,
      "grad_norm": 0.42041096091270447,
      "learning_rate": 0.00048080943847072876,
      "loss": 7.0098,
      "step": 258
    },
    {
      "epoch": 0.07737695122862051,
      "grad_norm": 0.4015696346759796,
      "learning_rate": 0.0004807347670250896,
      "loss": 6.8574,
      "step": 259
    },
    {
      "epoch": 0.07767570393606692,
      "grad_norm": 0.31781068444252014,
      "learning_rate": 0.00048066009557945044,
      "loss": 7.1729,
      "step": 260
    },
    {
      "epoch": 0.07797445664351334,
      "grad_norm": 0.33713677525520325,
      "learning_rate": 0.00048058542413381125,
      "loss": 7.2793,
      "step": 261
    },
    {
      "epoch": 0.07827320935095974,
      "grad_norm": 0.32415229082107544,
      "learning_rate": 0.00048051075268817206,
      "loss": 7.2158,
      "step": 262
    },
    {
      "epoch": 0.07857196205840615,
      "grad_norm": 0.3313237428665161,
      "learning_rate": 0.0004804360812425329,
      "loss": 7.7979,
      "step": 263
    },
    {
      "epoch": 0.07887071476585257,
      "grad_norm": 0.28631895780563354,
      "learning_rate": 0.0004803614097968937,
      "loss": 7.6855,
      "step": 264
    },
    {
      "epoch": 0.07916946747329898,
      "grad_norm": 0.34740912914276123,
      "learning_rate": 0.00048028673835125445,
      "loss": 7.2217,
      "step": 265
    },
    {
      "epoch": 0.07946822018074538,
      "grad_norm": 0.2941768765449524,
      "learning_rate": 0.0004802120669056153,
      "loss": 7.4521,
      "step": 266
    },
    {
      "epoch": 0.0797669728881918,
      "grad_norm": 0.3617632985115051,
      "learning_rate": 0.0004801373954599761,
      "loss": 7.1104,
      "step": 267
    },
    {
      "epoch": 0.08006572559563821,
      "grad_norm": 0.31673726439476013,
      "learning_rate": 0.00048006272401433694,
      "loss": 7.0762,
      "step": 268
    },
    {
      "epoch": 0.08036447830308462,
      "grad_norm": 0.3049163818359375,
      "learning_rate": 0.00047998805256869775,
      "loss": 7.334,
      "step": 269
    },
    {
      "epoch": 0.08066323101053104,
      "grad_norm": 0.3509286940097809,
      "learning_rate": 0.00047991338112305857,
      "loss": 7.0996,
      "step": 270
    },
    {
      "epoch": 0.08096198371797744,
      "grad_norm": 0.36868366599082947,
      "learning_rate": 0.0004798387096774194,
      "loss": 6.9736,
      "step": 271
    },
    {
      "epoch": 0.08126073642542385,
      "grad_norm": 0.4257866442203522,
      "learning_rate": 0.00047976403823178014,
      "loss": 6.5889,
      "step": 272
    },
    {
      "epoch": 0.08155948913287027,
      "grad_norm": 0.33760908246040344,
      "learning_rate": 0.000479689366786141,
      "loss": 6.8174,
      "step": 273
    },
    {
      "epoch": 0.08185824184031668,
      "grad_norm": 0.3890152871608734,
      "learning_rate": 0.00047961469534050176,
      "loss": 7.4414,
      "step": 274
    },
    {
      "epoch": 0.0821569945477631,
      "grad_norm": 0.4200459420681,
      "learning_rate": 0.00047954002389486263,
      "loss": 6.8447,
      "step": 275
    },
    {
      "epoch": 0.0824557472552095,
      "grad_norm": 0.32538771629333496,
      "learning_rate": 0.0004794653524492234,
      "loss": 7.3564,
      "step": 276
    },
    {
      "epoch": 0.08275449996265591,
      "grad_norm": 0.3766273856163025,
      "learning_rate": 0.00047939068100358426,
      "loss": 6.8135,
      "step": 277
    },
    {
      "epoch": 0.08305325267010233,
      "grad_norm": 0.2985558807849884,
      "learning_rate": 0.00047931600955794507,
      "loss": 7.3838,
      "step": 278
    },
    {
      "epoch": 0.08335200537754874,
      "grad_norm": 0.3554684817790985,
      "learning_rate": 0.0004792413381123059,
      "loss": 7.5234,
      "step": 279
    },
    {
      "epoch": 0.08365075808499514,
      "grad_norm": 0.31132543087005615,
      "learning_rate": 0.0004791666666666667,
      "loss": 7.2695,
      "step": 280
    },
    {
      "epoch": 0.08394951079244156,
      "grad_norm": 0.36146849393844604,
      "learning_rate": 0.00047909199522102745,
      "loss": 7.4658,
      "step": 281
    },
    {
      "epoch": 0.08424826349988797,
      "grad_norm": 0.32686829566955566,
      "learning_rate": 0.0004790173237753883,
      "loss": 7.4521,
      "step": 282
    },
    {
      "epoch": 0.08454701620733437,
      "grad_norm": 0.28750407695770264,
      "learning_rate": 0.0004789426523297491,
      "loss": 7.3955,
      "step": 283
    },
    {
      "epoch": 0.0848457689147808,
      "grad_norm": 0.36340340971946716,
      "learning_rate": 0.00047886798088410995,
      "loss": 7.5283,
      "step": 284
    },
    {
      "epoch": 0.0851445216222272,
      "grad_norm": 0.29256385564804077,
      "learning_rate": 0.0004787933094384707,
      "loss": 7.7695,
      "step": 285
    },
    {
      "epoch": 0.08544327432967361,
      "grad_norm": 0.29111194610595703,
      "learning_rate": 0.00047871863799283157,
      "loss": 7.4336,
      "step": 286
    },
    {
      "epoch": 0.08574202703712003,
      "grad_norm": 0.38031360507011414,
      "learning_rate": 0.0004786439665471924,
      "loss": 7.3438,
      "step": 287
    },
    {
      "epoch": 0.08604077974456643,
      "grad_norm": 0.5033569931983948,
      "learning_rate": 0.00047856929510155314,
      "loss": 6.9629,
      "step": 288
    },
    {
      "epoch": 0.08633953245201284,
      "grad_norm": 0.32559508085250854,
      "learning_rate": 0.000478494623655914,
      "loss": 7.1074,
      "step": 289
    },
    {
      "epoch": 0.08663828515945926,
      "grad_norm": 0.35896870493888855,
      "learning_rate": 0.00047841995221027477,
      "loss": 7.1719,
      "step": 290
    },
    {
      "epoch": 0.08693703786690567,
      "grad_norm": 0.3534773588180542,
      "learning_rate": 0.00047834528076463564,
      "loss": 7.4482,
      "step": 291
    },
    {
      "epoch": 0.08723579057435207,
      "grad_norm": 0.3783564567565918,
      "learning_rate": 0.0004782706093189964,
      "loss": 7.1924,
      "step": 292
    },
    {
      "epoch": 0.0875345432817985,
      "grad_norm": 0.33855393528938293,
      "learning_rate": 0.00047819593787335726,
      "loss": 7.1904,
      "step": 293
    },
    {
      "epoch": 0.0878332959892449,
      "grad_norm": 0.2666965126991272,
      "learning_rate": 0.000478121266427718,
      "loss": 7.3926,
      "step": 294
    },
    {
      "epoch": 0.08813204869669132,
      "grad_norm": 0.328940749168396,
      "learning_rate": 0.0004780465949820789,
      "loss": 7.2334,
      "step": 295
    },
    {
      "epoch": 0.08843080140413773,
      "grad_norm": 0.35646528005599976,
      "learning_rate": 0.0004779719235364397,
      "loss": 7.416,
      "step": 296
    },
    {
      "epoch": 0.08872955411158413,
      "grad_norm": 0.41437315940856934,
      "learning_rate": 0.00047789725209080046,
      "loss": 7.0273,
      "step": 297
    },
    {
      "epoch": 0.08902830681903055,
      "grad_norm": 0.32371196150779724,
      "learning_rate": 0.0004778225806451613,
      "loss": 7.1855,
      "step": 298
    },
    {
      "epoch": 0.08932705952647696,
      "grad_norm": 0.286600798368454,
      "learning_rate": 0.0004777479091995221,
      "loss": 7.3711,
      "step": 299
    },
    {
      "epoch": 0.08962581223392337,
      "grad_norm": 0.36828580498695374,
      "learning_rate": 0.00047767323775388295,
      "loss": 6.6221,
      "step": 300
    },
    {
      "epoch": 0.08992456494136979,
      "grad_norm": 0.3530673384666443,
      "learning_rate": 0.0004775985663082437,
      "loss": 7.2061,
      "step": 301
    },
    {
      "epoch": 0.09022331764881619,
      "grad_norm": 0.30934393405914307,
      "learning_rate": 0.0004775238948626046,
      "loss": 7.2178,
      "step": 302
    },
    {
      "epoch": 0.0905220703562626,
      "grad_norm": 0.3234090805053711,
      "learning_rate": 0.00047744922341696534,
      "loss": 7.6318,
      "step": 303
    },
    {
      "epoch": 0.09082082306370902,
      "grad_norm": 0.41697925329208374,
      "learning_rate": 0.00047737455197132615,
      "loss": 6.9072,
      "step": 304
    },
    {
      "epoch": 0.09111957577115543,
      "grad_norm": 0.40609318017959595,
      "learning_rate": 0.000477299880525687,
      "loss": 7.4297,
      "step": 305
    },
    {
      "epoch": 0.09141832847860183,
      "grad_norm": 0.33139768242836,
      "learning_rate": 0.0004772252090800478,
      "loss": 7.3711,
      "step": 306
    },
    {
      "epoch": 0.09171708118604825,
      "grad_norm": 0.3366144597530365,
      "learning_rate": 0.00047715053763440864,
      "loss": 7.4678,
      "step": 307
    },
    {
      "epoch": 0.09201583389349466,
      "grad_norm": 0.373856782913208,
      "learning_rate": 0.0004770758661887694,
      "loss": 6.9414,
      "step": 308
    },
    {
      "epoch": 0.09231458660094106,
      "grad_norm": 0.3595760762691498,
      "learning_rate": 0.00047700119474313027,
      "loss": 7.2773,
      "step": 309
    },
    {
      "epoch": 0.09261333930838749,
      "grad_norm": 0.38620641827583313,
      "learning_rate": 0.000476926523297491,
      "loss": 7.0107,
      "step": 310
    },
    {
      "epoch": 0.09291209201583389,
      "grad_norm": 0.28860586881637573,
      "learning_rate": 0.0004768518518518519,
      "loss": 7.3516,
      "step": 311
    },
    {
      "epoch": 0.0932108447232803,
      "grad_norm": 0.3319552540779114,
      "learning_rate": 0.00047677718040621265,
      "loss": 7.6865,
      "step": 312
    },
    {
      "epoch": 0.09350959743072672,
      "grad_norm": 0.35501229763031006,
      "learning_rate": 0.00047670250896057347,
      "loss": 7.1748,
      "step": 313
    },
    {
      "epoch": 0.09380835013817312,
      "grad_norm": 0.3376871347427368,
      "learning_rate": 0.00047662783751493433,
      "loss": 7.5293,
      "step": 314
    },
    {
      "epoch": 0.09410710284561954,
      "grad_norm": 0.34964245557785034,
      "learning_rate": 0.0004765531660692951,
      "loss": 7.1279,
      "step": 315
    },
    {
      "epoch": 0.09440585555306595,
      "grad_norm": 0.2928626835346222,
      "learning_rate": 0.00047647849462365596,
      "loss": 7.4678,
      "step": 316
    },
    {
      "epoch": 0.09470460826051236,
      "grad_norm": 0.27204424142837524,
      "learning_rate": 0.0004764038231780167,
      "loss": 7.457,
      "step": 317
    },
    {
      "epoch": 0.09500336096795878,
      "grad_norm": 0.34205707907676697,
      "learning_rate": 0.0004763291517323776,
      "loss": 7.3604,
      "step": 318
    },
    {
      "epoch": 0.09530211367540518,
      "grad_norm": 0.43689972162246704,
      "learning_rate": 0.00047625448028673834,
      "loss": 6.9404,
      "step": 319
    },
    {
      "epoch": 0.09560086638285159,
      "grad_norm": 0.34572312235832214,
      "learning_rate": 0.00047617980884109916,
      "loss": 7.1553,
      "step": 320
    },
    {
      "epoch": 0.09589961909029801,
      "grad_norm": 0.5221048593521118,
      "learning_rate": 0.00047610513739545997,
      "loss": 6.8701,
      "step": 321
    },
    {
      "epoch": 0.09619837179774442,
      "grad_norm": 0.32879388332366943,
      "learning_rate": 0.0004760304659498208,
      "loss": 7.1484,
      "step": 322
    },
    {
      "epoch": 0.09649712450519082,
      "grad_norm": 0.335427850484848,
      "learning_rate": 0.00047595579450418165,
      "loss": 7.4492,
      "step": 323
    },
    {
      "epoch": 0.09679587721263724,
      "grad_norm": 0.32932671904563904,
      "learning_rate": 0.0004758811230585424,
      "loss": 7.2305,
      "step": 324
    },
    {
      "epoch": 0.09709462992008365,
      "grad_norm": 0.4528493881225586,
      "learning_rate": 0.0004758064516129033,
      "loss": 6.5498,
      "step": 325
    },
    {
      "epoch": 0.09739338262753006,
      "grad_norm": 0.43360480666160583,
      "learning_rate": 0.00047573178016726403,
      "loss": 6.8643,
      "step": 326
    },
    {
      "epoch": 0.09769213533497648,
      "grad_norm": 0.37920624017715454,
      "learning_rate": 0.0004756571087216249,
      "loss": 6.8877,
      "step": 327
    },
    {
      "epoch": 0.09799088804242288,
      "grad_norm": 0.2969019114971161,
      "learning_rate": 0.00047558243727598566,
      "loss": 7.5674,
      "step": 328
    },
    {
      "epoch": 0.09828964074986929,
      "grad_norm": 0.35542958974838257,
      "learning_rate": 0.00047550776583034647,
      "loss": 7.251,
      "step": 329
    },
    {
      "epoch": 0.09858839345731571,
      "grad_norm": 0.4044097065925598,
      "learning_rate": 0.0004754330943847073,
      "loss": 6.9648,
      "step": 330
    },
    {
      "epoch": 0.09888714616476212,
      "grad_norm": 0.3543989956378937,
      "learning_rate": 0.0004753584229390681,
      "loss": 7.2373,
      "step": 331
    },
    {
      "epoch": 0.09918589887220854,
      "grad_norm": 0.39775583148002625,
      "learning_rate": 0.0004752837514934289,
      "loss": 6.7412,
      "step": 332
    },
    {
      "epoch": 0.09948465157965494,
      "grad_norm": 0.38995787501335144,
      "learning_rate": 0.0004752090800477897,
      "loss": 7.3545,
      "step": 333
    },
    {
      "epoch": 0.09978340428710135,
      "grad_norm": 0.3022054433822632,
      "learning_rate": 0.0004751344086021506,
      "loss": 7.4277,
      "step": 334
    },
    {
      "epoch": 0.10008215699454777,
      "grad_norm": 0.2856612503528595,
      "learning_rate": 0.00047505973715651135,
      "loss": 7.6006,
      "step": 335
    },
    {
      "epoch": 0.10038090970199418,
      "grad_norm": 0.3730432689189911,
      "learning_rate": 0.00047498506571087216,
      "loss": 7.1973,
      "step": 336
    },
    {
      "epoch": 0.10067966240944058,
      "grad_norm": 0.3948811888694763,
      "learning_rate": 0.000474910394265233,
      "loss": 6.9893,
      "step": 337
    },
    {
      "epoch": 0.100978415116887,
      "grad_norm": 0.31810784339904785,
      "learning_rate": 0.0004748357228195938,
      "loss": 7.5664,
      "step": 338
    },
    {
      "epoch": 0.10127716782433341,
      "grad_norm": 0.33774954080581665,
      "learning_rate": 0.0004747610513739546,
      "loss": 7.0801,
      "step": 339
    },
    {
      "epoch": 0.10157592053177981,
      "grad_norm": 0.3480311632156372,
      "learning_rate": 0.0004746863799283154,
      "loss": 7.2236,
      "step": 340
    },
    {
      "epoch": 0.10187467323922623,
      "grad_norm": 0.4085802733898163,
      "learning_rate": 0.0004746117084826762,
      "loss": 7.1309,
      "step": 341
    },
    {
      "epoch": 0.10217342594667264,
      "grad_norm": 0.3025522232055664,
      "learning_rate": 0.00047453703703703704,
      "loss": 7.1445,
      "step": 342
    },
    {
      "epoch": 0.10247217865411905,
      "grad_norm": 0.4028434753417969,
      "learning_rate": 0.0004744623655913979,
      "loss": 7.0723,
      "step": 343
    },
    {
      "epoch": 0.10277093136156547,
      "grad_norm": 0.34659409523010254,
      "learning_rate": 0.00047438769414575866,
      "loss": 7.4443,
      "step": 344
    },
    {
      "epoch": 0.10306968406901187,
      "grad_norm": 0.33226320147514343,
      "learning_rate": 0.0004743130227001195,
      "loss": 7.542,
      "step": 345
    },
    {
      "epoch": 0.10336843677645828,
      "grad_norm": 0.4248802363872528,
      "learning_rate": 0.0004742383512544803,
      "loss": 7.4424,
      "step": 346
    },
    {
      "epoch": 0.1036671894839047,
      "grad_norm": 0.3419833481311798,
      "learning_rate": 0.0004741636798088411,
      "loss": 7.3096,
      "step": 347
    },
    {
      "epoch": 0.10396594219135111,
      "grad_norm": 0.42626187205314636,
      "learning_rate": 0.0004740890083632019,
      "loss": 6.5156,
      "step": 348
    },
    {
      "epoch": 0.10426469489879751,
      "grad_norm": 0.3002922534942627,
      "learning_rate": 0.00047401433691756273,
      "loss": 7.5244,
      "step": 349
    },
    {
      "epoch": 0.10456344760624393,
      "grad_norm": 0.30830204486846924,
      "learning_rate": 0.00047393966547192354,
      "loss": 7.0938,
      "step": 350
    },
    {
      "epoch": 0.10486220031369034,
      "grad_norm": 0.3372485339641571,
      "learning_rate": 0.00047386499402628435,
      "loss": 7.4814,
      "step": 351
    },
    {
      "epoch": 0.10516095302113676,
      "grad_norm": 0.39707690477371216,
      "learning_rate": 0.00047379032258064517,
      "loss": 6.9561,
      "step": 352
    },
    {
      "epoch": 0.10545970572858317,
      "grad_norm": 0.32670754194259644,
      "learning_rate": 0.000473715651135006,
      "loss": 7.4912,
      "step": 353
    },
    {
      "epoch": 0.10575845843602957,
      "grad_norm": 0.32411718368530273,
      "learning_rate": 0.0004736409796893668,
      "loss": 7.1201,
      "step": 354
    },
    {
      "epoch": 0.106057211143476,
      "grad_norm": 0.30876603722572327,
      "learning_rate": 0.0004735663082437276,
      "loss": 7.6797,
      "step": 355
    },
    {
      "epoch": 0.1063559638509224,
      "grad_norm": 0.3555131256580353,
      "learning_rate": 0.0004734916367980884,
      "loss": 7.4648,
      "step": 356
    },
    {
      "epoch": 0.1066547165583688,
      "grad_norm": 0.38018181920051575,
      "learning_rate": 0.00047341696535244923,
      "loss": 7.1465,
      "step": 357
    },
    {
      "epoch": 0.10695346926581523,
      "grad_norm": 0.47448158264160156,
      "learning_rate": 0.00047334229390681004,
      "loss": 6.9893,
      "step": 358
    },
    {
      "epoch": 0.10725222197326163,
      "grad_norm": 0.3358205258846283,
      "learning_rate": 0.00047326762246117086,
      "loss": 7.1406,
      "step": 359
    },
    {
      "epoch": 0.10755097468070804,
      "grad_norm": 0.33809614181518555,
      "learning_rate": 0.00047319295101553167,
      "loss": 7.1885,
      "step": 360
    },
    {
      "epoch": 0.10784972738815446,
      "grad_norm": 0.3168056905269623,
      "learning_rate": 0.0004731182795698925,
      "loss": 7.5479,
      "step": 361
    },
    {
      "epoch": 0.10814848009560087,
      "grad_norm": 0.46827954053878784,
      "learning_rate": 0.0004730436081242533,
      "loss": 6.7139,
      "step": 362
    },
    {
      "epoch": 0.10844723280304727,
      "grad_norm": 0.40343886613845825,
      "learning_rate": 0.0004729689366786141,
      "loss": 6.8506,
      "step": 363
    },
    {
      "epoch": 0.10874598551049369,
      "grad_norm": 0.3390718102455139,
      "learning_rate": 0.0004728942652329749,
      "loss": 7.123,
      "step": 364
    },
    {
      "epoch": 0.1090447382179401,
      "grad_norm": 0.43942832946777344,
      "learning_rate": 0.00047281959378733574,
      "loss": 7.5029,
      "step": 365
    },
    {
      "epoch": 0.1093434909253865,
      "grad_norm": 0.3825077414512634,
      "learning_rate": 0.00047274492234169655,
      "loss": 7.2178,
      "step": 366
    },
    {
      "epoch": 0.10964224363283293,
      "grad_norm": 0.36402952671051025,
      "learning_rate": 0.00047267025089605736,
      "loss": 7.2979,
      "step": 367
    },
    {
      "epoch": 0.10994099634027933,
      "grad_norm": 0.36446794867515564,
      "learning_rate": 0.0004725955794504181,
      "loss": 7.251,
      "step": 368
    },
    {
      "epoch": 0.11023974904772575,
      "grad_norm": 0.3983413875102997,
      "learning_rate": 0.000472520908004779,
      "loss": 6.7393,
      "step": 369
    },
    {
      "epoch": 0.11053850175517216,
      "grad_norm": 0.3563667833805084,
      "learning_rate": 0.0004724462365591398,
      "loss": 6.9756,
      "step": 370
    },
    {
      "epoch": 0.11083725446261856,
      "grad_norm": 0.34414103627204895,
      "learning_rate": 0.0004723715651135006,
      "loss": 7.2285,
      "step": 371
    },
    {
      "epoch": 0.11113600717006498,
      "grad_norm": 0.4532737731933594,
      "learning_rate": 0.0004722968936678614,
      "loss": 6.9023,
      "step": 372
    },
    {
      "epoch": 0.11143475987751139,
      "grad_norm": 0.296292245388031,
      "learning_rate": 0.00047222222222222224,
      "loss": 7.3359,
      "step": 373
    },
    {
      "epoch": 0.1117335125849578,
      "grad_norm": 0.2869156301021576,
      "learning_rate": 0.00047214755077658305,
      "loss": 7.3701,
      "step": 374
    },
    {
      "epoch": 0.11203226529240422,
      "grad_norm": 0.3389177620410919,
      "learning_rate": 0.00047207287933094386,
      "loss": 7.3691,
      "step": 375
    },
    {
      "epoch": 0.11233101799985062,
      "grad_norm": 0.34489476680755615,
      "learning_rate": 0.0004719982078853047,
      "loss": 7.4756,
      "step": 376
    },
    {
      "epoch": 0.11262977070729703,
      "grad_norm": 0.31314054131507874,
      "learning_rate": 0.00047192353643966544,
      "loss": 7.6172,
      "step": 377
    },
    {
      "epoch": 0.11292852341474345,
      "grad_norm": 0.34616023302078247,
      "learning_rate": 0.0004718488649940263,
      "loss": 7.6846,
      "step": 378
    },
    {
      "epoch": 0.11322727612218986,
      "grad_norm": 0.3795040249824524,
      "learning_rate": 0.0004717741935483871,
      "loss": 7.3447,
      "step": 379
    },
    {
      "epoch": 0.11352602882963626,
      "grad_norm": 0.34627050161361694,
      "learning_rate": 0.00047169952210274793,
      "loss": 7.3418,
      "step": 380
    },
    {
      "epoch": 0.11382478153708268,
      "grad_norm": 0.47329452633857727,
      "learning_rate": 0.00047162485065710874,
      "loss": 6.9062,
      "step": 381
    },
    {
      "epoch": 0.11412353424452909,
      "grad_norm": 0.390525758266449,
      "learning_rate": 0.00047155017921146955,
      "loss": 6.7793,
      "step": 382
    },
    {
      "epoch": 0.1144222869519755,
      "grad_norm": 0.33880671858787537,
      "learning_rate": 0.00047147550776583037,
      "loss": 7.083,
      "step": 383
    },
    {
      "epoch": 0.11472103965942192,
      "grad_norm": 0.436779260635376,
      "learning_rate": 0.0004714008363201911,
      "loss": 7.0723,
      "step": 384
    },
    {
      "epoch": 0.11501979236686832,
      "grad_norm": 0.31517294049263,
      "learning_rate": 0.000471326164874552,
      "loss": 7.5107,
      "step": 385
    },
    {
      "epoch": 0.11531854507431473,
      "grad_norm": 0.35628557205200195,
      "learning_rate": 0.00047125149342891275,
      "loss": 7.3232,
      "step": 386
    },
    {
      "epoch": 0.11561729778176115,
      "grad_norm": 0.3770134747028351,
      "learning_rate": 0.0004711768219832736,
      "loss": 7.6406,
      "step": 387
    },
    {
      "epoch": 0.11591605048920756,
      "grad_norm": 0.3829798698425293,
      "learning_rate": 0.00047110215053763443,
      "loss": 7.2451,
      "step": 388
    },
    {
      "epoch": 0.11621480319665398,
      "grad_norm": 0.41653376817703247,
      "learning_rate": 0.00047102747909199524,
      "loss": 7.0498,
      "step": 389
    },
    {
      "epoch": 0.11651355590410038,
      "grad_norm": 0.3544130325317383,
      "learning_rate": 0.00047095280764635606,
      "loss": 7.0684,
      "step": 390
    },
    {
      "epoch": 0.11681230861154679,
      "grad_norm": 0.3239371180534363,
      "learning_rate": 0.00047087813620071687,
      "loss": 7.0918,
      "step": 391
    },
    {
      "epoch": 0.11711106131899321,
      "grad_norm": 0.3773457109928131,
      "learning_rate": 0.0004708034647550777,
      "loss": 7.2773,
      "step": 392
    },
    {
      "epoch": 0.11740981402643962,
      "grad_norm": 0.3058832883834839,
      "learning_rate": 0.00047072879330943844,
      "loss": 7.5107,
      "step": 393
    },
    {
      "epoch": 0.11770856673388602,
      "grad_norm": 0.38404345512390137,
      "learning_rate": 0.0004706541218637993,
      "loss": 6.7852,
      "step": 394
    },
    {
      "epoch": 0.11800731944133244,
      "grad_norm": 0.4163118004798889,
      "learning_rate": 0.00047057945041816007,
      "loss": 6.7666,
      "step": 395
    },
    {
      "epoch": 0.11830607214877885,
      "grad_norm": 0.2919059693813324,
      "learning_rate": 0.00047050477897252093,
      "loss": 7.4551,
      "step": 396
    },
    {
      "epoch": 0.11860482485622525,
      "grad_norm": 0.41334113478660583,
      "learning_rate": 0.00047043010752688175,
      "loss": 6.9238,
      "step": 397
    },
    {
      "epoch": 0.11890357756367168,
      "grad_norm": 0.330368310213089,
      "learning_rate": 0.00047035543608124256,
      "loss": 7.8213,
      "step": 398
    },
    {
      "epoch": 0.11920233027111808,
      "grad_norm": 0.3507167100906372,
      "learning_rate": 0.00047028076463560337,
      "loss": 7.3389,
      "step": 399
    },
    {
      "epoch": 0.11950108297856449,
      "grad_norm": 0.33408859372138977,
      "learning_rate": 0.00047020609318996413,
      "loss": 7.5156,
      "step": 400
    },
    {
      "epoch": 0.11950108297856449,
      "eval_bleu": 0.06314290633984992,
      "eval_loss": 7.07421875,
      "eval_runtime": 615.7724,
      "eval_samples_per_second": 2.288,
      "eval_steps_per_second": 0.145,
      "step": 400
    },
    {
      "epoch": 0.11979983568601091,
      "grad_norm": 0.3407604992389679,
      "learning_rate": 0.000470131421744325,
      "loss": 7.0703,
      "step": 401
    },
    {
      "epoch": 0.12009858839345731,
      "grad_norm": 0.352294921875,
      "learning_rate": 0.00047005675029868576,
      "loss": 7.248,
      "step": 402
    },
    {
      "epoch": 0.12039734110090372,
      "grad_norm": 0.4790753126144409,
      "learning_rate": 0.0004699820788530466,
      "loss": 6.5674,
      "step": 403
    },
    {
      "epoch": 0.12069609380835014,
      "grad_norm": 0.34463468194007874,
      "learning_rate": 0.0004699074074074074,
      "loss": 7.1855,
      "step": 404
    },
    {
      "epoch": 0.12099484651579655,
      "grad_norm": 0.34944072365760803,
      "learning_rate": 0.00046983273596176825,
      "loss": 7.2295,
      "step": 405
    },
    {
      "epoch": 0.12129359922324295,
      "grad_norm": 0.3610077202320099,
      "learning_rate": 0.00046975806451612906,
      "loss": 7.0049,
      "step": 406
    },
    {
      "epoch": 0.12159235193068937,
      "grad_norm": 0.3886076807975769,
      "learning_rate": 0.0004696833930704899,
      "loss": 7.4717,
      "step": 407
    },
    {
      "epoch": 0.12189110463813578,
      "grad_norm": 0.4166484475135803,
      "learning_rate": 0.0004696087216248507,
      "loss": 7.3281,
      "step": 408
    },
    {
      "epoch": 0.1221898573455822,
      "grad_norm": 0.37506043910980225,
      "learning_rate": 0.00046953405017921145,
      "loss": 7.4717,
      "step": 409
    },
    {
      "epoch": 0.12248861005302861,
      "grad_norm": 0.39027926325798035,
      "learning_rate": 0.0004694593787335723,
      "loss": 7.1924,
      "step": 410
    },
    {
      "epoch": 0.12278736276047501,
      "grad_norm": 0.3593742251396179,
      "learning_rate": 0.0004693847072879331,
      "loss": 7.2305,
      "step": 411
    },
    {
      "epoch": 0.12308611546792143,
      "grad_norm": 0.4341390132904053,
      "learning_rate": 0.00046931003584229394,
      "loss": 7.0459,
      "step": 412
    },
    {
      "epoch": 0.12338486817536784,
      "grad_norm": 0.45067688822746277,
      "learning_rate": 0.0004692353643966547,
      "loss": 7.1045,
      "step": 413
    },
    {
      "epoch": 0.12368362088281425,
      "grad_norm": 0.5502626299858093,
      "learning_rate": 0.00046916069295101557,
      "loss": 7.3369,
      "step": 414
    },
    {
      "epoch": 0.12398237359026067,
      "grad_norm": 0.33474794030189514,
      "learning_rate": 0.0004690860215053764,
      "loss": 7.4951,
      "step": 415
    },
    {
      "epoch": 0.12428112629770707,
      "grad_norm": 0.39838525652885437,
      "learning_rate": 0.00046901135005973714,
      "loss": 7.2979,
      "step": 416
    },
    {
      "epoch": 0.12457987900515348,
      "grad_norm": 0.37522467970848083,
      "learning_rate": 0.000468936678614098,
      "loss": 7.3535,
      "step": 417
    },
    {
      "epoch": 0.1248786317125999,
      "grad_norm": 0.38580048084259033,
      "learning_rate": 0.00046886200716845876,
      "loss": 7.335,
      "step": 418
    },
    {
      "epoch": 0.1251773844200463,
      "grad_norm": 0.42477577924728394,
      "learning_rate": 0.00046878733572281963,
      "loss": 7.3984,
      "step": 419
    },
    {
      "epoch": 0.12547613712749273,
      "grad_norm": 0.369338721036911,
      "learning_rate": 0.0004687126642771804,
      "loss": 7.3545,
      "step": 420
    },
    {
      "epoch": 0.12577488983493912,
      "grad_norm": 0.28646552562713623,
      "learning_rate": 0.00046863799283154126,
      "loss": 7.3369,
      "step": 421
    },
    {
      "epoch": 0.12607364254238554,
      "grad_norm": 0.33757954835891724,
      "learning_rate": 0.000468563321385902,
      "loss": 7.4355,
      "step": 422
    },
    {
      "epoch": 0.12637239524983196,
      "grad_norm": 0.43328431248664856,
      "learning_rate": 0.0004684886499402629,
      "loss": 6.9902,
      "step": 423
    },
    {
      "epoch": 0.12667114795727835,
      "grad_norm": 0.2920494079589844,
      "learning_rate": 0.00046841397849462364,
      "loss": 7.4912,
      "step": 424
    },
    {
      "epoch": 0.12696990066472477,
      "grad_norm": 0.39742282032966614,
      "learning_rate": 0.00046833930704898445,
      "loss": 6.8477,
      "step": 425
    },
    {
      "epoch": 0.1272686533721712,
      "grad_norm": 0.3749666213989258,
      "learning_rate": 0.0004682646356033453,
      "loss": 6.9873,
      "step": 426
    },
    {
      "epoch": 0.12756740607961758,
      "grad_norm": 0.35190659761428833,
      "learning_rate": 0.0004681899641577061,
      "loss": 7.4014,
      "step": 427
    },
    {
      "epoch": 0.127866158787064,
      "grad_norm": 0.42471548914909363,
      "learning_rate": 0.00046811529271206695,
      "loss": 7.2129,
      "step": 428
    },
    {
      "epoch": 0.12816491149451043,
      "grad_norm": 0.3918190002441406,
      "learning_rate": 0.0004680406212664277,
      "loss": 7.166,
      "step": 429
    },
    {
      "epoch": 0.12846366420195682,
      "grad_norm": 0.36660444736480713,
      "learning_rate": 0.00046796594982078857,
      "loss": 7.2119,
      "step": 430
    },
    {
      "epoch": 0.12876241690940324,
      "grad_norm": 0.3479898273944855,
      "learning_rate": 0.00046789127837514933,
      "loss": 7.7314,
      "step": 431
    },
    {
      "epoch": 0.12906116961684966,
      "grad_norm": 0.3594509959220886,
      "learning_rate": 0.00046781660692951014,
      "loss": 7.3701,
      "step": 432
    },
    {
      "epoch": 0.12935992232429605,
      "grad_norm": 0.3519911468029022,
      "learning_rate": 0.00046774193548387096,
      "loss": 7.668,
      "step": 433
    },
    {
      "epoch": 0.12965867503174247,
      "grad_norm": 0.46653422713279724,
      "learning_rate": 0.00046766726403823177,
      "loss": 6.6914,
      "step": 434
    },
    {
      "epoch": 0.1299574277391889,
      "grad_norm": 0.4118472635746002,
      "learning_rate": 0.00046759259259259264,
      "loss": 7.2285,
      "step": 435
    },
    {
      "epoch": 0.1302561804466353,
      "grad_norm": 0.39476075768470764,
      "learning_rate": 0.0004675179211469534,
      "loss": 7.166,
      "step": 436
    },
    {
      "epoch": 0.1305549331540817,
      "grad_norm": 0.45234039425849915,
      "learning_rate": 0.00046744324970131426,
      "loss": 7.0479,
      "step": 437
    },
    {
      "epoch": 0.13085368586152812,
      "grad_norm": 0.3527635335922241,
      "learning_rate": 0.000467368578255675,
      "loss": 7.4775,
      "step": 438
    },
    {
      "epoch": 0.13115243856897454,
      "grad_norm": 0.4166572093963623,
      "learning_rate": 0.0004672939068100359,
      "loss": 7.4004,
      "step": 439
    },
    {
      "epoch": 0.13145119127642094,
      "grad_norm": 0.5076642632484436,
      "learning_rate": 0.00046721923536439665,
      "loss": 7.1572,
      "step": 440
    },
    {
      "epoch": 0.13174994398386736,
      "grad_norm": 0.4013204872608185,
      "learning_rate": 0.00046714456391875746,
      "loss": 7.0508,
      "step": 441
    },
    {
      "epoch": 0.13204869669131378,
      "grad_norm": 0.319581001996994,
      "learning_rate": 0.00046706989247311827,
      "loss": 7.5166,
      "step": 442
    },
    {
      "epoch": 0.13234744939876017,
      "grad_norm": 0.3800637125968933,
      "learning_rate": 0.0004669952210274791,
      "loss": 7.124,
      "step": 443
    },
    {
      "epoch": 0.1326462021062066,
      "grad_norm": 0.34482765197753906,
      "learning_rate": 0.00046692054958183995,
      "loss": 7.2949,
      "step": 444
    },
    {
      "epoch": 0.132944954813653,
      "grad_norm": 0.3178139925003052,
      "learning_rate": 0.0004668458781362007,
      "loss": 7.7031,
      "step": 445
    },
    {
      "epoch": 0.1332437075210994,
      "grad_norm": 0.6007941365242004,
      "learning_rate": 0.0004667712066905616,
      "loss": 6.5684,
      "step": 446
    },
    {
      "epoch": 0.13354246022854582,
      "grad_norm": 0.38168299198150635,
      "learning_rate": 0.00046669653524492234,
      "loss": 7.3184,
      "step": 447
    },
    {
      "epoch": 0.13384121293599224,
      "grad_norm": 0.3468656539916992,
      "learning_rate": 0.00046662186379928315,
      "loss": 7.0537,
      "step": 448
    },
    {
      "epoch": 0.13413996564343864,
      "grad_norm": 0.3573014438152313,
      "learning_rate": 0.00046654719235364396,
      "loss": 7.2559,
      "step": 449
    },
    {
      "epoch": 0.13443871835088506,
      "grad_norm": 0.37460553646087646,
      "learning_rate": 0.0004664725209080048,
      "loss": 7.3516,
      "step": 450
    },
    {
      "epoch": 0.13473747105833148,
      "grad_norm": 0.37861835956573486,
      "learning_rate": 0.0004663978494623656,
      "loss": 7.1768,
      "step": 451
    },
    {
      "epoch": 0.13503622376577787,
      "grad_norm": 0.3449447751045227,
      "learning_rate": 0.0004663231780167264,
      "loss": 7.4678,
      "step": 452
    },
    {
      "epoch": 0.1353349764732243,
      "grad_norm": 0.3241126835346222,
      "learning_rate": 0.00046624850657108727,
      "loss": 7.3457,
      "step": 453
    },
    {
      "epoch": 0.1356337291806707,
      "grad_norm": 0.3104764521121979,
      "learning_rate": 0.000466173835125448,
      "loss": 7.3584,
      "step": 454
    },
    {
      "epoch": 0.1359324818881171,
      "grad_norm": 0.4853258430957794,
      "learning_rate": 0.0004660991636798089,
      "loss": 6.4795,
      "step": 455
    },
    {
      "epoch": 0.13623123459556352,
      "grad_norm": 0.3746856451034546,
      "learning_rate": 0.00046602449223416965,
      "loss": 7.5654,
      "step": 456
    },
    {
      "epoch": 0.13652998730300994,
      "grad_norm": 0.3340097963809967,
      "learning_rate": 0.00046594982078853047,
      "loss": 7.1611,
      "step": 457
    },
    {
      "epoch": 0.13682874001045633,
      "grad_norm": 0.3395751714706421,
      "learning_rate": 0.0004658751493428913,
      "loss": 7.1582,
      "step": 458
    },
    {
      "epoch": 0.13712749271790275,
      "grad_norm": 0.3319547772407532,
      "learning_rate": 0.0004658004778972521,
      "loss": 7.3584,
      "step": 459
    },
    {
      "epoch": 0.13742624542534917,
      "grad_norm": 0.34483182430267334,
      "learning_rate": 0.0004657258064516129,
      "loss": 7.5312,
      "step": 460
    },
    {
      "epoch": 0.13772499813279557,
      "grad_norm": 0.37583398818969727,
      "learning_rate": 0.0004656511350059737,
      "loss": 7.127,
      "step": 461
    },
    {
      "epoch": 0.138023750840242,
      "grad_norm": 0.3963184952735901,
      "learning_rate": 0.0004655764635603346,
      "loss": 6.9551,
      "step": 462
    },
    {
      "epoch": 0.1383225035476884,
      "grad_norm": 0.31748080253601074,
      "learning_rate": 0.00046550179211469534,
      "loss": 7.4551,
      "step": 463
    },
    {
      "epoch": 0.1386212562551348,
      "grad_norm": 0.3935743272304535,
      "learning_rate": 0.00046542712066905616,
      "loss": 7.5693,
      "step": 464
    },
    {
      "epoch": 0.13892000896258122,
      "grad_norm": 0.3253386914730072,
      "learning_rate": 0.00046535244922341697,
      "loss": 7.0908,
      "step": 465
    },
    {
      "epoch": 0.13921876167002764,
      "grad_norm": 0.3798268735408783,
      "learning_rate": 0.0004652777777777778,
      "loss": 7.4902,
      "step": 466
    },
    {
      "epoch": 0.13951751437747403,
      "grad_norm": 0.31957101821899414,
      "learning_rate": 0.0004652031063321386,
      "loss": 7.542,
      "step": 467
    },
    {
      "epoch": 0.13981626708492045,
      "grad_norm": 0.4815508723258972,
      "learning_rate": 0.0004651284348864994,
      "loss": 6.9668,
      "step": 468
    },
    {
      "epoch": 0.14011501979236687,
      "grad_norm": 0.4467724561691284,
      "learning_rate": 0.0004650537634408602,
      "loss": 7.0322,
      "step": 469
    },
    {
      "epoch": 0.14041377249981327,
      "grad_norm": 0.489765465259552,
      "learning_rate": 0.00046497909199522103,
      "loss": 7.1465,
      "step": 470
    },
    {
      "epoch": 0.1407125252072597,
      "grad_norm": 0.3792406916618347,
      "learning_rate": 0.0004649044205495819,
      "loss": 7.0361,
      "step": 471
    },
    {
      "epoch": 0.1410112779147061,
      "grad_norm": 0.44332659244537354,
      "learning_rate": 0.00046482974910394266,
      "loss": 7.3105,
      "step": 472
    },
    {
      "epoch": 0.14131003062215253,
      "grad_norm": 0.39608466625213623,
      "learning_rate": 0.00046475507765830347,
      "loss": 7.3779,
      "step": 473
    },
    {
      "epoch": 0.14160878332959892,
      "grad_norm": 0.42770296335220337,
      "learning_rate": 0.0004646804062126643,
      "loss": 7.3154,
      "step": 474
    },
    {
      "epoch": 0.14190753603704534,
      "grad_norm": 0.36429211497306824,
      "learning_rate": 0.0004646057347670251,
      "loss": 7.459,
      "step": 475
    },
    {
      "epoch": 0.14220628874449176,
      "grad_norm": 0.37419646978378296,
      "learning_rate": 0.0004645310633213859,
      "loss": 7.5225,
      "step": 476
    },
    {
      "epoch": 0.14250504145193815,
      "grad_norm": 0.36639687418937683,
      "learning_rate": 0.0004644563918757467,
      "loss": 7.3213,
      "step": 477
    },
    {
      "epoch": 0.14280379415938457,
      "grad_norm": 0.3705472946166992,
      "learning_rate": 0.00046438172043010754,
      "loss": 7.4824,
      "step": 478
    },
    {
      "epoch": 0.143102546866831,
      "grad_norm": 0.4703329801559448,
      "learning_rate": 0.00046430704898446835,
      "loss": 7.0781,
      "step": 479
    },
    {
      "epoch": 0.14340129957427739,
      "grad_norm": 0.4923274517059326,
      "learning_rate": 0.00046423237753882916,
      "loss": 6.5352,
      "step": 480
    },
    {
      "epoch": 0.1437000522817238,
      "grad_norm": 0.4432821273803711,
      "learning_rate": 0.00046415770609319,
      "loss": 6.8555,
      "step": 481
    },
    {
      "epoch": 0.14399880498917023,
      "grad_norm": 0.4010286331176758,
      "learning_rate": 0.0004640830346475508,
      "loss": 7.2812,
      "step": 482
    },
    {
      "epoch": 0.14429755769661662,
      "grad_norm": 0.34480640292167664,
      "learning_rate": 0.0004640083632019116,
      "loss": 7.4512,
      "step": 483
    },
    {
      "epoch": 0.14459631040406304,
      "grad_norm": 0.3909822702407837,
      "learning_rate": 0.0004639336917562724,
      "loss": 6.8369,
      "step": 484
    },
    {
      "epoch": 0.14489506311150946,
      "grad_norm": 0.35184359550476074,
      "learning_rate": 0.0004638590203106332,
      "loss": 7.3018,
      "step": 485
    },
    {
      "epoch": 0.14519381581895585,
      "grad_norm": 0.47893083095550537,
      "learning_rate": 0.00046378434886499404,
      "loss": 7.0566,
      "step": 486
    },
    {
      "epoch": 0.14549256852640227,
      "grad_norm": 0.4085986614227295,
      "learning_rate": 0.00046370967741935485,
      "loss": 6.96,
      "step": 487
    },
    {
      "epoch": 0.1457913212338487,
      "grad_norm": 0.36285147070884705,
      "learning_rate": 0.00046363500597371566,
      "loss": 7.5215,
      "step": 488
    },
    {
      "epoch": 0.14609007394129508,
      "grad_norm": 0.39923295378685,
      "learning_rate": 0.0004635603345280765,
      "loss": 7.29,
      "step": 489
    },
    {
      "epoch": 0.1463888266487415,
      "grad_norm": 0.4298717677593231,
      "learning_rate": 0.0004634856630824373,
      "loss": 6.9131,
      "step": 490
    },
    {
      "epoch": 0.14668757935618792,
      "grad_norm": 0.3964043855667114,
      "learning_rate": 0.0004634109916367981,
      "loss": 7.3252,
      "step": 491
    },
    {
      "epoch": 0.14698633206363432,
      "grad_norm": 0.34743672609329224,
      "learning_rate": 0.0004633363201911589,
      "loss": 7.1904,
      "step": 492
    },
    {
      "epoch": 0.14728508477108074,
      "grad_norm": 0.3526354134082794,
      "learning_rate": 0.00046326164874551973,
      "loss": 7.5615,
      "step": 493
    },
    {
      "epoch": 0.14758383747852716,
      "grad_norm": 0.4242677688598633,
      "learning_rate": 0.00046318697729988054,
      "loss": 7.4014,
      "step": 494
    },
    {
      "epoch": 0.14788259018597355,
      "grad_norm": 0.3925628960132599,
      "learning_rate": 0.00046311230585424135,
      "loss": 7.2998,
      "step": 495
    },
    {
      "epoch": 0.14818134289341997,
      "grad_norm": 0.3802424669265747,
      "learning_rate": 0.0004630376344086021,
      "loss": 7.2988,
      "step": 496
    },
    {
      "epoch": 0.1484800956008664,
      "grad_norm": 0.3823290169239044,
      "learning_rate": 0.000462962962962963,
      "loss": 7.3926,
      "step": 497
    },
    {
      "epoch": 0.14877884830831278,
      "grad_norm": 0.3304677903652191,
      "learning_rate": 0.0004628882915173238,
      "loss": 7.6025,
      "step": 498
    },
    {
      "epoch": 0.1490776010157592,
      "grad_norm": 0.4154568314552307,
      "learning_rate": 0.0004628136200716846,
      "loss": 7.291,
      "step": 499
    },
    {
      "epoch": 0.14937635372320562,
      "grad_norm": 0.4527128338813782,
      "learning_rate": 0.0004627389486260454,
      "loss": 7.3662,
      "step": 500
    },
    {
      "epoch": 0.14967510643065202,
      "grad_norm": 0.4651779234409332,
      "learning_rate": 0.00046266427718040623,
      "loss": 7.3271,
      "step": 501
    },
    {
      "epoch": 0.14997385913809844,
      "grad_norm": 0.43253764510154724,
      "learning_rate": 0.00046258960573476705,
      "loss": 7.2441,
      "step": 502
    },
    {
      "epoch": 0.15027261184554486,
      "grad_norm": 0.41578447818756104,
      "learning_rate": 0.00046251493428912786,
      "loss": 7.3076,
      "step": 503
    },
    {
      "epoch": 0.15057136455299125,
      "grad_norm": 0.33692219853401184,
      "learning_rate": 0.00046244026284348867,
      "loss": 7.5645,
      "step": 504
    },
    {
      "epoch": 0.15087011726043767,
      "grad_norm": 0.40838131308555603,
      "learning_rate": 0.00046236559139784943,
      "loss": 7.1182,
      "step": 505
    },
    {
      "epoch": 0.1511688699678841,
      "grad_norm": 0.4092601537704468,
      "learning_rate": 0.0004622909199522103,
      "loss": 6.8379,
      "step": 506
    },
    {
      "epoch": 0.15146762267533048,
      "grad_norm": 0.4096202850341797,
      "learning_rate": 0.0004622162485065711,
      "loss": 7.0781,
      "step": 507
    },
    {
      "epoch": 0.1517663753827769,
      "grad_norm": 0.3555811941623688,
      "learning_rate": 0.0004621415770609319,
      "loss": 7.2588,
      "step": 508
    },
    {
      "epoch": 0.15206512809022332,
      "grad_norm": 0.32640114426612854,
      "learning_rate": 0.00046206690561529274,
      "loss": 7.5889,
      "step": 509
    },
    {
      "epoch": 0.15236388079766972,
      "grad_norm": 0.35323473811149597,
      "learning_rate": 0.00046199223416965355,
      "loss": 7.3721,
      "step": 510
    },
    {
      "epoch": 0.15266263350511614,
      "grad_norm": 0.3315449059009552,
      "learning_rate": 0.00046191756272401436,
      "loss": 7.4639,
      "step": 511
    },
    {
      "epoch": 0.15296138621256256,
      "grad_norm": 0.4315294921398163,
      "learning_rate": 0.0004618428912783751,
      "loss": 7.5361,
      "step": 512
    },
    {
      "epoch": 0.15326013892000898,
      "grad_norm": 0.43292179703712463,
      "learning_rate": 0.000461768219832736,
      "loss": 7.292,
      "step": 513
    },
    {
      "epoch": 0.15355889162745537,
      "grad_norm": 0.4063829779624939,
      "learning_rate": 0.00046169354838709675,
      "loss": 7.0205,
      "step": 514
    },
    {
      "epoch": 0.1538576443349018,
      "grad_norm": 0.3746349513530731,
      "learning_rate": 0.0004616188769414576,
      "loss": 7.1104,
      "step": 515
    },
    {
      "epoch": 0.1541563970423482,
      "grad_norm": 0.4410933554172516,
      "learning_rate": 0.00046154420549581837,
      "loss": 6.8438,
      "step": 516
    },
    {
      "epoch": 0.1544551497497946,
      "grad_norm": 0.3735065162181854,
      "learning_rate": 0.00046146953405017924,
      "loss": 6.873,
      "step": 517
    },
    {
      "epoch": 0.15475390245724102,
      "grad_norm": 0.38182929158210754,
      "learning_rate": 0.00046139486260454005,
      "loss": 7.4883,
      "step": 518
    },
    {
      "epoch": 0.15505265516468744,
      "grad_norm": 0.34415221214294434,
      "learning_rate": 0.00046132019115890086,
      "loss": 7.5508,
      "step": 519
    },
    {
      "epoch": 0.15535140787213383,
      "grad_norm": 0.3995930850505829,
      "learning_rate": 0.0004612455197132617,
      "loss": 7.2061,
      "step": 520
    },
    {
      "epoch": 0.15565016057958025,
      "grad_norm": 0.37925347685813904,
      "learning_rate": 0.00046117084826762244,
      "loss": 7.252,
      "step": 521
    },
    {
      "epoch": 0.15594891328702667,
      "grad_norm": 0.46988874673843384,
      "learning_rate": 0.0004610961768219833,
      "loss": 7.207,
      "step": 522
    },
    {
      "epoch": 0.15624766599447307,
      "grad_norm": 0.39252549409866333,
      "learning_rate": 0.00046102150537634406,
      "loss": 7.3164,
      "step": 523
    },
    {
      "epoch": 0.1565464187019195,
      "grad_norm": 0.4602091610431671,
      "learning_rate": 0.00046094683393070493,
      "loss": 7.0176,
      "step": 524
    },
    {
      "epoch": 0.1568451714093659,
      "grad_norm": 0.4840177595615387,
      "learning_rate": 0.0004608721624850657,
      "loss": 6.6836,
      "step": 525
    },
    {
      "epoch": 0.1571439241168123,
      "grad_norm": 0.41737475991249084,
      "learning_rate": 0.00046079749103942655,
      "loss": 7.4014,
      "step": 526
    },
    {
      "epoch": 0.15744267682425872,
      "grad_norm": 0.33526843786239624,
      "learning_rate": 0.00046072281959378737,
      "loss": 7.7256,
      "step": 527
    },
    {
      "epoch": 0.15774142953170514,
      "grad_norm": 0.5068597793579102,
      "learning_rate": 0.0004606481481481481,
      "loss": 6.2852,
      "step": 528
    },
    {
      "epoch": 0.15804018223915153,
      "grad_norm": 0.3731829524040222,
      "learning_rate": 0.000460573476702509,
      "loss": 7.2822,
      "step": 529
    },
    {
      "epoch": 0.15833893494659795,
      "grad_norm": 0.35565051436424255,
      "learning_rate": 0.00046049880525686975,
      "loss": 7.4336,
      "step": 530
    },
    {
      "epoch": 0.15863768765404437,
      "grad_norm": 0.3721979558467865,
      "learning_rate": 0.0004604241338112306,
      "loss": 7.0195,
      "step": 531
    },
    {
      "epoch": 0.15893644036149077,
      "grad_norm": 0.3453672528266907,
      "learning_rate": 0.0004603494623655914,
      "loss": 7.6523,
      "step": 532
    },
    {
      "epoch": 0.1592351930689372,
      "grad_norm": 0.3656001687049866,
      "learning_rate": 0.00046027479091995224,
      "loss": 7.6797,
      "step": 533
    },
    {
      "epoch": 0.1595339457763836,
      "grad_norm": 0.39576786756515503,
      "learning_rate": 0.000460200119474313,
      "loss": 7.418,
      "step": 534
    },
    {
      "epoch": 0.15983269848383,
      "grad_norm": 0.39797618985176086,
      "learning_rate": 0.00046012544802867387,
      "loss": 7.4248,
      "step": 535
    },
    {
      "epoch": 0.16013145119127642,
      "grad_norm": 0.4094032347202301,
      "learning_rate": 0.0004600507765830347,
      "loss": 7.4854,
      "step": 536
    },
    {
      "epoch": 0.16043020389872284,
      "grad_norm": 0.38115039467811584,
      "learning_rate": 0.00045997610513739544,
      "loss": 7.5127,
      "step": 537
    },
    {
      "epoch": 0.16072895660616923,
      "grad_norm": 0.4406241774559021,
      "learning_rate": 0.0004599014336917563,
      "loss": 7.0752,
      "step": 538
    },
    {
      "epoch": 0.16102770931361565,
      "grad_norm": 0.3622308075428009,
      "learning_rate": 0.00045982676224611707,
      "loss": 7.3613,
      "step": 539
    },
    {
      "epoch": 0.16132646202106207,
      "grad_norm": 0.49825045466423035,
      "learning_rate": 0.00045975209080047793,
      "loss": 6.6807,
      "step": 540
    },
    {
      "epoch": 0.16162521472850847,
      "grad_norm": 0.38409075140953064,
      "learning_rate": 0.0004596774193548387,
      "loss": 7.2686,
      "step": 541
    },
    {
      "epoch": 0.16192396743595489,
      "grad_norm": 0.3863038122653961,
      "learning_rate": 0.00045960274790919956,
      "loss": 7.5938,
      "step": 542
    },
    {
      "epoch": 0.1622227201434013,
      "grad_norm": 0.4190234839916229,
      "learning_rate": 0.0004595280764635603,
      "loss": 7.2627,
      "step": 543
    },
    {
      "epoch": 0.1625214728508477,
      "grad_norm": 0.3243887722492218,
      "learning_rate": 0.00045945340501792113,
      "loss": 7.5244,
      "step": 544
    },
    {
      "epoch": 0.16282022555829412,
      "grad_norm": 0.4027411937713623,
      "learning_rate": 0.000459378733572282,
      "loss": 7.3145,
      "step": 545
    },
    {
      "epoch": 0.16311897826574054,
      "grad_norm": 0.4423351585865021,
      "learning_rate": 0.00045930406212664276,
      "loss": 6.7139,
      "step": 546
    },
    {
      "epoch": 0.16341773097318693,
      "grad_norm": 0.45697882771492004,
      "learning_rate": 0.0004592293906810036,
      "loss": 7.0107,
      "step": 547
    },
    {
      "epoch": 0.16371648368063335,
      "grad_norm": 0.3535310626029968,
      "learning_rate": 0.0004591547192353644,
      "loss": 7.5635,
      "step": 548
    },
    {
      "epoch": 0.16401523638807977,
      "grad_norm": 0.40528586506843567,
      "learning_rate": 0.00045908004778972525,
      "loss": 7.2578,
      "step": 549
    },
    {
      "epoch": 0.1643139890955262,
      "grad_norm": 0.3310297429561615,
      "learning_rate": 0.000459005376344086,
      "loss": 7.6299,
      "step": 550
    },
    {
      "epoch": 0.16461274180297258,
      "grad_norm": 0.36236876249313354,
      "learning_rate": 0.0004589307048984469,
      "loss": 7.2324,
      "step": 551
    },
    {
      "epoch": 0.164911494510419,
      "grad_norm": 0.3836616277694702,
      "learning_rate": 0.00045885603345280763,
      "loss": 7.1973,
      "step": 552
    },
    {
      "epoch": 0.16521024721786542,
      "grad_norm": 0.39154064655303955,
      "learning_rate": 0.00045878136200716845,
      "loss": 7.499,
      "step": 553
    },
    {
      "epoch": 0.16550899992531182,
      "grad_norm": 0.3483290374279022,
      "learning_rate": 0.0004587066905615293,
      "loss": 7.5039,
      "step": 554
    },
    {
      "epoch": 0.16580775263275824,
      "grad_norm": 0.38793253898620605,
      "learning_rate": 0.0004586320191158901,
      "loss": 7.3457,
      "step": 555
    },
    {
      "epoch": 0.16610650534020466,
      "grad_norm": 0.3821764290332794,
      "learning_rate": 0.00045855734767025094,
      "loss": 7.3965,
      "step": 556
    },
    {
      "epoch": 0.16640525804765105,
      "grad_norm": 0.340007483959198,
      "learning_rate": 0.0004584826762246117,
      "loss": 7.5859,
      "step": 557
    },
    {
      "epoch": 0.16670401075509747,
      "grad_norm": 0.40256068110466003,
      "learning_rate": 0.00045840800477897257,
      "loss": 7.25,
      "step": 558
    },
    {
      "epoch": 0.1670027634625439,
      "grad_norm": 0.5629507899284363,
      "learning_rate": 0.0004583333333333333,
      "loss": 7.0137,
      "step": 559
    },
    {
      "epoch": 0.16730151616999028,
      "grad_norm": 0.534640908241272,
      "learning_rate": 0.00045825866188769414,
      "loss": 6.8975,
      "step": 560
    },
    {
      "epoch": 0.1676002688774367,
      "grad_norm": 0.4003329873085022,
      "learning_rate": 0.00045818399044205495,
      "loss": 7.2285,
      "step": 561
    },
    {
      "epoch": 0.16789902158488312,
      "grad_norm": 0.41849029064178467,
      "learning_rate": 0.00045810931899641576,
      "loss": 7.4932,
      "step": 562
    },
    {
      "epoch": 0.16819777429232952,
      "grad_norm": 0.43317610025405884,
      "learning_rate": 0.00045803464755077663,
      "loss": 7.1191,
      "step": 563
    },
    {
      "epoch": 0.16849652699977594,
      "grad_norm": 0.36945387721061707,
      "learning_rate": 0.0004579599761051374,
      "loss": 7.4355,
      "step": 564
    },
    {
      "epoch": 0.16879527970722236,
      "grad_norm": 0.428309828042984,
      "learning_rate": 0.00045788530465949826,
      "loss": 6.957,
      "step": 565
    },
    {
      "epoch": 0.16909403241466875,
      "grad_norm": 0.43416914343833923,
      "learning_rate": 0.000457810633213859,
      "loss": 7.3232,
      "step": 566
    },
    {
      "epoch": 0.16939278512211517,
      "grad_norm": 0.3761763274669647,
      "learning_rate": 0.0004577359617682199,
      "loss": 7.4131,
      "step": 567
    },
    {
      "epoch": 0.1696915378295616,
      "grad_norm": 0.38552385568618774,
      "learning_rate": 0.00045766129032258064,
      "loss": 6.957,
      "step": 568
    },
    {
      "epoch": 0.16999029053700798,
      "grad_norm": 0.37265893816947937,
      "learning_rate": 0.00045758661887694145,
      "loss": 7.5215,
      "step": 569
    },
    {
      "epoch": 0.1702890432444544,
      "grad_norm": 0.3897472321987152,
      "learning_rate": 0.00045751194743130227,
      "loss": 7.3398,
      "step": 570
    },
    {
      "epoch": 0.17058779595190082,
      "grad_norm": 0.3238326609134674,
      "learning_rate": 0.0004574372759856631,
      "loss": 7.7412,
      "step": 571
    },
    {
      "epoch": 0.17088654865934721,
      "grad_norm": 0.42091798782348633,
      "learning_rate": 0.00045736260454002395,
      "loss": 7.376,
      "step": 572
    },
    {
      "epoch": 0.17118530136679364,
      "grad_norm": 0.4016720950603485,
      "learning_rate": 0.0004572879330943847,
      "loss": 7.5918,
      "step": 573
    },
    {
      "epoch": 0.17148405407424006,
      "grad_norm": 0.38962221145629883,
      "learning_rate": 0.00045721326164874557,
      "loss": 7.2871,
      "step": 574
    },
    {
      "epoch": 0.17178280678168645,
      "grad_norm": 0.39090844988822937,
      "learning_rate": 0.00045713859020310633,
      "loss": 7.4746,
      "step": 575
    },
    {
      "epoch": 0.17208155948913287,
      "grad_norm": 0.40731289982795715,
      "learning_rate": 0.00045706391875746714,
      "loss": 7.0703,
      "step": 576
    },
    {
      "epoch": 0.1723803121965793,
      "grad_norm": 0.4257664680480957,
      "learning_rate": 0.00045698924731182796,
      "loss": 7.0127,
      "step": 577
    },
    {
      "epoch": 0.17267906490402568,
      "grad_norm": 0.4589594602584839,
      "learning_rate": 0.00045691457586618877,
      "loss": 6.8076,
      "step": 578
    },
    {
      "epoch": 0.1729778176114721,
      "grad_norm": 0.37811416387557983,
      "learning_rate": 0.0004568399044205496,
      "loss": 7.1123,
      "step": 579
    },
    {
      "epoch": 0.17327657031891852,
      "grad_norm": 0.401071161031723,
      "learning_rate": 0.0004567652329749104,
      "loss": 7.0098,
      "step": 580
    },
    {
      "epoch": 0.1735753230263649,
      "grad_norm": 0.4928409457206726,
      "learning_rate": 0.00045669056152927126,
      "loss": 7.0869,
      "step": 581
    },
    {
      "epoch": 0.17387407573381133,
      "grad_norm": 0.3705659806728363,
      "learning_rate": 0.000456615890083632,
      "loss": 7.2725,
      "step": 582
    },
    {
      "epoch": 0.17417282844125775,
      "grad_norm": 0.35716262459754944,
      "learning_rate": 0.0004565412186379929,
      "loss": 7.5283,
      "step": 583
    },
    {
      "epoch": 0.17447158114870415,
      "grad_norm": 0.44968387484550476,
      "learning_rate": 0.00045646654719235365,
      "loss": 6.8555,
      "step": 584
    },
    {
      "epoch": 0.17477033385615057,
      "grad_norm": 0.4309314489364624,
      "learning_rate": 0.00045639187574671446,
      "loss": 6.9453,
      "step": 585
    },
    {
      "epoch": 0.175069086563597,
      "grad_norm": 0.5305352807044983,
      "learning_rate": 0.00045631720430107527,
      "loss": 6.3955,
      "step": 586
    },
    {
      "epoch": 0.1753678392710434,
      "grad_norm": 0.36297738552093506,
      "learning_rate": 0.0004562425328554361,
      "loss": 7.1943,
      "step": 587
    },
    {
      "epoch": 0.1756665919784898,
      "grad_norm": 0.39075636863708496,
      "learning_rate": 0.0004561678614097969,
      "loss": 7.2637,
      "step": 588
    },
    {
      "epoch": 0.17596534468593622,
      "grad_norm": 0.4245891571044922,
      "learning_rate": 0.0004560931899641577,
      "loss": 6.9678,
      "step": 589
    },
    {
      "epoch": 0.17626409739338264,
      "grad_norm": 0.3536333441734314,
      "learning_rate": 0.0004560185185185186,
      "loss": 7.6035,
      "step": 590
    },
    {
      "epoch": 0.17656285010082903,
      "grad_norm": 0.37007611989974976,
      "learning_rate": 0.00045594384707287934,
      "loss": 7.542,
      "step": 591
    },
    {
      "epoch": 0.17686160280827545,
      "grad_norm": 0.37865617871284485,
      "learning_rate": 0.00045586917562724015,
      "loss": 7.1719,
      "step": 592
    },
    {
      "epoch": 0.17716035551572187,
      "grad_norm": 0.41175633668899536,
      "learning_rate": 0.00045579450418160096,
      "loss": 7.4404,
      "step": 593
    },
    {
      "epoch": 0.17745910822316827,
      "grad_norm": 0.3914108872413635,
      "learning_rate": 0.0004557198327359618,
      "loss": 7.2676,
      "step": 594
    },
    {
      "epoch": 0.1777578609306147,
      "grad_norm": 0.45254841446876526,
      "learning_rate": 0.0004556451612903226,
      "loss": 6.8359,
      "step": 595
    },
    {
      "epoch": 0.1780566136380611,
      "grad_norm": 0.36402076482772827,
      "learning_rate": 0.0004555704898446834,
      "loss": 7.5029,
      "step": 596
    },
    {
      "epoch": 0.1783553663455075,
      "grad_norm": 0.3413120210170746,
      "learning_rate": 0.0004554958183990442,
      "loss": 7.5625,
      "step": 597
    },
    {
      "epoch": 0.17865411905295392,
      "grad_norm": 0.3583661615848541,
      "learning_rate": 0.000455421146953405,
      "loss": 7.4912,
      "step": 598
    },
    {
      "epoch": 0.17895287176040034,
      "grad_norm": 0.3917255103588104,
      "learning_rate": 0.0004553464755077659,
      "loss": 7.9229,
      "step": 599
    },
    {
      "epoch": 0.17925162446784673,
      "grad_norm": 0.46217119693756104,
      "learning_rate": 0.00045527180406212665,
      "loss": 6.7598,
      "step": 600
    },
    {
      "epoch": 0.17925162446784673,
      "eval_bleu": 0.07854102719186938,
      "eval_loss": 7.11328125,
      "eval_runtime": 527.0984,
      "eval_samples_per_second": 2.673,
      "eval_steps_per_second": 0.169,
      "step": 600
    },
    {
      "epoch": 0.17955037717529315,
      "grad_norm": 0.4248005449771881,
      "learning_rate": 0.00045519713261648747,
      "loss": 6.7539,
      "step": 601
    },
    {
      "epoch": 0.17984912988273957,
      "grad_norm": 0.32401132583618164,
      "learning_rate": 0.0004551224611708483,
      "loss": 7.6738,
      "step": 602
    },
    {
      "epoch": 0.18014788259018596,
      "grad_norm": 0.43907949328422546,
      "learning_rate": 0.0004550477897252091,
      "loss": 7.3604,
      "step": 603
    },
    {
      "epoch": 0.18044663529763239,
      "grad_norm": 0.38660985231399536,
      "learning_rate": 0.0004549731182795699,
      "loss": 7.416,
      "step": 604
    },
    {
      "epoch": 0.1807453880050788,
      "grad_norm": 0.4815957844257355,
      "learning_rate": 0.0004548984468339307,
      "loss": 6.6309,
      "step": 605
    },
    {
      "epoch": 0.1810441407125252,
      "grad_norm": 0.4087294042110443,
      "learning_rate": 0.00045482377538829153,
      "loss": 7.2529,
      "step": 606
    },
    {
      "epoch": 0.18134289341997162,
      "grad_norm": 0.4295148551464081,
      "learning_rate": 0.00045474910394265234,
      "loss": 7.3789,
      "step": 607
    },
    {
      "epoch": 0.18164164612741804,
      "grad_norm": 0.386086642742157,
      "learning_rate": 0.0004546744324970131,
      "loss": 7.2842,
      "step": 608
    },
    {
      "epoch": 0.18194039883486443,
      "grad_norm": 0.3591023087501526,
      "learning_rate": 0.00045459976105137397,
      "loss": 7.4424,
      "step": 609
    },
    {
      "epoch": 0.18223915154231085,
      "grad_norm": 0.3802921772003174,
      "learning_rate": 0.0004545250896057348,
      "loss": 7.1465,
      "step": 610
    },
    {
      "epoch": 0.18253790424975727,
      "grad_norm": 0.461904376745224,
      "learning_rate": 0.0004544504181600956,
      "loss": 7.1436,
      "step": 611
    },
    {
      "epoch": 0.18283665695720366,
      "grad_norm": 0.4196985363960266,
      "learning_rate": 0.0004543757467144564,
      "loss": 7.1875,
      "step": 612
    },
    {
      "epoch": 0.18313540966465008,
      "grad_norm": 0.4085301160812378,
      "learning_rate": 0.0004543010752688172,
      "loss": 7.2158,
      "step": 613
    },
    {
      "epoch": 0.1834341623720965,
      "grad_norm": 0.37223660945892334,
      "learning_rate": 0.00045422640382317803,
      "loss": 7.4443,
      "step": 614
    },
    {
      "epoch": 0.1837329150795429,
      "grad_norm": 0.3721599876880646,
      "learning_rate": 0.0004541517323775388,
      "loss": 7.0391,
      "step": 615
    },
    {
      "epoch": 0.18403166778698932,
      "grad_norm": 0.4309301972389221,
      "learning_rate": 0.00045407706093189966,
      "loss": 7.2725,
      "step": 616
    },
    {
      "epoch": 0.18433042049443574,
      "grad_norm": 0.36106613278388977,
      "learning_rate": 0.0004540023894862604,
      "loss": 7.6318,
      "step": 617
    },
    {
      "epoch": 0.18462917320188213,
      "grad_norm": 0.47131600975990295,
      "learning_rate": 0.0004539277180406213,
      "loss": 7.5039,
      "step": 618
    },
    {
      "epoch": 0.18492792590932855,
      "grad_norm": 0.39348429441452026,
      "learning_rate": 0.0004538530465949821,
      "loss": 7.4619,
      "step": 619
    },
    {
      "epoch": 0.18522667861677497,
      "grad_norm": 0.49011680483818054,
      "learning_rate": 0.0004537783751493429,
      "loss": 6.7764,
      "step": 620
    },
    {
      "epoch": 0.18552543132422136,
      "grad_norm": 0.4355608820915222,
      "learning_rate": 0.0004537037037037037,
      "loss": 6.8281,
      "step": 621
    },
    {
      "epoch": 0.18582418403166778,
      "grad_norm": 0.42373842000961304,
      "learning_rate": 0.00045362903225806454,
      "loss": 7.4287,
      "step": 622
    },
    {
      "epoch": 0.1861229367391142,
      "grad_norm": 0.3839108943939209,
      "learning_rate": 0.00045355436081242535,
      "loss": 7.1514,
      "step": 623
    },
    {
      "epoch": 0.1864216894465606,
      "grad_norm": 0.4588606357574463,
      "learning_rate": 0.0004534796893667861,
      "loss": 7.377,
      "step": 624
    },
    {
      "epoch": 0.18672044215400702,
      "grad_norm": 0.38743725419044495,
      "learning_rate": 0.000453405017921147,
      "loss": 7.2119,
      "step": 625
    },
    {
      "epoch": 0.18701919486145344,
      "grad_norm": 0.34786054491996765,
      "learning_rate": 0.00045333034647550773,
      "loss": 7.2021,
      "step": 626
    },
    {
      "epoch": 0.18731794756889986,
      "grad_norm": 0.5188442468643188,
      "learning_rate": 0.0004532556750298686,
      "loss": 6.8203,
      "step": 627
    },
    {
      "epoch": 0.18761670027634625,
      "grad_norm": 0.34678980708122253,
      "learning_rate": 0.0004531810035842294,
      "loss": 7.7139,
      "step": 628
    },
    {
      "epoch": 0.18791545298379267,
      "grad_norm": 0.38118764758110046,
      "learning_rate": 0.0004531063321385902,
      "loss": 7.3252,
      "step": 629
    },
    {
      "epoch": 0.1882142056912391,
      "grad_norm": 0.357207715511322,
      "learning_rate": 0.00045303166069295104,
      "loss": 7.6729,
      "step": 630
    },
    {
      "epoch": 0.18851295839868548,
      "grad_norm": 0.3754153251647949,
      "learning_rate": 0.0004529569892473118,
      "loss": 7.4785,
      "step": 631
    },
    {
      "epoch": 0.1888117111061319,
      "grad_norm": 0.3679674565792084,
      "learning_rate": 0.00045288231780167266,
      "loss": 7.1943,
      "step": 632
    },
    {
      "epoch": 0.18911046381357832,
      "grad_norm": 0.5033873319625854,
      "learning_rate": 0.0004528076463560334,
      "loss": 7.3643,
      "step": 633
    },
    {
      "epoch": 0.18940921652102471,
      "grad_norm": 0.38266322016716003,
      "learning_rate": 0.0004527329749103943,
      "loss": 7.5508,
      "step": 634
    },
    {
      "epoch": 0.18970796922847113,
      "grad_norm": 0.39511701464653015,
      "learning_rate": 0.00045265830346475505,
      "loss": 6.8467,
      "step": 635
    },
    {
      "epoch": 0.19000672193591756,
      "grad_norm": 0.4473613202571869,
      "learning_rate": 0.0004525836320191159,
      "loss": 7.0176,
      "step": 636
    },
    {
      "epoch": 0.19030547464336395,
      "grad_norm": 0.4341128170490265,
      "learning_rate": 0.00045250896057347673,
      "loss": 7.2852,
      "step": 637
    },
    {
      "epoch": 0.19060422735081037,
      "grad_norm": 0.3157351016998291,
      "learning_rate": 0.00045243428912783754,
      "loss": 7.6836,
      "step": 638
    },
    {
      "epoch": 0.1909029800582568,
      "grad_norm": 0.3980732858181,
      "learning_rate": 0.00045235961768219835,
      "loss": 7.1055,
      "step": 639
    },
    {
      "epoch": 0.19120173276570318,
      "grad_norm": 0.3441426157951355,
      "learning_rate": 0.0004522849462365591,
      "loss": 7.6016,
      "step": 640
    },
    {
      "epoch": 0.1915004854731496,
      "grad_norm": 0.3617788553237915,
      "learning_rate": 0.00045221027479092,
      "loss": 7.4297,
      "step": 641
    },
    {
      "epoch": 0.19179923818059602,
      "grad_norm": 0.4115547239780426,
      "learning_rate": 0.00045213560334528074,
      "loss": 6.957,
      "step": 642
    },
    {
      "epoch": 0.1920979908880424,
      "grad_norm": 0.3798518180847168,
      "learning_rate": 0.0004520609318996416,
      "loss": 7.5117,
      "step": 643
    },
    {
      "epoch": 0.19239674359548883,
      "grad_norm": 0.38557571172714233,
      "learning_rate": 0.00045198626045400237,
      "loss": 7.0547,
      "step": 644
    },
    {
      "epoch": 0.19269549630293525,
      "grad_norm": 0.5245217680931091,
      "learning_rate": 0.00045191158900836323,
      "loss": 6.6494,
      "step": 645
    },
    {
      "epoch": 0.19299424901038165,
      "grad_norm": 0.3415203094482422,
      "learning_rate": 0.00045183691756272405,
      "loss": 7.7441,
      "step": 646
    },
    {
      "epoch": 0.19329300171782807,
      "grad_norm": 0.4497454762458801,
      "learning_rate": 0.0004517622461170848,
      "loss": 7.4971,
      "step": 647
    },
    {
      "epoch": 0.1935917544252745,
      "grad_norm": 0.40038663148880005,
      "learning_rate": 0.00045168757467144567,
      "loss": 7.7158,
      "step": 648
    },
    {
      "epoch": 0.19389050713272088,
      "grad_norm": 0.38530421257019043,
      "learning_rate": 0.00045161290322580643,
      "loss": 7.3926,
      "step": 649
    },
    {
      "epoch": 0.1941892598401673,
      "grad_norm": 0.39906278252601624,
      "learning_rate": 0.0004515382317801673,
      "loss": 6.8848,
      "step": 650
    },
    {
      "epoch": 0.19448801254761372,
      "grad_norm": 0.4303952753543854,
      "learning_rate": 0.00045146356033452806,
      "loss": 7.4951,
      "step": 651
    },
    {
      "epoch": 0.1947867652550601,
      "grad_norm": 0.3416970372200012,
      "learning_rate": 0.0004513888888888889,
      "loss": 7.5947,
      "step": 652
    },
    {
      "epoch": 0.19508551796250653,
      "grad_norm": 0.3872196674346924,
      "learning_rate": 0.0004513142174432497,
      "loss": 7.1836,
      "step": 653
    },
    {
      "epoch": 0.19538427066995295,
      "grad_norm": 0.4784843623638153,
      "learning_rate": 0.00045123954599761055,
      "loss": 7.3574,
      "step": 654
    },
    {
      "epoch": 0.19568302337739935,
      "grad_norm": 0.3752526640892029,
      "learning_rate": 0.00045116487455197136,
      "loss": 7.666,
      "step": 655
    },
    {
      "epoch": 0.19598177608484577,
      "grad_norm": 0.3630761504173279,
      "learning_rate": 0.0004510902031063321,
      "loss": 7.5322,
      "step": 656
    },
    {
      "epoch": 0.19628052879229219,
      "grad_norm": 0.36428409814834595,
      "learning_rate": 0.000451015531660693,
      "loss": 7.4971,
      "step": 657
    },
    {
      "epoch": 0.19657928149973858,
      "grad_norm": 0.4428221583366394,
      "learning_rate": 0.00045094086021505375,
      "loss": 7.2744,
      "step": 658
    },
    {
      "epoch": 0.196878034207185,
      "grad_norm": 0.4339991509914398,
      "learning_rate": 0.0004508661887694146,
      "loss": 7.127,
      "step": 659
    },
    {
      "epoch": 0.19717678691463142,
      "grad_norm": 0.4080313444137573,
      "learning_rate": 0.00045079151732377537,
      "loss": 7.0605,
      "step": 660
    },
    {
      "epoch": 0.1974755396220778,
      "grad_norm": 0.35533300042152405,
      "learning_rate": 0.00045071684587813624,
      "loss": 7.7754,
      "step": 661
    },
    {
      "epoch": 0.19777429232952423,
      "grad_norm": 0.3933696746826172,
      "learning_rate": 0.000450642174432497,
      "loss": 7.1836,
      "step": 662
    },
    {
      "epoch": 0.19807304503697065,
      "grad_norm": 0.4357900321483612,
      "learning_rate": 0.0004505675029868578,
      "loss": 6.9248,
      "step": 663
    },
    {
      "epoch": 0.19837179774441707,
      "grad_norm": 0.35669243335723877,
      "learning_rate": 0.0004504928315412187,
      "loss": 7.4629,
      "step": 664
    },
    {
      "epoch": 0.19867055045186346,
      "grad_norm": 0.40406301617622375,
      "learning_rate": 0.00045041816009557944,
      "loss": 7.5498,
      "step": 665
    },
    {
      "epoch": 0.19896930315930988,
      "grad_norm": 0.37862446904182434,
      "learning_rate": 0.0004503434886499403,
      "loss": 7.1182,
      "step": 666
    },
    {
      "epoch": 0.1992680558667563,
      "grad_norm": 0.31080374121665955,
      "learning_rate": 0.00045026881720430106,
      "loss": 7.8965,
      "step": 667
    },
    {
      "epoch": 0.1995668085742027,
      "grad_norm": 0.3831155300140381,
      "learning_rate": 0.00045019414575866193,
      "loss": 7.127,
      "step": 668
    },
    {
      "epoch": 0.19986556128164912,
      "grad_norm": 0.35943517088890076,
      "learning_rate": 0.0004501194743130227,
      "loss": 7.6123,
      "step": 669
    },
    {
      "epoch": 0.20016431398909554,
      "grad_norm": 0.39862382411956787,
      "learning_rate": 0.00045004480286738355,
      "loss": 6.8672,
      "step": 670
    },
    {
      "epoch": 0.20046306669654193,
      "grad_norm": 0.3977946639060974,
      "learning_rate": 0.0004499701314217443,
      "loss": 7.2354,
      "step": 671
    },
    {
      "epoch": 0.20076181940398835,
      "grad_norm": 0.4058443009853363,
      "learning_rate": 0.0004498954599761051,
      "loss": 7.1074,
      "step": 672
    },
    {
      "epoch": 0.20106057211143477,
      "grad_norm": 0.4078497290611267,
      "learning_rate": 0.000449820788530466,
      "loss": 7.1729,
      "step": 673
    },
    {
      "epoch": 0.20135932481888116,
      "grad_norm": 0.42287686467170715,
      "learning_rate": 0.00044974611708482675,
      "loss": 7.375,
      "step": 674
    },
    {
      "epoch": 0.20165807752632758,
      "grad_norm": 0.4304882287979126,
      "learning_rate": 0.0004496714456391876,
      "loss": 7.0117,
      "step": 675
    },
    {
      "epoch": 0.201956830233774,
      "grad_norm": 0.42946475744247437,
      "learning_rate": 0.0004495967741935484,
      "loss": 7.2471,
      "step": 676
    },
    {
      "epoch": 0.2022555829412204,
      "grad_norm": 0.4023226499557495,
      "learning_rate": 0.00044952210274790924,
      "loss": 7.4561,
      "step": 677
    },
    {
      "epoch": 0.20255433564866682,
      "grad_norm": 0.4913168251514435,
      "learning_rate": 0.00044944743130227,
      "loss": 6.6797,
      "step": 678
    },
    {
      "epoch": 0.20285308835611324,
      "grad_norm": 0.47047677636146545,
      "learning_rate": 0.0004493727598566308,
      "loss": 7.1924,
      "step": 679
    },
    {
      "epoch": 0.20315184106355963,
      "grad_norm": 0.31947970390319824,
      "learning_rate": 0.00044929808841099163,
      "loss": 7.4912,
      "step": 680
    },
    {
      "epoch": 0.20345059377100605,
      "grad_norm": 0.3915490210056305,
      "learning_rate": 0.00044922341696535244,
      "loss": 7.1162,
      "step": 681
    },
    {
      "epoch": 0.20374934647845247,
      "grad_norm": 0.6536716222763062,
      "learning_rate": 0.0004491487455197133,
      "loss": 6.3047,
      "step": 682
    },
    {
      "epoch": 0.20404809918589886,
      "grad_norm": 0.4451655149459839,
      "learning_rate": 0.00044907407407407407,
      "loss": 7.1592,
      "step": 683
    },
    {
      "epoch": 0.20434685189334528,
      "grad_norm": 0.35187673568725586,
      "learning_rate": 0.00044899940262843493,
      "loss": 7.4609,
      "step": 684
    },
    {
      "epoch": 0.2046456046007917,
      "grad_norm": 0.3187737464904785,
      "learning_rate": 0.0004489247311827957,
      "loss": 7.8906,
      "step": 685
    },
    {
      "epoch": 0.2049443573082381,
      "grad_norm": 0.40084370970726013,
      "learning_rate": 0.00044885005973715656,
      "loss": 7.0869,
      "step": 686
    },
    {
      "epoch": 0.20524311001568452,
      "grad_norm": 0.42563003301620483,
      "learning_rate": 0.0004487753882915173,
      "loss": 7.374,
      "step": 687
    },
    {
      "epoch": 0.20554186272313094,
      "grad_norm": 0.41075465083122253,
      "learning_rate": 0.00044870071684587813,
      "loss": 7.1553,
      "step": 688
    },
    {
      "epoch": 0.20584061543057733,
      "grad_norm": 0.35735753178596497,
      "learning_rate": 0.00044862604540023894,
      "loss": 7.5332,
      "step": 689
    },
    {
      "epoch": 0.20613936813802375,
      "grad_norm": 0.337489515542984,
      "learning_rate": 0.00044855137395459976,
      "loss": 7.2363,
      "step": 690
    },
    {
      "epoch": 0.20643812084547017,
      "grad_norm": 0.3457932770252228,
      "learning_rate": 0.0004484767025089606,
      "loss": 7.5195,
      "step": 691
    },
    {
      "epoch": 0.20673687355291656,
      "grad_norm": 0.4107029139995575,
      "learning_rate": 0.0004484020310633214,
      "loss": 7.2256,
      "step": 692
    },
    {
      "epoch": 0.20703562626036298,
      "grad_norm": 0.39478421211242676,
      "learning_rate": 0.00044832735961768225,
      "loss": 7.5059,
      "step": 693
    },
    {
      "epoch": 0.2073343789678094,
      "grad_norm": 0.37058475613594055,
      "learning_rate": 0.000448252688172043,
      "loss": 7.248,
      "step": 694
    },
    {
      "epoch": 0.2076331316752558,
      "grad_norm": 0.4061013460159302,
      "learning_rate": 0.0004481780167264038,
      "loss": 7.3711,
      "step": 695
    },
    {
      "epoch": 0.20793188438270221,
      "grad_norm": 0.3810076117515564,
      "learning_rate": 0.00044810334528076463,
      "loss": 7.3574,
      "step": 696
    },
    {
      "epoch": 0.20823063709014863,
      "grad_norm": 0.4121871888637543,
      "learning_rate": 0.00044802867383512545,
      "loss": 7.043,
      "step": 697
    },
    {
      "epoch": 0.20852938979759503,
      "grad_norm": 0.4459388852119446,
      "learning_rate": 0.00044795400238948626,
      "loss": 7.2451,
      "step": 698
    },
    {
      "epoch": 0.20882814250504145,
      "grad_norm": 0.43536895513534546,
      "learning_rate": 0.0004478793309438471,
      "loss": 7.5674,
      "step": 699
    },
    {
      "epoch": 0.20912689521248787,
      "grad_norm": 0.46629413962364197,
      "learning_rate": 0.0004478046594982079,
      "loss": 7.043,
      "step": 700
    },
    {
      "epoch": 0.2094256479199343,
      "grad_norm": 0.5301429033279419,
      "learning_rate": 0.0004477299880525687,
      "loss": 6.2744,
      "step": 701
    },
    {
      "epoch": 0.20972440062738068,
      "grad_norm": 0.4304063022136688,
      "learning_rate": 0.00044765531660692957,
      "loss": 6.7617,
      "step": 702
    },
    {
      "epoch": 0.2100231533348271,
      "grad_norm": 0.4079777002334595,
      "learning_rate": 0.0004475806451612903,
      "loss": 7.5693,
      "step": 703
    },
    {
      "epoch": 0.21032190604227352,
      "grad_norm": 0.38183334469795227,
      "learning_rate": 0.00044750597371565114,
      "loss": 6.9434,
      "step": 704
    },
    {
      "epoch": 0.2106206587497199,
      "grad_norm": 0.36060017347335815,
      "learning_rate": 0.00044743130227001195,
      "loss": 7.583,
      "step": 705
    },
    {
      "epoch": 0.21091941145716633,
      "grad_norm": 0.36223557591438293,
      "learning_rate": 0.00044735663082437276,
      "loss": 7.4258,
      "step": 706
    },
    {
      "epoch": 0.21121816416461275,
      "grad_norm": 0.3928808271884918,
      "learning_rate": 0.0004472819593787336,
      "loss": 7.6377,
      "step": 707
    },
    {
      "epoch": 0.21151691687205915,
      "grad_norm": 0.4357273280620575,
      "learning_rate": 0.0004472072879330944,
      "loss": 7.3408,
      "step": 708
    },
    {
      "epoch": 0.21181566957950557,
      "grad_norm": 0.3856928050518036,
      "learning_rate": 0.0004471326164874552,
      "loss": 7.6719,
      "step": 709
    },
    {
      "epoch": 0.212114422286952,
      "grad_norm": 0.3722672760486603,
      "learning_rate": 0.000447057945041816,
      "loss": 7.7949,
      "step": 710
    },
    {
      "epoch": 0.21241317499439838,
      "grad_norm": 0.46537214517593384,
      "learning_rate": 0.00044698327359617683,
      "loss": 7.0049,
      "step": 711
    },
    {
      "epoch": 0.2127119277018448,
      "grad_norm": 0.4096795618534088,
      "learning_rate": 0.00044690860215053764,
      "loss": 7.2275,
      "step": 712
    },
    {
      "epoch": 0.21301068040929122,
      "grad_norm": 0.3321285545825958,
      "learning_rate": 0.00044683393070489845,
      "loss": 7.791,
      "step": 713
    },
    {
      "epoch": 0.2133094331167376,
      "grad_norm": 0.4175223112106323,
      "learning_rate": 0.00044675925925925927,
      "loss": 7.2383,
      "step": 714
    },
    {
      "epoch": 0.21360818582418403,
      "grad_norm": 0.554729700088501,
      "learning_rate": 0.0004466845878136201,
      "loss": 6.957,
      "step": 715
    },
    {
      "epoch": 0.21390693853163045,
      "grad_norm": 0.40223103761672974,
      "learning_rate": 0.0004466099163679809,
      "loss": 7.4307,
      "step": 716
    },
    {
      "epoch": 0.21420569123907685,
      "grad_norm": 0.4515427350997925,
      "learning_rate": 0.0004465352449223417,
      "loss": 7.3398,
      "step": 717
    },
    {
      "epoch": 0.21450444394652327,
      "grad_norm": 0.42579343914985657,
      "learning_rate": 0.0004464605734767025,
      "loss": 7.1123,
      "step": 718
    },
    {
      "epoch": 0.21480319665396969,
      "grad_norm": 0.3857142925262451,
      "learning_rate": 0.00044638590203106333,
      "loss": 7.0029,
      "step": 719
    },
    {
      "epoch": 0.21510194936141608,
      "grad_norm": 0.43541309237480164,
      "learning_rate": 0.00044631123058542414,
      "loss": 7.1973,
      "step": 720
    },
    {
      "epoch": 0.2154007020688625,
      "grad_norm": 0.41649940609931946,
      "learning_rate": 0.00044623655913978496,
      "loss": 7.0547,
      "step": 721
    },
    {
      "epoch": 0.21569945477630892,
      "grad_norm": 0.46136072278022766,
      "learning_rate": 0.00044616188769414577,
      "loss": 7.0449,
      "step": 722
    },
    {
      "epoch": 0.2159982074837553,
      "grad_norm": 0.4854053556919098,
      "learning_rate": 0.0004460872162485066,
      "loss": 6.6396,
      "step": 723
    },
    {
      "epoch": 0.21629696019120173,
      "grad_norm": 0.47906750440597534,
      "learning_rate": 0.0004460125448028674,
      "loss": 7.6074,
      "step": 724
    },
    {
      "epoch": 0.21659571289864815,
      "grad_norm": 0.41345587372779846,
      "learning_rate": 0.0004459378733572282,
      "loss": 6.9346,
      "step": 725
    },
    {
      "epoch": 0.21689446560609454,
      "grad_norm": 0.3388844132423401,
      "learning_rate": 0.000445863201911589,
      "loss": 7.6631,
      "step": 726
    },
    {
      "epoch": 0.21719321831354096,
      "grad_norm": 0.513288140296936,
      "learning_rate": 0.0004457885304659498,
      "loss": 6.4883,
      "step": 727
    },
    {
      "epoch": 0.21749197102098738,
      "grad_norm": 0.45356428623199463,
      "learning_rate": 0.00044571385902031065,
      "loss": 7.2061,
      "step": 728
    },
    {
      "epoch": 0.21779072372843378,
      "grad_norm": 0.32497695088386536,
      "learning_rate": 0.00044563918757467146,
      "loss": 7.5879,
      "step": 729
    },
    {
      "epoch": 0.2180894764358802,
      "grad_norm": 0.4006805419921875,
      "learning_rate": 0.00044556451612903227,
      "loss": 7.6592,
      "step": 730
    },
    {
      "epoch": 0.21838822914332662,
      "grad_norm": 0.5001377463340759,
      "learning_rate": 0.0004454898446833931,
      "loss": 6.6309,
      "step": 731
    },
    {
      "epoch": 0.218686981850773,
      "grad_norm": 0.40575700998306274,
      "learning_rate": 0.0004454151732377539,
      "loss": 7.1797,
      "step": 732
    },
    {
      "epoch": 0.21898573455821943,
      "grad_norm": 0.43372267484664917,
      "learning_rate": 0.0004453405017921147,
      "loss": 7.1787,
      "step": 733
    },
    {
      "epoch": 0.21928448726566585,
      "grad_norm": 0.3778757154941559,
      "learning_rate": 0.0004452658303464755,
      "loss": 7.3379,
      "step": 734
    },
    {
      "epoch": 0.21958323997311224,
      "grad_norm": 0.4057849645614624,
      "learning_rate": 0.00044519115890083634,
      "loss": 7.2871,
      "step": 735
    },
    {
      "epoch": 0.21988199268055866,
      "grad_norm": 0.3847145736217499,
      "learning_rate": 0.0004451164874551971,
      "loss": 7.5518,
      "step": 736
    },
    {
      "epoch": 0.22018074538800508,
      "grad_norm": 0.47657546401023865,
      "learning_rate": 0.00044504181600955796,
      "loss": 6.9531,
      "step": 737
    },
    {
      "epoch": 0.2204794980954515,
      "grad_norm": 0.49452465772628784,
      "learning_rate": 0.0004449671445639188,
      "loss": 6.9648,
      "step": 738
    },
    {
      "epoch": 0.2207782508028979,
      "grad_norm": 0.39857950806617737,
      "learning_rate": 0.0004448924731182796,
      "loss": 7.3818,
      "step": 739
    },
    {
      "epoch": 0.22107700351034432,
      "grad_norm": 0.5226014852523804,
      "learning_rate": 0.0004448178016726404,
      "loss": 6.9971,
      "step": 740
    },
    {
      "epoch": 0.22137575621779074,
      "grad_norm": 0.4519363045692444,
      "learning_rate": 0.0004447431302270012,
      "loss": 6.8096,
      "step": 741
    },
    {
      "epoch": 0.22167450892523713,
      "grad_norm": 0.43581196665763855,
      "learning_rate": 0.00044466845878136203,
      "loss": 7.5605,
      "step": 742
    },
    {
      "epoch": 0.22197326163268355,
      "grad_norm": 0.4889175295829773,
      "learning_rate": 0.0004445937873357228,
      "loss": 6.9033,
      "step": 743
    },
    {
      "epoch": 0.22227201434012997,
      "grad_norm": 0.486364483833313,
      "learning_rate": 0.00044451911589008365,
      "loss": 6.6016,
      "step": 744
    },
    {
      "epoch": 0.22257076704757636,
      "grad_norm": 0.41740578413009644,
      "learning_rate": 0.0004444444444444444,
      "loss": 7.1426,
      "step": 745
    },
    {
      "epoch": 0.22286951975502278,
      "grad_norm": 0.3550545871257782,
      "learning_rate": 0.0004443697729988053,
      "loss": 7.4805,
      "step": 746
    },
    {
      "epoch": 0.2231682724624692,
      "grad_norm": 0.3622879385948181,
      "learning_rate": 0.0004442951015531661,
      "loss": 7.2256,
      "step": 747
    },
    {
      "epoch": 0.2234670251699156,
      "grad_norm": 0.3899250626564026,
      "learning_rate": 0.0004442204301075269,
      "loss": 7.5566,
      "step": 748
    },
    {
      "epoch": 0.22376577787736202,
      "grad_norm": 0.48321664333343506,
      "learning_rate": 0.0004441457586618877,
      "loss": 6.832,
      "step": 749
    },
    {
      "epoch": 0.22406453058480844,
      "grad_norm": 0.3857758343219757,
      "learning_rate": 0.00044407108721624853,
      "loss": 7.2617,
      "step": 750
    },
    {
      "epoch": 0.22436328329225483,
      "grad_norm": 0.4406209886074066,
      "learning_rate": 0.00044399641577060934,
      "loss": 7.3379,
      "step": 751
    },
    {
      "epoch": 0.22466203599970125,
      "grad_norm": 0.5260152816772461,
      "learning_rate": 0.0004439217443249701,
      "loss": 6.6836,
      "step": 752
    },
    {
      "epoch": 0.22496078870714767,
      "grad_norm": 0.46181535720825195,
      "learning_rate": 0.00044384707287933097,
      "loss": 6.7881,
      "step": 753
    },
    {
      "epoch": 0.22525954141459406,
      "grad_norm": 0.6058606505393982,
      "learning_rate": 0.00044377240143369173,
      "loss": 6.416,
      "step": 754
    },
    {
      "epoch": 0.22555829412204048,
      "grad_norm": 0.4534761309623718,
      "learning_rate": 0.0004436977299880526,
      "loss": 7.2461,
      "step": 755
    },
    {
      "epoch": 0.2258570468294869,
      "grad_norm": 0.42217007279396057,
      "learning_rate": 0.0004436230585424134,
      "loss": 7.3545,
      "step": 756
    },
    {
      "epoch": 0.2261557995369333,
      "grad_norm": 0.36713290214538574,
      "learning_rate": 0.0004435483870967742,
      "loss": 7.8701,
      "step": 757
    },
    {
      "epoch": 0.22645455224437971,
      "grad_norm": 0.3474491238594055,
      "learning_rate": 0.00044347371565113503,
      "loss": 7.5928,
      "step": 758
    },
    {
      "epoch": 0.22675330495182613,
      "grad_norm": 0.3256765902042389,
      "learning_rate": 0.0004433990442054958,
      "loss": 7.7773,
      "step": 759
    },
    {
      "epoch": 0.22705205765927253,
      "grad_norm": 0.5174461007118225,
      "learning_rate": 0.00044332437275985666,
      "loss": 6.7617,
      "step": 760
    },
    {
      "epoch": 0.22735081036671895,
      "grad_norm": 0.3373779058456421,
      "learning_rate": 0.0004432497013142174,
      "loss": 7.6387,
      "step": 761
    },
    {
      "epoch": 0.22764956307416537,
      "grad_norm": 0.39637190103530884,
      "learning_rate": 0.0004431750298685783,
      "loss": 7.3125,
      "step": 762
    },
    {
      "epoch": 0.22794831578161176,
      "grad_norm": 0.42789018154144287,
      "learning_rate": 0.00044310035842293904,
      "loss": 7.2324,
      "step": 763
    },
    {
      "epoch": 0.22824706848905818,
      "grad_norm": 0.3285674750804901,
      "learning_rate": 0.0004430256869772999,
      "loss": 7.4385,
      "step": 764
    },
    {
      "epoch": 0.2285458211965046,
      "grad_norm": 0.4036949574947357,
      "learning_rate": 0.0004429510155316607,
      "loss": 7.5088,
      "step": 765
    },
    {
      "epoch": 0.228844573903951,
      "grad_norm": 0.4450298547744751,
      "learning_rate": 0.00044287634408602154,
      "loss": 7.5752,
      "step": 766
    },
    {
      "epoch": 0.2291433266113974,
      "grad_norm": 0.42466631531715393,
      "learning_rate": 0.00044280167264038235,
      "loss": 7.0586,
      "step": 767
    },
    {
      "epoch": 0.22944207931884383,
      "grad_norm": 0.37189003825187683,
      "learning_rate": 0.0004427270011947431,
      "loss": 7.2451,
      "step": 768
    },
    {
      "epoch": 0.22974083202629023,
      "grad_norm": 0.34833380579948425,
      "learning_rate": 0.000442652329749104,
      "loss": 7.8906,
      "step": 769
    },
    {
      "epoch": 0.23003958473373665,
      "grad_norm": 0.5377634167671204,
      "learning_rate": 0.00044257765830346473,
      "loss": 6.748,
      "step": 770
    },
    {
      "epoch": 0.23033833744118307,
      "grad_norm": 0.4270717203617096,
      "learning_rate": 0.0004425029868578256,
      "loss": 7.3311,
      "step": 771
    },
    {
      "epoch": 0.23063709014862946,
      "grad_norm": 0.4251841902732849,
      "learning_rate": 0.00044242831541218636,
      "loss": 7.3564,
      "step": 772
    },
    {
      "epoch": 0.23093584285607588,
      "grad_norm": 0.42088013887405396,
      "learning_rate": 0.0004423536439665472,
      "loss": 7.4062,
      "step": 773
    },
    {
      "epoch": 0.2312345955635223,
      "grad_norm": 0.4633598029613495,
      "learning_rate": 0.00044227897252090804,
      "loss": 7.458,
      "step": 774
    },
    {
      "epoch": 0.2315333482709687,
      "grad_norm": 0.43119463324546814,
      "learning_rate": 0.0004422043010752688,
      "loss": 6.8721,
      "step": 775
    },
    {
      "epoch": 0.2318321009784151,
      "grad_norm": 0.4418625235557556,
      "learning_rate": 0.00044212962962962966,
      "loss": 7.2168,
      "step": 776
    },
    {
      "epoch": 0.23213085368586153,
      "grad_norm": 0.46492862701416016,
      "learning_rate": 0.0004420549581839904,
      "loss": 6.9004,
      "step": 777
    },
    {
      "epoch": 0.23242960639330795,
      "grad_norm": 0.3611902892589569,
      "learning_rate": 0.0004419802867383513,
      "loss": 7.3301,
      "step": 778
    },
    {
      "epoch": 0.23272835910075435,
      "grad_norm": 0.32795652747154236,
      "learning_rate": 0.00044190561529271205,
      "loss": 7.8711,
      "step": 779
    },
    {
      "epoch": 0.23302711180820077,
      "grad_norm": 0.4667639434337616,
      "learning_rate": 0.0004418309438470729,
      "loss": 6.9668,
      "step": 780
    },
    {
      "epoch": 0.23332586451564719,
      "grad_norm": 0.43481895327568054,
      "learning_rate": 0.0004417562724014337,
      "loss": 7.0312,
      "step": 781
    },
    {
      "epoch": 0.23362461722309358,
      "grad_norm": 0.35526904463768005,
      "learning_rate": 0.00044168160095579454,
      "loss": 7.7061,
      "step": 782
    },
    {
      "epoch": 0.23392336993054,
      "grad_norm": 0.45083528757095337,
      "learning_rate": 0.00044160692951015536,
      "loss": 6.8311,
      "step": 783
    },
    {
      "epoch": 0.23422212263798642,
      "grad_norm": 0.45404699444770813,
      "learning_rate": 0.0004415322580645161,
      "loss": 6.9971,
      "step": 784
    },
    {
      "epoch": 0.2345208753454328,
      "grad_norm": 0.4249763786792755,
      "learning_rate": 0.000441457586618877,
      "loss": 7.0723,
      "step": 785
    },
    {
      "epoch": 0.23481962805287923,
      "grad_norm": 0.38304606080055237,
      "learning_rate": 0.00044138291517323774,
      "loss": 7.5439,
      "step": 786
    },
    {
      "epoch": 0.23511838076032565,
      "grad_norm": 0.3788539171218872,
      "learning_rate": 0.0004413082437275986,
      "loss": 7.5654,
      "step": 787
    },
    {
      "epoch": 0.23541713346777204,
      "grad_norm": 0.3718545138835907,
      "learning_rate": 0.00044123357228195937,
      "loss": 7.6309,
      "step": 788
    },
    {
      "epoch": 0.23571588617521846,
      "grad_norm": 0.39574339985847473,
      "learning_rate": 0.00044115890083632023,
      "loss": 7.208,
      "step": 789
    },
    {
      "epoch": 0.23601463888266488,
      "grad_norm": 0.3946608603000641,
      "learning_rate": 0.000441084229390681,
      "loss": 7.1035,
      "step": 790
    },
    {
      "epoch": 0.23631339159011128,
      "grad_norm": 0.6183930039405823,
      "learning_rate": 0.0004410095579450418,
      "loss": 6.5049,
      "step": 791
    },
    {
      "epoch": 0.2366121442975577,
      "grad_norm": 0.43061065673828125,
      "learning_rate": 0.0004409348864994026,
      "loss": 7.6357,
      "step": 792
    },
    {
      "epoch": 0.23691089700500412,
      "grad_norm": 0.3741077184677124,
      "learning_rate": 0.00044086021505376343,
      "loss": 7.2139,
      "step": 793
    },
    {
      "epoch": 0.2372096497124505,
      "grad_norm": 0.3502742350101471,
      "learning_rate": 0.0004407855436081243,
      "loss": 7.8447,
      "step": 794
    },
    {
      "epoch": 0.23750840241989693,
      "grad_norm": 0.4088904559612274,
      "learning_rate": 0.00044071087216248506,
      "loss": 7.1035,
      "step": 795
    },
    {
      "epoch": 0.23780715512734335,
      "grad_norm": 0.49940183758735657,
      "learning_rate": 0.0004406362007168459,
      "loss": 7.5742,
      "step": 796
    },
    {
      "epoch": 0.23810590783478974,
      "grad_norm": 0.4954121708869934,
      "learning_rate": 0.0004405615292712067,
      "loss": 7.3447,
      "step": 797
    },
    {
      "epoch": 0.23840466054223616,
      "grad_norm": 0.4448952078819275,
      "learning_rate": 0.00044048685782556755,
      "loss": 7.5518,
      "step": 798
    },
    {
      "epoch": 0.23870341324968258,
      "grad_norm": 0.4292686879634857,
      "learning_rate": 0.0004404121863799283,
      "loss": 6.8701,
      "step": 799
    },
    {
      "epoch": 0.23900216595712898,
      "grad_norm": 0.4051790237426758,
      "learning_rate": 0.0004403375149342891,
      "loss": 7.0977,
      "step": 800
    },
    {
      "epoch": 0.23900216595712898,
      "eval_bleu": 0.09268497379540871,
      "eval_loss": 7.11328125,
      "eval_runtime": 482.1033,
      "eval_samples_per_second": 2.923,
      "eval_steps_per_second": 0.185,
      "step": 800
    },
    {
      "epoch": 0.2393009186645754,
      "grad_norm": 0.4723067283630371,
      "learning_rate": 0.00044026284348864993,
      "loss": 6.8682,
      "step": 801
    },
    {
      "epoch": 0.23959967137202182,
      "grad_norm": 0.4082038402557373,
      "learning_rate": 0.00044018817204301075,
      "loss": 7.1895,
      "step": 802
    },
    {
      "epoch": 0.2398984240794682,
      "grad_norm": 0.496094286441803,
      "learning_rate": 0.0004401135005973716,
      "loss": 7.0098,
      "step": 803
    },
    {
      "epoch": 0.24019717678691463,
      "grad_norm": 0.49288639426231384,
      "learning_rate": 0.00044003882915173237,
      "loss": 7.1094,
      "step": 804
    },
    {
      "epoch": 0.24049592949436105,
      "grad_norm": 0.3998868465423584,
      "learning_rate": 0.00043996415770609324,
      "loss": 7.8066,
      "step": 805
    },
    {
      "epoch": 0.24079468220180744,
      "grad_norm": 0.42048683762550354,
      "learning_rate": 0.000439889486260454,
      "loss": 7.2666,
      "step": 806
    },
    {
      "epoch": 0.24109343490925386,
      "grad_norm": 0.5373194813728333,
      "learning_rate": 0.0004398148148148148,
      "loss": 6.9785,
      "step": 807
    },
    {
      "epoch": 0.24139218761670028,
      "grad_norm": 0.49598559737205505,
      "learning_rate": 0.0004397401433691756,
      "loss": 7.3652,
      "step": 808
    },
    {
      "epoch": 0.24169094032414667,
      "grad_norm": 0.3324628174304962,
      "learning_rate": 0.00043966547192353644,
      "loss": 8.1182,
      "step": 809
    },
    {
      "epoch": 0.2419896930315931,
      "grad_norm": 0.36435815691947937,
      "learning_rate": 0.00043959080047789725,
      "loss": 7.5576,
      "step": 810
    },
    {
      "epoch": 0.24228844573903952,
      "grad_norm": 0.38541746139526367,
      "learning_rate": 0.00043951612903225806,
      "loss": 7.4707,
      "step": 811
    },
    {
      "epoch": 0.2425871984464859,
      "grad_norm": 0.38939177989959717,
      "learning_rate": 0.00043944145758661893,
      "loss": 7.4492,
      "step": 812
    },
    {
      "epoch": 0.24288595115393233,
      "grad_norm": 0.7057454586029053,
      "learning_rate": 0.0004393667861409797,
      "loss": 7.1445,
      "step": 813
    },
    {
      "epoch": 0.24318470386137875,
      "grad_norm": 0.32446718215942383,
      "learning_rate": 0.00043929211469534055,
      "loss": 8.0498,
      "step": 814
    },
    {
      "epoch": 0.24348345656882517,
      "grad_norm": 0.5168993473052979,
      "learning_rate": 0.0004392174432497013,
      "loss": 6.957,
      "step": 815
    },
    {
      "epoch": 0.24378220927627156,
      "grad_norm": 0.41026151180267334,
      "learning_rate": 0.0004391427718040621,
      "loss": 7.291,
      "step": 816
    },
    {
      "epoch": 0.24408096198371798,
      "grad_norm": 0.4517073631286621,
      "learning_rate": 0.00043906810035842294,
      "loss": 7.1455,
      "step": 817
    },
    {
      "epoch": 0.2443797146911644,
      "grad_norm": 0.414474755525589,
      "learning_rate": 0.00043899342891278375,
      "loss": 7.3086,
      "step": 818
    },
    {
      "epoch": 0.2446784673986108,
      "grad_norm": 0.5586822628974915,
      "learning_rate": 0.00043891875746714456,
      "loss": 6.5918,
      "step": 819
    },
    {
      "epoch": 0.24497722010605721,
      "grad_norm": 0.46947115659713745,
      "learning_rate": 0.0004388440860215054,
      "loss": 7.2402,
      "step": 820
    },
    {
      "epoch": 0.24527597281350363,
      "grad_norm": 0.3789454698562622,
      "learning_rate": 0.00043876941457586624,
      "loss": 7.6982,
      "step": 821
    },
    {
      "epoch": 0.24557472552095003,
      "grad_norm": 0.4131413698196411,
      "learning_rate": 0.000438694743130227,
      "loss": 7.1621,
      "step": 822
    },
    {
      "epoch": 0.24587347822839645,
      "grad_norm": 0.382409930229187,
      "learning_rate": 0.0004386200716845878,
      "loss": 7.2578,
      "step": 823
    },
    {
      "epoch": 0.24617223093584287,
      "grad_norm": 0.5812870860099792,
      "learning_rate": 0.00043854540023894863,
      "loss": 6.4443,
      "step": 824
    },
    {
      "epoch": 0.24647098364328926,
      "grad_norm": 0.4468601942062378,
      "learning_rate": 0.00043847072879330944,
      "loss": 7.0801,
      "step": 825
    },
    {
      "epoch": 0.24676973635073568,
      "grad_norm": 0.3774029016494751,
      "learning_rate": 0.00043839605734767025,
      "loss": 7.3457,
      "step": 826
    },
    {
      "epoch": 0.2470684890581821,
      "grad_norm": 0.4216448962688446,
      "learning_rate": 0.00043832138590203107,
      "loss": 7.6172,
      "step": 827
    },
    {
      "epoch": 0.2473672417656285,
      "grad_norm": 0.3918052017688751,
      "learning_rate": 0.0004382467144563919,
      "loss": 7.2012,
      "step": 828
    },
    {
      "epoch": 0.2476659944730749,
      "grad_norm": 0.44214770197868347,
      "learning_rate": 0.0004381720430107527,
      "loss": 7.0391,
      "step": 829
    },
    {
      "epoch": 0.24796474718052133,
      "grad_norm": 0.38930222392082214,
      "learning_rate": 0.00043809737156511356,
      "loss": 7.375,
      "step": 830
    },
    {
      "epoch": 0.24826349988796773,
      "grad_norm": 0.3546565771102905,
      "learning_rate": 0.0004380227001194743,
      "loss": 7.7666,
      "step": 831
    },
    {
      "epoch": 0.24856225259541415,
      "grad_norm": 0.47644031047821045,
      "learning_rate": 0.00043794802867383513,
      "loss": 6.8301,
      "step": 832
    },
    {
      "epoch": 0.24886100530286057,
      "grad_norm": 0.3499388098716736,
      "learning_rate": 0.00043787335722819594,
      "loss": 7.418,
      "step": 833
    },
    {
      "epoch": 0.24915975801030696,
      "grad_norm": 0.4086293578147888,
      "learning_rate": 0.00043779868578255676,
      "loss": 7.0137,
      "step": 834
    },
    {
      "epoch": 0.24945851071775338,
      "grad_norm": 0.4585743546485901,
      "learning_rate": 0.00043772401433691757,
      "loss": 6.9082,
      "step": 835
    },
    {
      "epoch": 0.2497572634251998,
      "grad_norm": 0.4046449065208435,
      "learning_rate": 0.0004376493428912784,
      "loss": 7.3057,
      "step": 836
    },
    {
      "epoch": 0.2500560161326462,
      "grad_norm": 0.6411067247390747,
      "learning_rate": 0.0004375746714456392,
      "loss": 6.7051,
      "step": 837
    },
    {
      "epoch": 0.2503547688400926,
      "grad_norm": 0.36595621705055237,
      "learning_rate": 0.0004375,
      "loss": 7.2129,
      "step": 838
    },
    {
      "epoch": 0.250653521547539,
      "grad_norm": 0.4720175266265869,
      "learning_rate": 0.0004374253285543608,
      "loss": 7.0781,
      "step": 839
    },
    {
      "epoch": 0.25095227425498545,
      "grad_norm": 0.42469558119773865,
      "learning_rate": 0.00043735065710872163,
      "loss": 7.043,
      "step": 840
    },
    {
      "epoch": 0.25125102696243184,
      "grad_norm": 0.44197142124176025,
      "learning_rate": 0.00043727598566308245,
      "loss": 7.1924,
      "step": 841
    },
    {
      "epoch": 0.25154977966987824,
      "grad_norm": 0.4659299850463867,
      "learning_rate": 0.00043720131421744326,
      "loss": 7.1729,
      "step": 842
    },
    {
      "epoch": 0.2518485323773247,
      "grad_norm": 0.3998686671257019,
      "learning_rate": 0.0004371266427718041,
      "loss": 7.4912,
      "step": 843
    },
    {
      "epoch": 0.2521472850847711,
      "grad_norm": 0.3724229335784912,
      "learning_rate": 0.0004370519713261649,
      "loss": 7.9717,
      "step": 844
    },
    {
      "epoch": 0.25244603779221747,
      "grad_norm": 0.37791314721107483,
      "learning_rate": 0.0004369772998805257,
      "loss": 7.7568,
      "step": 845
    },
    {
      "epoch": 0.2527447904996639,
      "grad_norm": 0.48041200637817383,
      "learning_rate": 0.00043690262843488646,
      "loss": 6.8857,
      "step": 846
    },
    {
      "epoch": 0.2530435432071103,
      "grad_norm": 0.39764827489852905,
      "learning_rate": 0.0004368279569892473,
      "loss": 6.9229,
      "step": 847
    },
    {
      "epoch": 0.2533422959145567,
      "grad_norm": 0.40556612610816956,
      "learning_rate": 0.00043675328554360814,
      "loss": 7.082,
      "step": 848
    },
    {
      "epoch": 0.25364104862200315,
      "grad_norm": 0.36819037795066833,
      "learning_rate": 0.00043667861409796895,
      "loss": 7.6426,
      "step": 849
    },
    {
      "epoch": 0.25393980132944954,
      "grad_norm": 0.3755166232585907,
      "learning_rate": 0.00043660394265232976,
      "loss": 7.5801,
      "step": 850
    },
    {
      "epoch": 0.25423855403689594,
      "grad_norm": 0.33136412501335144,
      "learning_rate": 0.0004365292712066906,
      "loss": 7.8838,
      "step": 851
    },
    {
      "epoch": 0.2545373067443424,
      "grad_norm": 0.45201340317726135,
      "learning_rate": 0.0004364545997610514,
      "loss": 7.3086,
      "step": 852
    },
    {
      "epoch": 0.2548360594517888,
      "grad_norm": 0.39920032024383545,
      "learning_rate": 0.0004363799283154122,
      "loss": 7.2324,
      "step": 853
    },
    {
      "epoch": 0.25513481215923517,
      "grad_norm": 0.39989739656448364,
      "learning_rate": 0.000436305256869773,
      "loss": 7.6963,
      "step": 854
    },
    {
      "epoch": 0.2554335648666816,
      "grad_norm": 0.3279734253883362,
      "learning_rate": 0.0004362305854241338,
      "loss": 7.7969,
      "step": 855
    },
    {
      "epoch": 0.255732317574128,
      "grad_norm": 0.37907564640045166,
      "learning_rate": 0.00043615591397849464,
      "loss": 7.0273,
      "step": 856
    },
    {
      "epoch": 0.2560310702815744,
      "grad_norm": 0.38861390948295593,
      "learning_rate": 0.00043608124253285545,
      "loss": 7.5273,
      "step": 857
    },
    {
      "epoch": 0.25632982298902085,
      "grad_norm": 0.5566051602363586,
      "learning_rate": 0.00043600657108721627,
      "loss": 6.585,
      "step": 858
    },
    {
      "epoch": 0.25662857569646724,
      "grad_norm": 0.46195581555366516,
      "learning_rate": 0.0004359318996415771,
      "loss": 7.1416,
      "step": 859
    },
    {
      "epoch": 0.25692732840391364,
      "grad_norm": 0.4175141155719757,
      "learning_rate": 0.0004358572281959379,
      "loss": 6.9648,
      "step": 860
    },
    {
      "epoch": 0.2572260811113601,
      "grad_norm": 0.35858628153800964,
      "learning_rate": 0.0004357825567502987,
      "loss": 7.584,
      "step": 861
    },
    {
      "epoch": 0.2575248338188065,
      "grad_norm": 0.4719788432121277,
      "learning_rate": 0.00043570788530465946,
      "loss": 7.4131,
      "step": 862
    },
    {
      "epoch": 0.25782358652625287,
      "grad_norm": 0.42543765902519226,
      "learning_rate": 0.00043563321385902033,
      "loss": 7.5186,
      "step": 863
    },
    {
      "epoch": 0.2581223392336993,
      "grad_norm": 0.4605262279510498,
      "learning_rate": 0.0004355585424133811,
      "loss": 6.915,
      "step": 864
    },
    {
      "epoch": 0.2584210919411457,
      "grad_norm": 0.427592396736145,
      "learning_rate": 0.00043548387096774196,
      "loss": 7.2627,
      "step": 865
    },
    {
      "epoch": 0.2587198446485921,
      "grad_norm": 0.45586714148521423,
      "learning_rate": 0.00043540919952210277,
      "loss": 7.1318,
      "step": 866
    },
    {
      "epoch": 0.25901859735603855,
      "grad_norm": 0.44765621423721313,
      "learning_rate": 0.0004353345280764636,
      "loss": 6.625,
      "step": 867
    },
    {
      "epoch": 0.25931735006348494,
      "grad_norm": 0.37651553750038147,
      "learning_rate": 0.0004352598566308244,
      "loss": 7.5098,
      "step": 868
    },
    {
      "epoch": 0.25961610277093133,
      "grad_norm": 0.4246636629104614,
      "learning_rate": 0.0004351851851851852,
      "loss": 6.8955,
      "step": 869
    },
    {
      "epoch": 0.2599148554783778,
      "grad_norm": 0.3331461250782013,
      "learning_rate": 0.000435110513739546,
      "loss": 7.7314,
      "step": 870
    },
    {
      "epoch": 0.2602136081858242,
      "grad_norm": 0.34687379002571106,
      "learning_rate": 0.0004350358422939068,
      "loss": 7.7461,
      "step": 871
    },
    {
      "epoch": 0.2605123608932706,
      "grad_norm": 0.45022693276405334,
      "learning_rate": 0.00043496117084826765,
      "loss": 7.0283,
      "step": 872
    },
    {
      "epoch": 0.260811113600717,
      "grad_norm": 0.3949999213218689,
      "learning_rate": 0.0004348864994026284,
      "loss": 7.46,
      "step": 873
    },
    {
      "epoch": 0.2611098663081634,
      "grad_norm": 0.3527705669403076,
      "learning_rate": 0.00043481182795698927,
      "loss": 7.748,
      "step": 874
    },
    {
      "epoch": 0.26140861901560986,
      "grad_norm": 0.39960646629333496,
      "learning_rate": 0.0004347371565113501,
      "loss": 7.0283,
      "step": 875
    },
    {
      "epoch": 0.26170737172305625,
      "grad_norm": 0.45411694049835205,
      "learning_rate": 0.0004346624850657109,
      "loss": 7.3623,
      "step": 876
    },
    {
      "epoch": 0.26200612443050264,
      "grad_norm": 0.3744768798351288,
      "learning_rate": 0.0004345878136200717,
      "loss": 7.6484,
      "step": 877
    },
    {
      "epoch": 0.2623048771379491,
      "grad_norm": 0.3516564667224884,
      "learning_rate": 0.00043451314217443247,
      "loss": 7.4297,
      "step": 878
    },
    {
      "epoch": 0.2626036298453955,
      "grad_norm": 0.41869622468948364,
      "learning_rate": 0.00043443847072879334,
      "loss": 7.0605,
      "step": 879
    },
    {
      "epoch": 0.2629023825528419,
      "grad_norm": 0.4702286124229431,
      "learning_rate": 0.0004343637992831541,
      "loss": 6.999,
      "step": 880
    },
    {
      "epoch": 0.2632011352602883,
      "grad_norm": 0.39602258801460266,
      "learning_rate": 0.00043428912783751496,
      "loss": 7.5547,
      "step": 881
    },
    {
      "epoch": 0.2634998879677347,
      "grad_norm": 0.41770872473716736,
      "learning_rate": 0.0004342144563918757,
      "loss": 7.2539,
      "step": 882
    },
    {
      "epoch": 0.2637986406751811,
      "grad_norm": 0.37051114439964294,
      "learning_rate": 0.0004341397849462366,
      "loss": 7.751,
      "step": 883
    },
    {
      "epoch": 0.26409739338262755,
      "grad_norm": 0.5518862009048462,
      "learning_rate": 0.0004340651135005974,
      "loss": 6.8984,
      "step": 884
    },
    {
      "epoch": 0.26439614609007395,
      "grad_norm": 0.38869529962539673,
      "learning_rate": 0.0004339904420549582,
      "loss": 7.5859,
      "step": 885
    },
    {
      "epoch": 0.26469489879752034,
      "grad_norm": 0.5049735307693481,
      "learning_rate": 0.00043391577060931903,
      "loss": 6.9932,
      "step": 886
    },
    {
      "epoch": 0.2649936515049668,
      "grad_norm": 0.4229329526424408,
      "learning_rate": 0.0004338410991636798,
      "loss": 7.2998,
      "step": 887
    },
    {
      "epoch": 0.2652924042124132,
      "grad_norm": 0.44961467385292053,
      "learning_rate": 0.00043376642771804065,
      "loss": 7.0498,
      "step": 888
    },
    {
      "epoch": 0.2655911569198596,
      "grad_norm": 0.347825288772583,
      "learning_rate": 0.0004336917562724014,
      "loss": 7.8828,
      "step": 889
    },
    {
      "epoch": 0.265889909627306,
      "grad_norm": 0.4737858176231384,
      "learning_rate": 0.0004336170848267623,
      "loss": 7.1729,
      "step": 890
    },
    {
      "epoch": 0.2661886623347524,
      "grad_norm": 0.46223488450050354,
      "learning_rate": 0.00043354241338112304,
      "loss": 7.1943,
      "step": 891
    },
    {
      "epoch": 0.2664874150421988,
      "grad_norm": 0.434112548828125,
      "learning_rate": 0.0004334677419354839,
      "loss": 7.2539,
      "step": 892
    },
    {
      "epoch": 0.26678616774964525,
      "grad_norm": 0.41467055678367615,
      "learning_rate": 0.00043339307048984466,
      "loss": 7.4287,
      "step": 893
    },
    {
      "epoch": 0.26708492045709165,
      "grad_norm": 0.33384230732917786,
      "learning_rate": 0.0004333183990442055,
      "loss": 7.5605,
      "step": 894
    },
    {
      "epoch": 0.26738367316453804,
      "grad_norm": 0.4823230803012848,
      "learning_rate": 0.00043324372759856634,
      "loss": 7.3311,
      "step": 895
    },
    {
      "epoch": 0.2676824258719845,
      "grad_norm": 0.4141921401023865,
      "learning_rate": 0.0004331690561529271,
      "loss": 7.1357,
      "step": 896
    },
    {
      "epoch": 0.2679811785794309,
      "grad_norm": 0.449584037065506,
      "learning_rate": 0.00043309438470728797,
      "loss": 7.2646,
      "step": 897
    },
    {
      "epoch": 0.26827993128687727,
      "grad_norm": 0.4164203703403473,
      "learning_rate": 0.00043301971326164873,
      "loss": 7.1309,
      "step": 898
    },
    {
      "epoch": 0.2685786839943237,
      "grad_norm": 0.3806099593639374,
      "learning_rate": 0.0004329450418160096,
      "loss": 7.6475,
      "step": 899
    },
    {
      "epoch": 0.2688774367017701,
      "grad_norm": 0.4499588906764984,
      "learning_rate": 0.00043287037037037035,
      "loss": 6.8301,
      "step": 900
    },
    {
      "epoch": 0.2691761894092165,
      "grad_norm": 0.40806183218955994,
      "learning_rate": 0.0004327956989247312,
      "loss": 7.2188,
      "step": 901
    },
    {
      "epoch": 0.26947494211666295,
      "grad_norm": 0.4790497124195099,
      "learning_rate": 0.000432721027479092,
      "loss": 6.8428,
      "step": 902
    },
    {
      "epoch": 0.26977369482410934,
      "grad_norm": 0.3520163893699646,
      "learning_rate": 0.0004326463560334528,
      "loss": 7.6104,
      "step": 903
    },
    {
      "epoch": 0.27007244753155574,
      "grad_norm": 0.4220838248729706,
      "learning_rate": 0.00043257168458781366,
      "loss": 7.418,
      "step": 904
    },
    {
      "epoch": 0.2703712002390022,
      "grad_norm": 0.3603399097919464,
      "learning_rate": 0.0004324970131421744,
      "loss": 7.5361,
      "step": 905
    },
    {
      "epoch": 0.2706699529464486,
      "grad_norm": 0.4377238154411316,
      "learning_rate": 0.0004324223416965353,
      "loss": 7.2451,
      "step": 906
    },
    {
      "epoch": 0.27096870565389497,
      "grad_norm": 0.39507755637168884,
      "learning_rate": 0.00043234767025089604,
      "loss": 7.4395,
      "step": 907
    },
    {
      "epoch": 0.2712674583613414,
      "grad_norm": 0.42503637075424194,
      "learning_rate": 0.0004322729988052569,
      "loss": 7.3936,
      "step": 908
    },
    {
      "epoch": 0.2715662110687878,
      "grad_norm": 0.5348562002182007,
      "learning_rate": 0.00043219832735961767,
      "loss": 7.2559,
      "step": 909
    },
    {
      "epoch": 0.2718649637762342,
      "grad_norm": 0.39749211072921753,
      "learning_rate": 0.0004321236559139785,
      "loss": 7.3105,
      "step": 910
    },
    {
      "epoch": 0.27216371648368065,
      "grad_norm": 0.36132773756980896,
      "learning_rate": 0.0004320489844683393,
      "loss": 7.627,
      "step": 911
    },
    {
      "epoch": 0.27246246919112704,
      "grad_norm": 0.45232710242271423,
      "learning_rate": 0.0004319743130227001,
      "loss": 6.6787,
      "step": 912
    },
    {
      "epoch": 0.27276122189857344,
      "grad_norm": 0.427321195602417,
      "learning_rate": 0.000431899641577061,
      "loss": 7.2676,
      "step": 913
    },
    {
      "epoch": 0.2730599746060199,
      "grad_norm": 0.39951008558273315,
      "learning_rate": 0.00043182497013142173,
      "loss": 7.5352,
      "step": 914
    },
    {
      "epoch": 0.2733587273134663,
      "grad_norm": 0.4677272439002991,
      "learning_rate": 0.0004317502986857826,
      "loss": 7.1943,
      "step": 915
    },
    {
      "epoch": 0.27365748002091267,
      "grad_norm": 0.4327559173107147,
      "learning_rate": 0.00043167562724014336,
      "loss": 7.4766,
      "step": 916
    },
    {
      "epoch": 0.2739562327283591,
      "grad_norm": 0.5182842016220093,
      "learning_rate": 0.0004316009557945042,
      "loss": 7.2559,
      "step": 917
    },
    {
      "epoch": 0.2742549854358055,
      "grad_norm": 0.41771912574768066,
      "learning_rate": 0.000431526284348865,
      "loss": 7.3994,
      "step": 918
    },
    {
      "epoch": 0.2745537381432519,
      "grad_norm": 0.3690473735332489,
      "learning_rate": 0.0004314516129032258,
      "loss": 7.3652,
      "step": 919
    },
    {
      "epoch": 0.27485249085069835,
      "grad_norm": 0.43988996744155884,
      "learning_rate": 0.0004313769414575866,
      "loss": 7.0537,
      "step": 920
    },
    {
      "epoch": 0.27515124355814474,
      "grad_norm": 0.4160076975822449,
      "learning_rate": 0.0004313022700119474,
      "loss": 7.4883,
      "step": 921
    },
    {
      "epoch": 0.27544999626559113,
      "grad_norm": 0.40717995166778564,
      "learning_rate": 0.0004312275985663083,
      "loss": 7.501,
      "step": 922
    },
    {
      "epoch": 0.2757487489730376,
      "grad_norm": 0.35410597920417786,
      "learning_rate": 0.00043115292712066905,
      "loss": 7.4473,
      "step": 923
    },
    {
      "epoch": 0.276047501680484,
      "grad_norm": 0.41300350427627563,
      "learning_rate": 0.0004310782556750299,
      "loss": 6.9473,
      "step": 924
    },
    {
      "epoch": 0.27634625438793037,
      "grad_norm": 0.4854552149772644,
      "learning_rate": 0.0004310035842293907,
      "loss": 6.8984,
      "step": 925
    },
    {
      "epoch": 0.2766450070953768,
      "grad_norm": 0.4599839448928833,
      "learning_rate": 0.0004309289127837515,
      "loss": 7.1152,
      "step": 926
    },
    {
      "epoch": 0.2769437598028232,
      "grad_norm": 0.38379576802253723,
      "learning_rate": 0.0004308542413381123,
      "loss": 7.1592,
      "step": 927
    },
    {
      "epoch": 0.2772425125102696,
      "grad_norm": 0.47607821226119995,
      "learning_rate": 0.0004307795698924731,
      "loss": 6.7227,
      "step": 928
    },
    {
      "epoch": 0.27754126521771605,
      "grad_norm": 0.40904954075813293,
      "learning_rate": 0.0004307048984468339,
      "loss": 7.3203,
      "step": 929
    },
    {
      "epoch": 0.27784001792516244,
      "grad_norm": 0.37295886874198914,
      "learning_rate": 0.00043063022700119474,
      "loss": 7.9307,
      "step": 930
    },
    {
      "epoch": 0.27813877063260883,
      "grad_norm": 0.4743909239768982,
      "learning_rate": 0.0004305555555555556,
      "loss": 7.3359,
      "step": 931
    },
    {
      "epoch": 0.2784375233400553,
      "grad_norm": 0.41805747151374817,
      "learning_rate": 0.00043048088410991637,
      "loss": 7.4531,
      "step": 932
    },
    {
      "epoch": 0.2787362760475017,
      "grad_norm": 0.41753989458084106,
      "learning_rate": 0.00043040621266427723,
      "loss": 7.1289,
      "step": 933
    },
    {
      "epoch": 0.27903502875494807,
      "grad_norm": 0.3981003165245056,
      "learning_rate": 0.000430331541218638,
      "loss": 7.2656,
      "step": 934
    },
    {
      "epoch": 0.2793337814623945,
      "grad_norm": 0.5286235213279724,
      "learning_rate": 0.0004302568697729988,
      "loss": 6.6787,
      "step": 935
    },
    {
      "epoch": 0.2796325341698409,
      "grad_norm": 0.42176806926727295,
      "learning_rate": 0.0004301821983273596,
      "loss": 7.3369,
      "step": 936
    },
    {
      "epoch": 0.2799312868772873,
      "grad_norm": 0.3891243040561676,
      "learning_rate": 0.00043010752688172043,
      "loss": 7.1787,
      "step": 937
    },
    {
      "epoch": 0.28023003958473375,
      "grad_norm": 0.4196974039077759,
      "learning_rate": 0.00043003285543608124,
      "loss": 7.0068,
      "step": 938
    },
    {
      "epoch": 0.28052879229218014,
      "grad_norm": 0.3838033974170685,
      "learning_rate": 0.00042995818399044206,
      "loss": 7.2344,
      "step": 939
    },
    {
      "epoch": 0.28082754499962653,
      "grad_norm": 0.36370956897735596,
      "learning_rate": 0.0004298835125448029,
      "loss": 7.001,
      "step": 940
    },
    {
      "epoch": 0.281126297707073,
      "grad_norm": 0.4557884633541107,
      "learning_rate": 0.0004298088410991637,
      "loss": 7.2988,
      "step": 941
    },
    {
      "epoch": 0.2814250504145194,
      "grad_norm": 0.4014793634414673,
      "learning_rate": 0.0004297341696535245,
      "loss": 7.6152,
      "step": 942
    },
    {
      "epoch": 0.28172380312196577,
      "grad_norm": 0.4336600601673126,
      "learning_rate": 0.0004296594982078853,
      "loss": 7.1885,
      "step": 943
    },
    {
      "epoch": 0.2820225558294122,
      "grad_norm": 0.4273433983325958,
      "learning_rate": 0.0004295848267622461,
      "loss": 7.2988,
      "step": 944
    },
    {
      "epoch": 0.2823213085368586,
      "grad_norm": 0.41110217571258545,
      "learning_rate": 0.00042951015531660693,
      "loss": 7.4561,
      "step": 945
    },
    {
      "epoch": 0.28262006124430505,
      "grad_norm": 0.3183257579803467,
      "learning_rate": 0.00042943548387096775,
      "loss": 7.6865,
      "step": 946
    },
    {
      "epoch": 0.28291881395175145,
      "grad_norm": 0.5694592595100403,
      "learning_rate": 0.00042936081242532856,
      "loss": 6.9375,
      "step": 947
    },
    {
      "epoch": 0.28321756665919784,
      "grad_norm": 0.48623138666152954,
      "learning_rate": 0.00042928614097968937,
      "loss": 6.7793,
      "step": 948
    },
    {
      "epoch": 0.2835163193666443,
      "grad_norm": 0.30896005034446716,
      "learning_rate": 0.00042921146953405024,
      "loss": 7.8906,
      "step": 949
    },
    {
      "epoch": 0.2838150720740907,
      "grad_norm": 0.3366028964519501,
      "learning_rate": 0.000429136798088411,
      "loss": 7.6016,
      "step": 950
    },
    {
      "epoch": 0.28411382478153707,
      "grad_norm": 0.460205078125,
      "learning_rate": 0.0004290621266427718,
      "loss": 7.1436,
      "step": 951
    },
    {
      "epoch": 0.2844125774889835,
      "grad_norm": 0.3499848246574402,
      "learning_rate": 0.0004289874551971326,
      "loss": 7.501,
      "step": 952
    },
    {
      "epoch": 0.2847113301964299,
      "grad_norm": 0.40299978852272034,
      "learning_rate": 0.00042891278375149344,
      "loss": 7.6494,
      "step": 953
    },
    {
      "epoch": 0.2850100829038763,
      "grad_norm": 0.43178945779800415,
      "learning_rate": 0.00042883811230585425,
      "loss": 7.1963,
      "step": 954
    },
    {
      "epoch": 0.28530883561132275,
      "grad_norm": 0.3649863004684448,
      "learning_rate": 0.00042876344086021506,
      "loss": 7.6279,
      "step": 955
    },
    {
      "epoch": 0.28560758831876915,
      "grad_norm": 0.43270859122276306,
      "learning_rate": 0.0004286887694145759,
      "loss": 7.1582,
      "step": 956
    },
    {
      "epoch": 0.28590634102621554,
      "grad_norm": 0.4331178069114685,
      "learning_rate": 0.0004286140979689367,
      "loss": 7.165,
      "step": 957
    },
    {
      "epoch": 0.286205093733662,
      "grad_norm": 0.37868112325668335,
      "learning_rate": 0.0004285394265232975,
      "loss": 7.4316,
      "step": 958
    },
    {
      "epoch": 0.2865038464411084,
      "grad_norm": 0.3806072473526001,
      "learning_rate": 0.0004284647550776583,
      "loss": 7.3516,
      "step": 959
    },
    {
      "epoch": 0.28680259914855477,
      "grad_norm": 0.3835466206073761,
      "learning_rate": 0.0004283900836320191,
      "loss": 7.3818,
      "step": 960
    },
    {
      "epoch": 0.2871013518560012,
      "grad_norm": 0.45750972628593445,
      "learning_rate": 0.00042831541218637994,
      "loss": 7.1133,
      "step": 961
    },
    {
      "epoch": 0.2874001045634476,
      "grad_norm": 0.5838074684143066,
      "learning_rate": 0.00042824074074074075,
      "loss": 6.3604,
      "step": 962
    },
    {
      "epoch": 0.287698857270894,
      "grad_norm": 0.486063152551651,
      "learning_rate": 0.00042816606929510156,
      "loss": 6.7471,
      "step": 963
    },
    {
      "epoch": 0.28799760997834045,
      "grad_norm": 0.46097901463508606,
      "learning_rate": 0.0004280913978494624,
      "loss": 7.1152,
      "step": 964
    },
    {
      "epoch": 0.28829636268578684,
      "grad_norm": 0.4228213131427765,
      "learning_rate": 0.0004280167264038232,
      "loss": 7.4141,
      "step": 965
    },
    {
      "epoch": 0.28859511539323324,
      "grad_norm": 0.5176370143890381,
      "learning_rate": 0.000427942054958184,
      "loss": 7.0469,
      "step": 966
    },
    {
      "epoch": 0.2888938681006797,
      "grad_norm": 0.4281718134880066,
      "learning_rate": 0.0004278673835125448,
      "loss": 7.4385,
      "step": 967
    },
    {
      "epoch": 0.2891926208081261,
      "grad_norm": 0.4047592878341675,
      "learning_rate": 0.00042779271206690563,
      "loss": 7.8604,
      "step": 968
    },
    {
      "epoch": 0.28949137351557247,
      "grad_norm": 0.442004919052124,
      "learning_rate": 0.00042771804062126644,
      "loss": 7.042,
      "step": 969
    },
    {
      "epoch": 0.2897901262230189,
      "grad_norm": 0.36630216240882874,
      "learning_rate": 0.00042764336917562725,
      "loss": 7.3594,
      "step": 970
    },
    {
      "epoch": 0.2900888789304653,
      "grad_norm": 0.37489184737205505,
      "learning_rate": 0.00042756869772998807,
      "loss": 7.5254,
      "step": 971
    },
    {
      "epoch": 0.2903876316379117,
      "grad_norm": 0.4115922749042511,
      "learning_rate": 0.0004274940262843489,
      "loss": 7.3984,
      "step": 972
    },
    {
      "epoch": 0.29068638434535815,
      "grad_norm": 0.4399450421333313,
      "learning_rate": 0.0004274193548387097,
      "loss": 6.9121,
      "step": 973
    },
    {
      "epoch": 0.29098513705280454,
      "grad_norm": 0.45114240050315857,
      "learning_rate": 0.00042734468339307045,
      "loss": 7.624,
      "step": 974
    },
    {
      "epoch": 0.29128388976025094,
      "grad_norm": 0.4363076686859131,
      "learning_rate": 0.0004272700119474313,
      "loss": 7.125,
      "step": 975
    },
    {
      "epoch": 0.2915826424676974,
      "grad_norm": 0.47879043221473694,
      "learning_rate": 0.00042719534050179213,
      "loss": 7.0078,
      "step": 976
    },
    {
      "epoch": 0.2918813951751438,
      "grad_norm": 0.44835165143013,
      "learning_rate": 0.00042712066905615294,
      "loss": 6.9941,
      "step": 977
    },
    {
      "epoch": 0.29218014788259017,
      "grad_norm": 0.5064224004745483,
      "learning_rate": 0.00042704599761051376,
      "loss": 7.1309,
      "step": 978
    },
    {
      "epoch": 0.2924789005900366,
      "grad_norm": 0.4334956109523773,
      "learning_rate": 0.00042697132616487457,
      "loss": 6.915,
      "step": 979
    },
    {
      "epoch": 0.292777653297483,
      "grad_norm": 0.4073925316333771,
      "learning_rate": 0.0004268966547192354,
      "loss": 7.625,
      "step": 980
    },
    {
      "epoch": 0.2930764060049294,
      "grad_norm": 0.419558048248291,
      "learning_rate": 0.0004268219832735962,
      "loss": 7.2295,
      "step": 981
    },
    {
      "epoch": 0.29337515871237585,
      "grad_norm": 0.37737566232681274,
      "learning_rate": 0.000426747311827957,
      "loss": 7.3828,
      "step": 982
    },
    {
      "epoch": 0.29367391141982224,
      "grad_norm": 0.42369920015335083,
      "learning_rate": 0.00042667264038231777,
      "loss": 7.4023,
      "step": 983
    },
    {
      "epoch": 0.29397266412726863,
      "grad_norm": 0.5877148509025574,
      "learning_rate": 0.00042659796893667863,
      "loss": 7.0264,
      "step": 984
    },
    {
      "epoch": 0.2942714168347151,
      "grad_norm": 0.4579118490219116,
      "learning_rate": 0.0004265232974910394,
      "loss": 7.2715,
      "step": 985
    },
    {
      "epoch": 0.2945701695421615,
      "grad_norm": 0.43164610862731934,
      "learning_rate": 0.00042644862604540026,
      "loss": 7.542,
      "step": 986
    },
    {
      "epoch": 0.29486892224960787,
      "grad_norm": 0.4240492284297943,
      "learning_rate": 0.0004263739545997611,
      "loss": 7.5625,
      "step": 987
    },
    {
      "epoch": 0.2951676749570543,
      "grad_norm": 0.5499091148376465,
      "learning_rate": 0.0004262992831541219,
      "loss": 7.0068,
      "step": 988
    },
    {
      "epoch": 0.2954664276645007,
      "grad_norm": 0.49844327569007874,
      "learning_rate": 0.0004262246117084827,
      "loss": 6.4561,
      "step": 989
    },
    {
      "epoch": 0.2957651803719471,
      "grad_norm": 0.3727582097053528,
      "learning_rate": 0.00042614994026284346,
      "loss": 7.3701,
      "step": 990
    },
    {
      "epoch": 0.29606393307939355,
      "grad_norm": 0.5158160924911499,
      "learning_rate": 0.0004260752688172043,
      "loss": 6.9229,
      "step": 991
    },
    {
      "epoch": 0.29636268578683994,
      "grad_norm": 0.8223013877868652,
      "learning_rate": 0.0004260005973715651,
      "loss": 6.585,
      "step": 992
    },
    {
      "epoch": 0.29666143849428633,
      "grad_norm": 0.44930702447891235,
      "learning_rate": 0.00042592592592592595,
      "loss": 7.3584,
      "step": 993
    },
    {
      "epoch": 0.2969601912017328,
      "grad_norm": 0.44352638721466064,
      "learning_rate": 0.0004258512544802867,
      "loss": 6.8203,
      "step": 994
    },
    {
      "epoch": 0.2972589439091792,
      "grad_norm": 0.47012829780578613,
      "learning_rate": 0.0004257765830346476,
      "loss": 6.8076,
      "step": 995
    },
    {
      "epoch": 0.29755769661662557,
      "grad_norm": 0.5783194899559021,
      "learning_rate": 0.0004257019115890084,
      "loss": 6.4414,
      "step": 996
    },
    {
      "epoch": 0.297856449324072,
      "grad_norm": 0.456613689661026,
      "learning_rate": 0.0004256272401433692,
      "loss": 7.1377,
      "step": 997
    },
    {
      "epoch": 0.2981552020315184,
      "grad_norm": 0.4158826172351837,
      "learning_rate": 0.00042555256869773,
      "loss": 7.2705,
      "step": 998
    },
    {
      "epoch": 0.2984539547389648,
      "grad_norm": 0.4144705832004547,
      "learning_rate": 0.0004254778972520908,
      "loss": 7.4863,
      "step": 999
    },
    {
      "epoch": 0.29875270744641125,
      "grad_norm": 0.4350469410419464,
      "learning_rate": 0.00042540322580645164,
      "loss": 7.1953,
      "step": 1000
    },
    {
      "epoch": 0.29875270744641125,
      "eval_bleu": 0.0852744815912739,
      "eval_loss": 7.0859375,
      "eval_runtime": 569.3102,
      "eval_samples_per_second": 2.475,
      "eval_steps_per_second": 0.156,
      "step": 1000
    },
    {
      "epoch": 0.29905146015385764,
      "grad_norm": 0.36561650037765503,
      "learning_rate": 0.0004253285543608124,
      "loss": 7.2129,
      "step": 1001
    },
    {
      "epoch": 0.29935021286130403,
      "grad_norm": 0.3895508348941803,
      "learning_rate": 0.00042525388291517327,
      "loss": 7.252,
      "step": 1002
    },
    {
      "epoch": 0.2996489655687505,
      "grad_norm": 0.45288950204849243,
      "learning_rate": 0.000425179211469534,
      "loss": 7.6357,
      "step": 1003
    },
    {
      "epoch": 0.2999477182761969,
      "grad_norm": 0.45206037163734436,
      "learning_rate": 0.0004251045400238949,
      "loss": 6.8604,
      "step": 1004
    },
    {
      "epoch": 0.30024647098364327,
      "grad_norm": 0.4485305845737457,
      "learning_rate": 0.0004250298685782557,
      "loss": 6.5996,
      "step": 1005
    },
    {
      "epoch": 0.3005452236910897,
      "grad_norm": 0.3345312774181366,
      "learning_rate": 0.00042495519713261646,
      "loss": 7.4072,
      "step": 1006
    },
    {
      "epoch": 0.3008439763985361,
      "grad_norm": 0.5029211044311523,
      "learning_rate": 0.00042488052568697733,
      "loss": 7.1133,
      "step": 1007
    },
    {
      "epoch": 0.3011427291059825,
      "grad_norm": 0.5049786567687988,
      "learning_rate": 0.0004248058542413381,
      "loss": 6.8262,
      "step": 1008
    },
    {
      "epoch": 0.30144148181342895,
      "grad_norm": 0.4324814975261688,
      "learning_rate": 0.00042473118279569896,
      "loss": 7.1533,
      "step": 1009
    },
    {
      "epoch": 0.30174023452087534,
      "grad_norm": 0.4080350995063782,
      "learning_rate": 0.0004246565113500597,
      "loss": 7.4619,
      "step": 1010
    },
    {
      "epoch": 0.30203898722832173,
      "grad_norm": 0.37531614303588867,
      "learning_rate": 0.0004245818399044206,
      "loss": 7.0762,
      "step": 1011
    },
    {
      "epoch": 0.3023377399357682,
      "grad_norm": 0.40993693470954895,
      "learning_rate": 0.00042450716845878134,
      "loss": 7.4434,
      "step": 1012
    },
    {
      "epoch": 0.30263649264321457,
      "grad_norm": 0.4353625476360321,
      "learning_rate": 0.0004244324970131422,
      "loss": 7.0479,
      "step": 1013
    },
    {
      "epoch": 0.30293524535066096,
      "grad_norm": 0.4394516348838806,
      "learning_rate": 0.000424357825567503,
      "loss": 7.0205,
      "step": 1014
    },
    {
      "epoch": 0.3032339980581074,
      "grad_norm": 0.35799428820610046,
      "learning_rate": 0.0004242831541218638,
      "loss": 7.3789,
      "step": 1015
    },
    {
      "epoch": 0.3035327507655538,
      "grad_norm": 0.40728357434272766,
      "learning_rate": 0.00042420848267622465,
      "loss": 7.0654,
      "step": 1016
    },
    {
      "epoch": 0.3038315034730002,
      "grad_norm": 0.45761191844940186,
      "learning_rate": 0.0004241338112305854,
      "loss": 7.1104,
      "step": 1017
    },
    {
      "epoch": 0.30413025618044665,
      "grad_norm": 0.48977231979370117,
      "learning_rate": 0.00042405913978494627,
      "loss": 6.7881,
      "step": 1018
    },
    {
      "epoch": 0.30442900888789304,
      "grad_norm": 0.42500731348991394,
      "learning_rate": 0.00042398446833930703,
      "loss": 7.208,
      "step": 1019
    },
    {
      "epoch": 0.30472776159533943,
      "grad_norm": 0.4306623339653015,
      "learning_rate": 0.0004239097968936679,
      "loss": 7.5918,
      "step": 1020
    },
    {
      "epoch": 0.3050265143027859,
      "grad_norm": 0.5146358609199524,
      "learning_rate": 0.00042383512544802866,
      "loss": 6.9805,
      "step": 1021
    },
    {
      "epoch": 0.30532526701023227,
      "grad_norm": 0.39700937271118164,
      "learning_rate": 0.00042376045400238947,
      "loss": 7.4883,
      "step": 1022
    },
    {
      "epoch": 0.3056240197176787,
      "grad_norm": 0.4184378385543823,
      "learning_rate": 0.00042368578255675034,
      "loss": 7.2793,
      "step": 1023
    },
    {
      "epoch": 0.3059227724251251,
      "grad_norm": 0.39305630326271057,
      "learning_rate": 0.0004236111111111111,
      "loss": 7.5068,
      "step": 1024
    },
    {
      "epoch": 0.3062215251325715,
      "grad_norm": 0.44654279947280884,
      "learning_rate": 0.00042353643966547196,
      "loss": 6.8379,
      "step": 1025
    },
    {
      "epoch": 0.30652027784001795,
      "grad_norm": 0.29086920619010925,
      "learning_rate": 0.0004234617682198327,
      "loss": 7.9746,
      "step": 1026
    },
    {
      "epoch": 0.30681903054746434,
      "grad_norm": 0.4487977921962738,
      "learning_rate": 0.0004233870967741936,
      "loss": 6.9561,
      "step": 1027
    },
    {
      "epoch": 0.30711778325491074,
      "grad_norm": 0.38400325179100037,
      "learning_rate": 0.00042331242532855435,
      "loss": 7.874,
      "step": 1028
    },
    {
      "epoch": 0.3074165359623572,
      "grad_norm": 0.5237321257591248,
      "learning_rate": 0.0004232377538829152,
      "loss": 6.9717,
      "step": 1029
    },
    {
      "epoch": 0.3077152886698036,
      "grad_norm": 0.4198107421398163,
      "learning_rate": 0.000423163082437276,
      "loss": 7.6621,
      "step": 1030
    },
    {
      "epoch": 0.30801404137724997,
      "grad_norm": 0.4032559394836426,
      "learning_rate": 0.0004230884109916368,
      "loss": 7.5762,
      "step": 1031
    },
    {
      "epoch": 0.3083127940846964,
      "grad_norm": 0.35650745034217834,
      "learning_rate": 0.00042301373954599765,
      "loss": 7.7373,
      "step": 1032
    },
    {
      "epoch": 0.3086115467921428,
      "grad_norm": 0.4206409752368927,
      "learning_rate": 0.0004229390681003584,
      "loss": 7.3008,
      "step": 1033
    },
    {
      "epoch": 0.3089102994995892,
      "grad_norm": 0.4199439585208893,
      "learning_rate": 0.0004228643966547193,
      "loss": 7.2432,
      "step": 1034
    },
    {
      "epoch": 0.30920905220703565,
      "grad_norm": 0.43988296389579773,
      "learning_rate": 0.00042278972520908004,
      "loss": 7.4736,
      "step": 1035
    },
    {
      "epoch": 0.30950780491448204,
      "grad_norm": 0.38467374444007874,
      "learning_rate": 0.0004227150537634409,
      "loss": 7.4219,
      "step": 1036
    },
    {
      "epoch": 0.30980655762192844,
      "grad_norm": 0.44611185789108276,
      "learning_rate": 0.00042264038231780166,
      "loss": 7.1689,
      "step": 1037
    },
    {
      "epoch": 0.3101053103293749,
      "grad_norm": 0.4784609079360962,
      "learning_rate": 0.0004225657108721625,
      "loss": 7.0791,
      "step": 1038
    },
    {
      "epoch": 0.3104040630368213,
      "grad_norm": 0.44399359822273254,
      "learning_rate": 0.0004224910394265233,
      "loss": 7.1221,
      "step": 1039
    },
    {
      "epoch": 0.31070281574426767,
      "grad_norm": 0.43030330538749695,
      "learning_rate": 0.0004224163679808841,
      "loss": 7.123,
      "step": 1040
    },
    {
      "epoch": 0.3110015684517141,
      "grad_norm": 0.41098731756210327,
      "learning_rate": 0.00042234169653524497,
      "loss": 7.1465,
      "step": 1041
    },
    {
      "epoch": 0.3113003211591605,
      "grad_norm": 0.40267035365104675,
      "learning_rate": 0.00042226702508960573,
      "loss": 7.3945,
      "step": 1042
    },
    {
      "epoch": 0.3115990738666069,
      "grad_norm": 0.4354916214942932,
      "learning_rate": 0.0004221923536439666,
      "loss": 6.8838,
      "step": 1043
    },
    {
      "epoch": 0.31189782657405335,
      "grad_norm": 0.390175461769104,
      "learning_rate": 0.00042211768219832735,
      "loss": 7.4346,
      "step": 1044
    },
    {
      "epoch": 0.31219657928149974,
      "grad_norm": 0.42380452156066895,
      "learning_rate": 0.0004220430107526882,
      "loss": 7.5732,
      "step": 1045
    },
    {
      "epoch": 0.31249533198894613,
      "grad_norm": 0.3856702148914337,
      "learning_rate": 0.000421968339307049,
      "loss": 7.5078,
      "step": 1046
    },
    {
      "epoch": 0.3127940846963926,
      "grad_norm": 0.3865894675254822,
      "learning_rate": 0.0004218936678614098,
      "loss": 7.4785,
      "step": 1047
    },
    {
      "epoch": 0.313092837403839,
      "grad_norm": 0.4473107159137726,
      "learning_rate": 0.0004218189964157706,
      "loss": 7.4834,
      "step": 1048
    },
    {
      "epoch": 0.31339159011128537,
      "grad_norm": 0.42065873742103577,
      "learning_rate": 0.0004217443249701314,
      "loss": 7.3008,
      "step": 1049
    },
    {
      "epoch": 0.3136903428187318,
      "grad_norm": 0.4139236807823181,
      "learning_rate": 0.0004216696535244923,
      "loss": 7.5684,
      "step": 1050
    },
    {
      "epoch": 0.3139890955261782,
      "grad_norm": 0.5097725987434387,
      "learning_rate": 0.00042159498207885304,
      "loss": 6.8262,
      "step": 1051
    },
    {
      "epoch": 0.3142878482336246,
      "grad_norm": 0.49422988295555115,
      "learning_rate": 0.0004215203106332139,
      "loss": 7.1826,
      "step": 1052
    },
    {
      "epoch": 0.31458660094107105,
      "grad_norm": 0.35531729459762573,
      "learning_rate": 0.00042144563918757467,
      "loss": 7.7021,
      "step": 1053
    },
    {
      "epoch": 0.31488535364851744,
      "grad_norm": 0.4253123700618744,
      "learning_rate": 0.0004213709677419355,
      "loss": 7.332,
      "step": 1054
    },
    {
      "epoch": 0.31518410635596383,
      "grad_norm": 0.37399572134017944,
      "learning_rate": 0.0004212962962962963,
      "loss": 7.3076,
      "step": 1055
    },
    {
      "epoch": 0.3154828590634103,
      "grad_norm": 0.35754701495170593,
      "learning_rate": 0.0004212216248506571,
      "loss": 7.8271,
      "step": 1056
    },
    {
      "epoch": 0.3157816117708567,
      "grad_norm": 0.38742172718048096,
      "learning_rate": 0.0004211469534050179,
      "loss": 7.5771,
      "step": 1057
    },
    {
      "epoch": 0.31608036447830307,
      "grad_norm": 0.5507731437683105,
      "learning_rate": 0.00042107228195937873,
      "loss": 6.5117,
      "step": 1058
    },
    {
      "epoch": 0.3163791171857495,
      "grad_norm": 0.420071005821228,
      "learning_rate": 0.0004209976105137396,
      "loss": 7.1768,
      "step": 1059
    },
    {
      "epoch": 0.3166778698931959,
      "grad_norm": 0.39536240696907043,
      "learning_rate": 0.00042092293906810036,
      "loss": 7.2031,
      "step": 1060
    },
    {
      "epoch": 0.3169766226006423,
      "grad_norm": 0.452719122171402,
      "learning_rate": 0.0004208482676224612,
      "loss": 7.2637,
      "step": 1061
    },
    {
      "epoch": 0.31727537530808875,
      "grad_norm": 0.40127918124198914,
      "learning_rate": 0.000420773596176822,
      "loss": 7.3857,
      "step": 1062
    },
    {
      "epoch": 0.31757412801553514,
      "grad_norm": 0.4113979935646057,
      "learning_rate": 0.0004206989247311828,
      "loss": 7.1787,
      "step": 1063
    },
    {
      "epoch": 0.31787288072298153,
      "grad_norm": 0.3689039647579193,
      "learning_rate": 0.0004206242532855436,
      "loss": 7.3633,
      "step": 1064
    },
    {
      "epoch": 0.318171633430428,
      "grad_norm": 0.35421326756477356,
      "learning_rate": 0.0004205495818399044,
      "loss": 7.5059,
      "step": 1065
    },
    {
      "epoch": 0.3184703861378744,
      "grad_norm": 0.46921083331108093,
      "learning_rate": 0.00042047491039426524,
      "loss": 7.2822,
      "step": 1066
    },
    {
      "epoch": 0.31876913884532077,
      "grad_norm": 0.4237115681171417,
      "learning_rate": 0.00042040023894862605,
      "loss": 7.0098,
      "step": 1067
    },
    {
      "epoch": 0.3190678915527672,
      "grad_norm": 0.45442110300064087,
      "learning_rate": 0.0004203255675029869,
      "loss": 7.2881,
      "step": 1068
    },
    {
      "epoch": 0.3193666442602136,
      "grad_norm": 0.4490732252597809,
      "learning_rate": 0.0004202508960573477,
      "loss": 7.1074,
      "step": 1069
    },
    {
      "epoch": 0.31966539696766,
      "grad_norm": 0.3271319568157196,
      "learning_rate": 0.0004201762246117085,
      "loss": 7.832,
      "step": 1070
    },
    {
      "epoch": 0.31996414967510645,
      "grad_norm": 0.41599899530410767,
      "learning_rate": 0.0004201015531660693,
      "loss": 7.4219,
      "step": 1071
    },
    {
      "epoch": 0.32026290238255284,
      "grad_norm": 0.45124301314353943,
      "learning_rate": 0.0004200268817204301,
      "loss": 7.3955,
      "step": 1072
    },
    {
      "epoch": 0.32056165508999923,
      "grad_norm": 0.3407822251319885,
      "learning_rate": 0.0004199522102747909,
      "loss": 7.9688,
      "step": 1073
    },
    {
      "epoch": 0.3208604077974457,
      "grad_norm": 0.36545518040657043,
      "learning_rate": 0.00041987753882915174,
      "loss": 7.4639,
      "step": 1074
    },
    {
      "epoch": 0.32115916050489207,
      "grad_norm": 0.3354385793209076,
      "learning_rate": 0.00041980286738351255,
      "loss": 7.8682,
      "step": 1075
    },
    {
      "epoch": 0.32145791321233846,
      "grad_norm": 0.4444710314273834,
      "learning_rate": 0.00041972819593787337,
      "loss": 6.9795,
      "step": 1076
    },
    {
      "epoch": 0.3217566659197849,
      "grad_norm": 0.45291373133659363,
      "learning_rate": 0.0004196535244922341,
      "loss": 7.1318,
      "step": 1077
    },
    {
      "epoch": 0.3220554186272313,
      "grad_norm": 0.5061197280883789,
      "learning_rate": 0.000419578853046595,
      "loss": 7.042,
      "step": 1078
    },
    {
      "epoch": 0.3223541713346777,
      "grad_norm": 0.46306324005126953,
      "learning_rate": 0.0004195041816009558,
      "loss": 7.373,
      "step": 1079
    },
    {
      "epoch": 0.32265292404212415,
      "grad_norm": 0.36244407296180725,
      "learning_rate": 0.0004194295101553166,
      "loss": 7.6348,
      "step": 1080
    },
    {
      "epoch": 0.32295167674957054,
      "grad_norm": 0.430561900138855,
      "learning_rate": 0.00041935483870967743,
      "loss": 7.418,
      "step": 1081
    },
    {
      "epoch": 0.32325042945701693,
      "grad_norm": 0.43227821588516235,
      "learning_rate": 0.00041928016726403824,
      "loss": 7.1846,
      "step": 1082
    },
    {
      "epoch": 0.3235491821644634,
      "grad_norm": 0.4208293855190277,
      "learning_rate": 0.00041920549581839906,
      "loss": 7.6465,
      "step": 1083
    },
    {
      "epoch": 0.32384793487190977,
      "grad_norm": 0.39111223816871643,
      "learning_rate": 0.00041913082437275987,
      "loss": 7.6377,
      "step": 1084
    },
    {
      "epoch": 0.32414668757935616,
      "grad_norm": 0.4090277850627899,
      "learning_rate": 0.0004190561529271207,
      "loss": 7.1055,
      "step": 1085
    },
    {
      "epoch": 0.3244454402868026,
      "grad_norm": 0.3865348994731903,
      "learning_rate": 0.00041898148148148144,
      "loss": 7.3691,
      "step": 1086
    },
    {
      "epoch": 0.324744192994249,
      "grad_norm": 0.4322788417339325,
      "learning_rate": 0.0004189068100358423,
      "loss": 7.1475,
      "step": 1087
    },
    {
      "epoch": 0.3250429457016954,
      "grad_norm": 0.35430046916007996,
      "learning_rate": 0.0004188321385902031,
      "loss": 7.4434,
      "step": 1088
    },
    {
      "epoch": 0.32534169840914184,
      "grad_norm": 0.4211747646331787,
      "learning_rate": 0.00041875746714456393,
      "loss": 7.4053,
      "step": 1089
    },
    {
      "epoch": 0.32564045111658824,
      "grad_norm": 0.39679154753685,
      "learning_rate": 0.00041868279569892475,
      "loss": 7.6865,
      "step": 1090
    },
    {
      "epoch": 0.32593920382403463,
      "grad_norm": 0.34410324692726135,
      "learning_rate": 0.00041860812425328556,
      "loss": 7.8857,
      "step": 1091
    },
    {
      "epoch": 0.3262379565314811,
      "grad_norm": 0.4657333195209503,
      "learning_rate": 0.00041853345280764637,
      "loss": 6.833,
      "step": 1092
    },
    {
      "epoch": 0.32653670923892747,
      "grad_norm": 0.32042425870895386,
      "learning_rate": 0.00041845878136200713,
      "loss": 7.8672,
      "step": 1093
    },
    {
      "epoch": 0.32683546194637386,
      "grad_norm": 0.3664456009864807,
      "learning_rate": 0.000418384109916368,
      "loss": 7.46,
      "step": 1094
    },
    {
      "epoch": 0.3271342146538203,
      "grad_norm": 0.5140801668167114,
      "learning_rate": 0.00041830943847072876,
      "loss": 7.1807,
      "step": 1095
    },
    {
      "epoch": 0.3274329673612667,
      "grad_norm": 0.3706851601600647,
      "learning_rate": 0.0004182347670250896,
      "loss": 7.793,
      "step": 1096
    },
    {
      "epoch": 0.32773172006871315,
      "grad_norm": 0.41141581535339355,
      "learning_rate": 0.00041816009557945044,
      "loss": 7.4648,
      "step": 1097
    },
    {
      "epoch": 0.32803047277615954,
      "grad_norm": 0.3611508309841156,
      "learning_rate": 0.00041808542413381125,
      "loss": 7.584,
      "step": 1098
    },
    {
      "epoch": 0.32832922548360594,
      "grad_norm": 0.4680461287498474,
      "learning_rate": 0.00041801075268817206,
      "loss": 7.1572,
      "step": 1099
    },
    {
      "epoch": 0.3286279781910524,
      "grad_norm": 0.4653686285018921,
      "learning_rate": 0.0004179360812425329,
      "loss": 6.7275,
      "step": 1100
    },
    {
      "epoch": 0.3289267308984988,
      "grad_norm": 0.4809640347957611,
      "learning_rate": 0.0004178614097968937,
      "loss": 6.9189,
      "step": 1101
    },
    {
      "epoch": 0.32922548360594517,
      "grad_norm": 0.46765419840812683,
      "learning_rate": 0.00041778673835125445,
      "loss": 7.0791,
      "step": 1102
    },
    {
      "epoch": 0.3295242363133916,
      "grad_norm": 0.42379406094551086,
      "learning_rate": 0.0004177120669056153,
      "loss": 7.5078,
      "step": 1103
    },
    {
      "epoch": 0.329822989020838,
      "grad_norm": 0.36727920174598694,
      "learning_rate": 0.00041763739545997607,
      "loss": 7.6504,
      "step": 1104
    },
    {
      "epoch": 0.3301217417282844,
      "grad_norm": 0.46467524766921997,
      "learning_rate": 0.00041756272401433694,
      "loss": 7.3955,
      "step": 1105
    },
    {
      "epoch": 0.33042049443573085,
      "grad_norm": 0.4351022243499756,
      "learning_rate": 0.00041748805256869775,
      "loss": 7.1455,
      "step": 1106
    },
    {
      "epoch": 0.33071924714317724,
      "grad_norm": 0.5749213099479675,
      "learning_rate": 0.00041741338112305856,
      "loss": 6.9209,
      "step": 1107
    },
    {
      "epoch": 0.33101799985062363,
      "grad_norm": 0.539630651473999,
      "learning_rate": 0.0004173387096774194,
      "loss": 6.8828,
      "step": 1108
    },
    {
      "epoch": 0.3313167525580701,
      "grad_norm": 0.4563591480255127,
      "learning_rate": 0.00041726403823178014,
      "loss": 6.9453,
      "step": 1109
    },
    {
      "epoch": 0.3316155052655165,
      "grad_norm": 0.5736159682273865,
      "learning_rate": 0.000417189366786141,
      "loss": 7.1055,
      "step": 1110
    },
    {
      "epoch": 0.33191425797296287,
      "grad_norm": 0.42450249195098877,
      "learning_rate": 0.00041711469534050176,
      "loss": 7.4746,
      "step": 1111
    },
    {
      "epoch": 0.3322130106804093,
      "grad_norm": 0.40547314286231995,
      "learning_rate": 0.00041704002389486263,
      "loss": 7.1455,
      "step": 1112
    },
    {
      "epoch": 0.3325117633878557,
      "grad_norm": 0.37994298338890076,
      "learning_rate": 0.0004169653524492234,
      "loss": 7.4209,
      "step": 1113
    },
    {
      "epoch": 0.3328105160953021,
      "grad_norm": 0.4664217233657837,
      "learning_rate": 0.00041689068100358425,
      "loss": 7.7168,
      "step": 1114
    },
    {
      "epoch": 0.33310926880274855,
      "grad_norm": 0.3890913128852844,
      "learning_rate": 0.00041681600955794507,
      "loss": 7.6094,
      "step": 1115
    },
    {
      "epoch": 0.33340802151019494,
      "grad_norm": 0.3892643451690674,
      "learning_rate": 0.0004167413381123059,
      "loss": 7.0322,
      "step": 1116
    },
    {
      "epoch": 0.33370677421764133,
      "grad_norm": 0.5475536584854126,
      "learning_rate": 0.0004166666666666667,
      "loss": 6.9141,
      "step": 1117
    },
    {
      "epoch": 0.3340055269250878,
      "grad_norm": 0.40577182173728943,
      "learning_rate": 0.00041659199522102745,
      "loss": 7.1914,
      "step": 1118
    },
    {
      "epoch": 0.3343042796325342,
      "grad_norm": 0.3819344639778137,
      "learning_rate": 0.0004165173237753883,
      "loss": 7.3027,
      "step": 1119
    },
    {
      "epoch": 0.33460303233998057,
      "grad_norm": 0.42910104990005493,
      "learning_rate": 0.0004164426523297491,
      "loss": 7.1895,
      "step": 1120
    },
    {
      "epoch": 0.334901785047427,
      "grad_norm": 0.37336793541908264,
      "learning_rate": 0.00041636798088410994,
      "loss": 7.2764,
      "step": 1121
    },
    {
      "epoch": 0.3352005377548734,
      "grad_norm": 0.44528958201408386,
      "learning_rate": 0.0004162933094384707,
      "loss": 7.4395,
      "step": 1122
    },
    {
      "epoch": 0.3354992904623198,
      "grad_norm": 0.399472177028656,
      "learning_rate": 0.00041621863799283157,
      "loss": 7.3965,
      "step": 1123
    },
    {
      "epoch": 0.33579804316976625,
      "grad_norm": 0.45034775137901306,
      "learning_rate": 0.0004161439665471924,
      "loss": 6.9238,
      "step": 1124
    },
    {
      "epoch": 0.33609679587721264,
      "grad_norm": 0.33687517046928406,
      "learning_rate": 0.00041606929510155314,
      "loss": 7.5479,
      "step": 1125
    },
    {
      "epoch": 0.33639554858465903,
      "grad_norm": 0.42495161294937134,
      "learning_rate": 0.000415994623655914,
      "loss": 7.0088,
      "step": 1126
    },
    {
      "epoch": 0.3366943012921055,
      "grad_norm": 0.44581952691078186,
      "learning_rate": 0.00041591995221027477,
      "loss": 7.2061,
      "step": 1127
    },
    {
      "epoch": 0.3369930539995519,
      "grad_norm": 0.4555799067020416,
      "learning_rate": 0.00041584528076463564,
      "loss": 7.0127,
      "step": 1128
    },
    {
      "epoch": 0.33729180670699827,
      "grad_norm": 0.4673580825328827,
      "learning_rate": 0.0004157706093189964,
      "loss": 7.4307,
      "step": 1129
    },
    {
      "epoch": 0.3375905594144447,
      "grad_norm": 0.35866519808769226,
      "learning_rate": 0.00041569593787335726,
      "loss": 7.5967,
      "step": 1130
    },
    {
      "epoch": 0.3378893121218911,
      "grad_norm": 0.4254395067691803,
      "learning_rate": 0.000415621266427718,
      "loss": 7.1182,
      "step": 1131
    },
    {
      "epoch": 0.3381880648293375,
      "grad_norm": 0.39071184396743774,
      "learning_rate": 0.0004155465949820789,
      "loss": 7.8086,
      "step": 1132
    },
    {
      "epoch": 0.33848681753678395,
      "grad_norm": 0.4330123960971832,
      "learning_rate": 0.0004154719235364397,
      "loss": 7.2852,
      "step": 1133
    },
    {
      "epoch": 0.33878557024423034,
      "grad_norm": 0.5372425317764282,
      "learning_rate": 0.00041539725209080046,
      "loss": 6.6484,
      "step": 1134
    },
    {
      "epoch": 0.33908432295167673,
      "grad_norm": 0.45645973086357117,
      "learning_rate": 0.0004153225806451613,
      "loss": 7.2041,
      "step": 1135
    },
    {
      "epoch": 0.3393830756591232,
      "grad_norm": 0.3294666111469269,
      "learning_rate": 0.0004152479091995221,
      "loss": 7.5527,
      "step": 1136
    },
    {
      "epoch": 0.33968182836656957,
      "grad_norm": 0.4823763370513916,
      "learning_rate": 0.00041517323775388295,
      "loss": 7.4375,
      "step": 1137
    },
    {
      "epoch": 0.33998058107401596,
      "grad_norm": 0.5532384514808655,
      "learning_rate": 0.0004150985663082437,
      "loss": 6.5391,
      "step": 1138
    },
    {
      "epoch": 0.3402793337814624,
      "grad_norm": 0.557755172252655,
      "learning_rate": 0.0004150238948626046,
      "loss": 6.9922,
      "step": 1139
    },
    {
      "epoch": 0.3405780864889088,
      "grad_norm": 0.4971378445625305,
      "learning_rate": 0.00041494922341696534,
      "loss": 7.1543,
      "step": 1140
    },
    {
      "epoch": 0.3408768391963552,
      "grad_norm": 0.2950311303138733,
      "learning_rate": 0.00041487455197132615,
      "loss": 7.7617,
      "step": 1141
    },
    {
      "epoch": 0.34117559190380164,
      "grad_norm": 0.35271570086479187,
      "learning_rate": 0.000414799880525687,
      "loss": 7.3711,
      "step": 1142
    },
    {
      "epoch": 0.34147434461124804,
      "grad_norm": 0.38431084156036377,
      "learning_rate": 0.0004147252090800478,
      "loss": 7.4404,
      "step": 1143
    },
    {
      "epoch": 0.34177309731869443,
      "grad_norm": 0.48284491896629333,
      "learning_rate": 0.00041465053763440864,
      "loss": 6.8359,
      "step": 1144
    },
    {
      "epoch": 0.3420718500261409,
      "grad_norm": 0.41182276606559753,
      "learning_rate": 0.0004145758661887694,
      "loss": 7.3369,
      "step": 1145
    },
    {
      "epoch": 0.34237060273358727,
      "grad_norm": 0.4213884770870209,
      "learning_rate": 0.00041450119474313027,
      "loss": 7.2598,
      "step": 1146
    },
    {
      "epoch": 0.34266935544103366,
      "grad_norm": 0.45883122086524963,
      "learning_rate": 0.000414426523297491,
      "loss": 7.3926,
      "step": 1147
    },
    {
      "epoch": 0.3429681081484801,
      "grad_norm": 0.45671749114990234,
      "learning_rate": 0.0004143518518518519,
      "loss": 7.1357,
      "step": 1148
    },
    {
      "epoch": 0.3432668608559265,
      "grad_norm": 0.4320181906223297,
      "learning_rate": 0.00041427718040621265,
      "loss": 7.6631,
      "step": 1149
    },
    {
      "epoch": 0.3435656135633729,
      "grad_norm": 0.4781048893928528,
      "learning_rate": 0.00041420250896057346,
      "loss": 7.1143,
      "step": 1150
    },
    {
      "epoch": 0.34386436627081934,
      "grad_norm": 0.35291406512260437,
      "learning_rate": 0.00041412783751493433,
      "loss": 7.4746,
      "step": 1151
    },
    {
      "epoch": 0.34416311897826574,
      "grad_norm": 0.447512686252594,
      "learning_rate": 0.0004140531660692951,
      "loss": 7.0127,
      "step": 1152
    },
    {
      "epoch": 0.34446187168571213,
      "grad_norm": 0.3887690603733063,
      "learning_rate": 0.00041397849462365596,
      "loss": 7.4551,
      "step": 1153
    },
    {
      "epoch": 0.3447606243931586,
      "grad_norm": 0.4299403131008148,
      "learning_rate": 0.0004139038231780167,
      "loss": 7.5117,
      "step": 1154
    },
    {
      "epoch": 0.34505937710060497,
      "grad_norm": 0.41844820976257324,
      "learning_rate": 0.0004138291517323776,
      "loss": 7.3887,
      "step": 1155
    },
    {
      "epoch": 0.34535812980805136,
      "grad_norm": 0.5585400462150574,
      "learning_rate": 0.00041375448028673834,
      "loss": 6.7627,
      "step": 1156
    },
    {
      "epoch": 0.3456568825154978,
      "grad_norm": 0.39746180176734924,
      "learning_rate": 0.00041367980884109915,
      "loss": 7.4424,
      "step": 1157
    },
    {
      "epoch": 0.3459556352229442,
      "grad_norm": 0.44526970386505127,
      "learning_rate": 0.00041360513739545997,
      "loss": 7.2939,
      "step": 1158
    },
    {
      "epoch": 0.3462543879303906,
      "grad_norm": 0.4681883454322815,
      "learning_rate": 0.0004135304659498208,
      "loss": 7.2197,
      "step": 1159
    },
    {
      "epoch": 0.34655314063783704,
      "grad_norm": 0.3992255628108978,
      "learning_rate": 0.00041345579450418165,
      "loss": 7.3076,
      "step": 1160
    },
    {
      "epoch": 0.34685189334528344,
      "grad_norm": 0.39443346858024597,
      "learning_rate": 0.0004133811230585424,
      "loss": 7.5547,
      "step": 1161
    },
    {
      "epoch": 0.3471506460527298,
      "grad_norm": 0.43411120772361755,
      "learning_rate": 0.00041330645161290327,
      "loss": 6.7852,
      "step": 1162
    },
    {
      "epoch": 0.3474493987601763,
      "grad_norm": 0.4030504524707794,
      "learning_rate": 0.00041323178016726403,
      "loss": 6.9648,
      "step": 1163
    },
    {
      "epoch": 0.34774815146762267,
      "grad_norm": 0.3786243796348572,
      "learning_rate": 0.0004131571087216249,
      "loss": 7.3066,
      "step": 1164
    },
    {
      "epoch": 0.34804690417506906,
      "grad_norm": 0.39617952704429626,
      "learning_rate": 0.00041308243727598566,
      "loss": 7.5703,
      "step": 1165
    },
    {
      "epoch": 0.3483456568825155,
      "grad_norm": 0.5793432593345642,
      "learning_rate": 0.00041300776583034647,
      "loss": 6.7637,
      "step": 1166
    },
    {
      "epoch": 0.3486444095899619,
      "grad_norm": 0.5101607441902161,
      "learning_rate": 0.0004129330943847073,
      "loss": 7.0479,
      "step": 1167
    },
    {
      "epoch": 0.3489431622974083,
      "grad_norm": 0.5196777582168579,
      "learning_rate": 0.0004128584229390681,
      "loss": 7.0762,
      "step": 1168
    },
    {
      "epoch": 0.34924191500485474,
      "grad_norm": 0.40564852952957153,
      "learning_rate": 0.0004127837514934289,
      "loss": 7.5732,
      "step": 1169
    },
    {
      "epoch": 0.34954066771230113,
      "grad_norm": 0.4312015175819397,
      "learning_rate": 0.0004127090800477897,
      "loss": 6.9189,
      "step": 1170
    },
    {
      "epoch": 0.3498394204197475,
      "grad_norm": 0.4284078776836395,
      "learning_rate": 0.0004126344086021506,
      "loss": 7.4395,
      "step": 1171
    },
    {
      "epoch": 0.350138173127194,
      "grad_norm": 0.38022831082344055,
      "learning_rate": 0.00041255973715651135,
      "loss": 7.4209,
      "step": 1172
    },
    {
      "epoch": 0.35043692583464037,
      "grad_norm": 0.36888185143470764,
      "learning_rate": 0.00041248506571087216,
      "loss": 7.5293,
      "step": 1173
    },
    {
      "epoch": 0.3507356785420868,
      "grad_norm": 0.4749763309955597,
      "learning_rate": 0.000412410394265233,
      "loss": 7.1055,
      "step": 1174
    },
    {
      "epoch": 0.3510344312495332,
      "grad_norm": 0.3912242650985718,
      "learning_rate": 0.0004123357228195938,
      "loss": 7.209,
      "step": 1175
    },
    {
      "epoch": 0.3513331839569796,
      "grad_norm": 0.4033750593662262,
      "learning_rate": 0.0004122610513739546,
      "loss": 7.3994,
      "step": 1176
    },
    {
      "epoch": 0.35163193666442605,
      "grad_norm": 0.472583144903183,
      "learning_rate": 0.0004121863799283154,
      "loss": 7.0225,
      "step": 1177
    },
    {
      "epoch": 0.35193068937187244,
      "grad_norm": 0.4219514727592468,
      "learning_rate": 0.0004121117084826762,
      "loss": 6.9229,
      "step": 1178
    },
    {
      "epoch": 0.35222944207931883,
      "grad_norm": 0.5255980491638184,
      "learning_rate": 0.00041203703703703704,
      "loss": 6.6982,
      "step": 1179
    },
    {
      "epoch": 0.3525281947867653,
      "grad_norm": 0.5178106427192688,
      "learning_rate": 0.0004119623655913979,
      "loss": 6.7852,
      "step": 1180
    },
    {
      "epoch": 0.3528269474942117,
      "grad_norm": 0.40459030866622925,
      "learning_rate": 0.00041188769414575866,
      "loss": 7.667,
      "step": 1181
    },
    {
      "epoch": 0.35312570020165807,
      "grad_norm": 0.41773372888565063,
      "learning_rate": 0.0004118130227001195,
      "loss": 7.2959,
      "step": 1182
    },
    {
      "epoch": 0.3534244529091045,
      "grad_norm": 0.5378082990646362,
      "learning_rate": 0.0004117383512544803,
      "loss": 6.7588,
      "step": 1183
    },
    {
      "epoch": 0.3537232056165509,
      "grad_norm": 0.4928402900695801,
      "learning_rate": 0.0004116636798088411,
      "loss": 6.959,
      "step": 1184
    },
    {
      "epoch": 0.3540219583239973,
      "grad_norm": 0.42796409130096436,
      "learning_rate": 0.0004115890083632019,
      "loss": 7.1572,
      "step": 1185
    },
    {
      "epoch": 0.35432071103144375,
      "grad_norm": 0.476517915725708,
      "learning_rate": 0.00041151433691756273,
      "loss": 7.2139,
      "step": 1186
    },
    {
      "epoch": 0.35461946373889014,
      "grad_norm": 0.37012097239494324,
      "learning_rate": 0.00041143966547192354,
      "loss": 7.249,
      "step": 1187
    },
    {
      "epoch": 0.35491821644633653,
      "grad_norm": 0.46436527371406555,
      "learning_rate": 0.00041136499402628435,
      "loss": 7.2373,
      "step": 1188
    },
    {
      "epoch": 0.355216969153783,
      "grad_norm": 0.4384196698665619,
      "learning_rate": 0.00041129032258064517,
      "loss": 7.1592,
      "step": 1189
    },
    {
      "epoch": 0.3555157218612294,
      "grad_norm": 0.38760754466056824,
      "learning_rate": 0.000411215651135006,
      "loss": 7.5557,
      "step": 1190
    },
    {
      "epoch": 0.35581447456867576,
      "grad_norm": 0.541159451007843,
      "learning_rate": 0.0004111409796893668,
      "loss": 7.3984,
      "step": 1191
    },
    {
      "epoch": 0.3561132272761222,
      "grad_norm": 0.42061081528663635,
      "learning_rate": 0.0004110663082437276,
      "loss": 6.8076,
      "step": 1192
    },
    {
      "epoch": 0.3564119799835686,
      "grad_norm": 0.43756312131881714,
      "learning_rate": 0.0004109916367980884,
      "loss": 7.3623,
      "step": 1193
    },
    {
      "epoch": 0.356710732691015,
      "grad_norm": 0.42711007595062256,
      "learning_rate": 0.00041091696535244923,
      "loss": 7.501,
      "step": 1194
    },
    {
      "epoch": 0.35700948539846145,
      "grad_norm": 0.4982943832874298,
      "learning_rate": 0.00041084229390681004,
      "loss": 7.0596,
      "step": 1195
    },
    {
      "epoch": 0.35730823810590784,
      "grad_norm": 0.43600234389305115,
      "learning_rate": 0.00041076762246117086,
      "loss": 7.4717,
      "step": 1196
    },
    {
      "epoch": 0.35760699081335423,
      "grad_norm": 0.46614736318588257,
      "learning_rate": 0.00041069295101553167,
      "loss": 7.1045,
      "step": 1197
    },
    {
      "epoch": 0.3579057435208007,
      "grad_norm": 0.3765042722225189,
      "learning_rate": 0.0004106182795698925,
      "loss": 7.626,
      "step": 1198
    },
    {
      "epoch": 0.35820449622824707,
      "grad_norm": 0.342729777097702,
      "learning_rate": 0.0004105436081242533,
      "loss": 7.7578,
      "step": 1199
    },
    {
      "epoch": 0.35850324893569346,
      "grad_norm": 0.396738201379776,
      "learning_rate": 0.0004104689366786141,
      "loss": 7.6025,
      "step": 1200
    },
    {
      "epoch": 0.35850324893569346,
      "eval_bleu": 0.09957964562434878,
      "eval_loss": 7.05859375,
      "eval_runtime": 536.1864,
      "eval_samples_per_second": 2.628,
      "eval_steps_per_second": 0.166,
      "step": 1200
    },
    {
      "epoch": 0.3588020016431399,
      "grad_norm": 0.4535461366176605,
      "learning_rate": 0.0004103942652329749,
      "loss": 7.0723,
      "step": 1201
    },
    {
      "epoch": 0.3591007543505863,
      "grad_norm": 0.4690917730331421,
      "learning_rate": 0.00041031959378733573,
      "loss": 7.0166,
      "step": 1202
    },
    {
      "epoch": 0.3593995070580327,
      "grad_norm": 0.46025314927101135,
      "learning_rate": 0.00041024492234169655,
      "loss": 7.0312,
      "step": 1203
    },
    {
      "epoch": 0.35969825976547914,
      "grad_norm": 0.5319485068321228,
      "learning_rate": 0.00041017025089605736,
      "loss": 7.1094,
      "step": 1204
    },
    {
      "epoch": 0.35999701247292554,
      "grad_norm": 0.4770788252353668,
      "learning_rate": 0.0004100955794504181,
      "loss": 6.793,
      "step": 1205
    },
    {
      "epoch": 0.36029576518037193,
      "grad_norm": 0.45234036445617676,
      "learning_rate": 0.000410020908004779,
      "loss": 7.0449,
      "step": 1206
    },
    {
      "epoch": 0.3605945178878184,
      "grad_norm": 0.462645024061203,
      "learning_rate": 0.0004099462365591398,
      "loss": 7.083,
      "step": 1207
    },
    {
      "epoch": 0.36089327059526477,
      "grad_norm": 0.34814587235450745,
      "learning_rate": 0.0004098715651135006,
      "loss": 7.6074,
      "step": 1208
    },
    {
      "epoch": 0.36119202330271116,
      "grad_norm": 0.4286554455757141,
      "learning_rate": 0.0004097968936678614,
      "loss": 6.9717,
      "step": 1209
    },
    {
      "epoch": 0.3614907760101576,
      "grad_norm": 0.46662402153015137,
      "learning_rate": 0.00040972222222222224,
      "loss": 6.9395,
      "step": 1210
    },
    {
      "epoch": 0.361789528717604,
      "grad_norm": 0.3659725785255432,
      "learning_rate": 0.00040964755077658305,
      "loss": 7.6621,
      "step": 1211
    },
    {
      "epoch": 0.3620882814250504,
      "grad_norm": 0.38391178846359253,
      "learning_rate": 0.00040957287933094386,
      "loss": 7.5254,
      "step": 1212
    },
    {
      "epoch": 0.36238703413249684,
      "grad_norm": 0.36423176527023315,
      "learning_rate": 0.0004094982078853047,
      "loss": 7.4727,
      "step": 1213
    },
    {
      "epoch": 0.36268578683994324,
      "grad_norm": 0.470901757478714,
      "learning_rate": 0.00040942353643966543,
      "loss": 6.7119,
      "step": 1214
    },
    {
      "epoch": 0.36298453954738963,
      "grad_norm": 0.418977290391922,
      "learning_rate": 0.0004093488649940263,
      "loss": 7.1006,
      "step": 1215
    },
    {
      "epoch": 0.3632832922548361,
      "grad_norm": 0.35365229845046997,
      "learning_rate": 0.0004092741935483871,
      "loss": 7.6094,
      "step": 1216
    },
    {
      "epoch": 0.36358204496228247,
      "grad_norm": 0.42185428738594055,
      "learning_rate": 0.0004091995221027479,
      "loss": 7.1006,
      "step": 1217
    },
    {
      "epoch": 0.36388079766972886,
      "grad_norm": 0.42102837562561035,
      "learning_rate": 0.00040912485065710874,
      "loss": 7.0137,
      "step": 1218
    },
    {
      "epoch": 0.3641795503771753,
      "grad_norm": 0.40738722681999207,
      "learning_rate": 0.00040905017921146955,
      "loss": 7.3848,
      "step": 1219
    },
    {
      "epoch": 0.3644783030846217,
      "grad_norm": 0.3942583203315735,
      "learning_rate": 0.00040897550776583037,
      "loss": 7.1016,
      "step": 1220
    },
    {
      "epoch": 0.3647770557920681,
      "grad_norm": 0.5242077112197876,
      "learning_rate": 0.0004089008363201911,
      "loss": 6.1992,
      "step": 1221
    },
    {
      "epoch": 0.36507580849951454,
      "grad_norm": 0.39056196808815,
      "learning_rate": 0.000408826164874552,
      "loss": 7.3682,
      "step": 1222
    },
    {
      "epoch": 0.36537456120696093,
      "grad_norm": 0.4657384157180786,
      "learning_rate": 0.00040875149342891275,
      "loss": 7.4365,
      "step": 1223
    },
    {
      "epoch": 0.3656733139144073,
      "grad_norm": 0.3703705072402954,
      "learning_rate": 0.0004086768219832736,
      "loss": 7.0771,
      "step": 1224
    },
    {
      "epoch": 0.3659720666218538,
      "grad_norm": 0.3990827202796936,
      "learning_rate": 0.00040860215053763443,
      "loss": 7.6914,
      "step": 1225
    },
    {
      "epoch": 0.36627081932930017,
      "grad_norm": 0.4852645695209503,
      "learning_rate": 0.00040852747909199524,
      "loss": 7.3174,
      "step": 1226
    },
    {
      "epoch": 0.36656957203674656,
      "grad_norm": 0.3915609121322632,
      "learning_rate": 0.00040845280764635606,
      "loss": 7.168,
      "step": 1227
    },
    {
      "epoch": 0.366868324744193,
      "grad_norm": 0.3578599691390991,
      "learning_rate": 0.00040837813620071687,
      "loss": 7.3574,
      "step": 1228
    },
    {
      "epoch": 0.3671670774516394,
      "grad_norm": 0.33268851041793823,
      "learning_rate": 0.0004083034647550777,
      "loss": 7.8438,
      "step": 1229
    },
    {
      "epoch": 0.3674658301590858,
      "grad_norm": 0.5153006911277771,
      "learning_rate": 0.00040822879330943844,
      "loss": 6.4248,
      "step": 1230
    },
    {
      "epoch": 0.36776458286653224,
      "grad_norm": 0.4173300266265869,
      "learning_rate": 0.0004081541218637993,
      "loss": 7.5537,
      "step": 1231
    },
    {
      "epoch": 0.36806333557397863,
      "grad_norm": 0.7996116876602173,
      "learning_rate": 0.00040807945041816007,
      "loss": 7.0811,
      "step": 1232
    },
    {
      "epoch": 0.368362088281425,
      "grad_norm": 0.40460270643234253,
      "learning_rate": 0.00040800477897252093,
      "loss": 7.4697,
      "step": 1233
    },
    {
      "epoch": 0.3686608409888715,
      "grad_norm": 0.388881117105484,
      "learning_rate": 0.00040793010752688175,
      "loss": 7.5977,
      "step": 1234
    },
    {
      "epoch": 0.36895959369631787,
      "grad_norm": 0.5645081996917725,
      "learning_rate": 0.00040785543608124256,
      "loss": 6.6699,
      "step": 1235
    },
    {
      "epoch": 0.36925834640376426,
      "grad_norm": 0.3587989807128906,
      "learning_rate": 0.00040778076463560337,
      "loss": 7.5693,
      "step": 1236
    },
    {
      "epoch": 0.3695570991112107,
      "grad_norm": 0.4457625448703766,
      "learning_rate": 0.00040770609318996413,
      "loss": 7.1895,
      "step": 1237
    },
    {
      "epoch": 0.3698558518186571,
      "grad_norm": 0.39566147327423096,
      "learning_rate": 0.000407631421744325,
      "loss": 7.2549,
      "step": 1238
    },
    {
      "epoch": 0.3701546045261035,
      "grad_norm": 0.38304874300956726,
      "learning_rate": 0.00040755675029868576,
      "loss": 7.2744,
      "step": 1239
    },
    {
      "epoch": 0.37045335723354994,
      "grad_norm": 0.35393014550209045,
      "learning_rate": 0.0004074820788530466,
      "loss": 7.7246,
      "step": 1240
    },
    {
      "epoch": 0.37075210994099633,
      "grad_norm": 0.44617149233818054,
      "learning_rate": 0.0004074074074074074,
      "loss": 6.9346,
      "step": 1241
    },
    {
      "epoch": 0.3710508626484427,
      "grad_norm": 0.35063305497169495,
      "learning_rate": 0.00040733273596176825,
      "loss": 7.3965,
      "step": 1242
    },
    {
      "epoch": 0.3713496153558892,
      "grad_norm": 0.40441280603408813,
      "learning_rate": 0.00040725806451612906,
      "loss": 7.0293,
      "step": 1243
    },
    {
      "epoch": 0.37164836806333557,
      "grad_norm": 0.43735045194625854,
      "learning_rate": 0.0004071833930704899,
      "loss": 7.29,
      "step": 1244
    },
    {
      "epoch": 0.37194712077078196,
      "grad_norm": 0.39322859048843384,
      "learning_rate": 0.0004071087216248507,
      "loss": 7.4727,
      "step": 1245
    },
    {
      "epoch": 0.3722458734782284,
      "grad_norm": 0.3413228988647461,
      "learning_rate": 0.00040703405017921145,
      "loss": 7.8809,
      "step": 1246
    },
    {
      "epoch": 0.3725446261856748,
      "grad_norm": 0.44855207204818726,
      "learning_rate": 0.0004069593787335723,
      "loss": 6.8701,
      "step": 1247
    },
    {
      "epoch": 0.3728433788931212,
      "grad_norm": 0.37545856833457947,
      "learning_rate": 0.00040688470728793307,
      "loss": 7.6055,
      "step": 1248
    },
    {
      "epoch": 0.37314213160056764,
      "grad_norm": 0.5158642530441284,
      "learning_rate": 0.00040681003584229394,
      "loss": 6.8008,
      "step": 1249
    },
    {
      "epoch": 0.37344088430801403,
      "grad_norm": 0.46901288628578186,
      "learning_rate": 0.0004067353643966547,
      "loss": 7.6367,
      "step": 1250
    },
    {
      "epoch": 0.3737396370154605,
      "grad_norm": 0.4072977304458618,
      "learning_rate": 0.00040666069295101556,
      "loss": 7.374,
      "step": 1251
    },
    {
      "epoch": 0.37403838972290687,
      "grad_norm": 0.3788970410823822,
      "learning_rate": 0.0004065860215053764,
      "loss": 7.6416,
      "step": 1252
    },
    {
      "epoch": 0.37433714243035326,
      "grad_norm": 0.4254037141799927,
      "learning_rate": 0.00040651135005973714,
      "loss": 7.1738,
      "step": 1253
    },
    {
      "epoch": 0.3746358951377997,
      "grad_norm": 0.38370802998542786,
      "learning_rate": 0.000406436678614098,
      "loss": 7.2568,
      "step": 1254
    },
    {
      "epoch": 0.3749346478452461,
      "grad_norm": 0.5000751614570618,
      "learning_rate": 0.00040636200716845876,
      "loss": 6.7842,
      "step": 1255
    },
    {
      "epoch": 0.3752334005526925,
      "grad_norm": 0.42839884757995605,
      "learning_rate": 0.00040628733572281963,
      "loss": 6.9023,
      "step": 1256
    },
    {
      "epoch": 0.37553215326013895,
      "grad_norm": 0.4234052300453186,
      "learning_rate": 0.0004062126642771804,
      "loss": 7.2412,
      "step": 1257
    },
    {
      "epoch": 0.37583090596758534,
      "grad_norm": 0.44725269079208374,
      "learning_rate": 0.00040613799283154125,
      "loss": 6.7627,
      "step": 1258
    },
    {
      "epoch": 0.37612965867503173,
      "grad_norm": 0.3868129551410675,
      "learning_rate": 0.000406063321385902,
      "loss": 7.1943,
      "step": 1259
    },
    {
      "epoch": 0.3764284113824782,
      "grad_norm": 0.4219895899295807,
      "learning_rate": 0.0004059886499402629,
      "loss": 7.0674,
      "step": 1260
    },
    {
      "epoch": 0.37672716408992457,
      "grad_norm": 0.5258481502532959,
      "learning_rate": 0.00040591397849462364,
      "loss": 6.6855,
      "step": 1261
    },
    {
      "epoch": 0.37702591679737096,
      "grad_norm": 0.4593670666217804,
      "learning_rate": 0.00040583930704898445,
      "loss": 7.4365,
      "step": 1262
    },
    {
      "epoch": 0.3773246695048174,
      "grad_norm": 0.44170162081718445,
      "learning_rate": 0.0004057646356033453,
      "loss": 7.1719,
      "step": 1263
    },
    {
      "epoch": 0.3776234222122638,
      "grad_norm": 0.4755409359931946,
      "learning_rate": 0.0004056899641577061,
      "loss": 6.7842,
      "step": 1264
    },
    {
      "epoch": 0.3779221749197102,
      "grad_norm": 0.33215969800949097,
      "learning_rate": 0.00040561529271206695,
      "loss": 7.8789,
      "step": 1265
    },
    {
      "epoch": 0.37822092762715664,
      "grad_norm": 0.5002089738845825,
      "learning_rate": 0.0004055406212664277,
      "loss": 6.9922,
      "step": 1266
    },
    {
      "epoch": 0.37851968033460304,
      "grad_norm": 0.45952701568603516,
      "learning_rate": 0.00040546594982078857,
      "loss": 7.376,
      "step": 1267
    },
    {
      "epoch": 0.37881843304204943,
      "grad_norm": 0.422726571559906,
      "learning_rate": 0.00040539127837514933,
      "loss": 7.1562,
      "step": 1268
    },
    {
      "epoch": 0.3791171857494959,
      "grad_norm": 0.3700873553752899,
      "learning_rate": 0.00040531660692951014,
      "loss": 7.4639,
      "step": 1269
    },
    {
      "epoch": 0.37941593845694227,
      "grad_norm": 0.47478920221328735,
      "learning_rate": 0.00040524193548387096,
      "loss": 7.1934,
      "step": 1270
    },
    {
      "epoch": 0.37971469116438866,
      "grad_norm": 0.4316105842590332,
      "learning_rate": 0.00040516726403823177,
      "loss": 6.8086,
      "step": 1271
    },
    {
      "epoch": 0.3800134438718351,
      "grad_norm": 0.37591585516929626,
      "learning_rate": 0.00040509259259259264,
      "loss": 7.1211,
      "step": 1272
    },
    {
      "epoch": 0.3803121965792815,
      "grad_norm": 0.4020209312438965,
      "learning_rate": 0.0004050179211469534,
      "loss": 7.3271,
      "step": 1273
    },
    {
      "epoch": 0.3806109492867279,
      "grad_norm": 0.46013426780700684,
      "learning_rate": 0.00040494324970131426,
      "loss": 6.7715,
      "step": 1274
    },
    {
      "epoch": 0.38090970199417434,
      "grad_norm": 0.41175949573516846,
      "learning_rate": 0.000404868578255675,
      "loss": 7.2119,
      "step": 1275
    },
    {
      "epoch": 0.38120845470162074,
      "grad_norm": 0.44741788506507874,
      "learning_rate": 0.0004047939068100359,
      "loss": 6.5898,
      "step": 1276
    },
    {
      "epoch": 0.38150720740906713,
      "grad_norm": 0.5059889554977417,
      "learning_rate": 0.00040471923536439665,
      "loss": 6.2715,
      "step": 1277
    },
    {
      "epoch": 0.3818059601165136,
      "grad_norm": 0.3805628716945648,
      "learning_rate": 0.00040464456391875746,
      "loss": 7.3359,
      "step": 1278
    },
    {
      "epoch": 0.38210471282395997,
      "grad_norm": 0.3913379907608032,
      "learning_rate": 0.00040456989247311827,
      "loss": 7.4951,
      "step": 1279
    },
    {
      "epoch": 0.38240346553140636,
      "grad_norm": 0.50736403465271,
      "learning_rate": 0.0004044952210274791,
      "loss": 6.7881,
      "step": 1280
    },
    {
      "epoch": 0.3827022182388528,
      "grad_norm": 0.4632420241832733,
      "learning_rate": 0.00040442054958183995,
      "loss": 7.4248,
      "step": 1281
    },
    {
      "epoch": 0.3830009709462992,
      "grad_norm": 0.509141743183136,
      "learning_rate": 0.0004043458781362007,
      "loss": 7.2266,
      "step": 1282
    },
    {
      "epoch": 0.3832997236537456,
      "grad_norm": 0.4201907217502594,
      "learning_rate": 0.0004042712066905616,
      "loss": 7.2373,
      "step": 1283
    },
    {
      "epoch": 0.38359847636119204,
      "grad_norm": 0.4298637807369232,
      "learning_rate": 0.00040419653524492234,
      "loss": 7.2402,
      "step": 1284
    },
    {
      "epoch": 0.38389722906863843,
      "grad_norm": 0.4859130382537842,
      "learning_rate": 0.00040412186379928315,
      "loss": 6.8516,
      "step": 1285
    },
    {
      "epoch": 0.3841959817760848,
      "grad_norm": 0.45485153794288635,
      "learning_rate": 0.00040404719235364396,
      "loss": 7.5576,
      "step": 1286
    },
    {
      "epoch": 0.3844947344835313,
      "grad_norm": 0.5605916380882263,
      "learning_rate": 0.0004039725209080048,
      "loss": 7.2344,
      "step": 1287
    },
    {
      "epoch": 0.38479348719097767,
      "grad_norm": 0.3534426987171173,
      "learning_rate": 0.0004038978494623656,
      "loss": 7.5381,
      "step": 1288
    },
    {
      "epoch": 0.38509223989842406,
      "grad_norm": 0.3818137049674988,
      "learning_rate": 0.0004038231780167264,
      "loss": 7.3857,
      "step": 1289
    },
    {
      "epoch": 0.3853909926058705,
      "grad_norm": 0.45477986335754395,
      "learning_rate": 0.00040374850657108727,
      "loss": 6.9775,
      "step": 1290
    },
    {
      "epoch": 0.3856897453133169,
      "grad_norm": 0.49868372082710266,
      "learning_rate": 0.000403673835125448,
      "loss": 7.1729,
      "step": 1291
    },
    {
      "epoch": 0.3859884980207633,
      "grad_norm": 0.4422799050807953,
      "learning_rate": 0.0004035991636798089,
      "loss": 6.876,
      "step": 1292
    },
    {
      "epoch": 0.38628725072820974,
      "grad_norm": 0.4107644855976105,
      "learning_rate": 0.00040352449223416965,
      "loss": 6.8789,
      "step": 1293
    },
    {
      "epoch": 0.38658600343565613,
      "grad_norm": 0.3871323764324188,
      "learning_rate": 0.00040344982078853046,
      "loss": 7.4941,
      "step": 1294
    },
    {
      "epoch": 0.3868847561431025,
      "grad_norm": 0.38297879695892334,
      "learning_rate": 0.0004033751493428913,
      "loss": 7.5391,
      "step": 1295
    },
    {
      "epoch": 0.387183508850549,
      "grad_norm": 0.4868331849575043,
      "learning_rate": 0.0004033004778972521,
      "loss": 6.5713,
      "step": 1296
    },
    {
      "epoch": 0.38748226155799537,
      "grad_norm": 0.4602886438369751,
      "learning_rate": 0.0004032258064516129,
      "loss": 7.2021,
      "step": 1297
    },
    {
      "epoch": 0.38778101426544176,
      "grad_norm": 0.4704173505306244,
      "learning_rate": 0.0004031511350059737,
      "loss": 7.1123,
      "step": 1298
    },
    {
      "epoch": 0.3880797669728882,
      "grad_norm": 0.42659884691238403,
      "learning_rate": 0.0004030764635603346,
      "loss": 7.3271,
      "step": 1299
    },
    {
      "epoch": 0.3883785196803346,
      "grad_norm": 0.39889708161354065,
      "learning_rate": 0.00040300179211469534,
      "loss": 7.4795,
      "step": 1300
    },
    {
      "epoch": 0.388677272387781,
      "grad_norm": 0.32731691002845764,
      "learning_rate": 0.00040292712066905615,
      "loss": 7.959,
      "step": 1301
    },
    {
      "epoch": 0.38897602509522744,
      "grad_norm": 0.4377132058143616,
      "learning_rate": 0.00040285244922341697,
      "loss": 7.1455,
      "step": 1302
    },
    {
      "epoch": 0.38927477780267383,
      "grad_norm": 0.5914089679718018,
      "learning_rate": 0.0004027777777777778,
      "loss": 7.3555,
      "step": 1303
    },
    {
      "epoch": 0.3895735305101202,
      "grad_norm": 0.4718615412712097,
      "learning_rate": 0.0004027031063321386,
      "loss": 7.1504,
      "step": 1304
    },
    {
      "epoch": 0.3898722832175667,
      "grad_norm": 0.3611397445201874,
      "learning_rate": 0.0004026284348864994,
      "loss": 7.8145,
      "step": 1305
    },
    {
      "epoch": 0.39017103592501307,
      "grad_norm": 0.4058666229248047,
      "learning_rate": 0.0004025537634408602,
      "loss": 7.3154,
      "step": 1306
    },
    {
      "epoch": 0.39046978863245946,
      "grad_norm": 0.3440166711807251,
      "learning_rate": 0.00040247909199522103,
      "loss": 7.8584,
      "step": 1307
    },
    {
      "epoch": 0.3907685413399059,
      "grad_norm": 0.47709521651268005,
      "learning_rate": 0.0004024044205495819,
      "loss": 7.248,
      "step": 1308
    },
    {
      "epoch": 0.3910672940473523,
      "grad_norm": 0.3559253513813019,
      "learning_rate": 0.00040232974910394266,
      "loss": 7.7441,
      "step": 1309
    },
    {
      "epoch": 0.3913660467547987,
      "grad_norm": 0.38514524698257446,
      "learning_rate": 0.00040225507765830347,
      "loss": 7.1475,
      "step": 1310
    },
    {
      "epoch": 0.39166479946224514,
      "grad_norm": 0.3566279113292694,
      "learning_rate": 0.0004021804062126643,
      "loss": 7.3672,
      "step": 1311
    },
    {
      "epoch": 0.39196355216969153,
      "grad_norm": 0.4191759526729584,
      "learning_rate": 0.0004021057347670251,
      "loss": 7.3203,
      "step": 1312
    },
    {
      "epoch": 0.3922623048771379,
      "grad_norm": 0.4021904170513153,
      "learning_rate": 0.0004020310633213859,
      "loss": 7.4287,
      "step": 1313
    },
    {
      "epoch": 0.39256105758458437,
      "grad_norm": 0.4226718842983246,
      "learning_rate": 0.0004019563918757467,
      "loss": 7.0361,
      "step": 1314
    },
    {
      "epoch": 0.39285981029203076,
      "grad_norm": 0.39375734329223633,
      "learning_rate": 0.00040188172043010753,
      "loss": 7.5264,
      "step": 1315
    },
    {
      "epoch": 0.39315856299947716,
      "grad_norm": 0.3949388861656189,
      "learning_rate": 0.00040180704898446835,
      "loss": 7.3955,
      "step": 1316
    },
    {
      "epoch": 0.3934573157069236,
      "grad_norm": 0.37904563546180725,
      "learning_rate": 0.00040173237753882916,
      "loss": 7.377,
      "step": 1317
    },
    {
      "epoch": 0.39375606841437,
      "grad_norm": 0.45754674077033997,
      "learning_rate": 0.00040165770609319,
      "loss": 6.9434,
      "step": 1318
    },
    {
      "epoch": 0.3940548211218164,
      "grad_norm": 0.35402339696884155,
      "learning_rate": 0.0004015830346475508,
      "loss": 7.3818,
      "step": 1319
    },
    {
      "epoch": 0.39435357382926284,
      "grad_norm": 0.4285252094268799,
      "learning_rate": 0.0004015083632019116,
      "loss": 7.3291,
      "step": 1320
    },
    {
      "epoch": 0.39465232653670923,
      "grad_norm": 0.4927235543727875,
      "learning_rate": 0.0004014336917562724,
      "loss": 6.7354,
      "step": 1321
    },
    {
      "epoch": 0.3949510792441556,
      "grad_norm": 0.44210943579673767,
      "learning_rate": 0.0004013590203106332,
      "loss": 7.3525,
      "step": 1322
    },
    {
      "epoch": 0.39524983195160207,
      "grad_norm": 0.3810325562953949,
      "learning_rate": 0.00040128434886499404,
      "loss": 7.5098,
      "step": 1323
    },
    {
      "epoch": 0.39554858465904846,
      "grad_norm": 0.555059015750885,
      "learning_rate": 0.00040120967741935485,
      "loss": 6.1357,
      "step": 1324
    },
    {
      "epoch": 0.3958473373664949,
      "grad_norm": 0.32264047861099243,
      "learning_rate": 0.00040113500597371566,
      "loss": 7.9697,
      "step": 1325
    },
    {
      "epoch": 0.3961460900739413,
      "grad_norm": 0.47967639565467834,
      "learning_rate": 0.0004010603345280765,
      "loss": 7.2236,
      "step": 1326
    },
    {
      "epoch": 0.3964448427813877,
      "grad_norm": 0.3806140720844269,
      "learning_rate": 0.0004009856630824373,
      "loss": 7.3457,
      "step": 1327
    },
    {
      "epoch": 0.39674359548883414,
      "grad_norm": 0.39954668283462524,
      "learning_rate": 0.0004009109916367981,
      "loss": 7.5098,
      "step": 1328
    },
    {
      "epoch": 0.39704234819628054,
      "grad_norm": 0.561843991279602,
      "learning_rate": 0.0004008363201911589,
      "loss": 6.3154,
      "step": 1329
    },
    {
      "epoch": 0.39734110090372693,
      "grad_norm": 0.4274856448173523,
      "learning_rate": 0.00040076164874551973,
      "loss": 7.0205,
      "step": 1330
    },
    {
      "epoch": 0.3976398536111734,
      "grad_norm": 0.4248376488685608,
      "learning_rate": 0.00040068697729988054,
      "loss": 7.1123,
      "step": 1331
    },
    {
      "epoch": 0.39793860631861977,
      "grad_norm": 0.326907217502594,
      "learning_rate": 0.00040061230585424135,
      "loss": 7.7168,
      "step": 1332
    },
    {
      "epoch": 0.39823735902606616,
      "grad_norm": 0.4475764036178589,
      "learning_rate": 0.0004005376344086021,
      "loss": 7.3125,
      "step": 1333
    },
    {
      "epoch": 0.3985361117335126,
      "grad_norm": 0.46090999245643616,
      "learning_rate": 0.000400462962962963,
      "loss": 7.0557,
      "step": 1334
    },
    {
      "epoch": 0.398834864440959,
      "grad_norm": 0.4872890114784241,
      "learning_rate": 0.0004003882915173238,
      "loss": 6.9736,
      "step": 1335
    },
    {
      "epoch": 0.3991336171484054,
      "grad_norm": 0.5997264981269836,
      "learning_rate": 0.0004003136200716846,
      "loss": 6.5508,
      "step": 1336
    },
    {
      "epoch": 0.39943236985585184,
      "grad_norm": 0.4190281629562378,
      "learning_rate": 0.0004002389486260454,
      "loss": 7.0723,
      "step": 1337
    },
    {
      "epoch": 0.39973112256329824,
      "grad_norm": 0.6724498271942139,
      "learning_rate": 0.00040016427718040623,
      "loss": 6.4873,
      "step": 1338
    },
    {
      "epoch": 0.40002987527074463,
      "grad_norm": 0.37512078881263733,
      "learning_rate": 0.00040008960573476704,
      "loss": 7.749,
      "step": 1339
    },
    {
      "epoch": 0.4003286279781911,
      "grad_norm": 0.43471014499664307,
      "learning_rate": 0.00040001493428912786,
      "loss": 6.959,
      "step": 1340
    },
    {
      "epoch": 0.40062738068563747,
      "grad_norm": 0.35706111788749695,
      "learning_rate": 0.00039994026284348867,
      "loss": 7.3916,
      "step": 1341
    },
    {
      "epoch": 0.40092613339308386,
      "grad_norm": 0.46811357140541077,
      "learning_rate": 0.00039986559139784943,
      "loss": 7.1846,
      "step": 1342
    },
    {
      "epoch": 0.4012248861005303,
      "grad_norm": 0.46734628081321716,
      "learning_rate": 0.0003997909199522103,
      "loss": 7.0674,
      "step": 1343
    },
    {
      "epoch": 0.4015236388079767,
      "grad_norm": 0.4335964322090149,
      "learning_rate": 0.0003997162485065711,
      "loss": 7.3281,
      "step": 1344
    },
    {
      "epoch": 0.4018223915154231,
      "grad_norm": 0.3685634136199951,
      "learning_rate": 0.0003996415770609319,
      "loss": 7.3633,
      "step": 1345
    },
    {
      "epoch": 0.40212114422286954,
      "grad_norm": 0.4093611538410187,
      "learning_rate": 0.00039956690561529273,
      "loss": 7.0645,
      "step": 1346
    },
    {
      "epoch": 0.40241989693031593,
      "grad_norm": 0.4455241560935974,
      "learning_rate": 0.00039949223416965355,
      "loss": 7.25,
      "step": 1347
    },
    {
      "epoch": 0.4027186496377623,
      "grad_norm": 0.3848409950733185,
      "learning_rate": 0.00039941756272401436,
      "loss": 7.4502,
      "step": 1348
    },
    {
      "epoch": 0.4030174023452088,
      "grad_norm": 0.5473566651344299,
      "learning_rate": 0.0003993428912783751,
      "loss": 6.9189,
      "step": 1349
    },
    {
      "epoch": 0.40331615505265517,
      "grad_norm": 0.5039893984794617,
      "learning_rate": 0.000399268219832736,
      "loss": 7.3633,
      "step": 1350
    },
    {
      "epoch": 0.40361490776010156,
      "grad_norm": 0.3977317810058594,
      "learning_rate": 0.00039919354838709674,
      "loss": 7.5449,
      "step": 1351
    },
    {
      "epoch": 0.403913660467548,
      "grad_norm": 0.40589627623558044,
      "learning_rate": 0.0003991188769414576,
      "loss": 7.1777,
      "step": 1352
    },
    {
      "epoch": 0.4042124131749944,
      "grad_norm": 0.4793681502342224,
      "learning_rate": 0.00039904420549581837,
      "loss": 6.998,
      "step": 1353
    },
    {
      "epoch": 0.4045111658824408,
      "grad_norm": 0.41409528255462646,
      "learning_rate": 0.00039896953405017924,
      "loss": 7.1094,
      "step": 1354
    },
    {
      "epoch": 0.40480991858988724,
      "grad_norm": 0.4545752704143524,
      "learning_rate": 0.00039889486260454005,
      "loss": 7.1152,
      "step": 1355
    },
    {
      "epoch": 0.40510867129733363,
      "grad_norm": 0.4737352430820465,
      "learning_rate": 0.00039882019115890086,
      "loss": 7.1484,
      "step": 1356
    },
    {
      "epoch": 0.40540742400478,
      "grad_norm": 0.4616076350212097,
      "learning_rate": 0.0003987455197132617,
      "loss": 7.1885,
      "step": 1357
    },
    {
      "epoch": 0.4057061767122265,
      "grad_norm": 0.5174769759178162,
      "learning_rate": 0.00039867084826762243,
      "loss": 7.3711,
      "step": 1358
    },
    {
      "epoch": 0.40600492941967287,
      "grad_norm": 0.9302103519439697,
      "learning_rate": 0.0003985961768219833,
      "loss": 7.5518,
      "step": 1359
    },
    {
      "epoch": 0.40630368212711926,
      "grad_norm": 0.41146960854530334,
      "learning_rate": 0.00039852150537634406,
      "loss": 7.0127,
      "step": 1360
    },
    {
      "epoch": 0.4066024348345657,
      "grad_norm": 0.48491713404655457,
      "learning_rate": 0.0003984468339307049,
      "loss": 6.6348,
      "step": 1361
    },
    {
      "epoch": 0.4069011875420121,
      "grad_norm": 0.4004208743572235,
      "learning_rate": 0.0003983721624850657,
      "loss": 7.2295,
      "step": 1362
    },
    {
      "epoch": 0.4071999402494585,
      "grad_norm": 0.4112577438354492,
      "learning_rate": 0.00039829749103942655,
      "loss": 7.2393,
      "step": 1363
    },
    {
      "epoch": 0.40749869295690494,
      "grad_norm": 0.44645434617996216,
      "learning_rate": 0.00039822281959378737,
      "loss": 7.0498,
      "step": 1364
    },
    {
      "epoch": 0.40779744566435133,
      "grad_norm": 0.34510186314582825,
      "learning_rate": 0.0003981481481481481,
      "loss": 7.6406,
      "step": 1365
    },
    {
      "epoch": 0.4080961983717977,
      "grad_norm": 0.3635798394680023,
      "learning_rate": 0.000398073476702509,
      "loss": 7.2959,
      "step": 1366
    },
    {
      "epoch": 0.4083949510792442,
      "grad_norm": 0.42905423045158386,
      "learning_rate": 0.00039799880525686975,
      "loss": 7.5518,
      "step": 1367
    },
    {
      "epoch": 0.40869370378669057,
      "grad_norm": 0.40726009011268616,
      "learning_rate": 0.0003979241338112306,
      "loss": 7.3691,
      "step": 1368
    },
    {
      "epoch": 0.40899245649413696,
      "grad_norm": 0.38802382349967957,
      "learning_rate": 0.0003978494623655914,
      "loss": 7.2246,
      "step": 1369
    },
    {
      "epoch": 0.4092912092015834,
      "grad_norm": 0.4276600480079651,
      "learning_rate": 0.00039777479091995224,
      "loss": 7.3037,
      "step": 1370
    },
    {
      "epoch": 0.4095899619090298,
      "grad_norm": 0.5946275591850281,
      "learning_rate": 0.000397700119474313,
      "loss": 7.0117,
      "step": 1371
    },
    {
      "epoch": 0.4098887146164762,
      "grad_norm": 0.42682212591171265,
      "learning_rate": 0.00039762544802867387,
      "loss": 7.4971,
      "step": 1372
    },
    {
      "epoch": 0.41018746732392264,
      "grad_norm": 0.5076514482498169,
      "learning_rate": 0.0003975507765830347,
      "loss": 6.5869,
      "step": 1373
    },
    {
      "epoch": 0.41048622003136903,
      "grad_norm": 0.4785946309566498,
      "learning_rate": 0.00039747610513739544,
      "loss": 6.8467,
      "step": 1374
    },
    {
      "epoch": 0.4107849727388154,
      "grad_norm": 0.45732253789901733,
      "learning_rate": 0.0003974014336917563,
      "loss": 7.2354,
      "step": 1375
    },
    {
      "epoch": 0.41108372544626187,
      "grad_norm": 0.3913795053958893,
      "learning_rate": 0.00039732676224611707,
      "loss": 7.4395,
      "step": 1376
    },
    {
      "epoch": 0.41138247815370826,
      "grad_norm": 0.4227626919746399,
      "learning_rate": 0.00039725209080047793,
      "loss": 7.4932,
      "step": 1377
    },
    {
      "epoch": 0.41168123086115466,
      "grad_norm": 0.4357970952987671,
      "learning_rate": 0.0003971774193548387,
      "loss": 6.9922,
      "step": 1378
    },
    {
      "epoch": 0.4119799835686011,
      "grad_norm": 0.48738133907318115,
      "learning_rate": 0.00039710274790919956,
      "loss": 7.2588,
      "step": 1379
    },
    {
      "epoch": 0.4122787362760475,
      "grad_norm": 0.37713372707366943,
      "learning_rate": 0.0003970280764635603,
      "loss": 7.3613,
      "step": 1380
    },
    {
      "epoch": 0.4125774889834939,
      "grad_norm": 0.34404146671295166,
      "learning_rate": 0.00039695340501792113,
      "loss": 7.5693,
      "step": 1381
    },
    {
      "epoch": 0.41287624169094034,
      "grad_norm": 0.372784823179245,
      "learning_rate": 0.000396878733572282,
      "loss": 7.4463,
      "step": 1382
    },
    {
      "epoch": 0.41317499439838673,
      "grad_norm": 0.441914439201355,
      "learning_rate": 0.00039680406212664276,
      "loss": 7.4346,
      "step": 1383
    },
    {
      "epoch": 0.4134737471058331,
      "grad_norm": 0.5862779021263123,
      "learning_rate": 0.0003967293906810036,
      "loss": 6.3184,
      "step": 1384
    },
    {
      "epoch": 0.41377249981327957,
      "grad_norm": 0.5438957810401917,
      "learning_rate": 0.0003966547192353644,
      "loss": 6.5625,
      "step": 1385
    },
    {
      "epoch": 0.41407125252072596,
      "grad_norm": 0.4764567017555237,
      "learning_rate": 0.00039658004778972525,
      "loss": 7.0889,
      "step": 1386
    },
    {
      "epoch": 0.41437000522817236,
      "grad_norm": 0.43036162853240967,
      "learning_rate": 0.000396505376344086,
      "loss": 7.1484,
      "step": 1387
    },
    {
      "epoch": 0.4146687579356188,
      "grad_norm": 0.5068739652633667,
      "learning_rate": 0.0003964307048984469,
      "loss": 7.333,
      "step": 1388
    },
    {
      "epoch": 0.4149675106430652,
      "grad_norm": 0.42902594804763794,
      "learning_rate": 0.00039635603345280763,
      "loss": 7.0908,
      "step": 1389
    },
    {
      "epoch": 0.4152662633505116,
      "grad_norm": 0.5386912822723389,
      "learning_rate": 0.00039628136200716845,
      "loss": 6.9756,
      "step": 1390
    },
    {
      "epoch": 0.41556501605795804,
      "grad_norm": 0.4063040316104889,
      "learning_rate": 0.0003962066905615293,
      "loss": 6.9092,
      "step": 1391
    },
    {
      "epoch": 0.41586376876540443,
      "grad_norm": 0.4669172465801239,
      "learning_rate": 0.00039613201911589007,
      "loss": 6.8789,
      "step": 1392
    },
    {
      "epoch": 0.4161625214728508,
      "grad_norm": 0.45136263966560364,
      "learning_rate": 0.00039605734767025094,
      "loss": 7.2637,
      "step": 1393
    },
    {
      "epoch": 0.41646127418029727,
      "grad_norm": 0.4837387502193451,
      "learning_rate": 0.0003959826762246117,
      "loss": 7.1143,
      "step": 1394
    },
    {
      "epoch": 0.41676002688774366,
      "grad_norm": 0.5211097002029419,
      "learning_rate": 0.00039590800477897256,
      "loss": 7.1338,
      "step": 1395
    },
    {
      "epoch": 0.41705877959519005,
      "grad_norm": 0.48205679655075073,
      "learning_rate": 0.0003958333333333333,
      "loss": 7.2842,
      "step": 1396
    },
    {
      "epoch": 0.4173575323026365,
      "grad_norm": 0.4614178240299225,
      "learning_rate": 0.00039575866188769414,
      "loss": 7.251,
      "step": 1397
    },
    {
      "epoch": 0.4176562850100829,
      "grad_norm": 0.5243740677833557,
      "learning_rate": 0.00039568399044205495,
      "loss": 6.6172,
      "step": 1398
    },
    {
      "epoch": 0.4179550377175293,
      "grad_norm": 0.5515962243080139,
      "learning_rate": 0.00039560931899641576,
      "loss": 7.1748,
      "step": 1399
    },
    {
      "epoch": 0.41825379042497574,
      "grad_norm": 0.3449063301086426,
      "learning_rate": 0.00039553464755077663,
      "loss": 7.7109,
      "step": 1400
    },
    {
      "epoch": 0.41825379042497574,
      "eval_bleu": 0.10192604878975882,
      "eval_loss": 7.0625,
      "eval_runtime": 580.6345,
      "eval_samples_per_second": 2.427,
      "eval_steps_per_second": 0.153,
      "step": 1400
    },
    {
      "epoch": 0.41855254313242213,
      "grad_norm": 0.29618170857429504,
      "learning_rate": 0.0003954599761051374,
      "loss": 7.9424,
      "step": 1401
    },
    {
      "epoch": 0.4188512958398686,
      "grad_norm": 0.47894224524497986,
      "learning_rate": 0.00039538530465949825,
      "loss": 7.1123,
      "step": 1402
    },
    {
      "epoch": 0.41915004854731497,
      "grad_norm": 0.5301428437232971,
      "learning_rate": 0.000395310633213859,
      "loss": 6.8223,
      "step": 1403
    },
    {
      "epoch": 0.41944880125476136,
      "grad_norm": 0.4700724482536316,
      "learning_rate": 0.0003952359617682199,
      "loss": 6.958,
      "step": 1404
    },
    {
      "epoch": 0.4197475539622078,
      "grad_norm": 0.38836222887039185,
      "learning_rate": 0.00039516129032258064,
      "loss": 7.3467,
      "step": 1405
    },
    {
      "epoch": 0.4200463066696542,
      "grad_norm": 0.484891414642334,
      "learning_rate": 0.00039508661887694145,
      "loss": 6.9355,
      "step": 1406
    },
    {
      "epoch": 0.4203450593771006,
      "grad_norm": 0.44520285725593567,
      "learning_rate": 0.00039501194743130227,
      "loss": 6.8867,
      "step": 1407
    },
    {
      "epoch": 0.42064381208454704,
      "grad_norm": 0.47022169828414917,
      "learning_rate": 0.0003949372759856631,
      "loss": 7.3936,
      "step": 1408
    },
    {
      "epoch": 0.42094256479199343,
      "grad_norm": 0.43711212277412415,
      "learning_rate": 0.00039486260454002395,
      "loss": 7.0361,
      "step": 1409
    },
    {
      "epoch": 0.4212413174994398,
      "grad_norm": 0.4393630623817444,
      "learning_rate": 0.0003947879330943847,
      "loss": 7.415,
      "step": 1410
    },
    {
      "epoch": 0.4215400702068863,
      "grad_norm": 0.4215008616447449,
      "learning_rate": 0.00039471326164874557,
      "loss": 7.2227,
      "step": 1411
    },
    {
      "epoch": 0.42183882291433267,
      "grad_norm": 0.4265718162059784,
      "learning_rate": 0.00039463859020310633,
      "loss": 7.1836,
      "step": 1412
    },
    {
      "epoch": 0.42213757562177906,
      "grad_norm": 0.3926502466201782,
      "learning_rate": 0.00039456391875746714,
      "loss": 7.3721,
      "step": 1413
    },
    {
      "epoch": 0.4224363283292255,
      "grad_norm": 0.4213932454586029,
      "learning_rate": 0.00039448924731182796,
      "loss": 7.1289,
      "step": 1414
    },
    {
      "epoch": 0.4227350810366719,
      "grad_norm": 0.4301188588142395,
      "learning_rate": 0.00039441457586618877,
      "loss": 7.4014,
      "step": 1415
    },
    {
      "epoch": 0.4230338337441183,
      "grad_norm": 0.371794730424881,
      "learning_rate": 0.0003943399044205496,
      "loss": 7.2754,
      "step": 1416
    },
    {
      "epoch": 0.42333258645156474,
      "grad_norm": 0.6214315891265869,
      "learning_rate": 0.0003942652329749104,
      "loss": 6.8594,
      "step": 1417
    },
    {
      "epoch": 0.42363133915901113,
      "grad_norm": 0.44963952898979187,
      "learning_rate": 0.00039419056152927126,
      "loss": 7.084,
      "step": 1418
    },
    {
      "epoch": 0.4239300918664575,
      "grad_norm": 0.4310494661331177,
      "learning_rate": 0.000394115890083632,
      "loss": 6.8223,
      "step": 1419
    },
    {
      "epoch": 0.424228844573904,
      "grad_norm": 0.47870320081710815,
      "learning_rate": 0.0003940412186379929,
      "loss": 6.9307,
      "step": 1420
    },
    {
      "epoch": 0.42452759728135037,
      "grad_norm": 0.4860629439353943,
      "learning_rate": 0.00039396654719235365,
      "loss": 7.1289,
      "step": 1421
    },
    {
      "epoch": 0.42482634998879676,
      "grad_norm": 0.5410910248756409,
      "learning_rate": 0.00039389187574671446,
      "loss": 6.7949,
      "step": 1422
    },
    {
      "epoch": 0.4251251026962432,
      "grad_norm": 0.4135812222957611,
      "learning_rate": 0.00039381720430107527,
      "loss": 7.3281,
      "step": 1423
    },
    {
      "epoch": 0.4254238554036896,
      "grad_norm": 0.38667285442352295,
      "learning_rate": 0.0003937425328554361,
      "loss": 7.6709,
      "step": 1424
    },
    {
      "epoch": 0.425722608111136,
      "grad_norm": 0.4448122978210449,
      "learning_rate": 0.0003936678614097969,
      "loss": 7.459,
      "step": 1425
    },
    {
      "epoch": 0.42602136081858244,
      "grad_norm": 0.4631573557853699,
      "learning_rate": 0.0003935931899641577,
      "loss": 7.1768,
      "step": 1426
    },
    {
      "epoch": 0.42632011352602883,
      "grad_norm": 0.41764986515045166,
      "learning_rate": 0.0003935185185185186,
      "loss": 7.3408,
      "step": 1427
    },
    {
      "epoch": 0.4266188662334752,
      "grad_norm": 0.4320964515209198,
      "learning_rate": 0.00039344384707287934,
      "loss": 7.3252,
      "step": 1428
    },
    {
      "epoch": 0.4269176189409217,
      "grad_norm": 0.415304958820343,
      "learning_rate": 0.00039336917562724015,
      "loss": 7.5439,
      "step": 1429
    },
    {
      "epoch": 0.42721637164836807,
      "grad_norm": 0.4885118007659912,
      "learning_rate": 0.00039329450418160096,
      "loss": 6.9395,
      "step": 1430
    },
    {
      "epoch": 0.42751512435581446,
      "grad_norm": 0.4191299378871918,
      "learning_rate": 0.0003932198327359618,
      "loss": 7.084,
      "step": 1431
    },
    {
      "epoch": 0.4278138770632609,
      "grad_norm": 0.5862101912498474,
      "learning_rate": 0.0003931451612903226,
      "loss": 6.6201,
      "step": 1432
    },
    {
      "epoch": 0.4281126297707073,
      "grad_norm": 0.41920557618141174,
      "learning_rate": 0.0003930704898446834,
      "loss": 7.5469,
      "step": 1433
    },
    {
      "epoch": 0.4284113824781537,
      "grad_norm": 0.41364428400993347,
      "learning_rate": 0.0003929958183990442,
      "loss": 7.4619,
      "step": 1434
    },
    {
      "epoch": 0.42871013518560014,
      "grad_norm": 0.5411252975463867,
      "learning_rate": 0.000392921146953405,
      "loss": 6.3047,
      "step": 1435
    },
    {
      "epoch": 0.42900888789304653,
      "grad_norm": 0.4260885417461395,
      "learning_rate": 0.0003928464755077659,
      "loss": 7.0703,
      "step": 1436
    },
    {
      "epoch": 0.4293076406004929,
      "grad_norm": 0.4201105833053589,
      "learning_rate": 0.00039277180406212665,
      "loss": 7.6582,
      "step": 1437
    },
    {
      "epoch": 0.42960639330793937,
      "grad_norm": 0.4451907277107239,
      "learning_rate": 0.00039269713261648746,
      "loss": 7.0469,
      "step": 1438
    },
    {
      "epoch": 0.42990514601538576,
      "grad_norm": 0.395119845867157,
      "learning_rate": 0.0003926224611708483,
      "loss": 7.3975,
      "step": 1439
    },
    {
      "epoch": 0.43020389872283216,
      "grad_norm": 0.4948483109474182,
      "learning_rate": 0.0003925477897252091,
      "loss": 6.8301,
      "step": 1440
    },
    {
      "epoch": 0.4305026514302786,
      "grad_norm": 0.7064464688301086,
      "learning_rate": 0.0003924731182795699,
      "loss": 6.5498,
      "step": 1441
    },
    {
      "epoch": 0.430801404137725,
      "grad_norm": 0.43144655227661133,
      "learning_rate": 0.0003923984468339307,
      "loss": 7.3545,
      "step": 1442
    },
    {
      "epoch": 0.4311001568451714,
      "grad_norm": 0.34789925813674927,
      "learning_rate": 0.00039232377538829153,
      "loss": 7.875,
      "step": 1443
    },
    {
      "epoch": 0.43139890955261784,
      "grad_norm": 0.5104612112045288,
      "learning_rate": 0.00039224910394265234,
      "loss": 6.6914,
      "step": 1444
    },
    {
      "epoch": 0.43169766226006423,
      "grad_norm": 0.4577682316303253,
      "learning_rate": 0.0003921744324970131,
      "loss": 7.124,
      "step": 1445
    },
    {
      "epoch": 0.4319964149675106,
      "grad_norm": 0.5320565700531006,
      "learning_rate": 0.00039209976105137397,
      "loss": 7.0469,
      "step": 1446
    },
    {
      "epoch": 0.43229516767495707,
      "grad_norm": 0.46176111698150635,
      "learning_rate": 0.0003920250896057348,
      "loss": 7.248,
      "step": 1447
    },
    {
      "epoch": 0.43259392038240346,
      "grad_norm": 0.4152930676937103,
      "learning_rate": 0.0003919504181600956,
      "loss": 7.4053,
      "step": 1448
    },
    {
      "epoch": 0.43289267308984986,
      "grad_norm": 0.4140941798686981,
      "learning_rate": 0.0003918757467144564,
      "loss": 7.0186,
      "step": 1449
    },
    {
      "epoch": 0.4331914257972963,
      "grad_norm": 0.41071006655693054,
      "learning_rate": 0.0003918010752688172,
      "loss": 7.0977,
      "step": 1450
    },
    {
      "epoch": 0.4334901785047427,
      "grad_norm": 0.48309236764907837,
      "learning_rate": 0.00039172640382317803,
      "loss": 7.3096,
      "step": 1451
    },
    {
      "epoch": 0.4337889312121891,
      "grad_norm": 0.5486384034156799,
      "learning_rate": 0.0003916517323775388,
      "loss": 6.6719,
      "step": 1452
    },
    {
      "epoch": 0.43408768391963554,
      "grad_norm": 0.36855727434158325,
      "learning_rate": 0.00039157706093189966,
      "loss": 7.8008,
      "step": 1453
    },
    {
      "epoch": 0.43438643662708193,
      "grad_norm": 0.39983585476875305,
      "learning_rate": 0.0003915023894862604,
      "loss": 7.6367,
      "step": 1454
    },
    {
      "epoch": 0.4346851893345283,
      "grad_norm": 0.4070429801940918,
      "learning_rate": 0.0003914277180406213,
      "loss": 6.916,
      "step": 1455
    },
    {
      "epoch": 0.43498394204197477,
      "grad_norm": 0.4203869700431824,
      "learning_rate": 0.0003913530465949821,
      "loss": 7.1865,
      "step": 1456
    },
    {
      "epoch": 0.43528269474942116,
      "grad_norm": 0.43473830819129944,
      "learning_rate": 0.0003912783751493429,
      "loss": 7.2969,
      "step": 1457
    },
    {
      "epoch": 0.43558144745686755,
      "grad_norm": 0.4258636236190796,
      "learning_rate": 0.0003912037037037037,
      "loss": 6.9053,
      "step": 1458
    },
    {
      "epoch": 0.435880200164314,
      "grad_norm": 0.356250524520874,
      "learning_rate": 0.00039112903225806453,
      "loss": 7.9023,
      "step": 1459
    },
    {
      "epoch": 0.4361789528717604,
      "grad_norm": 0.38025471568107605,
      "learning_rate": 0.00039105436081242535,
      "loss": 7.5059,
      "step": 1460
    },
    {
      "epoch": 0.4364777055792068,
      "grad_norm": 0.45830583572387695,
      "learning_rate": 0.0003909796893667861,
      "loss": 7.0801,
      "step": 1461
    },
    {
      "epoch": 0.43677645828665324,
      "grad_norm": 0.4040442407131195,
      "learning_rate": 0.000390905017921147,
      "loss": 7.582,
      "step": 1462
    },
    {
      "epoch": 0.4370752109940996,
      "grad_norm": 0.417721152305603,
      "learning_rate": 0.00039083034647550773,
      "loss": 7.2656,
      "step": 1463
    },
    {
      "epoch": 0.437373963701546,
      "grad_norm": 0.392659068107605,
      "learning_rate": 0.0003907556750298686,
      "loss": 7.0557,
      "step": 1464
    },
    {
      "epoch": 0.43767271640899247,
      "grad_norm": 0.4259890019893646,
      "learning_rate": 0.0003906810035842294,
      "loss": 6.9326,
      "step": 1465
    },
    {
      "epoch": 0.43797146911643886,
      "grad_norm": 0.4372883439064026,
      "learning_rate": 0.0003906063321385902,
      "loss": 7.4209,
      "step": 1466
    },
    {
      "epoch": 0.43827022182388525,
      "grad_norm": 0.42887693643569946,
      "learning_rate": 0.00039053166069295104,
      "loss": 7.2559,
      "step": 1467
    },
    {
      "epoch": 0.4385689745313317,
      "grad_norm": 0.5071214437484741,
      "learning_rate": 0.0003904569892473118,
      "loss": 6.96,
      "step": 1468
    },
    {
      "epoch": 0.4388677272387781,
      "grad_norm": 0.459905207157135,
      "learning_rate": 0.00039038231780167266,
      "loss": 7.1504,
      "step": 1469
    },
    {
      "epoch": 0.4391664799462245,
      "grad_norm": 0.3931758403778076,
      "learning_rate": 0.0003903076463560334,
      "loss": 7.4209,
      "step": 1470
    },
    {
      "epoch": 0.43946523265367093,
      "grad_norm": 0.4178850054740906,
      "learning_rate": 0.0003902329749103943,
      "loss": 7.335,
      "step": 1471
    },
    {
      "epoch": 0.4397639853611173,
      "grad_norm": 0.41972818970680237,
      "learning_rate": 0.00039015830346475505,
      "loss": 7.4014,
      "step": 1472
    },
    {
      "epoch": 0.4400627380685637,
      "grad_norm": 0.47802576422691345,
      "learning_rate": 0.0003900836320191159,
      "loss": 6.6436,
      "step": 1473
    },
    {
      "epoch": 0.44036149077601017,
      "grad_norm": 0.5412635207176208,
      "learning_rate": 0.00039000896057347673,
      "loss": 7.0889,
      "step": 1474
    },
    {
      "epoch": 0.44066024348345656,
      "grad_norm": 0.42846620082855225,
      "learning_rate": 0.00038993428912783754,
      "loss": 7.499,
      "step": 1475
    },
    {
      "epoch": 0.440958996190903,
      "grad_norm": 0.5721824765205383,
      "learning_rate": 0.00038985961768219835,
      "loss": 6.418,
      "step": 1476
    },
    {
      "epoch": 0.4412577488983494,
      "grad_norm": 0.4607917368412018,
      "learning_rate": 0.0003897849462365591,
      "loss": 7.1562,
      "step": 1477
    },
    {
      "epoch": 0.4415565016057958,
      "grad_norm": 0.4025934338569641,
      "learning_rate": 0.00038971027479092,
      "loss": 7.0049,
      "step": 1478
    },
    {
      "epoch": 0.44185525431324224,
      "grad_norm": 0.4372842013835907,
      "learning_rate": 0.00038963560334528074,
      "loss": 7.126,
      "step": 1479
    },
    {
      "epoch": 0.44215400702068863,
      "grad_norm": 0.3944029211997986,
      "learning_rate": 0.0003895609318996416,
      "loss": 7.0342,
      "step": 1480
    },
    {
      "epoch": 0.442452759728135,
      "grad_norm": 0.4079132676124573,
      "learning_rate": 0.00038948626045400236,
      "loss": 7.2969,
      "step": 1481
    },
    {
      "epoch": 0.4427515124355815,
      "grad_norm": 0.4577992856502533,
      "learning_rate": 0.00038941158900836323,
      "loss": 6.9082,
      "step": 1482
    },
    {
      "epoch": 0.44305026514302787,
      "grad_norm": 0.4519190788269043,
      "learning_rate": 0.00038933691756272404,
      "loss": 7.2207,
      "step": 1483
    },
    {
      "epoch": 0.44334901785047426,
      "grad_norm": 0.44804441928863525,
      "learning_rate": 0.0003892622461170848,
      "loss": 7.1738,
      "step": 1484
    },
    {
      "epoch": 0.4436477705579207,
      "grad_norm": 0.39885643124580383,
      "learning_rate": 0.00038918757467144567,
      "loss": 7.4561,
      "step": 1485
    },
    {
      "epoch": 0.4439465232653671,
      "grad_norm": 0.38609951734542847,
      "learning_rate": 0.00038911290322580643,
      "loss": 7.4102,
      "step": 1486
    },
    {
      "epoch": 0.4442452759728135,
      "grad_norm": 0.4150185286998749,
      "learning_rate": 0.0003890382317801673,
      "loss": 7.4277,
      "step": 1487
    },
    {
      "epoch": 0.44454402868025994,
      "grad_norm": 0.4368774890899658,
      "learning_rate": 0.00038896356033452805,
      "loss": 7.1982,
      "step": 1488
    },
    {
      "epoch": 0.44484278138770633,
      "grad_norm": 0.4504484236240387,
      "learning_rate": 0.0003888888888888889,
      "loss": 7.0254,
      "step": 1489
    },
    {
      "epoch": 0.4451415340951527,
      "grad_norm": 0.40636709332466125,
      "learning_rate": 0.0003888142174432497,
      "loss": 7.5166,
      "step": 1490
    },
    {
      "epoch": 0.4454402868025992,
      "grad_norm": 0.4728342890739441,
      "learning_rate": 0.00038873954599761055,
      "loss": 6.9961,
      "step": 1491
    },
    {
      "epoch": 0.44573903951004556,
      "grad_norm": 0.3699270486831665,
      "learning_rate": 0.00038866487455197136,
      "loss": 7.2422,
      "step": 1492
    },
    {
      "epoch": 0.44603779221749196,
      "grad_norm": 0.46431079506874084,
      "learning_rate": 0.0003885902031063321,
      "loss": 7.25,
      "step": 1493
    },
    {
      "epoch": 0.4463365449249384,
      "grad_norm": 0.46854496002197266,
      "learning_rate": 0.000388515531660693,
      "loss": 7.2285,
      "step": 1494
    },
    {
      "epoch": 0.4466352976323848,
      "grad_norm": 0.44297081232070923,
      "learning_rate": 0.00038844086021505374,
      "loss": 7.4873,
      "step": 1495
    },
    {
      "epoch": 0.4469340503398312,
      "grad_norm": 0.4125458300113678,
      "learning_rate": 0.0003883661887694146,
      "loss": 7.3418,
      "step": 1496
    },
    {
      "epoch": 0.44723280304727764,
      "grad_norm": 0.5128099918365479,
      "learning_rate": 0.00038829151732377537,
      "loss": 7.6797,
      "step": 1497
    },
    {
      "epoch": 0.44753155575472403,
      "grad_norm": 0.43388837575912476,
      "learning_rate": 0.00038821684587813624,
      "loss": 7.3955,
      "step": 1498
    },
    {
      "epoch": 0.4478303084621704,
      "grad_norm": 0.4634524881839752,
      "learning_rate": 0.000388142174432497,
      "loss": 6.6748,
      "step": 1499
    },
    {
      "epoch": 0.44812906116961687,
      "grad_norm": 0.39982128143310547,
      "learning_rate": 0.0003880675029868578,
      "loss": 7.7998,
      "step": 1500
    },
    {
      "epoch": 0.44842781387706326,
      "grad_norm": 0.5062081217765808,
      "learning_rate": 0.0003879928315412187,
      "loss": 6.8584,
      "step": 1501
    },
    {
      "epoch": 0.44872656658450966,
      "grad_norm": 0.47585707902908325,
      "learning_rate": 0.00038791816009557943,
      "loss": 7.0098,
      "step": 1502
    },
    {
      "epoch": 0.4490253192919561,
      "grad_norm": 1.420627236366272,
      "learning_rate": 0.0003878434886499403,
      "loss": 7.6436,
      "step": 1503
    },
    {
      "epoch": 0.4493240719994025,
      "grad_norm": 0.47976231575012207,
      "learning_rate": 0.00038776881720430106,
      "loss": 6.8135,
      "step": 1504
    },
    {
      "epoch": 0.4496228247068489,
      "grad_norm": 0.8474008440971375,
      "learning_rate": 0.00038769414575866193,
      "loss": 7.3418,
      "step": 1505
    },
    {
      "epoch": 0.44992157741429534,
      "grad_norm": 0.6583009958267212,
      "learning_rate": 0.0003876194743130227,
      "loss": 7.5088,
      "step": 1506
    },
    {
      "epoch": 0.45022033012174173,
      "grad_norm": 0.42941582202911377,
      "learning_rate": 0.00038754480286738355,
      "loss": 7.3857,
      "step": 1507
    },
    {
      "epoch": 0.4505190828291881,
      "grad_norm": 0.47223004698753357,
      "learning_rate": 0.0003874701314217443,
      "loss": 7.0225,
      "step": 1508
    },
    {
      "epoch": 0.45081783553663457,
      "grad_norm": 0.4547000527381897,
      "learning_rate": 0.0003873954599761051,
      "loss": 7.5049,
      "step": 1509
    },
    {
      "epoch": 0.45111658824408096,
      "grad_norm": 0.3728136420249939,
      "learning_rate": 0.000387320788530466,
      "loss": 7.5859,
      "step": 1510
    },
    {
      "epoch": 0.45141534095152736,
      "grad_norm": 0.5134193897247314,
      "learning_rate": 0.00038724611708482675,
      "loss": 7.3125,
      "step": 1511
    },
    {
      "epoch": 0.4517140936589738,
      "grad_norm": 0.4077126681804657,
      "learning_rate": 0.0003871714456391876,
      "loss": 6.9385,
      "step": 1512
    },
    {
      "epoch": 0.4520128463664202,
      "grad_norm": 0.44684311747550964,
      "learning_rate": 0.0003870967741935484,
      "loss": 6.9766,
      "step": 1513
    },
    {
      "epoch": 0.4523115990738666,
      "grad_norm": 0.36017173528671265,
      "learning_rate": 0.00038702210274790924,
      "loss": 7.4189,
      "step": 1514
    },
    {
      "epoch": 0.45261035178131304,
      "grad_norm": 0.4248254597187042,
      "learning_rate": 0.00038694743130227,
      "loss": 7.626,
      "step": 1515
    },
    {
      "epoch": 0.45290910448875943,
      "grad_norm": 0.48187077045440674,
      "learning_rate": 0.0003868727598566308,
      "loss": 7.1973,
      "step": 1516
    },
    {
      "epoch": 0.4532078571962058,
      "grad_norm": 2.935636281967163,
      "learning_rate": 0.00038679808841099163,
      "loss": 7.5361,
      "step": 1517
    },
    {
      "epoch": 0.45350660990365227,
      "grad_norm": 0.43481627106666565,
      "learning_rate": 0.00038672341696535244,
      "loss": 7.3496,
      "step": 1518
    },
    {
      "epoch": 0.45380536261109866,
      "grad_norm": 0.44335079193115234,
      "learning_rate": 0.0003866487455197133,
      "loss": 7.1279,
      "step": 1519
    },
    {
      "epoch": 0.45410411531854505,
      "grad_norm": 0.5495507717132568,
      "learning_rate": 0.00038657407407407407,
      "loss": 6.8936,
      "step": 1520
    },
    {
      "epoch": 0.4544028680259915,
      "grad_norm": 0.47012919187545776,
      "learning_rate": 0.00038649940262843493,
      "loss": 6.9209,
      "step": 1521
    },
    {
      "epoch": 0.4547016207334379,
      "grad_norm": 0.47491782903671265,
      "learning_rate": 0.0003864247311827957,
      "loss": 6.8428,
      "step": 1522
    },
    {
      "epoch": 0.4550003734408843,
      "grad_norm": 0.5752939581871033,
      "learning_rate": 0.00038635005973715656,
      "loss": 6.9219,
      "step": 1523
    },
    {
      "epoch": 0.45529912614833074,
      "grad_norm": 0.41006824374198914,
      "learning_rate": 0.0003862753882915173,
      "loss": 7.3184,
      "step": 1524
    },
    {
      "epoch": 0.4555978788557771,
      "grad_norm": 0.5327703356742859,
      "learning_rate": 0.00038620071684587813,
      "loss": 7.0781,
      "step": 1525
    },
    {
      "epoch": 0.4558966315632235,
      "grad_norm": 0.4031495153903961,
      "learning_rate": 0.00038612604540023894,
      "loss": 7.3115,
      "step": 1526
    },
    {
      "epoch": 0.45619538427066997,
      "grad_norm": 0.4971637427806854,
      "learning_rate": 0.00038605137395459976,
      "loss": 7.3564,
      "step": 1527
    },
    {
      "epoch": 0.45649413697811636,
      "grad_norm": 0.4081481397151947,
      "learning_rate": 0.0003859767025089606,
      "loss": 7.2217,
      "step": 1528
    },
    {
      "epoch": 0.45679288968556275,
      "grad_norm": 0.4533267915248871,
      "learning_rate": 0.0003859020310633214,
      "loss": 6.9922,
      "step": 1529
    },
    {
      "epoch": 0.4570916423930092,
      "grad_norm": 0.40585553646087646,
      "learning_rate": 0.00038582735961768225,
      "loss": 7.0586,
      "step": 1530
    },
    {
      "epoch": 0.4573903951004556,
      "grad_norm": 0.4111849069595337,
      "learning_rate": 0.000385752688172043,
      "loss": 7.4912,
      "step": 1531
    },
    {
      "epoch": 0.457689147807902,
      "grad_norm": 0.4559098482131958,
      "learning_rate": 0.0003856780167264038,
      "loss": 7.0,
      "step": 1532
    },
    {
      "epoch": 0.45798790051534843,
      "grad_norm": 0.46705034375190735,
      "learning_rate": 0.00038560334528076463,
      "loss": 6.8965,
      "step": 1533
    },
    {
      "epoch": 0.4582866532227948,
      "grad_norm": 0.375968337059021,
      "learning_rate": 0.00038552867383512545,
      "loss": 7.3066,
      "step": 1534
    },
    {
      "epoch": 0.4585854059302412,
      "grad_norm": 0.4177662432193756,
      "learning_rate": 0.00038545400238948626,
      "loss": 7.1875,
      "step": 1535
    },
    {
      "epoch": 0.45888415863768767,
      "grad_norm": 0.4532710015773773,
      "learning_rate": 0.00038537933094384707,
      "loss": 7.3604,
      "step": 1536
    },
    {
      "epoch": 0.45918291134513406,
      "grad_norm": 0.4226815104484558,
      "learning_rate": 0.0003853046594982079,
      "loss": 7.3857,
      "step": 1537
    },
    {
      "epoch": 0.45948166405258045,
      "grad_norm": 0.4782011806964874,
      "learning_rate": 0.0003852299880525687,
      "loss": 7.2686,
      "step": 1538
    },
    {
      "epoch": 0.4597804167600269,
      "grad_norm": 0.5404102802276611,
      "learning_rate": 0.00038515531660692956,
      "loss": 6.6885,
      "step": 1539
    },
    {
      "epoch": 0.4600791694674733,
      "grad_norm": 0.48159152269363403,
      "learning_rate": 0.0003850806451612903,
      "loss": 7.1201,
      "step": 1540
    },
    {
      "epoch": 0.4603779221749197,
      "grad_norm": 0.49900856614112854,
      "learning_rate": 0.00038500597371565114,
      "loss": 7.0068,
      "step": 1541
    },
    {
      "epoch": 0.46067667488236613,
      "grad_norm": 0.41682982444763184,
      "learning_rate": 0.00038493130227001195,
      "loss": 7.2432,
      "step": 1542
    },
    {
      "epoch": 0.4609754275898125,
      "grad_norm": 0.3852193355560303,
      "learning_rate": 0.00038485663082437276,
      "loss": 7.3633,
      "step": 1543
    },
    {
      "epoch": 0.4612741802972589,
      "grad_norm": 0.45564955472946167,
      "learning_rate": 0.0003847819593787336,
      "loss": 7.3809,
      "step": 1544
    },
    {
      "epoch": 0.46157293300470537,
      "grad_norm": 0.4794396460056305,
      "learning_rate": 0.0003847072879330944,
      "loss": 7.0928,
      "step": 1545
    },
    {
      "epoch": 0.46187168571215176,
      "grad_norm": 0.4758355915546417,
      "learning_rate": 0.0003846326164874552,
      "loss": 7.166,
      "step": 1546
    },
    {
      "epoch": 0.46217043841959815,
      "grad_norm": 0.4977599084377289,
      "learning_rate": 0.000384557945041816,
      "loss": 7.3652,
      "step": 1547
    },
    {
      "epoch": 0.4624691911270446,
      "grad_norm": 0.4405752420425415,
      "learning_rate": 0.0003844832735961768,
      "loss": 7.1064,
      "step": 1548
    },
    {
      "epoch": 0.462767943834491,
      "grad_norm": 0.41783133149147034,
      "learning_rate": 0.00038440860215053764,
      "loss": 7.1553,
      "step": 1549
    },
    {
      "epoch": 0.4630666965419374,
      "grad_norm": 0.4130675494670868,
      "learning_rate": 0.00038433393070489845,
      "loss": 7.4248,
      "step": 1550
    },
    {
      "epoch": 0.46336544924938383,
      "grad_norm": 0.33401474356651306,
      "learning_rate": 0.00038425925925925927,
      "loss": 7.8047,
      "step": 1551
    },
    {
      "epoch": 0.4636642019568302,
      "grad_norm": 0.4267752170562744,
      "learning_rate": 0.0003841845878136201,
      "loss": 7.1846,
      "step": 1552
    },
    {
      "epoch": 0.46396295466427667,
      "grad_norm": 0.37683624029159546,
      "learning_rate": 0.0003841099163679809,
      "loss": 7.6699,
      "step": 1553
    },
    {
      "epoch": 0.46426170737172306,
      "grad_norm": 0.49098363518714905,
      "learning_rate": 0.0003840352449223417,
      "loss": 6.9229,
      "step": 1554
    },
    {
      "epoch": 0.46456046007916946,
      "grad_norm": 1.7839118242263794,
      "learning_rate": 0.0003839605734767025,
      "loss": 7.085,
      "step": 1555
    },
    {
      "epoch": 0.4648592127866159,
      "grad_norm": 0.43354126811027527,
      "learning_rate": 0.00038388590203106333,
      "loss": 7.1406,
      "step": 1556
    },
    {
      "epoch": 0.4651579654940623,
      "grad_norm": 0.5115739703178406,
      "learning_rate": 0.00038381123058542414,
      "loss": 6.667,
      "step": 1557
    },
    {
      "epoch": 0.4654567182015087,
      "grad_norm": 0.43596702814102173,
      "learning_rate": 0.00038373655913978496,
      "loss": 6.9033,
      "step": 1558
    },
    {
      "epoch": 0.46575547090895514,
      "grad_norm": 0.4076865017414093,
      "learning_rate": 0.00038366188769414577,
      "loss": 7.0498,
      "step": 1559
    },
    {
      "epoch": 0.46605422361640153,
      "grad_norm": 0.44397029280662537,
      "learning_rate": 0.0003835872162485066,
      "loss": 7.2275,
      "step": 1560
    },
    {
      "epoch": 0.4663529763238479,
      "grad_norm": 0.4232134521007538,
      "learning_rate": 0.0003835125448028674,
      "loss": 7.1035,
      "step": 1561
    },
    {
      "epoch": 0.46665172903129437,
      "grad_norm": 0.3880419135093689,
      "learning_rate": 0.0003834378733572282,
      "loss": 7.3623,
      "step": 1562
    },
    {
      "epoch": 0.46695048173874076,
      "grad_norm": 0.4763181209564209,
      "learning_rate": 0.000383363201911589,
      "loss": 7.0146,
      "step": 1563
    },
    {
      "epoch": 0.46724923444618716,
      "grad_norm": 0.4966956377029419,
      "learning_rate": 0.0003832885304659498,
      "loss": 7.1055,
      "step": 1564
    },
    {
      "epoch": 0.4675479871536336,
      "grad_norm": 0.4368298649787903,
      "learning_rate": 0.00038321385902031065,
      "loss": 6.9629,
      "step": 1565
    },
    {
      "epoch": 0.46784673986108,
      "grad_norm": 0.402526319026947,
      "learning_rate": 0.00038313918757467146,
      "loss": 7.4209,
      "step": 1566
    },
    {
      "epoch": 0.4681454925685264,
      "grad_norm": 0.5572559833526611,
      "learning_rate": 0.00038306451612903227,
      "loss": 7.1885,
      "step": 1567
    },
    {
      "epoch": 0.46844424527597284,
      "grad_norm": 0.4194570481777191,
      "learning_rate": 0.0003829898446833931,
      "loss": 7.4424,
      "step": 1568
    },
    {
      "epoch": 0.46874299798341923,
      "grad_norm": 0.45749661326408386,
      "learning_rate": 0.0003829151732377539,
      "loss": 7.0029,
      "step": 1569
    },
    {
      "epoch": 0.4690417506908656,
      "grad_norm": 0.4213704764842987,
      "learning_rate": 0.0003828405017921147,
      "loss": 7.9043,
      "step": 1570
    },
    {
      "epoch": 0.46934050339831207,
      "grad_norm": 0.5370138883590698,
      "learning_rate": 0.0003827658303464755,
      "loss": 6.4551,
      "step": 1571
    },
    {
      "epoch": 0.46963925610575846,
      "grad_norm": 0.4237613081932068,
      "learning_rate": 0.00038269115890083634,
      "loss": 7.1172,
      "step": 1572
    },
    {
      "epoch": 0.46993800881320485,
      "grad_norm": 0.4011322557926178,
      "learning_rate": 0.0003826164874551971,
      "loss": 7.5107,
      "step": 1573
    },
    {
      "epoch": 0.4702367615206513,
      "grad_norm": 0.40216371417045593,
      "learning_rate": 0.00038254181600955796,
      "loss": 7.4482,
      "step": 1574
    },
    {
      "epoch": 0.4705355142280977,
      "grad_norm": 0.4055023789405823,
      "learning_rate": 0.0003824671445639188,
      "loss": 7.4473,
      "step": 1575
    },
    {
      "epoch": 0.4708342669355441,
      "grad_norm": 0.4075641632080078,
      "learning_rate": 0.0003823924731182796,
      "loss": 7.0342,
      "step": 1576
    },
    {
      "epoch": 0.47113301964299054,
      "grad_norm": 0.4685882031917572,
      "learning_rate": 0.0003823178016726404,
      "loss": 6.8643,
      "step": 1577
    },
    {
      "epoch": 0.47143177235043693,
      "grad_norm": 0.3861036002635956,
      "learning_rate": 0.0003822431302270012,
      "loss": 7.7178,
      "step": 1578
    },
    {
      "epoch": 0.4717305250578833,
      "grad_norm": 0.4381156265735626,
      "learning_rate": 0.000382168458781362,
      "loss": 7.1016,
      "step": 1579
    },
    {
      "epoch": 0.47202927776532977,
      "grad_norm": 0.4354434013366699,
      "learning_rate": 0.0003820937873357228,
      "loss": 7.3584,
      "step": 1580
    },
    {
      "epoch": 0.47232803047277616,
      "grad_norm": 0.5318011045455933,
      "learning_rate": 0.00038201911589008365,
      "loss": 7.248,
      "step": 1581
    },
    {
      "epoch": 0.47262678318022255,
      "grad_norm": 0.45766961574554443,
      "learning_rate": 0.0003819444444444444,
      "loss": 6.6904,
      "step": 1582
    },
    {
      "epoch": 0.472925535887669,
      "grad_norm": 0.4763125479221344,
      "learning_rate": 0.0003818697729988053,
      "loss": 7.0547,
      "step": 1583
    },
    {
      "epoch": 0.4732242885951154,
      "grad_norm": 0.5579553842544556,
      "learning_rate": 0.0003817951015531661,
      "loss": 6.7578,
      "step": 1584
    },
    {
      "epoch": 0.4735230413025618,
      "grad_norm": 0.3823830187320709,
      "learning_rate": 0.0003817204301075269,
      "loss": 7.1504,
      "step": 1585
    },
    {
      "epoch": 0.47382179401000823,
      "grad_norm": 0.4992998540401459,
      "learning_rate": 0.0003816457586618877,
      "loss": 7.1543,
      "step": 1586
    },
    {
      "epoch": 0.4741205467174546,
      "grad_norm": 0.3813839256763458,
      "learning_rate": 0.00038157108721624853,
      "loss": 7.6768,
      "step": 1587
    },
    {
      "epoch": 0.474419299424901,
      "grad_norm": 0.48052138090133667,
      "learning_rate": 0.00038149641577060934,
      "loss": 7.5488,
      "step": 1588
    },
    {
      "epoch": 0.47471805213234747,
      "grad_norm": 0.4030061960220337,
      "learning_rate": 0.0003814217443249701,
      "loss": 7.3096,
      "step": 1589
    },
    {
      "epoch": 0.47501680483979386,
      "grad_norm": 0.45032036304473877,
      "learning_rate": 0.00038134707287933097,
      "loss": 7.4072,
      "step": 1590
    },
    {
      "epoch": 0.47531555754724025,
      "grad_norm": 2.397012948989868,
      "learning_rate": 0.0003812724014336917,
      "loss": 7.3496,
      "step": 1591
    },
    {
      "epoch": 0.4756143102546867,
      "grad_norm": 0.4579375386238098,
      "learning_rate": 0.0003811977299880526,
      "loss": 7.6504,
      "step": 1592
    },
    {
      "epoch": 0.4759130629621331,
      "grad_norm": 0.503897488117218,
      "learning_rate": 0.0003811230585424134,
      "loss": 6.96,
      "step": 1593
    },
    {
      "epoch": 0.4762118156695795,
      "grad_norm": 0.3505367040634155,
      "learning_rate": 0.0003810483870967742,
      "loss": 7.8223,
      "step": 1594
    },
    {
      "epoch": 0.47651056837702593,
      "grad_norm": 0.5027017593383789,
      "learning_rate": 0.00038097371565113503,
      "loss": 7.3535,
      "step": 1595
    },
    {
      "epoch": 0.4768093210844723,
      "grad_norm": 0.38443219661712646,
      "learning_rate": 0.0003808990442054958,
      "loss": 7.2354,
      "step": 1596
    },
    {
      "epoch": 0.4771080737919187,
      "grad_norm": 0.4328257441520691,
      "learning_rate": 0.00038082437275985666,
      "loss": 7.2881,
      "step": 1597
    },
    {
      "epoch": 0.47740682649936517,
      "grad_norm": 0.3895472288131714,
      "learning_rate": 0.0003807497013142174,
      "loss": 7.3994,
      "step": 1598
    },
    {
      "epoch": 0.47770557920681156,
      "grad_norm": 0.4991438388824463,
      "learning_rate": 0.0003806750298685783,
      "loss": 7.0303,
      "step": 1599
    },
    {
      "epoch": 0.47800433191425795,
      "grad_norm": 0.41270527243614197,
      "learning_rate": 0.00038060035842293904,
      "loss": 7.2773,
      "step": 1600
    },
    {
      "epoch": 0.47800433191425795,
      "eval_bleu": 0.1126648493790043,
      "eval_loss": 7.04296875,
      "eval_runtime": 521.5956,
      "eval_samples_per_second": 2.701,
      "eval_steps_per_second": 0.171,
      "step": 1600
    }
  ],
  "logging_steps": 1,
  "max_steps": 6696,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 200,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 3478963775078400.0,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
