{
  "best_global_step": 200,
  "best_metric": 7.1328125,
  "best_model_checkpoint": "training-nllb-tgl-to-bicol-working\\checkpoint-200",
  "epoch": 0.059750541489282244,
  "eval_steps": 200,
  "global_step": 200,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.00029875270744641124,
      "grad_norm": 0.10821348428726196,
      "learning_rate": 0.0005,
      "loss": 10.5107,
      "step": 1
    },
    {
      "epoch": 0.0005975054148928225,
      "grad_norm": 0.11005502939224243,
      "learning_rate": 0.0004999253285543608,
      "loss": 11.3281,
      "step": 2
    },
    {
      "epoch": 0.0008962581223392337,
      "grad_norm": 0.17715798318386078,
      "learning_rate": 0.0004998506571087216,
      "loss": 10.7266,
      "step": 3
    },
    {
      "epoch": 0.001195010829785645,
      "grad_norm": 0.18851852416992188,
      "learning_rate": 0.0004997759856630824,
      "loss": 10.7266,
      "step": 4
    },
    {
      "epoch": 0.001493763537232056,
      "grad_norm": 0.27457132935523987,
      "learning_rate": 0.0004997013142174433,
      "loss": 11.1172,
      "step": 5
    },
    {
      "epoch": 0.0017925162446784674,
      "grad_norm": 0.32356637716293335,
      "learning_rate": 0.0004996266427718041,
      "loss": 11.127,
      "step": 6
    },
    {
      "epoch": 0.002091268952124879,
      "grad_norm": 0.38698747754096985,
      "learning_rate": 0.0004995519713261649,
      "loss": 11.0762,
      "step": 7
    },
    {
      "epoch": 0.00239002165957129,
      "grad_norm": 0.4688027501106262,
      "learning_rate": 0.0004994772998805257,
      "loss": 11.0762,
      "step": 8
    },
    {
      "epoch": 0.002688774367017701,
      "grad_norm": 0.5016260147094727,
      "learning_rate": 0.0004994026284348865,
      "loss": 10.5703,
      "step": 9
    },
    {
      "epoch": 0.002987527074464112,
      "grad_norm": 0.6383640170097351,
      "learning_rate": 0.0004993279569892473,
      "loss": 10.9414,
      "step": 10
    },
    {
      "epoch": 0.0032862797819105238,
      "grad_norm": 0.6936139464378357,
      "learning_rate": 0.0004992532855436081,
      "loss": 11.3438,
      "step": 11
    },
    {
      "epoch": 0.003585032489356935,
      "grad_norm": 0.6707635521888733,
      "learning_rate": 0.000499178614097969,
      "loss": 10.9121,
      "step": 12
    },
    {
      "epoch": 0.003883785196803346,
      "grad_norm": 0.6898233890533447,
      "learning_rate": 0.0004991039426523298,
      "loss": 10.5566,
      "step": 13
    },
    {
      "epoch": 0.004182537904249758,
      "grad_norm": 0.6574143767356873,
      "learning_rate": 0.0004990292712066906,
      "loss": 11.2637,
      "step": 14
    },
    {
      "epoch": 0.004481290611696168,
      "grad_norm": 0.5881863832473755,
      "learning_rate": 0.0004989545997610514,
      "loss": 10.5469,
      "step": 15
    },
    {
      "epoch": 0.00478004331914258,
      "grad_norm": 0.5066705942153931,
      "learning_rate": 0.0004988799283154122,
      "loss": 10.5879,
      "step": 16
    },
    {
      "epoch": 0.0050787960265889906,
      "grad_norm": 0.5587491989135742,
      "learning_rate": 0.000498805256869773,
      "loss": 11.0312,
      "step": 17
    },
    {
      "epoch": 0.005377548734035402,
      "grad_norm": 0.4914754033088684,
      "learning_rate": 0.0004987305854241338,
      "loss": 10.9629,
      "step": 18
    },
    {
      "epoch": 0.005676301441481814,
      "grad_norm": 0.47820284962654114,
      "learning_rate": 0.0004986559139784946,
      "loss": 10.7637,
      "step": 19
    },
    {
      "epoch": 0.005975054148928224,
      "grad_norm": 0.6641180515289307,
      "learning_rate": 0.0004985812425328555,
      "loss": 11.5117,
      "step": 20
    },
    {
      "epoch": 0.006273806856374636,
      "grad_norm": 0.4952411949634552,
      "learning_rate": 0.0004985065710872163,
      "loss": 10.7676,
      "step": 21
    },
    {
      "epoch": 0.0065725595638210475,
      "grad_norm": 0.436328262090683,
      "learning_rate": 0.0004984318996415771,
      "loss": 9.9941,
      "step": 22
    },
    {
      "epoch": 0.006871312271267458,
      "grad_norm": 0.5016461610794067,
      "learning_rate": 0.0004983572281959379,
      "loss": 10.2441,
      "step": 23
    },
    {
      "epoch": 0.00717006497871387,
      "grad_norm": 0.48784565925598145,
      "learning_rate": 0.0004982825567502987,
      "loss": 10.7227,
      "step": 24
    },
    {
      "epoch": 0.0074688176861602805,
      "grad_norm": 0.46044033765792847,
      "learning_rate": 0.0004982078853046595,
      "loss": 9.875,
      "step": 25
    },
    {
      "epoch": 0.007767570393606692,
      "grad_norm": 0.4846183657646179,
      "learning_rate": 0.0004981332138590203,
      "loss": 10.5723,
      "step": 26
    },
    {
      "epoch": 0.008066323101053104,
      "grad_norm": 0.45062729716300964,
      "learning_rate": 0.0004980585424133811,
      "loss": 9.9922,
      "step": 27
    },
    {
      "epoch": 0.008365075808499515,
      "grad_norm": 0.5014210343360901,
      "learning_rate": 0.000497983870967742,
      "loss": 10.6836,
      "step": 28
    },
    {
      "epoch": 0.008663828515945925,
      "grad_norm": 0.5104053616523743,
      "learning_rate": 0.0004979091995221028,
      "loss": 10.3477,
      "step": 29
    },
    {
      "epoch": 0.008962581223392337,
      "grad_norm": 0.4748744070529938,
      "learning_rate": 0.0004978345280764636,
      "loss": 9.6602,
      "step": 30
    },
    {
      "epoch": 0.009261333930838748,
      "grad_norm": 0.49339559674263,
      "learning_rate": 0.0004977598566308244,
      "loss": 10.7129,
      "step": 31
    },
    {
      "epoch": 0.00956008663828516,
      "grad_norm": 0.39628931879997253,
      "learning_rate": 0.0004976851851851852,
      "loss": 9.627,
      "step": 32
    },
    {
      "epoch": 0.009858839345731571,
      "grad_norm": 0.42897528409957886,
      "learning_rate": 0.000497610513739546,
      "loss": 10.4668,
      "step": 33
    },
    {
      "epoch": 0.010157592053177981,
      "grad_norm": 0.4145178198814392,
      "learning_rate": 0.0004975358422939068,
      "loss": 10.3262,
      "step": 34
    },
    {
      "epoch": 0.010456344760624393,
      "grad_norm": 0.4588963985443115,
      "learning_rate": 0.0004974611708482676,
      "loss": 11.2031,
      "step": 35
    },
    {
      "epoch": 0.010755097468070804,
      "grad_norm": 0.4078393578529358,
      "learning_rate": 0.0004973864994026285,
      "loss": 9.9121,
      "step": 36
    },
    {
      "epoch": 0.011053850175517216,
      "grad_norm": 0.36339741945266724,
      "learning_rate": 0.0004973118279569893,
      "loss": 9.2246,
      "step": 37
    },
    {
      "epoch": 0.011352602882963627,
      "grad_norm": 0.40116608142852783,
      "learning_rate": 0.0004972371565113501,
      "loss": 9.7773,
      "step": 38
    },
    {
      "epoch": 0.011651355590410037,
      "grad_norm": 0.379043847322464,
      "learning_rate": 0.0004971624850657109,
      "loss": 9.5439,
      "step": 39
    },
    {
      "epoch": 0.011950108297856449,
      "grad_norm": 0.38495877385139465,
      "learning_rate": 0.0004970878136200717,
      "loss": 9.5215,
      "step": 40
    },
    {
      "epoch": 0.01224886100530286,
      "grad_norm": 0.41120296716690063,
      "learning_rate": 0.0004970131421744325,
      "loss": 10.4277,
      "step": 41
    },
    {
      "epoch": 0.012547613712749272,
      "grad_norm": 0.40164628624916077,
      "learning_rate": 0.0004969384707287933,
      "loss": 10.0625,
      "step": 42
    },
    {
      "epoch": 0.012846366420195683,
      "grad_norm": 0.4104005694389343,
      "learning_rate": 0.0004968637992831542,
      "loss": 9.9902,
      "step": 43
    },
    {
      "epoch": 0.013145119127642095,
      "grad_norm": 0.4140368103981018,
      "learning_rate": 0.000496789127837515,
      "loss": 9.8848,
      "step": 44
    },
    {
      "epoch": 0.013443871835088505,
      "grad_norm": 0.3986252248287201,
      "learning_rate": 0.0004967144563918758,
      "loss": 9.3008,
      "step": 45
    },
    {
      "epoch": 0.013742624542534916,
      "grad_norm": 0.40926143527030945,
      "learning_rate": 0.0004966397849462366,
      "loss": 9.6934,
      "step": 46
    },
    {
      "epoch": 0.014041377249981328,
      "grad_norm": 0.39645010232925415,
      "learning_rate": 0.0004965651135005974,
      "loss": 8.3613,
      "step": 47
    },
    {
      "epoch": 0.01434012995742774,
      "grad_norm": 0.4267260730266571,
      "learning_rate": 0.0004964904420549582,
      "loss": 9.5488,
      "step": 48
    },
    {
      "epoch": 0.014638882664874151,
      "grad_norm": 0.3833109736442566,
      "learning_rate": 0.000496415770609319,
      "loss": 8.9131,
      "step": 49
    },
    {
      "epoch": 0.014937635372320561,
      "grad_norm": 0.38322240114212036,
      "learning_rate": 0.0004963410991636798,
      "loss": 9.1816,
      "step": 50
    },
    {
      "epoch": 0.015236388079766973,
      "grad_norm": 0.41379764676094055,
      "learning_rate": 0.0004962664277180407,
      "loss": 9.498,
      "step": 51
    },
    {
      "epoch": 0.015535140787213384,
      "grad_norm": 0.36599066853523254,
      "learning_rate": 0.0004961917562724015,
      "loss": 8.71,
      "step": 52
    },
    {
      "epoch": 0.015833893494659794,
      "grad_norm": 0.3643648028373718,
      "learning_rate": 0.0004961170848267623,
      "loss": 8.5176,
      "step": 53
    },
    {
      "epoch": 0.016132646202106207,
      "grad_norm": 0.361419677734375,
      "learning_rate": 0.0004960424133811231,
      "loss": 8.8027,
      "step": 54
    },
    {
      "epoch": 0.016431398909552617,
      "grad_norm": 0.3777402341365814,
      "learning_rate": 0.0004959677419354839,
      "loss": 8.957,
      "step": 55
    },
    {
      "epoch": 0.01673015161699903,
      "grad_norm": 0.3822852373123169,
      "learning_rate": 0.0004958930704898447,
      "loss": 8.3691,
      "step": 56
    },
    {
      "epoch": 0.01702890432444544,
      "grad_norm": 0.3792678415775299,
      "learning_rate": 0.0004958183990442055,
      "loss": 9.1738,
      "step": 57
    },
    {
      "epoch": 0.01732765703189185,
      "grad_norm": 0.37306538224220276,
      "learning_rate": 0.0004957437275985663,
      "loss": 8.4873,
      "step": 58
    },
    {
      "epoch": 0.017626409739338263,
      "grad_norm": 0.38344499468803406,
      "learning_rate": 0.0004956690561529272,
      "loss": 9.1914,
      "step": 59
    },
    {
      "epoch": 0.017925162446784673,
      "grad_norm": 0.38386160135269165,
      "learning_rate": 0.000495594384707288,
      "loss": 8.8926,
      "step": 60
    },
    {
      "epoch": 0.018223915154231086,
      "grad_norm": 0.348595529794693,
      "learning_rate": 0.0004955197132616488,
      "loss": 7.8936,
      "step": 61
    },
    {
      "epoch": 0.018522667861677496,
      "grad_norm": 0.403987854719162,
      "learning_rate": 0.0004954450418160096,
      "loss": 8.7227,
      "step": 62
    },
    {
      "epoch": 0.018821420569123906,
      "grad_norm": 0.3449745774269104,
      "learning_rate": 0.0004953703703703704,
      "loss": 8.5898,
      "step": 63
    },
    {
      "epoch": 0.01912017327657032,
      "grad_norm": 0.35804587602615356,
      "learning_rate": 0.0004952956989247312,
      "loss": 8.5215,
      "step": 64
    },
    {
      "epoch": 0.01941892598401673,
      "grad_norm": 0.34505829215049744,
      "learning_rate": 0.0004952210274790919,
      "loss": 8.1533,
      "step": 65
    },
    {
      "epoch": 0.019717678691463143,
      "grad_norm": 0.34219563007354736,
      "learning_rate": 0.0004951463560334528,
      "loss": 7.9258,
      "step": 66
    },
    {
      "epoch": 0.020016431398909552,
      "grad_norm": 0.33780527114868164,
      "learning_rate": 0.0004950716845878137,
      "loss": 8.4648,
      "step": 67
    },
    {
      "epoch": 0.020315184106355962,
      "grad_norm": 0.3311116397380829,
      "learning_rate": 0.0004949970131421745,
      "loss": 8.3604,
      "step": 68
    },
    {
      "epoch": 0.020613936813802376,
      "grad_norm": 0.3329460918903351,
      "learning_rate": 0.0004949223416965353,
      "loss": 8.0234,
      "step": 69
    },
    {
      "epoch": 0.020912689521248785,
      "grad_norm": 0.3190329968929291,
      "learning_rate": 0.0004948476702508961,
      "loss": 8.4531,
      "step": 70
    },
    {
      "epoch": 0.0212114422286952,
      "grad_norm": 0.33799904584884644,
      "learning_rate": 0.0004947729988052569,
      "loss": 8.2656,
      "step": 71
    },
    {
      "epoch": 0.02151019493614161,
      "grad_norm": 0.31499767303466797,
      "learning_rate": 0.0004946983273596177,
      "loss": 8.4141,
      "step": 72
    },
    {
      "epoch": 0.02180894764358802,
      "grad_norm": 0.31318432092666626,
      "learning_rate": 0.0004946236559139785,
      "loss": 8.0254,
      "step": 73
    },
    {
      "epoch": 0.02210770035103443,
      "grad_norm": 0.3035445213317871,
      "learning_rate": 0.0004945489844683392,
      "loss": 7.7998,
      "step": 74
    },
    {
      "epoch": 0.02240645305848084,
      "grad_norm": 0.3642057776451111,
      "learning_rate": 0.0004944743130227002,
      "loss": 7.5146,
      "step": 75
    },
    {
      "epoch": 0.022705205765927255,
      "grad_norm": 0.39204663038253784,
      "learning_rate": 0.000494399641577061,
      "loss": 7.9434,
      "step": 76
    },
    {
      "epoch": 0.023003958473373665,
      "grad_norm": 0.3150036334991455,
      "learning_rate": 0.0004943249701314218,
      "loss": 7.7666,
      "step": 77
    },
    {
      "epoch": 0.023302711180820074,
      "grad_norm": 0.3041803240776062,
      "learning_rate": 0.0004942502986857826,
      "loss": 8.1553,
      "step": 78
    },
    {
      "epoch": 0.023601463888266488,
      "grad_norm": 0.32640504837036133,
      "learning_rate": 0.0004941756272401434,
      "loss": 8.2227,
      "step": 79
    },
    {
      "epoch": 0.023900216595712898,
      "grad_norm": 0.3017626404762268,
      "learning_rate": 0.0004941009557945042,
      "loss": 8.0449,
      "step": 80
    },
    {
      "epoch": 0.02419896930315931,
      "grad_norm": 0.2896050810813904,
      "learning_rate": 0.0004940262843488649,
      "loss": 7.8027,
      "step": 81
    },
    {
      "epoch": 0.02449772201060572,
      "grad_norm": 0.2856549918651581,
      "learning_rate": 0.0004939516129032259,
      "loss": 7.5742,
      "step": 82
    },
    {
      "epoch": 0.024796474718052134,
      "grad_norm": 0.2921745479106903,
      "learning_rate": 0.0004938769414575866,
      "loss": 7.7959,
      "step": 83
    },
    {
      "epoch": 0.025095227425498544,
      "grad_norm": 0.2688756585121155,
      "learning_rate": 0.0004938022700119475,
      "loss": 8.083,
      "step": 84
    },
    {
      "epoch": 0.025393980132944954,
      "grad_norm": 0.275928258895874,
      "learning_rate": 0.0004937275985663083,
      "loss": 7.583,
      "step": 85
    },
    {
      "epoch": 0.025692732840391367,
      "grad_norm": 0.272701621055603,
      "learning_rate": 0.0004936529271206691,
      "loss": 7.8018,
      "step": 86
    },
    {
      "epoch": 0.025991485547837777,
      "grad_norm": 0.2676122784614563,
      "learning_rate": 0.0004935782556750299,
      "loss": 8.0166,
      "step": 87
    },
    {
      "epoch": 0.02629023825528419,
      "grad_norm": 0.3009990453720093,
      "learning_rate": 0.0004935035842293907,
      "loss": 7.5195,
      "step": 88
    },
    {
      "epoch": 0.0265889909627306,
      "grad_norm": 0.32099971175193787,
      "learning_rate": 0.0004934289127837515,
      "loss": 7.2861,
      "step": 89
    },
    {
      "epoch": 0.02688774367017701,
      "grad_norm": 0.2675405740737915,
      "learning_rate": 0.0004933542413381122,
      "loss": 7.667,
      "step": 90
    },
    {
      "epoch": 0.027186496377623423,
      "grad_norm": 0.3011398911476135,
      "learning_rate": 0.0004932795698924732,
      "loss": 7.2764,
      "step": 91
    },
    {
      "epoch": 0.027485249085069833,
      "grad_norm": 0.24202702939510345,
      "learning_rate": 0.0004932048984468339,
      "loss": 7.7969,
      "step": 92
    },
    {
      "epoch": 0.027784001792516246,
      "grad_norm": 0.2650386393070221,
      "learning_rate": 0.0004931302270011948,
      "loss": 7.8994,
      "step": 93
    },
    {
      "epoch": 0.028082754499962656,
      "grad_norm": 0.2493761032819748,
      "learning_rate": 0.0004930555555555556,
      "loss": 7.7256,
      "step": 94
    },
    {
      "epoch": 0.028381507207409066,
      "grad_norm": 0.24684636294841766,
      "learning_rate": 0.0004929808841099164,
      "loss": 7.6885,
      "step": 95
    },
    {
      "epoch": 0.02868025991485548,
      "grad_norm": 0.32382333278656006,
      "learning_rate": 0.0004929062126642772,
      "loss": 7.3018,
      "step": 96
    },
    {
      "epoch": 0.02897901262230189,
      "grad_norm": 0.2478044331073761,
      "learning_rate": 0.0004928315412186379,
      "loss": 8.1084,
      "step": 97
    },
    {
      "epoch": 0.029277765329748302,
      "grad_norm": 0.28959137201309204,
      "learning_rate": 0.0004927568697729989,
      "loss": 7.8066,
      "step": 98
    },
    {
      "epoch": 0.029576518037194712,
      "grad_norm": 0.26750457286834717,
      "learning_rate": 0.0004926821983273596,
      "loss": 7.3613,
      "step": 99
    },
    {
      "epoch": 0.029875270744641122,
      "grad_norm": 0.24644550681114197,
      "learning_rate": 0.0004926075268817205,
      "loss": 7.8154,
      "step": 100
    },
    {
      "epoch": 0.030174023452087535,
      "grad_norm": 0.21514536440372467,
      "learning_rate": 0.0004925328554360812,
      "loss": 8.0107,
      "step": 101
    },
    {
      "epoch": 0.030472776159533945,
      "grad_norm": 0.24907834827899933,
      "learning_rate": 0.0004924581839904421,
      "loss": 7.8574,
      "step": 102
    },
    {
      "epoch": 0.03077152886698036,
      "grad_norm": 0.2640770375728607,
      "learning_rate": 0.0004923835125448029,
      "loss": 7.7686,
      "step": 103
    },
    {
      "epoch": 0.031070281574426768,
      "grad_norm": 0.3616735637187958,
      "learning_rate": 0.0004923088410991637,
      "loss": 7.6465,
      "step": 104
    },
    {
      "epoch": 0.03136903428187318,
      "grad_norm": 0.47297847270965576,
      "learning_rate": 0.0004922341696535245,
      "loss": 7.2412,
      "step": 105
    },
    {
      "epoch": 0.03166778698931959,
      "grad_norm": 0.2724394202232361,
      "learning_rate": 0.0004921594982078853,
      "loss": 7.6572,
      "step": 106
    },
    {
      "epoch": 0.031966539696766,
      "grad_norm": 0.23566237092018127,
      "learning_rate": 0.0004920848267622462,
      "loss": 7.7812,
      "step": 107
    },
    {
      "epoch": 0.032265292404212415,
      "grad_norm": 0.24848216772079468,
      "learning_rate": 0.0004920101553166069,
      "loss": 7.4004,
      "step": 108
    },
    {
      "epoch": 0.03256404511165883,
      "grad_norm": 0.3075593113899231,
      "learning_rate": 0.0004919354838709678,
      "loss": 6.8564,
      "step": 109
    },
    {
      "epoch": 0.032862797819105234,
      "grad_norm": 0.3262488543987274,
      "learning_rate": 0.0004918608124253285,
      "loss": 7.0518,
      "step": 110
    },
    {
      "epoch": 0.03316155052655165,
      "grad_norm": 0.2236112356185913,
      "learning_rate": 0.0004917861409796894,
      "loss": 7.7959,
      "step": 111
    },
    {
      "epoch": 0.03346030323399806,
      "grad_norm": 0.2831442952156067,
      "learning_rate": 0.0004917114695340502,
      "loss": 7.1348,
      "step": 112
    },
    {
      "epoch": 0.03375905594144447,
      "grad_norm": 0.24397167563438416,
      "learning_rate": 0.0004916367980884109,
      "loss": 7.8721,
      "step": 113
    },
    {
      "epoch": 0.03405780864889088,
      "grad_norm": 0.2545168697834015,
      "learning_rate": 0.0004915621266427719,
      "loss": 7.4492,
      "step": 114
    },
    {
      "epoch": 0.034356561356337294,
      "grad_norm": 0.22379270195960999,
      "learning_rate": 0.0004914874551971326,
      "loss": 7.6738,
      "step": 115
    },
    {
      "epoch": 0.0346553140637837,
      "grad_norm": 0.36304497718811035,
      "learning_rate": 0.0004914127837514935,
      "loss": 6.917,
      "step": 116
    },
    {
      "epoch": 0.03495406677123011,
      "grad_norm": 0.24335503578186035,
      "learning_rate": 0.0004913381123058542,
      "loss": 7.46,
      "step": 117
    },
    {
      "epoch": 0.03525281947867653,
      "grad_norm": 0.19890791177749634,
      "learning_rate": 0.0004912634408602151,
      "loss": 7.8145,
      "step": 118
    },
    {
      "epoch": 0.03555157218612294,
      "grad_norm": 0.2813917398452759,
      "learning_rate": 0.0004911887694145758,
      "loss": 7.0918,
      "step": 119
    },
    {
      "epoch": 0.035850324893569346,
      "grad_norm": 0.2551155686378479,
      "learning_rate": 0.0004911140979689367,
      "loss": 7.415,
      "step": 120
    },
    {
      "epoch": 0.03614907760101576,
      "grad_norm": 0.21404586732387543,
      "learning_rate": 0.0004910394265232976,
      "loss": 7.7285,
      "step": 121
    },
    {
      "epoch": 0.03644783030846217,
      "grad_norm": 0.2606382369995117,
      "learning_rate": 0.0004909647550776583,
      "loss": 7.4502,
      "step": 122
    },
    {
      "epoch": 0.03674658301590858,
      "grad_norm": 0.27905747294425964,
      "learning_rate": 0.0004908900836320192,
      "loss": 7.3135,
      "step": 123
    },
    {
      "epoch": 0.03704533572335499,
      "grad_norm": 0.27660632133483887,
      "learning_rate": 0.0004908154121863799,
      "loss": 7.1143,
      "step": 124
    },
    {
      "epoch": 0.037344088430801406,
      "grad_norm": 0.26993387937545776,
      "learning_rate": 0.0004907407407407408,
      "loss": 7.5371,
      "step": 125
    },
    {
      "epoch": 0.03764284113824781,
      "grad_norm": 0.20496590435504913,
      "learning_rate": 0.0004906660692951015,
      "loss": 7.7764,
      "step": 126
    },
    {
      "epoch": 0.037941593845694226,
      "grad_norm": 0.26359736919403076,
      "learning_rate": 0.0004905913978494624,
      "loss": 7.5264,
      "step": 127
    },
    {
      "epoch": 0.03824034655314064,
      "grad_norm": 0.2587951421737671,
      "learning_rate": 0.0004905167264038231,
      "loss": 7.4697,
      "step": 128
    },
    {
      "epoch": 0.03853909926058705,
      "grad_norm": 0.27789872884750366,
      "learning_rate": 0.000490442054958184,
      "loss": 7.0439,
      "step": 129
    },
    {
      "epoch": 0.03883785196803346,
      "grad_norm": 0.24561715126037598,
      "learning_rate": 0.0004903673835125449,
      "loss": 7.5801,
      "step": 130
    },
    {
      "epoch": 0.03913660467547987,
      "grad_norm": 0.29134175181388855,
      "learning_rate": 0.0004902927120669056,
      "loss": 7.2666,
      "step": 131
    },
    {
      "epoch": 0.039435357382926285,
      "grad_norm": 0.29421618580818176,
      "learning_rate": 0.0004902180406212665,
      "loss": 7.6748,
      "step": 132
    },
    {
      "epoch": 0.03973411009037269,
      "grad_norm": 0.30324870347976685,
      "learning_rate": 0.0004901433691756272,
      "loss": 7.2764,
      "step": 133
    },
    {
      "epoch": 0.040032862797819105,
      "grad_norm": 0.24822299182415009,
      "learning_rate": 0.0004900686977299881,
      "loss": 7.5156,
      "step": 134
    },
    {
      "epoch": 0.04033161550526552,
      "grad_norm": 0.3090725243091583,
      "learning_rate": 0.0004899940262843488,
      "loss": 7.4834,
      "step": 135
    },
    {
      "epoch": 0.040630368212711925,
      "grad_norm": 0.2992507517337799,
      "learning_rate": 0.0004899193548387097,
      "loss": 7.3252,
      "step": 136
    },
    {
      "epoch": 0.04092912092015834,
      "grad_norm": 0.3073965907096863,
      "learning_rate": 0.0004898446833930705,
      "loss": 7.2656,
      "step": 137
    },
    {
      "epoch": 0.04122787362760475,
      "grad_norm": 0.32228711247444153,
      "learning_rate": 0.0004897700119474313,
      "loss": 6.9688,
      "step": 138
    },
    {
      "epoch": 0.041526626335051164,
      "grad_norm": 0.290147066116333,
      "learning_rate": 0.0004896953405017922,
      "loss": 7.46,
      "step": 139
    },
    {
      "epoch": 0.04182537904249757,
      "grad_norm": 0.3048872947692871,
      "learning_rate": 0.0004896206690561529,
      "loss": 7.1982,
      "step": 140
    },
    {
      "epoch": 0.042124131749943984,
      "grad_norm": 0.2502696216106415,
      "learning_rate": 0.0004895459976105138,
      "loss": 7.7822,
      "step": 141
    },
    {
      "epoch": 0.0424228844573904,
      "grad_norm": 0.277056485414505,
      "learning_rate": 0.0004894713261648745,
      "loss": 6.9238,
      "step": 142
    },
    {
      "epoch": 0.042721637164836804,
      "grad_norm": 0.3534789979457855,
      "learning_rate": 0.0004893966547192354,
      "loss": 7.1211,
      "step": 143
    },
    {
      "epoch": 0.04302038987228322,
      "grad_norm": 0.2861802875995636,
      "learning_rate": 0.0004893219832735961,
      "loss": 7.2949,
      "step": 144
    },
    {
      "epoch": 0.04331914257972963,
      "grad_norm": 0.2737436890602112,
      "learning_rate": 0.000489247311827957,
      "loss": 7.168,
      "step": 145
    },
    {
      "epoch": 0.04361789528717604,
      "grad_norm": 0.3162184953689575,
      "learning_rate": 0.0004891726403823178,
      "loss": 7.1924,
      "step": 146
    },
    {
      "epoch": 0.04391664799462245,
      "grad_norm": 0.2845972776412964,
      "learning_rate": 0.0004890979689366786,
      "loss": 7.0381,
      "step": 147
    },
    {
      "epoch": 0.04421540070206886,
      "grad_norm": 0.35005950927734375,
      "learning_rate": 0.0004890232974910394,
      "loss": 7.3398,
      "step": 148
    },
    {
      "epoch": 0.04451415340951528,
      "grad_norm": 0.25128173828125,
      "learning_rate": 0.0004889486260454002,
      "loss": 7.3232,
      "step": 149
    },
    {
      "epoch": 0.04481290611696168,
      "grad_norm": 0.2506413757801056,
      "learning_rate": 0.0004888739545997611,
      "loss": 7.5654,
      "step": 150
    },
    {
      "epoch": 0.045111658824408096,
      "grad_norm": 0.3365420699119568,
      "learning_rate": 0.0004887992831541218,
      "loss": 7.0918,
      "step": 151
    },
    {
      "epoch": 0.04541041153185451,
      "grad_norm": 0.30113887786865234,
      "learning_rate": 0.0004887246117084828,
      "loss": 7.0449,
      "step": 152
    },
    {
      "epoch": 0.045709164239300916,
      "grad_norm": 0.31116804480552673,
      "learning_rate": 0.0004886499402628435,
      "loss": 7.4502,
      "step": 153
    },
    {
      "epoch": 0.04600791694674733,
      "grad_norm": 0.30247560143470764,
      "learning_rate": 0.0004885752688172043,
      "loss": 7.1523,
      "step": 154
    },
    {
      "epoch": 0.04630666965419374,
      "grad_norm": 0.21981866657733917,
      "learning_rate": 0.0004885005973715651,
      "loss": 7.5488,
      "step": 155
    },
    {
      "epoch": 0.04660542236164015,
      "grad_norm": 0.3004038631916046,
      "learning_rate": 0.0004884259259259259,
      "loss": 7.5127,
      "step": 156
    },
    {
      "epoch": 0.04690417506908656,
      "grad_norm": 0.25513529777526855,
      "learning_rate": 0.0004883512544802867,
      "loss": 7.4385,
      "step": 157
    },
    {
      "epoch": 0.047202927776532976,
      "grad_norm": 0.33559417724609375,
      "learning_rate": 0.0004882765830346476,
      "loss": 7.4717,
      "step": 158
    },
    {
      "epoch": 0.04750168048397939,
      "grad_norm": 0.26063433289527893,
      "learning_rate": 0.0004882019115890084,
      "loss": 7.6895,
      "step": 159
    },
    {
      "epoch": 0.047800433191425795,
      "grad_norm": 0.39380547404289246,
      "learning_rate": 0.0004881272401433692,
      "loss": 7.2266,
      "step": 160
    },
    {
      "epoch": 0.04809918589887221,
      "grad_norm": 0.33631765842437744,
      "learning_rate": 0.00048805256869773,
      "loss": 6.8701,
      "step": 161
    },
    {
      "epoch": 0.04839793860631862,
      "grad_norm": 0.2894502878189087,
      "learning_rate": 0.0004879778972520908,
      "loss": 7.4658,
      "step": 162
    },
    {
      "epoch": 0.04869669131376503,
      "grad_norm": 0.3700374364852905,
      "learning_rate": 0.00048790322580645164,
      "loss": 6.8213,
      "step": 163
    },
    {
      "epoch": 0.04899544402121144,
      "grad_norm": 0.2870594263076782,
      "learning_rate": 0.0004878285543608124,
      "loss": 7.2793,
      "step": 164
    },
    {
      "epoch": 0.049294196728657855,
      "grad_norm": 0.2825789153575897,
      "learning_rate": 0.00048775388291517327,
      "loss": 7.5088,
      "step": 165
    },
    {
      "epoch": 0.04959294943610427,
      "grad_norm": 0.24907280504703522,
      "learning_rate": 0.000487679211469534,
      "loss": 7.5146,
      "step": 166
    },
    {
      "epoch": 0.049891702143550674,
      "grad_norm": 0.257586807012558,
      "learning_rate": 0.0004876045400238949,
      "loss": 7.5244,
      "step": 167
    },
    {
      "epoch": 0.05019045485099709,
      "grad_norm": 0.2747199535369873,
      "learning_rate": 0.0004875298685782557,
      "loss": 7.7803,
      "step": 168
    },
    {
      "epoch": 0.0504892075584435,
      "grad_norm": 0.2723439633846283,
      "learning_rate": 0.00048745519713261647,
      "loss": 7.4814,
      "step": 169
    },
    {
      "epoch": 0.05078796026588991,
      "grad_norm": 0.3231061100959778,
      "learning_rate": 0.00048738052568697733,
      "loss": 7.7725,
      "step": 170
    },
    {
      "epoch": 0.05108671297333632,
      "grad_norm": 0.25988444685935974,
      "learning_rate": 0.0004873058542413381,
      "loss": 7.7178,
      "step": 171
    },
    {
      "epoch": 0.051385465680782734,
      "grad_norm": 0.3010958433151245,
      "learning_rate": 0.00048723118279569896,
      "loss": 7.2129,
      "step": 172
    },
    {
      "epoch": 0.05168421838822914,
      "grad_norm": 0.292570561170578,
      "learning_rate": 0.0004871565113500597,
      "loss": 7.3984,
      "step": 173
    },
    {
      "epoch": 0.051982971095675554,
      "grad_norm": 0.2666943669319153,
      "learning_rate": 0.0004870818399044206,
      "loss": 7.2725,
      "step": 174
    },
    {
      "epoch": 0.05228172380312197,
      "grad_norm": 0.3564080595970154,
      "learning_rate": 0.00048700716845878134,
      "loss": 7.4648,
      "step": 175
    },
    {
      "epoch": 0.05258047651056838,
      "grad_norm": 0.3185379207134247,
      "learning_rate": 0.0004869324970131422,
      "loss": 7.1289,
      "step": 176
    },
    {
      "epoch": 0.05287922921801479,
      "grad_norm": 0.2607346773147583,
      "learning_rate": 0.000486857825567503,
      "loss": 7.3965,
      "step": 177
    },
    {
      "epoch": 0.0531779819254612,
      "grad_norm": 0.3033246695995331,
      "learning_rate": 0.0004867831541218638,
      "loss": 7.2402,
      "step": 178
    },
    {
      "epoch": 0.05347673463290761,
      "grad_norm": 0.29828473925590515,
      "learning_rate": 0.00048670848267622465,
      "loss": 7.3066,
      "step": 179
    },
    {
      "epoch": 0.05377548734035402,
      "grad_norm": 0.46538135409355164,
      "learning_rate": 0.0004866338112305854,
      "loss": 7.1484,
      "step": 180
    },
    {
      "epoch": 0.05407424004780043,
      "grad_norm": 0.34644702076911926,
      "learning_rate": 0.0004865591397849463,
      "loss": 7.0264,
      "step": 181
    },
    {
      "epoch": 0.054372992755246846,
      "grad_norm": 0.2750479578971863,
      "learning_rate": 0.00048648446833930703,
      "loss": 7.6006,
      "step": 182
    },
    {
      "epoch": 0.05467174546269325,
      "grad_norm": 0.39400234818458557,
      "learning_rate": 0.0004864097968936679,
      "loss": 6.7461,
      "step": 183
    },
    {
      "epoch": 0.054970498170139666,
      "grad_norm": 0.2518077492713928,
      "learning_rate": 0.00048633512544802866,
      "loss": 7.4766,
      "step": 184
    },
    {
      "epoch": 0.05526925087758608,
      "grad_norm": 0.31082457304000854,
      "learning_rate": 0.00048626045400238947,
      "loss": 7.292,
      "step": 185
    },
    {
      "epoch": 0.05556800358503249,
      "grad_norm": 0.24337168037891388,
      "learning_rate": 0.00048618578255675034,
      "loss": 7.6064,
      "step": 186
    },
    {
      "epoch": 0.0558667562924789,
      "grad_norm": 0.26724016666412354,
      "learning_rate": 0.0004861111111111111,
      "loss": 7.6484,
      "step": 187
    },
    {
      "epoch": 0.05616550899992531,
      "grad_norm": 0.33405354619026184,
      "learning_rate": 0.00048603643966547196,
      "loss": 7.5039,
      "step": 188
    },
    {
      "epoch": 0.056464261707371725,
      "grad_norm": 0.3647632300853729,
      "learning_rate": 0.0004859617682198327,
      "loss": 7.1309,
      "step": 189
    },
    {
      "epoch": 0.05676301441481813,
      "grad_norm": 0.3169994652271271,
      "learning_rate": 0.0004858870967741936,
      "loss": 7.7275,
      "step": 190
    },
    {
      "epoch": 0.057061767122264545,
      "grad_norm": 0.3262353837490082,
      "learning_rate": 0.00048581242532855435,
      "loss": 7.2393,
      "step": 191
    },
    {
      "epoch": 0.05736051982971096,
      "grad_norm": 0.28487446904182434,
      "learning_rate": 0.0004857377538829152,
      "loss": 7.4795,
      "step": 192
    },
    {
      "epoch": 0.057659272537157365,
      "grad_norm": 0.2790040373802185,
      "learning_rate": 0.000485663082437276,
      "loss": 7.5664,
      "step": 193
    },
    {
      "epoch": 0.05795802524460378,
      "grad_norm": 0.4375808537006378,
      "learning_rate": 0.0004855884109916368,
      "loss": 7.2256,
      "step": 194
    },
    {
      "epoch": 0.05825677795205019,
      "grad_norm": 0.2864578664302826,
      "learning_rate": 0.00048551373954599765,
      "loss": 7.1836,
      "step": 195
    },
    {
      "epoch": 0.058555530659496605,
      "grad_norm": 0.36548492312431335,
      "learning_rate": 0.0004854390681003584,
      "loss": 7.1211,
      "step": 196
    },
    {
      "epoch": 0.05885428336694301,
      "grad_norm": 0.4068611264228821,
      "learning_rate": 0.0004853643966547193,
      "loss": 7.3398,
      "step": 197
    },
    {
      "epoch": 0.059153036074389424,
      "grad_norm": 0.34425196051597595,
      "learning_rate": 0.00048528972520908004,
      "loss": 6.7324,
      "step": 198
    },
    {
      "epoch": 0.05945178878183584,
      "grad_norm": 0.3324277102947235,
      "learning_rate": 0.0004852150537634409,
      "loss": 7.2988,
      "step": 199
    },
    {
      "epoch": 0.059750541489282244,
      "grad_norm": 0.3078002631664276,
      "learning_rate": 0.00048514038231780166,
      "loss": 7.2334,
      "step": 200
    },
    {
      "epoch": 0.059750541489282244,
      "eval_bleu": 0.03907982849907875,
      "eval_loss": 7.1328125,
      "eval_runtime": 2607.2509,
      "eval_samples_per_second": 0.54,
      "eval_steps_per_second": 0.034,
      "step": 200
    }
  ],
  "logging_steps": 1,
  "max_steps": 6696,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 200,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 434870471884800.0,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
