{
  "best_global_step": 3400,
  "best_metric": 7.01953125,
  "best_model_checkpoint": "training-nllb-tgl-to-bicol-working\\checkpoint-3400",
  "epoch": 1.1947867652550601,
  "eval_steps": 200,
  "global_step": 4000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.00029875270744641124,
      "grad_norm": 0.10821348428726196,
      "learning_rate": 0.0005,
      "loss": 10.5107,
      "step": 1
    },
    {
      "epoch": 0.0005975054148928225,
      "grad_norm": 0.11005502939224243,
      "learning_rate": 0.0004999253285543608,
      "loss": 11.3281,
      "step": 2
    },
    {
      "epoch": 0.0008962581223392337,
      "grad_norm": 0.17715798318386078,
      "learning_rate": 0.0004998506571087216,
      "loss": 10.7266,
      "step": 3
    },
    {
      "epoch": 0.001195010829785645,
      "grad_norm": 0.18851852416992188,
      "learning_rate": 0.0004997759856630824,
      "loss": 10.7266,
      "step": 4
    },
    {
      "epoch": 0.001493763537232056,
      "grad_norm": 0.27457132935523987,
      "learning_rate": 0.0004997013142174433,
      "loss": 11.1172,
      "step": 5
    },
    {
      "epoch": 0.0017925162446784674,
      "grad_norm": 0.32356637716293335,
      "learning_rate": 0.0004996266427718041,
      "loss": 11.127,
      "step": 6
    },
    {
      "epoch": 0.002091268952124879,
      "grad_norm": 0.38698747754096985,
      "learning_rate": 0.0004995519713261649,
      "loss": 11.0762,
      "step": 7
    },
    {
      "epoch": 0.00239002165957129,
      "grad_norm": 0.4688027501106262,
      "learning_rate": 0.0004994772998805257,
      "loss": 11.0762,
      "step": 8
    },
    {
      "epoch": 0.002688774367017701,
      "grad_norm": 0.5016260147094727,
      "learning_rate": 0.0004994026284348865,
      "loss": 10.5703,
      "step": 9
    },
    {
      "epoch": 0.002987527074464112,
      "grad_norm": 0.6383640170097351,
      "learning_rate": 0.0004993279569892473,
      "loss": 10.9414,
      "step": 10
    },
    {
      "epoch": 0.0032862797819105238,
      "grad_norm": 0.6936139464378357,
      "learning_rate": 0.0004992532855436081,
      "loss": 11.3438,
      "step": 11
    },
    {
      "epoch": 0.003585032489356935,
      "grad_norm": 0.6707635521888733,
      "learning_rate": 0.000499178614097969,
      "loss": 10.9121,
      "step": 12
    },
    {
      "epoch": 0.003883785196803346,
      "grad_norm": 0.6898233890533447,
      "learning_rate": 0.0004991039426523298,
      "loss": 10.5566,
      "step": 13
    },
    {
      "epoch": 0.004182537904249758,
      "grad_norm": 0.6574143767356873,
      "learning_rate": 0.0004990292712066906,
      "loss": 11.2637,
      "step": 14
    },
    {
      "epoch": 0.004481290611696168,
      "grad_norm": 0.5881863832473755,
      "learning_rate": 0.0004989545997610514,
      "loss": 10.5469,
      "step": 15
    },
    {
      "epoch": 0.00478004331914258,
      "grad_norm": 0.5066705942153931,
      "learning_rate": 0.0004988799283154122,
      "loss": 10.5879,
      "step": 16
    },
    {
      "epoch": 0.0050787960265889906,
      "grad_norm": 0.5587491989135742,
      "learning_rate": 0.000498805256869773,
      "loss": 11.0312,
      "step": 17
    },
    {
      "epoch": 0.005377548734035402,
      "grad_norm": 0.4914754033088684,
      "learning_rate": 0.0004987305854241338,
      "loss": 10.9629,
      "step": 18
    },
    {
      "epoch": 0.005676301441481814,
      "grad_norm": 0.47820284962654114,
      "learning_rate": 0.0004986559139784946,
      "loss": 10.7637,
      "step": 19
    },
    {
      "epoch": 0.005975054148928224,
      "grad_norm": 0.6641180515289307,
      "learning_rate": 0.0004985812425328555,
      "loss": 11.5117,
      "step": 20
    },
    {
      "epoch": 0.006273806856374636,
      "grad_norm": 0.4952411949634552,
      "learning_rate": 0.0004985065710872163,
      "loss": 10.7676,
      "step": 21
    },
    {
      "epoch": 0.0065725595638210475,
      "grad_norm": 0.436328262090683,
      "learning_rate": 0.0004984318996415771,
      "loss": 9.9941,
      "step": 22
    },
    {
      "epoch": 0.006871312271267458,
      "grad_norm": 0.5016461610794067,
      "learning_rate": 0.0004983572281959379,
      "loss": 10.2441,
      "step": 23
    },
    {
      "epoch": 0.00717006497871387,
      "grad_norm": 0.48784565925598145,
      "learning_rate": 0.0004982825567502987,
      "loss": 10.7227,
      "step": 24
    },
    {
      "epoch": 0.0074688176861602805,
      "grad_norm": 0.46044033765792847,
      "learning_rate": 0.0004982078853046595,
      "loss": 9.875,
      "step": 25
    },
    {
      "epoch": 0.007767570393606692,
      "grad_norm": 0.4846183657646179,
      "learning_rate": 0.0004981332138590203,
      "loss": 10.5723,
      "step": 26
    },
    {
      "epoch": 0.008066323101053104,
      "grad_norm": 0.45062729716300964,
      "learning_rate": 0.0004980585424133811,
      "loss": 9.9922,
      "step": 27
    },
    {
      "epoch": 0.008365075808499515,
      "grad_norm": 0.5014210343360901,
      "learning_rate": 0.000497983870967742,
      "loss": 10.6836,
      "step": 28
    },
    {
      "epoch": 0.008663828515945925,
      "grad_norm": 0.5104053616523743,
      "learning_rate": 0.0004979091995221028,
      "loss": 10.3477,
      "step": 29
    },
    {
      "epoch": 0.008962581223392337,
      "grad_norm": 0.4748744070529938,
      "learning_rate": 0.0004978345280764636,
      "loss": 9.6602,
      "step": 30
    },
    {
      "epoch": 0.009261333930838748,
      "grad_norm": 0.49339559674263,
      "learning_rate": 0.0004977598566308244,
      "loss": 10.7129,
      "step": 31
    },
    {
      "epoch": 0.00956008663828516,
      "grad_norm": 0.39628931879997253,
      "learning_rate": 0.0004976851851851852,
      "loss": 9.627,
      "step": 32
    },
    {
      "epoch": 0.009858839345731571,
      "grad_norm": 0.42897528409957886,
      "learning_rate": 0.000497610513739546,
      "loss": 10.4668,
      "step": 33
    },
    {
      "epoch": 0.010157592053177981,
      "grad_norm": 0.4145178198814392,
      "learning_rate": 0.0004975358422939068,
      "loss": 10.3262,
      "step": 34
    },
    {
      "epoch": 0.010456344760624393,
      "grad_norm": 0.4588963985443115,
      "learning_rate": 0.0004974611708482676,
      "loss": 11.2031,
      "step": 35
    },
    {
      "epoch": 0.010755097468070804,
      "grad_norm": 0.4078393578529358,
      "learning_rate": 0.0004973864994026285,
      "loss": 9.9121,
      "step": 36
    },
    {
      "epoch": 0.011053850175517216,
      "grad_norm": 0.36339741945266724,
      "learning_rate": 0.0004973118279569893,
      "loss": 9.2246,
      "step": 37
    },
    {
      "epoch": 0.011352602882963627,
      "grad_norm": 0.40116608142852783,
      "learning_rate": 0.0004972371565113501,
      "loss": 9.7773,
      "step": 38
    },
    {
      "epoch": 0.011651355590410037,
      "grad_norm": 0.379043847322464,
      "learning_rate": 0.0004971624850657109,
      "loss": 9.5439,
      "step": 39
    },
    {
      "epoch": 0.011950108297856449,
      "grad_norm": 0.38495877385139465,
      "learning_rate": 0.0004970878136200717,
      "loss": 9.5215,
      "step": 40
    },
    {
      "epoch": 0.01224886100530286,
      "grad_norm": 0.41120296716690063,
      "learning_rate": 0.0004970131421744325,
      "loss": 10.4277,
      "step": 41
    },
    {
      "epoch": 0.012547613712749272,
      "grad_norm": 0.40164628624916077,
      "learning_rate": 0.0004969384707287933,
      "loss": 10.0625,
      "step": 42
    },
    {
      "epoch": 0.012846366420195683,
      "grad_norm": 0.4104005694389343,
      "learning_rate": 0.0004968637992831542,
      "loss": 9.9902,
      "step": 43
    },
    {
      "epoch": 0.013145119127642095,
      "grad_norm": 0.4140368103981018,
      "learning_rate": 0.000496789127837515,
      "loss": 9.8848,
      "step": 44
    },
    {
      "epoch": 0.013443871835088505,
      "grad_norm": 0.3986252248287201,
      "learning_rate": 0.0004967144563918758,
      "loss": 9.3008,
      "step": 45
    },
    {
      "epoch": 0.013742624542534916,
      "grad_norm": 0.40926143527030945,
      "learning_rate": 0.0004966397849462366,
      "loss": 9.6934,
      "step": 46
    },
    {
      "epoch": 0.014041377249981328,
      "grad_norm": 0.39645010232925415,
      "learning_rate": 0.0004965651135005974,
      "loss": 8.3613,
      "step": 47
    },
    {
      "epoch": 0.01434012995742774,
      "grad_norm": 0.4267260730266571,
      "learning_rate": 0.0004964904420549582,
      "loss": 9.5488,
      "step": 48
    },
    {
      "epoch": 0.014638882664874151,
      "grad_norm": 0.3833109736442566,
      "learning_rate": 0.000496415770609319,
      "loss": 8.9131,
      "step": 49
    },
    {
      "epoch": 0.014937635372320561,
      "grad_norm": 0.38322240114212036,
      "learning_rate": 0.0004963410991636798,
      "loss": 9.1816,
      "step": 50
    },
    {
      "epoch": 0.015236388079766973,
      "grad_norm": 0.41379764676094055,
      "learning_rate": 0.0004962664277180407,
      "loss": 9.498,
      "step": 51
    },
    {
      "epoch": 0.015535140787213384,
      "grad_norm": 0.36599066853523254,
      "learning_rate": 0.0004961917562724015,
      "loss": 8.71,
      "step": 52
    },
    {
      "epoch": 0.015833893494659794,
      "grad_norm": 0.3643648028373718,
      "learning_rate": 0.0004961170848267623,
      "loss": 8.5176,
      "step": 53
    },
    {
      "epoch": 0.016132646202106207,
      "grad_norm": 0.361419677734375,
      "learning_rate": 0.0004960424133811231,
      "loss": 8.8027,
      "step": 54
    },
    {
      "epoch": 0.016431398909552617,
      "grad_norm": 0.3777402341365814,
      "learning_rate": 0.0004959677419354839,
      "loss": 8.957,
      "step": 55
    },
    {
      "epoch": 0.01673015161699903,
      "grad_norm": 0.3822852373123169,
      "learning_rate": 0.0004958930704898447,
      "loss": 8.3691,
      "step": 56
    },
    {
      "epoch": 0.01702890432444544,
      "grad_norm": 0.3792678415775299,
      "learning_rate": 0.0004958183990442055,
      "loss": 9.1738,
      "step": 57
    },
    {
      "epoch": 0.01732765703189185,
      "grad_norm": 0.37306538224220276,
      "learning_rate": 0.0004957437275985663,
      "loss": 8.4873,
      "step": 58
    },
    {
      "epoch": 0.017626409739338263,
      "grad_norm": 0.38344499468803406,
      "learning_rate": 0.0004956690561529272,
      "loss": 9.1914,
      "step": 59
    },
    {
      "epoch": 0.017925162446784673,
      "grad_norm": 0.38386160135269165,
      "learning_rate": 0.000495594384707288,
      "loss": 8.8926,
      "step": 60
    },
    {
      "epoch": 0.018223915154231086,
      "grad_norm": 0.348595529794693,
      "learning_rate": 0.0004955197132616488,
      "loss": 7.8936,
      "step": 61
    },
    {
      "epoch": 0.018522667861677496,
      "grad_norm": 0.403987854719162,
      "learning_rate": 0.0004954450418160096,
      "loss": 8.7227,
      "step": 62
    },
    {
      "epoch": 0.018821420569123906,
      "grad_norm": 0.3449745774269104,
      "learning_rate": 0.0004953703703703704,
      "loss": 8.5898,
      "step": 63
    },
    {
      "epoch": 0.01912017327657032,
      "grad_norm": 0.35804587602615356,
      "learning_rate": 0.0004952956989247312,
      "loss": 8.5215,
      "step": 64
    },
    {
      "epoch": 0.01941892598401673,
      "grad_norm": 0.34505829215049744,
      "learning_rate": 0.0004952210274790919,
      "loss": 8.1533,
      "step": 65
    },
    {
      "epoch": 0.019717678691463143,
      "grad_norm": 0.34219563007354736,
      "learning_rate": 0.0004951463560334528,
      "loss": 7.9258,
      "step": 66
    },
    {
      "epoch": 0.020016431398909552,
      "grad_norm": 0.33780527114868164,
      "learning_rate": 0.0004950716845878137,
      "loss": 8.4648,
      "step": 67
    },
    {
      "epoch": 0.020315184106355962,
      "grad_norm": 0.3311116397380829,
      "learning_rate": 0.0004949970131421745,
      "loss": 8.3604,
      "step": 68
    },
    {
      "epoch": 0.020613936813802376,
      "grad_norm": 0.3329460918903351,
      "learning_rate": 0.0004949223416965353,
      "loss": 8.0234,
      "step": 69
    },
    {
      "epoch": 0.020912689521248785,
      "grad_norm": 0.3190329968929291,
      "learning_rate": 0.0004948476702508961,
      "loss": 8.4531,
      "step": 70
    },
    {
      "epoch": 0.0212114422286952,
      "grad_norm": 0.33799904584884644,
      "learning_rate": 0.0004947729988052569,
      "loss": 8.2656,
      "step": 71
    },
    {
      "epoch": 0.02151019493614161,
      "grad_norm": 0.31499767303466797,
      "learning_rate": 0.0004946983273596177,
      "loss": 8.4141,
      "step": 72
    },
    {
      "epoch": 0.02180894764358802,
      "grad_norm": 0.31318432092666626,
      "learning_rate": 0.0004946236559139785,
      "loss": 8.0254,
      "step": 73
    },
    {
      "epoch": 0.02210770035103443,
      "grad_norm": 0.3035445213317871,
      "learning_rate": 0.0004945489844683392,
      "loss": 7.7998,
      "step": 74
    },
    {
      "epoch": 0.02240645305848084,
      "grad_norm": 0.3642057776451111,
      "learning_rate": 0.0004944743130227002,
      "loss": 7.5146,
      "step": 75
    },
    {
      "epoch": 0.022705205765927255,
      "grad_norm": 0.39204663038253784,
      "learning_rate": 0.000494399641577061,
      "loss": 7.9434,
      "step": 76
    },
    {
      "epoch": 0.023003958473373665,
      "grad_norm": 0.3150036334991455,
      "learning_rate": 0.0004943249701314218,
      "loss": 7.7666,
      "step": 77
    },
    {
      "epoch": 0.023302711180820074,
      "grad_norm": 0.3041803240776062,
      "learning_rate": 0.0004942502986857826,
      "loss": 8.1553,
      "step": 78
    },
    {
      "epoch": 0.023601463888266488,
      "grad_norm": 0.32640504837036133,
      "learning_rate": 0.0004941756272401434,
      "loss": 8.2227,
      "step": 79
    },
    {
      "epoch": 0.023900216595712898,
      "grad_norm": 0.3017626404762268,
      "learning_rate": 0.0004941009557945042,
      "loss": 8.0449,
      "step": 80
    },
    {
      "epoch": 0.02419896930315931,
      "grad_norm": 0.2896050810813904,
      "learning_rate": 0.0004940262843488649,
      "loss": 7.8027,
      "step": 81
    },
    {
      "epoch": 0.02449772201060572,
      "grad_norm": 0.2856549918651581,
      "learning_rate": 0.0004939516129032259,
      "loss": 7.5742,
      "step": 82
    },
    {
      "epoch": 0.024796474718052134,
      "grad_norm": 0.2921745479106903,
      "learning_rate": 0.0004938769414575866,
      "loss": 7.7959,
      "step": 83
    },
    {
      "epoch": 0.025095227425498544,
      "grad_norm": 0.2688756585121155,
      "learning_rate": 0.0004938022700119475,
      "loss": 8.083,
      "step": 84
    },
    {
      "epoch": 0.025393980132944954,
      "grad_norm": 0.275928258895874,
      "learning_rate": 0.0004937275985663083,
      "loss": 7.583,
      "step": 85
    },
    {
      "epoch": 0.025692732840391367,
      "grad_norm": 0.272701621055603,
      "learning_rate": 0.0004936529271206691,
      "loss": 7.8018,
      "step": 86
    },
    {
      "epoch": 0.025991485547837777,
      "grad_norm": 0.2676122784614563,
      "learning_rate": 0.0004935782556750299,
      "loss": 8.0166,
      "step": 87
    },
    {
      "epoch": 0.02629023825528419,
      "grad_norm": 0.3009990453720093,
      "learning_rate": 0.0004935035842293907,
      "loss": 7.5195,
      "step": 88
    },
    {
      "epoch": 0.0265889909627306,
      "grad_norm": 0.32099971175193787,
      "learning_rate": 0.0004934289127837515,
      "loss": 7.2861,
      "step": 89
    },
    {
      "epoch": 0.02688774367017701,
      "grad_norm": 0.2675405740737915,
      "learning_rate": 0.0004933542413381122,
      "loss": 7.667,
      "step": 90
    },
    {
      "epoch": 0.027186496377623423,
      "grad_norm": 0.3011398911476135,
      "learning_rate": 0.0004932795698924732,
      "loss": 7.2764,
      "step": 91
    },
    {
      "epoch": 0.027485249085069833,
      "grad_norm": 0.24202702939510345,
      "learning_rate": 0.0004932048984468339,
      "loss": 7.7969,
      "step": 92
    },
    {
      "epoch": 0.027784001792516246,
      "grad_norm": 0.2650386393070221,
      "learning_rate": 0.0004931302270011948,
      "loss": 7.8994,
      "step": 93
    },
    {
      "epoch": 0.028082754499962656,
      "grad_norm": 0.2493761032819748,
      "learning_rate": 0.0004930555555555556,
      "loss": 7.7256,
      "step": 94
    },
    {
      "epoch": 0.028381507207409066,
      "grad_norm": 0.24684636294841766,
      "learning_rate": 0.0004929808841099164,
      "loss": 7.6885,
      "step": 95
    },
    {
      "epoch": 0.02868025991485548,
      "grad_norm": 0.32382333278656006,
      "learning_rate": 0.0004929062126642772,
      "loss": 7.3018,
      "step": 96
    },
    {
      "epoch": 0.02897901262230189,
      "grad_norm": 0.2478044331073761,
      "learning_rate": 0.0004928315412186379,
      "loss": 8.1084,
      "step": 97
    },
    {
      "epoch": 0.029277765329748302,
      "grad_norm": 0.28959137201309204,
      "learning_rate": 0.0004927568697729989,
      "loss": 7.8066,
      "step": 98
    },
    {
      "epoch": 0.029576518037194712,
      "grad_norm": 0.26750457286834717,
      "learning_rate": 0.0004926821983273596,
      "loss": 7.3613,
      "step": 99
    },
    {
      "epoch": 0.029875270744641122,
      "grad_norm": 0.24644550681114197,
      "learning_rate": 0.0004926075268817205,
      "loss": 7.8154,
      "step": 100
    },
    {
      "epoch": 0.030174023452087535,
      "grad_norm": 0.21514536440372467,
      "learning_rate": 0.0004925328554360812,
      "loss": 8.0107,
      "step": 101
    },
    {
      "epoch": 0.030472776159533945,
      "grad_norm": 0.24907834827899933,
      "learning_rate": 0.0004924581839904421,
      "loss": 7.8574,
      "step": 102
    },
    {
      "epoch": 0.03077152886698036,
      "grad_norm": 0.2640770375728607,
      "learning_rate": 0.0004923835125448029,
      "loss": 7.7686,
      "step": 103
    },
    {
      "epoch": 0.031070281574426768,
      "grad_norm": 0.3616735637187958,
      "learning_rate": 0.0004923088410991637,
      "loss": 7.6465,
      "step": 104
    },
    {
      "epoch": 0.03136903428187318,
      "grad_norm": 0.47297847270965576,
      "learning_rate": 0.0004922341696535245,
      "loss": 7.2412,
      "step": 105
    },
    {
      "epoch": 0.03166778698931959,
      "grad_norm": 0.2724394202232361,
      "learning_rate": 0.0004921594982078853,
      "loss": 7.6572,
      "step": 106
    },
    {
      "epoch": 0.031966539696766,
      "grad_norm": 0.23566237092018127,
      "learning_rate": 0.0004920848267622462,
      "loss": 7.7812,
      "step": 107
    },
    {
      "epoch": 0.032265292404212415,
      "grad_norm": 0.24848216772079468,
      "learning_rate": 0.0004920101553166069,
      "loss": 7.4004,
      "step": 108
    },
    {
      "epoch": 0.03256404511165883,
      "grad_norm": 0.3075593113899231,
      "learning_rate": 0.0004919354838709678,
      "loss": 6.8564,
      "step": 109
    },
    {
      "epoch": 0.032862797819105234,
      "grad_norm": 0.3262488543987274,
      "learning_rate": 0.0004918608124253285,
      "loss": 7.0518,
      "step": 110
    },
    {
      "epoch": 0.03316155052655165,
      "grad_norm": 0.2236112356185913,
      "learning_rate": 0.0004917861409796894,
      "loss": 7.7959,
      "step": 111
    },
    {
      "epoch": 0.03346030323399806,
      "grad_norm": 0.2831442952156067,
      "learning_rate": 0.0004917114695340502,
      "loss": 7.1348,
      "step": 112
    },
    {
      "epoch": 0.03375905594144447,
      "grad_norm": 0.24397167563438416,
      "learning_rate": 0.0004916367980884109,
      "loss": 7.8721,
      "step": 113
    },
    {
      "epoch": 0.03405780864889088,
      "grad_norm": 0.2545168697834015,
      "learning_rate": 0.0004915621266427719,
      "loss": 7.4492,
      "step": 114
    },
    {
      "epoch": 0.034356561356337294,
      "grad_norm": 0.22379270195960999,
      "learning_rate": 0.0004914874551971326,
      "loss": 7.6738,
      "step": 115
    },
    {
      "epoch": 0.0346553140637837,
      "grad_norm": 0.36304497718811035,
      "learning_rate": 0.0004914127837514935,
      "loss": 6.917,
      "step": 116
    },
    {
      "epoch": 0.03495406677123011,
      "grad_norm": 0.24335503578186035,
      "learning_rate": 0.0004913381123058542,
      "loss": 7.46,
      "step": 117
    },
    {
      "epoch": 0.03525281947867653,
      "grad_norm": 0.19890791177749634,
      "learning_rate": 0.0004912634408602151,
      "loss": 7.8145,
      "step": 118
    },
    {
      "epoch": 0.03555157218612294,
      "grad_norm": 0.2813917398452759,
      "learning_rate": 0.0004911887694145758,
      "loss": 7.0918,
      "step": 119
    },
    {
      "epoch": 0.035850324893569346,
      "grad_norm": 0.2551155686378479,
      "learning_rate": 0.0004911140979689367,
      "loss": 7.415,
      "step": 120
    },
    {
      "epoch": 0.03614907760101576,
      "grad_norm": 0.21404586732387543,
      "learning_rate": 0.0004910394265232976,
      "loss": 7.7285,
      "step": 121
    },
    {
      "epoch": 0.03644783030846217,
      "grad_norm": 0.2606382369995117,
      "learning_rate": 0.0004909647550776583,
      "loss": 7.4502,
      "step": 122
    },
    {
      "epoch": 0.03674658301590858,
      "grad_norm": 0.27905747294425964,
      "learning_rate": 0.0004908900836320192,
      "loss": 7.3135,
      "step": 123
    },
    {
      "epoch": 0.03704533572335499,
      "grad_norm": 0.27660632133483887,
      "learning_rate": 0.0004908154121863799,
      "loss": 7.1143,
      "step": 124
    },
    {
      "epoch": 0.037344088430801406,
      "grad_norm": 0.26993387937545776,
      "learning_rate": 0.0004907407407407408,
      "loss": 7.5371,
      "step": 125
    },
    {
      "epoch": 0.03764284113824781,
      "grad_norm": 0.20496590435504913,
      "learning_rate": 0.0004906660692951015,
      "loss": 7.7764,
      "step": 126
    },
    {
      "epoch": 0.037941593845694226,
      "grad_norm": 0.26359736919403076,
      "learning_rate": 0.0004905913978494624,
      "loss": 7.5264,
      "step": 127
    },
    {
      "epoch": 0.03824034655314064,
      "grad_norm": 0.2587951421737671,
      "learning_rate": 0.0004905167264038231,
      "loss": 7.4697,
      "step": 128
    },
    {
      "epoch": 0.03853909926058705,
      "grad_norm": 0.27789872884750366,
      "learning_rate": 0.000490442054958184,
      "loss": 7.0439,
      "step": 129
    },
    {
      "epoch": 0.03883785196803346,
      "grad_norm": 0.24561715126037598,
      "learning_rate": 0.0004903673835125449,
      "loss": 7.5801,
      "step": 130
    },
    {
      "epoch": 0.03913660467547987,
      "grad_norm": 0.29134175181388855,
      "learning_rate": 0.0004902927120669056,
      "loss": 7.2666,
      "step": 131
    },
    {
      "epoch": 0.039435357382926285,
      "grad_norm": 0.29421618580818176,
      "learning_rate": 0.0004902180406212665,
      "loss": 7.6748,
      "step": 132
    },
    {
      "epoch": 0.03973411009037269,
      "grad_norm": 0.30324870347976685,
      "learning_rate": 0.0004901433691756272,
      "loss": 7.2764,
      "step": 133
    },
    {
      "epoch": 0.040032862797819105,
      "grad_norm": 0.24822299182415009,
      "learning_rate": 0.0004900686977299881,
      "loss": 7.5156,
      "step": 134
    },
    {
      "epoch": 0.04033161550526552,
      "grad_norm": 0.3090725243091583,
      "learning_rate": 0.0004899940262843488,
      "loss": 7.4834,
      "step": 135
    },
    {
      "epoch": 0.040630368212711925,
      "grad_norm": 0.2992507517337799,
      "learning_rate": 0.0004899193548387097,
      "loss": 7.3252,
      "step": 136
    },
    {
      "epoch": 0.04092912092015834,
      "grad_norm": 0.3073965907096863,
      "learning_rate": 0.0004898446833930705,
      "loss": 7.2656,
      "step": 137
    },
    {
      "epoch": 0.04122787362760475,
      "grad_norm": 0.32228711247444153,
      "learning_rate": 0.0004897700119474313,
      "loss": 6.9688,
      "step": 138
    },
    {
      "epoch": 0.041526626335051164,
      "grad_norm": 0.290147066116333,
      "learning_rate": 0.0004896953405017922,
      "loss": 7.46,
      "step": 139
    },
    {
      "epoch": 0.04182537904249757,
      "grad_norm": 0.3048872947692871,
      "learning_rate": 0.0004896206690561529,
      "loss": 7.1982,
      "step": 140
    },
    {
      "epoch": 0.042124131749943984,
      "grad_norm": 0.2502696216106415,
      "learning_rate": 0.0004895459976105138,
      "loss": 7.7822,
      "step": 141
    },
    {
      "epoch": 0.0424228844573904,
      "grad_norm": 0.277056485414505,
      "learning_rate": 0.0004894713261648745,
      "loss": 6.9238,
      "step": 142
    },
    {
      "epoch": 0.042721637164836804,
      "grad_norm": 0.3534789979457855,
      "learning_rate": 0.0004893966547192354,
      "loss": 7.1211,
      "step": 143
    },
    {
      "epoch": 0.04302038987228322,
      "grad_norm": 0.2861802875995636,
      "learning_rate": 0.0004893219832735961,
      "loss": 7.2949,
      "step": 144
    },
    {
      "epoch": 0.04331914257972963,
      "grad_norm": 0.2737436890602112,
      "learning_rate": 0.000489247311827957,
      "loss": 7.168,
      "step": 145
    },
    {
      "epoch": 0.04361789528717604,
      "grad_norm": 0.3162184953689575,
      "learning_rate": 0.0004891726403823178,
      "loss": 7.1924,
      "step": 146
    },
    {
      "epoch": 0.04391664799462245,
      "grad_norm": 0.2845972776412964,
      "learning_rate": 0.0004890979689366786,
      "loss": 7.0381,
      "step": 147
    },
    {
      "epoch": 0.04421540070206886,
      "grad_norm": 0.35005950927734375,
      "learning_rate": 0.0004890232974910394,
      "loss": 7.3398,
      "step": 148
    },
    {
      "epoch": 0.04451415340951528,
      "grad_norm": 0.25128173828125,
      "learning_rate": 0.0004889486260454002,
      "loss": 7.3232,
      "step": 149
    },
    {
      "epoch": 0.04481290611696168,
      "grad_norm": 0.2506413757801056,
      "learning_rate": 0.0004888739545997611,
      "loss": 7.5654,
      "step": 150
    },
    {
      "epoch": 0.045111658824408096,
      "grad_norm": 0.3365420699119568,
      "learning_rate": 0.0004887992831541218,
      "loss": 7.0918,
      "step": 151
    },
    {
      "epoch": 0.04541041153185451,
      "grad_norm": 0.30113887786865234,
      "learning_rate": 0.0004887246117084828,
      "loss": 7.0449,
      "step": 152
    },
    {
      "epoch": 0.045709164239300916,
      "grad_norm": 0.31116804480552673,
      "learning_rate": 0.0004886499402628435,
      "loss": 7.4502,
      "step": 153
    },
    {
      "epoch": 0.04600791694674733,
      "grad_norm": 0.30247560143470764,
      "learning_rate": 0.0004885752688172043,
      "loss": 7.1523,
      "step": 154
    },
    {
      "epoch": 0.04630666965419374,
      "grad_norm": 0.21981866657733917,
      "learning_rate": 0.0004885005973715651,
      "loss": 7.5488,
      "step": 155
    },
    {
      "epoch": 0.04660542236164015,
      "grad_norm": 0.3004038631916046,
      "learning_rate": 0.0004884259259259259,
      "loss": 7.5127,
      "step": 156
    },
    {
      "epoch": 0.04690417506908656,
      "grad_norm": 0.25513529777526855,
      "learning_rate": 0.0004883512544802867,
      "loss": 7.4385,
      "step": 157
    },
    {
      "epoch": 0.047202927776532976,
      "grad_norm": 0.33559417724609375,
      "learning_rate": 0.0004882765830346476,
      "loss": 7.4717,
      "step": 158
    },
    {
      "epoch": 0.04750168048397939,
      "grad_norm": 0.26063433289527893,
      "learning_rate": 0.0004882019115890084,
      "loss": 7.6895,
      "step": 159
    },
    {
      "epoch": 0.047800433191425795,
      "grad_norm": 0.39380547404289246,
      "learning_rate": 0.0004881272401433692,
      "loss": 7.2266,
      "step": 160
    },
    {
      "epoch": 0.04809918589887221,
      "grad_norm": 0.33631765842437744,
      "learning_rate": 0.00048805256869773,
      "loss": 6.8701,
      "step": 161
    },
    {
      "epoch": 0.04839793860631862,
      "grad_norm": 0.2894502878189087,
      "learning_rate": 0.0004879778972520908,
      "loss": 7.4658,
      "step": 162
    },
    {
      "epoch": 0.04869669131376503,
      "grad_norm": 0.3700374364852905,
      "learning_rate": 0.00048790322580645164,
      "loss": 6.8213,
      "step": 163
    },
    {
      "epoch": 0.04899544402121144,
      "grad_norm": 0.2870594263076782,
      "learning_rate": 0.0004878285543608124,
      "loss": 7.2793,
      "step": 164
    },
    {
      "epoch": 0.049294196728657855,
      "grad_norm": 0.2825789153575897,
      "learning_rate": 0.00048775388291517327,
      "loss": 7.5088,
      "step": 165
    },
    {
      "epoch": 0.04959294943610427,
      "grad_norm": 0.24907280504703522,
      "learning_rate": 0.000487679211469534,
      "loss": 7.5146,
      "step": 166
    },
    {
      "epoch": 0.049891702143550674,
      "grad_norm": 0.257586807012558,
      "learning_rate": 0.0004876045400238949,
      "loss": 7.5244,
      "step": 167
    },
    {
      "epoch": 0.05019045485099709,
      "grad_norm": 0.2747199535369873,
      "learning_rate": 0.0004875298685782557,
      "loss": 7.7803,
      "step": 168
    },
    {
      "epoch": 0.0504892075584435,
      "grad_norm": 0.2723439633846283,
      "learning_rate": 0.00048745519713261647,
      "loss": 7.4814,
      "step": 169
    },
    {
      "epoch": 0.05078796026588991,
      "grad_norm": 0.3231061100959778,
      "learning_rate": 0.00048738052568697733,
      "loss": 7.7725,
      "step": 170
    },
    {
      "epoch": 0.05108671297333632,
      "grad_norm": 0.25988444685935974,
      "learning_rate": 0.0004873058542413381,
      "loss": 7.7178,
      "step": 171
    },
    {
      "epoch": 0.051385465680782734,
      "grad_norm": 0.3010958433151245,
      "learning_rate": 0.00048723118279569896,
      "loss": 7.2129,
      "step": 172
    },
    {
      "epoch": 0.05168421838822914,
      "grad_norm": 0.292570561170578,
      "learning_rate": 0.0004871565113500597,
      "loss": 7.3984,
      "step": 173
    },
    {
      "epoch": 0.051982971095675554,
      "grad_norm": 0.2666943669319153,
      "learning_rate": 0.0004870818399044206,
      "loss": 7.2725,
      "step": 174
    },
    {
      "epoch": 0.05228172380312197,
      "grad_norm": 0.3564080595970154,
      "learning_rate": 0.00048700716845878134,
      "loss": 7.4648,
      "step": 175
    },
    {
      "epoch": 0.05258047651056838,
      "grad_norm": 0.3185379207134247,
      "learning_rate": 0.0004869324970131422,
      "loss": 7.1289,
      "step": 176
    },
    {
      "epoch": 0.05287922921801479,
      "grad_norm": 0.2607346773147583,
      "learning_rate": 0.000486857825567503,
      "loss": 7.3965,
      "step": 177
    },
    {
      "epoch": 0.0531779819254612,
      "grad_norm": 0.3033246695995331,
      "learning_rate": 0.0004867831541218638,
      "loss": 7.2402,
      "step": 178
    },
    {
      "epoch": 0.05347673463290761,
      "grad_norm": 0.29828473925590515,
      "learning_rate": 0.00048670848267622465,
      "loss": 7.3066,
      "step": 179
    },
    {
      "epoch": 0.05377548734035402,
      "grad_norm": 0.46538135409355164,
      "learning_rate": 0.0004866338112305854,
      "loss": 7.1484,
      "step": 180
    },
    {
      "epoch": 0.05407424004780043,
      "grad_norm": 0.34644702076911926,
      "learning_rate": 0.0004865591397849463,
      "loss": 7.0264,
      "step": 181
    },
    {
      "epoch": 0.054372992755246846,
      "grad_norm": 0.2750479578971863,
      "learning_rate": 0.00048648446833930703,
      "loss": 7.6006,
      "step": 182
    },
    {
      "epoch": 0.05467174546269325,
      "grad_norm": 0.39400234818458557,
      "learning_rate": 0.0004864097968936679,
      "loss": 6.7461,
      "step": 183
    },
    {
      "epoch": 0.054970498170139666,
      "grad_norm": 0.2518077492713928,
      "learning_rate": 0.00048633512544802866,
      "loss": 7.4766,
      "step": 184
    },
    {
      "epoch": 0.05526925087758608,
      "grad_norm": 0.31082457304000854,
      "learning_rate": 0.00048626045400238947,
      "loss": 7.292,
      "step": 185
    },
    {
      "epoch": 0.05556800358503249,
      "grad_norm": 0.24337168037891388,
      "learning_rate": 0.00048618578255675034,
      "loss": 7.6064,
      "step": 186
    },
    {
      "epoch": 0.0558667562924789,
      "grad_norm": 0.26724016666412354,
      "learning_rate": 0.0004861111111111111,
      "loss": 7.6484,
      "step": 187
    },
    {
      "epoch": 0.05616550899992531,
      "grad_norm": 0.33405354619026184,
      "learning_rate": 0.00048603643966547196,
      "loss": 7.5039,
      "step": 188
    },
    {
      "epoch": 0.056464261707371725,
      "grad_norm": 0.3647632300853729,
      "learning_rate": 0.0004859617682198327,
      "loss": 7.1309,
      "step": 189
    },
    {
      "epoch": 0.05676301441481813,
      "grad_norm": 0.3169994652271271,
      "learning_rate": 0.0004858870967741936,
      "loss": 7.7275,
      "step": 190
    },
    {
      "epoch": 0.057061767122264545,
      "grad_norm": 0.3262353837490082,
      "learning_rate": 0.00048581242532855435,
      "loss": 7.2393,
      "step": 191
    },
    {
      "epoch": 0.05736051982971096,
      "grad_norm": 0.28487446904182434,
      "learning_rate": 0.0004857377538829152,
      "loss": 7.4795,
      "step": 192
    },
    {
      "epoch": 0.057659272537157365,
      "grad_norm": 0.2790040373802185,
      "learning_rate": 0.000485663082437276,
      "loss": 7.5664,
      "step": 193
    },
    {
      "epoch": 0.05795802524460378,
      "grad_norm": 0.4375808537006378,
      "learning_rate": 0.0004855884109916368,
      "loss": 7.2256,
      "step": 194
    },
    {
      "epoch": 0.05825677795205019,
      "grad_norm": 0.2864578664302826,
      "learning_rate": 0.00048551373954599765,
      "loss": 7.1836,
      "step": 195
    },
    {
      "epoch": 0.058555530659496605,
      "grad_norm": 0.36548492312431335,
      "learning_rate": 0.0004854390681003584,
      "loss": 7.1211,
      "step": 196
    },
    {
      "epoch": 0.05885428336694301,
      "grad_norm": 0.4068611264228821,
      "learning_rate": 0.0004853643966547193,
      "loss": 7.3398,
      "step": 197
    },
    {
      "epoch": 0.059153036074389424,
      "grad_norm": 0.34425196051597595,
      "learning_rate": 0.00048528972520908004,
      "loss": 6.7324,
      "step": 198
    },
    {
      "epoch": 0.05945178878183584,
      "grad_norm": 0.3324277102947235,
      "learning_rate": 0.0004852150537634409,
      "loss": 7.2988,
      "step": 199
    },
    {
      "epoch": 0.059750541489282244,
      "grad_norm": 0.3078002631664276,
      "learning_rate": 0.00048514038231780166,
      "loss": 7.2334,
      "step": 200
    },
    {
      "epoch": 0.059750541489282244,
      "eval_bleu": 0.03907982849907875,
      "eval_loss": 7.1328125,
      "eval_runtime": 2607.2509,
      "eval_samples_per_second": 0.54,
      "eval_steps_per_second": 0.034,
      "step": 200
    },
    {
      "epoch": 0.06004929419672866,
      "grad_norm": 0.3123011589050293,
      "learning_rate": 0.0004850657108721625,
      "loss": 7.2529,
      "step": 201
    },
    {
      "epoch": 0.06034804690417507,
      "grad_norm": 0.2943539619445801,
      "learning_rate": 0.0004849910394265233,
      "loss": 7.2393,
      "step": 202
    },
    {
      "epoch": 0.06064679961162148,
      "grad_norm": 0.2779425084590912,
      "learning_rate": 0.0004849163679808841,
      "loss": 7.3496,
      "step": 203
    },
    {
      "epoch": 0.06094555231906789,
      "grad_norm": 0.29054632782936096,
      "learning_rate": 0.00048484169653524497,
      "loss": 7.4219,
      "step": 204
    },
    {
      "epoch": 0.061244305026514304,
      "grad_norm": 0.26427578926086426,
      "learning_rate": 0.00048476702508960573,
      "loss": 7.4111,
      "step": 205
    },
    {
      "epoch": 0.06154305773396072,
      "grad_norm": 0.3190177083015442,
      "learning_rate": 0.0004846923536439666,
      "loss": 7.2773,
      "step": 206
    },
    {
      "epoch": 0.06184181044140712,
      "grad_norm": 0.28507933020591736,
      "learning_rate": 0.00048461768219832735,
      "loss": 7.21,
      "step": 207
    },
    {
      "epoch": 0.062140563148853536,
      "grad_norm": 0.28055888414382935,
      "learning_rate": 0.0004845430107526882,
      "loss": 7.5439,
      "step": 208
    },
    {
      "epoch": 0.06243931585629995,
      "grad_norm": 0.305084228515625,
      "learning_rate": 0.000484468339307049,
      "loss": 7.2793,
      "step": 209
    },
    {
      "epoch": 0.06273806856374636,
      "grad_norm": 0.3277842700481415,
      "learning_rate": 0.0004843936678614098,
      "loss": 7.7324,
      "step": 210
    },
    {
      "epoch": 0.06303682127119277,
      "grad_norm": 0.2979891300201416,
      "learning_rate": 0.0004843189964157706,
      "loss": 7.0898,
      "step": 211
    },
    {
      "epoch": 0.06333557397863918,
      "grad_norm": 0.31598925590515137,
      "learning_rate": 0.0004842443249701314,
      "loss": 7.291,
      "step": 212
    },
    {
      "epoch": 0.0636343266860856,
      "grad_norm": 0.2918887734413147,
      "learning_rate": 0.0004841696535244923,
      "loss": 7.4854,
      "step": 213
    },
    {
      "epoch": 0.063933079393532,
      "grad_norm": 0.30401235818862915,
      "learning_rate": 0.00048409498207885304,
      "loss": 7.2002,
      "step": 214
    },
    {
      "epoch": 0.06423183210097841,
      "grad_norm": 0.3682520389556885,
      "learning_rate": 0.0004840203106332139,
      "loss": 7.2324,
      "step": 215
    },
    {
      "epoch": 0.06453058480842483,
      "grad_norm": 0.3752498924732208,
      "learning_rate": 0.00048394563918757467,
      "loss": 7.0205,
      "step": 216
    },
    {
      "epoch": 0.06482933751587124,
      "grad_norm": 0.3327663838863373,
      "learning_rate": 0.0004838709677419355,
      "loss": 7.4141,
      "step": 217
    },
    {
      "epoch": 0.06512809022331766,
      "grad_norm": 0.38402241468429565,
      "learning_rate": 0.0004837962962962963,
      "loss": 6.9395,
      "step": 218
    },
    {
      "epoch": 0.06542684293076406,
      "grad_norm": 0.271368145942688,
      "learning_rate": 0.0004837216248506571,
      "loss": 7.5869,
      "step": 219
    },
    {
      "epoch": 0.06572559563821047,
      "grad_norm": 0.25743579864501953,
      "learning_rate": 0.0004836469534050179,
      "loss": 7.5557,
      "step": 220
    },
    {
      "epoch": 0.06602434834565689,
      "grad_norm": 0.28932467103004456,
      "learning_rate": 0.00048357228195937873,
      "loss": 7.6787,
      "step": 221
    },
    {
      "epoch": 0.0663231010531033,
      "grad_norm": 0.33489927649497986,
      "learning_rate": 0.0004834976105137396,
      "loss": 7.4746,
      "step": 222
    },
    {
      "epoch": 0.0666218537605497,
      "grad_norm": 0.3144931197166443,
      "learning_rate": 0.00048342293906810036,
      "loss": 7.459,
      "step": 223
    },
    {
      "epoch": 0.06692060646799612,
      "grad_norm": 0.3177618980407715,
      "learning_rate": 0.00048334826762246123,
      "loss": 7.585,
      "step": 224
    },
    {
      "epoch": 0.06721935917544253,
      "grad_norm": 0.40274351835250854,
      "learning_rate": 0.000483273596176822,
      "loss": 7.2705,
      "step": 225
    },
    {
      "epoch": 0.06751811188288893,
      "grad_norm": 0.3625737130641937,
      "learning_rate": 0.0004831989247311828,
      "loss": 7.0977,
      "step": 226
    },
    {
      "epoch": 0.06781686459033535,
      "grad_norm": 0.3747721016407013,
      "learning_rate": 0.0004831242532855436,
      "loss": 7.3496,
      "step": 227
    },
    {
      "epoch": 0.06811561729778176,
      "grad_norm": 0.30746516585350037,
      "learning_rate": 0.0004830495818399044,
      "loss": 7.4443,
      "step": 228
    },
    {
      "epoch": 0.06841437000522817,
      "grad_norm": 0.35161441564559937,
      "learning_rate": 0.00048297491039426524,
      "loss": 6.709,
      "step": 229
    },
    {
      "epoch": 0.06871312271267459,
      "grad_norm": 0.3419186770915985,
      "learning_rate": 0.00048290023894862605,
      "loss": 7.2734,
      "step": 230
    },
    {
      "epoch": 0.069011875420121,
      "grad_norm": 0.32175663113594055,
      "learning_rate": 0.0004828255675029869,
      "loss": 7.5088,
      "step": 231
    },
    {
      "epoch": 0.0693106281275674,
      "grad_norm": 0.310066819190979,
      "learning_rate": 0.0004827508960573477,
      "loss": 7.2734,
      "step": 232
    },
    {
      "epoch": 0.06960938083501382,
      "grad_norm": 0.38308191299438477,
      "learning_rate": 0.0004826762246117085,
      "loss": 7.2852,
      "step": 233
    },
    {
      "epoch": 0.06990813354246023,
      "grad_norm": 0.33334752917289734,
      "learning_rate": 0.0004826015531660693,
      "loss": 7.2979,
      "step": 234
    },
    {
      "epoch": 0.07020688624990663,
      "grad_norm": 0.3677629828453064,
      "learning_rate": 0.0004825268817204301,
      "loss": 6.959,
      "step": 235
    },
    {
      "epoch": 0.07050563895735305,
      "grad_norm": 0.6028388142585754,
      "learning_rate": 0.00048245221027479093,
      "loss": 6.6934,
      "step": 236
    },
    {
      "epoch": 0.07080439166479946,
      "grad_norm": 0.30435067415237427,
      "learning_rate": 0.00048237753882915174,
      "loss": 7.4219,
      "step": 237
    },
    {
      "epoch": 0.07110314437224588,
      "grad_norm": 0.3424874246120453,
      "learning_rate": 0.00048230286738351255,
      "loss": 6.9922,
      "step": 238
    },
    {
      "epoch": 0.07140189707969229,
      "grad_norm": 0.38234105706214905,
      "learning_rate": 0.00048222819593787337,
      "loss": 7.5273,
      "step": 239
    },
    {
      "epoch": 0.07170064978713869,
      "grad_norm": 0.34200963377952576,
      "learning_rate": 0.0004821535244922341,
      "loss": 7.0391,
      "step": 240
    },
    {
      "epoch": 0.07199940249458511,
      "grad_norm": 0.38949450850486755,
      "learning_rate": 0.000482078853046595,
      "loss": 7.6494,
      "step": 241
    },
    {
      "epoch": 0.07229815520203152,
      "grad_norm": 0.3774808943271637,
      "learning_rate": 0.0004820041816009558,
      "loss": 6.9707,
      "step": 242
    },
    {
      "epoch": 0.07259690790947793,
      "grad_norm": 0.3394872844219208,
      "learning_rate": 0.0004819295101553166,
      "loss": 7.375,
      "step": 243
    },
    {
      "epoch": 0.07289566061692435,
      "grad_norm": 0.2433769851922989,
      "learning_rate": 0.00048185483870967743,
      "loss": 7.415,
      "step": 244
    },
    {
      "epoch": 0.07319441332437075,
      "grad_norm": 0.5029091238975525,
      "learning_rate": 0.00048178016726403824,
      "loss": 6.7168,
      "step": 245
    },
    {
      "epoch": 0.07349316603181716,
      "grad_norm": 0.2827988564968109,
      "learning_rate": 0.00048170549581839906,
      "loss": 7.499,
      "step": 246
    },
    {
      "epoch": 0.07379191873926358,
      "grad_norm": 0.3065880537033081,
      "learning_rate": 0.00048163082437275987,
      "loss": 7.5312,
      "step": 247
    },
    {
      "epoch": 0.07409067144670999,
      "grad_norm": 0.2757122218608856,
      "learning_rate": 0.0004815561529271207,
      "loss": 7.4023,
      "step": 248
    },
    {
      "epoch": 0.07438942415415639,
      "grad_norm": 0.35145482420921326,
      "learning_rate": 0.00048148148148148144,
      "loss": 7.1299,
      "step": 249
    },
    {
      "epoch": 0.07468817686160281,
      "grad_norm": 0.3465346097946167,
      "learning_rate": 0.0004814068100358423,
      "loss": 7.1553,
      "step": 250
    },
    {
      "epoch": 0.07498692956904922,
      "grad_norm": 0.32320737838745117,
      "learning_rate": 0.0004813321385902031,
      "loss": 7.1562,
      "step": 251
    },
    {
      "epoch": 0.07528568227649562,
      "grad_norm": 0.2975021004676819,
      "learning_rate": 0.00048125746714456393,
      "loss": 7.1943,
      "step": 252
    },
    {
      "epoch": 0.07558443498394204,
      "grad_norm": 0.30543196201324463,
      "learning_rate": 0.00048118279569892475,
      "loss": 7.6523,
      "step": 253
    },
    {
      "epoch": 0.07588318769138845,
      "grad_norm": 0.31832584738731384,
      "learning_rate": 0.00048110812425328556,
      "loss": 7.7295,
      "step": 254
    },
    {
      "epoch": 0.07618194039883486,
      "grad_norm": 0.45462581515312195,
      "learning_rate": 0.00048103345280764637,
      "loss": 6.6504,
      "step": 255
    },
    {
      "epoch": 0.07648069310628128,
      "grad_norm": 0.29367244243621826,
      "learning_rate": 0.00048095878136200713,
      "loss": 7.5576,
      "step": 256
    },
    {
      "epoch": 0.07677944581372768,
      "grad_norm": 0.3785807490348816,
      "learning_rate": 0.000480884109916368,
      "loss": 6.9893,
      "step": 257
    },
    {
      "epoch": 0.0770781985211741,
      "grad_norm": 0.42041096091270447,
      "learning_rate": 0.00048080943847072876,
      "loss": 7.0098,
      "step": 258
    },
    {
      "epoch": 0.07737695122862051,
      "grad_norm": 0.4015696346759796,
      "learning_rate": 0.0004807347670250896,
      "loss": 6.8574,
      "step": 259
    },
    {
      "epoch": 0.07767570393606692,
      "grad_norm": 0.31781068444252014,
      "learning_rate": 0.00048066009557945044,
      "loss": 7.1729,
      "step": 260
    },
    {
      "epoch": 0.07797445664351334,
      "grad_norm": 0.33713677525520325,
      "learning_rate": 0.00048058542413381125,
      "loss": 7.2793,
      "step": 261
    },
    {
      "epoch": 0.07827320935095974,
      "grad_norm": 0.32415229082107544,
      "learning_rate": 0.00048051075268817206,
      "loss": 7.2158,
      "step": 262
    },
    {
      "epoch": 0.07857196205840615,
      "grad_norm": 0.3313237428665161,
      "learning_rate": 0.0004804360812425329,
      "loss": 7.7979,
      "step": 263
    },
    {
      "epoch": 0.07887071476585257,
      "grad_norm": 0.28631895780563354,
      "learning_rate": 0.0004803614097968937,
      "loss": 7.6855,
      "step": 264
    },
    {
      "epoch": 0.07916946747329898,
      "grad_norm": 0.34740912914276123,
      "learning_rate": 0.00048028673835125445,
      "loss": 7.2217,
      "step": 265
    },
    {
      "epoch": 0.07946822018074538,
      "grad_norm": 0.2941768765449524,
      "learning_rate": 0.0004802120669056153,
      "loss": 7.4521,
      "step": 266
    },
    {
      "epoch": 0.0797669728881918,
      "grad_norm": 0.3617632985115051,
      "learning_rate": 0.0004801373954599761,
      "loss": 7.1104,
      "step": 267
    },
    {
      "epoch": 0.08006572559563821,
      "grad_norm": 0.31673726439476013,
      "learning_rate": 0.00048006272401433694,
      "loss": 7.0762,
      "step": 268
    },
    {
      "epoch": 0.08036447830308462,
      "grad_norm": 0.3049163818359375,
      "learning_rate": 0.00047998805256869775,
      "loss": 7.334,
      "step": 269
    },
    {
      "epoch": 0.08066323101053104,
      "grad_norm": 0.3509286940097809,
      "learning_rate": 0.00047991338112305857,
      "loss": 7.0996,
      "step": 270
    },
    {
      "epoch": 0.08096198371797744,
      "grad_norm": 0.36868366599082947,
      "learning_rate": 0.0004798387096774194,
      "loss": 6.9736,
      "step": 271
    },
    {
      "epoch": 0.08126073642542385,
      "grad_norm": 0.4257866442203522,
      "learning_rate": 0.00047976403823178014,
      "loss": 6.5889,
      "step": 272
    },
    {
      "epoch": 0.08155948913287027,
      "grad_norm": 0.33760908246040344,
      "learning_rate": 0.000479689366786141,
      "loss": 6.8174,
      "step": 273
    },
    {
      "epoch": 0.08185824184031668,
      "grad_norm": 0.3890152871608734,
      "learning_rate": 0.00047961469534050176,
      "loss": 7.4414,
      "step": 274
    },
    {
      "epoch": 0.0821569945477631,
      "grad_norm": 0.4200459420681,
      "learning_rate": 0.00047954002389486263,
      "loss": 6.8447,
      "step": 275
    },
    {
      "epoch": 0.0824557472552095,
      "grad_norm": 0.32538771629333496,
      "learning_rate": 0.0004794653524492234,
      "loss": 7.3564,
      "step": 276
    },
    {
      "epoch": 0.08275449996265591,
      "grad_norm": 0.3766273856163025,
      "learning_rate": 0.00047939068100358426,
      "loss": 6.8135,
      "step": 277
    },
    {
      "epoch": 0.08305325267010233,
      "grad_norm": 0.2985558807849884,
      "learning_rate": 0.00047931600955794507,
      "loss": 7.3838,
      "step": 278
    },
    {
      "epoch": 0.08335200537754874,
      "grad_norm": 0.3554684817790985,
      "learning_rate": 0.0004792413381123059,
      "loss": 7.5234,
      "step": 279
    },
    {
      "epoch": 0.08365075808499514,
      "grad_norm": 0.31132543087005615,
      "learning_rate": 0.0004791666666666667,
      "loss": 7.2695,
      "step": 280
    },
    {
      "epoch": 0.08394951079244156,
      "grad_norm": 0.36146849393844604,
      "learning_rate": 0.00047909199522102745,
      "loss": 7.4658,
      "step": 281
    },
    {
      "epoch": 0.08424826349988797,
      "grad_norm": 0.32686829566955566,
      "learning_rate": 0.0004790173237753883,
      "loss": 7.4521,
      "step": 282
    },
    {
      "epoch": 0.08454701620733437,
      "grad_norm": 0.28750407695770264,
      "learning_rate": 0.0004789426523297491,
      "loss": 7.3955,
      "step": 283
    },
    {
      "epoch": 0.0848457689147808,
      "grad_norm": 0.36340340971946716,
      "learning_rate": 0.00047886798088410995,
      "loss": 7.5283,
      "step": 284
    },
    {
      "epoch": 0.0851445216222272,
      "grad_norm": 0.29256385564804077,
      "learning_rate": 0.0004787933094384707,
      "loss": 7.7695,
      "step": 285
    },
    {
      "epoch": 0.08544327432967361,
      "grad_norm": 0.29111194610595703,
      "learning_rate": 0.00047871863799283157,
      "loss": 7.4336,
      "step": 286
    },
    {
      "epoch": 0.08574202703712003,
      "grad_norm": 0.38031360507011414,
      "learning_rate": 0.0004786439665471924,
      "loss": 7.3438,
      "step": 287
    },
    {
      "epoch": 0.08604077974456643,
      "grad_norm": 0.5033569931983948,
      "learning_rate": 0.00047856929510155314,
      "loss": 6.9629,
      "step": 288
    },
    {
      "epoch": 0.08633953245201284,
      "grad_norm": 0.32559508085250854,
      "learning_rate": 0.000478494623655914,
      "loss": 7.1074,
      "step": 289
    },
    {
      "epoch": 0.08663828515945926,
      "grad_norm": 0.35896870493888855,
      "learning_rate": 0.00047841995221027477,
      "loss": 7.1719,
      "step": 290
    },
    {
      "epoch": 0.08693703786690567,
      "grad_norm": 0.3534773588180542,
      "learning_rate": 0.00047834528076463564,
      "loss": 7.4482,
      "step": 291
    },
    {
      "epoch": 0.08723579057435207,
      "grad_norm": 0.3783564567565918,
      "learning_rate": 0.0004782706093189964,
      "loss": 7.1924,
      "step": 292
    },
    {
      "epoch": 0.0875345432817985,
      "grad_norm": 0.33855393528938293,
      "learning_rate": 0.00047819593787335726,
      "loss": 7.1904,
      "step": 293
    },
    {
      "epoch": 0.0878332959892449,
      "grad_norm": 0.2666965126991272,
      "learning_rate": 0.000478121266427718,
      "loss": 7.3926,
      "step": 294
    },
    {
      "epoch": 0.08813204869669132,
      "grad_norm": 0.328940749168396,
      "learning_rate": 0.0004780465949820789,
      "loss": 7.2334,
      "step": 295
    },
    {
      "epoch": 0.08843080140413773,
      "grad_norm": 0.35646528005599976,
      "learning_rate": 0.0004779719235364397,
      "loss": 7.416,
      "step": 296
    },
    {
      "epoch": 0.08872955411158413,
      "grad_norm": 0.41437315940856934,
      "learning_rate": 0.00047789725209080046,
      "loss": 7.0273,
      "step": 297
    },
    {
      "epoch": 0.08902830681903055,
      "grad_norm": 0.32371196150779724,
      "learning_rate": 0.0004778225806451613,
      "loss": 7.1855,
      "step": 298
    },
    {
      "epoch": 0.08932705952647696,
      "grad_norm": 0.286600798368454,
      "learning_rate": 0.0004777479091995221,
      "loss": 7.3711,
      "step": 299
    },
    {
      "epoch": 0.08962581223392337,
      "grad_norm": 0.36828580498695374,
      "learning_rate": 0.00047767323775388295,
      "loss": 6.6221,
      "step": 300
    },
    {
      "epoch": 0.08992456494136979,
      "grad_norm": 0.3530673384666443,
      "learning_rate": 0.0004775985663082437,
      "loss": 7.2061,
      "step": 301
    },
    {
      "epoch": 0.09022331764881619,
      "grad_norm": 0.30934393405914307,
      "learning_rate": 0.0004775238948626046,
      "loss": 7.2178,
      "step": 302
    },
    {
      "epoch": 0.0905220703562626,
      "grad_norm": 0.3234090805053711,
      "learning_rate": 0.00047744922341696534,
      "loss": 7.6318,
      "step": 303
    },
    {
      "epoch": 0.09082082306370902,
      "grad_norm": 0.41697925329208374,
      "learning_rate": 0.00047737455197132615,
      "loss": 6.9072,
      "step": 304
    },
    {
      "epoch": 0.09111957577115543,
      "grad_norm": 0.40609318017959595,
      "learning_rate": 0.000477299880525687,
      "loss": 7.4297,
      "step": 305
    },
    {
      "epoch": 0.09141832847860183,
      "grad_norm": 0.33139768242836,
      "learning_rate": 0.0004772252090800478,
      "loss": 7.3711,
      "step": 306
    },
    {
      "epoch": 0.09171708118604825,
      "grad_norm": 0.3366144597530365,
      "learning_rate": 0.00047715053763440864,
      "loss": 7.4678,
      "step": 307
    },
    {
      "epoch": 0.09201583389349466,
      "grad_norm": 0.373856782913208,
      "learning_rate": 0.0004770758661887694,
      "loss": 6.9414,
      "step": 308
    },
    {
      "epoch": 0.09231458660094106,
      "grad_norm": 0.3595760762691498,
      "learning_rate": 0.00047700119474313027,
      "loss": 7.2773,
      "step": 309
    },
    {
      "epoch": 0.09261333930838749,
      "grad_norm": 0.38620641827583313,
      "learning_rate": 0.000476926523297491,
      "loss": 7.0107,
      "step": 310
    },
    {
      "epoch": 0.09291209201583389,
      "grad_norm": 0.28860586881637573,
      "learning_rate": 0.0004768518518518519,
      "loss": 7.3516,
      "step": 311
    },
    {
      "epoch": 0.0932108447232803,
      "grad_norm": 0.3319552540779114,
      "learning_rate": 0.00047677718040621265,
      "loss": 7.6865,
      "step": 312
    },
    {
      "epoch": 0.09350959743072672,
      "grad_norm": 0.35501229763031006,
      "learning_rate": 0.00047670250896057347,
      "loss": 7.1748,
      "step": 313
    },
    {
      "epoch": 0.09380835013817312,
      "grad_norm": 0.3376871347427368,
      "learning_rate": 0.00047662783751493433,
      "loss": 7.5293,
      "step": 314
    },
    {
      "epoch": 0.09410710284561954,
      "grad_norm": 0.34964245557785034,
      "learning_rate": 0.0004765531660692951,
      "loss": 7.1279,
      "step": 315
    },
    {
      "epoch": 0.09440585555306595,
      "grad_norm": 0.2928626835346222,
      "learning_rate": 0.00047647849462365596,
      "loss": 7.4678,
      "step": 316
    },
    {
      "epoch": 0.09470460826051236,
      "grad_norm": 0.27204424142837524,
      "learning_rate": 0.0004764038231780167,
      "loss": 7.457,
      "step": 317
    },
    {
      "epoch": 0.09500336096795878,
      "grad_norm": 0.34205707907676697,
      "learning_rate": 0.0004763291517323776,
      "loss": 7.3604,
      "step": 318
    },
    {
      "epoch": 0.09530211367540518,
      "grad_norm": 0.43689972162246704,
      "learning_rate": 0.00047625448028673834,
      "loss": 6.9404,
      "step": 319
    },
    {
      "epoch": 0.09560086638285159,
      "grad_norm": 0.34572312235832214,
      "learning_rate": 0.00047617980884109916,
      "loss": 7.1553,
      "step": 320
    },
    {
      "epoch": 0.09589961909029801,
      "grad_norm": 0.5221048593521118,
      "learning_rate": 0.00047610513739545997,
      "loss": 6.8701,
      "step": 321
    },
    {
      "epoch": 0.09619837179774442,
      "grad_norm": 0.32879388332366943,
      "learning_rate": 0.0004760304659498208,
      "loss": 7.1484,
      "step": 322
    },
    {
      "epoch": 0.09649712450519082,
      "grad_norm": 0.335427850484848,
      "learning_rate": 0.00047595579450418165,
      "loss": 7.4492,
      "step": 323
    },
    {
      "epoch": 0.09679587721263724,
      "grad_norm": 0.32932671904563904,
      "learning_rate": 0.0004758811230585424,
      "loss": 7.2305,
      "step": 324
    },
    {
      "epoch": 0.09709462992008365,
      "grad_norm": 0.4528493881225586,
      "learning_rate": 0.0004758064516129033,
      "loss": 6.5498,
      "step": 325
    },
    {
      "epoch": 0.09739338262753006,
      "grad_norm": 0.43360480666160583,
      "learning_rate": 0.00047573178016726403,
      "loss": 6.8643,
      "step": 326
    },
    {
      "epoch": 0.09769213533497648,
      "grad_norm": 0.37920624017715454,
      "learning_rate": 0.0004756571087216249,
      "loss": 6.8877,
      "step": 327
    },
    {
      "epoch": 0.09799088804242288,
      "grad_norm": 0.2969019114971161,
      "learning_rate": 0.00047558243727598566,
      "loss": 7.5674,
      "step": 328
    },
    {
      "epoch": 0.09828964074986929,
      "grad_norm": 0.35542958974838257,
      "learning_rate": 0.00047550776583034647,
      "loss": 7.251,
      "step": 329
    },
    {
      "epoch": 0.09858839345731571,
      "grad_norm": 0.4044097065925598,
      "learning_rate": 0.0004754330943847073,
      "loss": 6.9648,
      "step": 330
    },
    {
      "epoch": 0.09888714616476212,
      "grad_norm": 0.3543989956378937,
      "learning_rate": 0.0004753584229390681,
      "loss": 7.2373,
      "step": 331
    },
    {
      "epoch": 0.09918589887220854,
      "grad_norm": 0.39775583148002625,
      "learning_rate": 0.0004752837514934289,
      "loss": 6.7412,
      "step": 332
    },
    {
      "epoch": 0.09948465157965494,
      "grad_norm": 0.38995787501335144,
      "learning_rate": 0.0004752090800477897,
      "loss": 7.3545,
      "step": 333
    },
    {
      "epoch": 0.09978340428710135,
      "grad_norm": 0.3022054433822632,
      "learning_rate": 0.0004751344086021506,
      "loss": 7.4277,
      "step": 334
    },
    {
      "epoch": 0.10008215699454777,
      "grad_norm": 0.2856612503528595,
      "learning_rate": 0.00047505973715651135,
      "loss": 7.6006,
      "step": 335
    },
    {
      "epoch": 0.10038090970199418,
      "grad_norm": 0.3730432689189911,
      "learning_rate": 0.00047498506571087216,
      "loss": 7.1973,
      "step": 336
    },
    {
      "epoch": 0.10067966240944058,
      "grad_norm": 0.3948811888694763,
      "learning_rate": 0.000474910394265233,
      "loss": 6.9893,
      "step": 337
    },
    {
      "epoch": 0.100978415116887,
      "grad_norm": 0.31810784339904785,
      "learning_rate": 0.0004748357228195938,
      "loss": 7.5664,
      "step": 338
    },
    {
      "epoch": 0.10127716782433341,
      "grad_norm": 0.33774954080581665,
      "learning_rate": 0.0004747610513739546,
      "loss": 7.0801,
      "step": 339
    },
    {
      "epoch": 0.10157592053177981,
      "grad_norm": 0.3480311632156372,
      "learning_rate": 0.0004746863799283154,
      "loss": 7.2236,
      "step": 340
    },
    {
      "epoch": 0.10187467323922623,
      "grad_norm": 0.4085802733898163,
      "learning_rate": 0.0004746117084826762,
      "loss": 7.1309,
      "step": 341
    },
    {
      "epoch": 0.10217342594667264,
      "grad_norm": 0.3025522232055664,
      "learning_rate": 0.00047453703703703704,
      "loss": 7.1445,
      "step": 342
    },
    {
      "epoch": 0.10247217865411905,
      "grad_norm": 0.4028434753417969,
      "learning_rate": 0.0004744623655913979,
      "loss": 7.0723,
      "step": 343
    },
    {
      "epoch": 0.10277093136156547,
      "grad_norm": 0.34659409523010254,
      "learning_rate": 0.00047438769414575866,
      "loss": 7.4443,
      "step": 344
    },
    {
      "epoch": 0.10306968406901187,
      "grad_norm": 0.33226320147514343,
      "learning_rate": 0.0004743130227001195,
      "loss": 7.542,
      "step": 345
    },
    {
      "epoch": 0.10336843677645828,
      "grad_norm": 0.4248802363872528,
      "learning_rate": 0.0004742383512544803,
      "loss": 7.4424,
      "step": 346
    },
    {
      "epoch": 0.1036671894839047,
      "grad_norm": 0.3419833481311798,
      "learning_rate": 0.0004741636798088411,
      "loss": 7.3096,
      "step": 347
    },
    {
      "epoch": 0.10396594219135111,
      "grad_norm": 0.42626187205314636,
      "learning_rate": 0.0004740890083632019,
      "loss": 6.5156,
      "step": 348
    },
    {
      "epoch": 0.10426469489879751,
      "grad_norm": 0.3002922534942627,
      "learning_rate": 0.00047401433691756273,
      "loss": 7.5244,
      "step": 349
    },
    {
      "epoch": 0.10456344760624393,
      "grad_norm": 0.30830204486846924,
      "learning_rate": 0.00047393966547192354,
      "loss": 7.0938,
      "step": 350
    },
    {
      "epoch": 0.10486220031369034,
      "grad_norm": 0.3372485339641571,
      "learning_rate": 0.00047386499402628435,
      "loss": 7.4814,
      "step": 351
    },
    {
      "epoch": 0.10516095302113676,
      "grad_norm": 0.39707690477371216,
      "learning_rate": 0.00047379032258064517,
      "loss": 6.9561,
      "step": 352
    },
    {
      "epoch": 0.10545970572858317,
      "grad_norm": 0.32670754194259644,
      "learning_rate": 0.000473715651135006,
      "loss": 7.4912,
      "step": 353
    },
    {
      "epoch": 0.10575845843602957,
      "grad_norm": 0.32411718368530273,
      "learning_rate": 0.0004736409796893668,
      "loss": 7.1201,
      "step": 354
    },
    {
      "epoch": 0.106057211143476,
      "grad_norm": 0.30876603722572327,
      "learning_rate": 0.0004735663082437276,
      "loss": 7.6797,
      "step": 355
    },
    {
      "epoch": 0.1063559638509224,
      "grad_norm": 0.3555131256580353,
      "learning_rate": 0.0004734916367980884,
      "loss": 7.4648,
      "step": 356
    },
    {
      "epoch": 0.1066547165583688,
      "grad_norm": 0.38018181920051575,
      "learning_rate": 0.00047341696535244923,
      "loss": 7.1465,
      "step": 357
    },
    {
      "epoch": 0.10695346926581523,
      "grad_norm": 0.47448158264160156,
      "learning_rate": 0.00047334229390681004,
      "loss": 6.9893,
      "step": 358
    },
    {
      "epoch": 0.10725222197326163,
      "grad_norm": 0.3358205258846283,
      "learning_rate": 0.00047326762246117086,
      "loss": 7.1406,
      "step": 359
    },
    {
      "epoch": 0.10755097468070804,
      "grad_norm": 0.33809614181518555,
      "learning_rate": 0.00047319295101553167,
      "loss": 7.1885,
      "step": 360
    },
    {
      "epoch": 0.10784972738815446,
      "grad_norm": 0.3168056905269623,
      "learning_rate": 0.0004731182795698925,
      "loss": 7.5479,
      "step": 361
    },
    {
      "epoch": 0.10814848009560087,
      "grad_norm": 0.46827954053878784,
      "learning_rate": 0.0004730436081242533,
      "loss": 6.7139,
      "step": 362
    },
    {
      "epoch": 0.10844723280304727,
      "grad_norm": 0.40343886613845825,
      "learning_rate": 0.0004729689366786141,
      "loss": 6.8506,
      "step": 363
    },
    {
      "epoch": 0.10874598551049369,
      "grad_norm": 0.3390718102455139,
      "learning_rate": 0.0004728942652329749,
      "loss": 7.123,
      "step": 364
    },
    {
      "epoch": 0.1090447382179401,
      "grad_norm": 0.43942832946777344,
      "learning_rate": 0.00047281959378733574,
      "loss": 7.5029,
      "step": 365
    },
    {
      "epoch": 0.1093434909253865,
      "grad_norm": 0.3825077414512634,
      "learning_rate": 0.00047274492234169655,
      "loss": 7.2178,
      "step": 366
    },
    {
      "epoch": 0.10964224363283293,
      "grad_norm": 0.36402952671051025,
      "learning_rate": 0.00047267025089605736,
      "loss": 7.2979,
      "step": 367
    },
    {
      "epoch": 0.10994099634027933,
      "grad_norm": 0.36446794867515564,
      "learning_rate": 0.0004725955794504181,
      "loss": 7.251,
      "step": 368
    },
    {
      "epoch": 0.11023974904772575,
      "grad_norm": 0.3983413875102997,
      "learning_rate": 0.000472520908004779,
      "loss": 6.7393,
      "step": 369
    },
    {
      "epoch": 0.11053850175517216,
      "grad_norm": 0.3563667833805084,
      "learning_rate": 0.0004724462365591398,
      "loss": 6.9756,
      "step": 370
    },
    {
      "epoch": 0.11083725446261856,
      "grad_norm": 0.34414103627204895,
      "learning_rate": 0.0004723715651135006,
      "loss": 7.2285,
      "step": 371
    },
    {
      "epoch": 0.11113600717006498,
      "grad_norm": 0.4532737731933594,
      "learning_rate": 0.0004722968936678614,
      "loss": 6.9023,
      "step": 372
    },
    {
      "epoch": 0.11143475987751139,
      "grad_norm": 0.296292245388031,
      "learning_rate": 0.00047222222222222224,
      "loss": 7.3359,
      "step": 373
    },
    {
      "epoch": 0.1117335125849578,
      "grad_norm": 0.2869156301021576,
      "learning_rate": 0.00047214755077658305,
      "loss": 7.3701,
      "step": 374
    },
    {
      "epoch": 0.11203226529240422,
      "grad_norm": 0.3389177620410919,
      "learning_rate": 0.00047207287933094386,
      "loss": 7.3691,
      "step": 375
    },
    {
      "epoch": 0.11233101799985062,
      "grad_norm": 0.34489476680755615,
      "learning_rate": 0.0004719982078853047,
      "loss": 7.4756,
      "step": 376
    },
    {
      "epoch": 0.11262977070729703,
      "grad_norm": 0.31314054131507874,
      "learning_rate": 0.00047192353643966544,
      "loss": 7.6172,
      "step": 377
    },
    {
      "epoch": 0.11292852341474345,
      "grad_norm": 0.34616023302078247,
      "learning_rate": 0.0004718488649940263,
      "loss": 7.6846,
      "step": 378
    },
    {
      "epoch": 0.11322727612218986,
      "grad_norm": 0.3795040249824524,
      "learning_rate": 0.0004717741935483871,
      "loss": 7.3447,
      "step": 379
    },
    {
      "epoch": 0.11352602882963626,
      "grad_norm": 0.34627050161361694,
      "learning_rate": 0.00047169952210274793,
      "loss": 7.3418,
      "step": 380
    },
    {
      "epoch": 0.11382478153708268,
      "grad_norm": 0.47329452633857727,
      "learning_rate": 0.00047162485065710874,
      "loss": 6.9062,
      "step": 381
    },
    {
      "epoch": 0.11412353424452909,
      "grad_norm": 0.390525758266449,
      "learning_rate": 0.00047155017921146955,
      "loss": 6.7793,
      "step": 382
    },
    {
      "epoch": 0.1144222869519755,
      "grad_norm": 0.33880671858787537,
      "learning_rate": 0.00047147550776583037,
      "loss": 7.083,
      "step": 383
    },
    {
      "epoch": 0.11472103965942192,
      "grad_norm": 0.436779260635376,
      "learning_rate": 0.0004714008363201911,
      "loss": 7.0723,
      "step": 384
    },
    {
      "epoch": 0.11501979236686832,
      "grad_norm": 0.31517294049263,
      "learning_rate": 0.000471326164874552,
      "loss": 7.5107,
      "step": 385
    },
    {
      "epoch": 0.11531854507431473,
      "grad_norm": 0.35628557205200195,
      "learning_rate": 0.00047125149342891275,
      "loss": 7.3232,
      "step": 386
    },
    {
      "epoch": 0.11561729778176115,
      "grad_norm": 0.3770134747028351,
      "learning_rate": 0.0004711768219832736,
      "loss": 7.6406,
      "step": 387
    },
    {
      "epoch": 0.11591605048920756,
      "grad_norm": 0.3829798698425293,
      "learning_rate": 0.00047110215053763443,
      "loss": 7.2451,
      "step": 388
    },
    {
      "epoch": 0.11621480319665398,
      "grad_norm": 0.41653376817703247,
      "learning_rate": 0.00047102747909199524,
      "loss": 7.0498,
      "step": 389
    },
    {
      "epoch": 0.11651355590410038,
      "grad_norm": 0.3544130325317383,
      "learning_rate": 0.00047095280764635606,
      "loss": 7.0684,
      "step": 390
    },
    {
      "epoch": 0.11681230861154679,
      "grad_norm": 0.3239371180534363,
      "learning_rate": 0.00047087813620071687,
      "loss": 7.0918,
      "step": 391
    },
    {
      "epoch": 0.11711106131899321,
      "grad_norm": 0.3773457109928131,
      "learning_rate": 0.0004708034647550777,
      "loss": 7.2773,
      "step": 392
    },
    {
      "epoch": 0.11740981402643962,
      "grad_norm": 0.3058832883834839,
      "learning_rate": 0.00047072879330943844,
      "loss": 7.5107,
      "step": 393
    },
    {
      "epoch": 0.11770856673388602,
      "grad_norm": 0.38404345512390137,
      "learning_rate": 0.0004706541218637993,
      "loss": 6.7852,
      "step": 394
    },
    {
      "epoch": 0.11800731944133244,
      "grad_norm": 0.4163118004798889,
      "learning_rate": 0.00047057945041816007,
      "loss": 6.7666,
      "step": 395
    },
    {
      "epoch": 0.11830607214877885,
      "grad_norm": 0.2919059693813324,
      "learning_rate": 0.00047050477897252093,
      "loss": 7.4551,
      "step": 396
    },
    {
      "epoch": 0.11860482485622525,
      "grad_norm": 0.41334113478660583,
      "learning_rate": 0.00047043010752688175,
      "loss": 6.9238,
      "step": 397
    },
    {
      "epoch": 0.11890357756367168,
      "grad_norm": 0.330368310213089,
      "learning_rate": 0.00047035543608124256,
      "loss": 7.8213,
      "step": 398
    },
    {
      "epoch": 0.11920233027111808,
      "grad_norm": 0.3507167100906372,
      "learning_rate": 0.00047028076463560337,
      "loss": 7.3389,
      "step": 399
    },
    {
      "epoch": 0.11950108297856449,
      "grad_norm": 0.33408859372138977,
      "learning_rate": 0.00047020609318996413,
      "loss": 7.5156,
      "step": 400
    },
    {
      "epoch": 0.11950108297856449,
      "eval_bleu": 0.06314290633984992,
      "eval_loss": 7.07421875,
      "eval_runtime": 615.7724,
      "eval_samples_per_second": 2.288,
      "eval_steps_per_second": 0.145,
      "step": 400
    },
    {
      "epoch": 0.11979983568601091,
      "grad_norm": 0.3407604992389679,
      "learning_rate": 0.000470131421744325,
      "loss": 7.0703,
      "step": 401
    },
    {
      "epoch": 0.12009858839345731,
      "grad_norm": 0.352294921875,
      "learning_rate": 0.00047005675029868576,
      "loss": 7.248,
      "step": 402
    },
    {
      "epoch": 0.12039734110090372,
      "grad_norm": 0.4790753126144409,
      "learning_rate": 0.0004699820788530466,
      "loss": 6.5674,
      "step": 403
    },
    {
      "epoch": 0.12069609380835014,
      "grad_norm": 0.34463468194007874,
      "learning_rate": 0.0004699074074074074,
      "loss": 7.1855,
      "step": 404
    },
    {
      "epoch": 0.12099484651579655,
      "grad_norm": 0.34944072365760803,
      "learning_rate": 0.00046983273596176825,
      "loss": 7.2295,
      "step": 405
    },
    {
      "epoch": 0.12129359922324295,
      "grad_norm": 0.3610077202320099,
      "learning_rate": 0.00046975806451612906,
      "loss": 7.0049,
      "step": 406
    },
    {
      "epoch": 0.12159235193068937,
      "grad_norm": 0.3886076807975769,
      "learning_rate": 0.0004696833930704899,
      "loss": 7.4717,
      "step": 407
    },
    {
      "epoch": 0.12189110463813578,
      "grad_norm": 0.4166484475135803,
      "learning_rate": 0.0004696087216248507,
      "loss": 7.3281,
      "step": 408
    },
    {
      "epoch": 0.1221898573455822,
      "grad_norm": 0.37506043910980225,
      "learning_rate": 0.00046953405017921145,
      "loss": 7.4717,
      "step": 409
    },
    {
      "epoch": 0.12248861005302861,
      "grad_norm": 0.39027926325798035,
      "learning_rate": 0.0004694593787335723,
      "loss": 7.1924,
      "step": 410
    },
    {
      "epoch": 0.12278736276047501,
      "grad_norm": 0.3593742251396179,
      "learning_rate": 0.0004693847072879331,
      "loss": 7.2305,
      "step": 411
    },
    {
      "epoch": 0.12308611546792143,
      "grad_norm": 0.4341390132904053,
      "learning_rate": 0.00046931003584229394,
      "loss": 7.0459,
      "step": 412
    },
    {
      "epoch": 0.12338486817536784,
      "grad_norm": 0.45067688822746277,
      "learning_rate": 0.0004692353643966547,
      "loss": 7.1045,
      "step": 413
    },
    {
      "epoch": 0.12368362088281425,
      "grad_norm": 0.5502626299858093,
      "learning_rate": 0.00046916069295101557,
      "loss": 7.3369,
      "step": 414
    },
    {
      "epoch": 0.12398237359026067,
      "grad_norm": 0.33474794030189514,
      "learning_rate": 0.0004690860215053764,
      "loss": 7.4951,
      "step": 415
    },
    {
      "epoch": 0.12428112629770707,
      "grad_norm": 0.39838525652885437,
      "learning_rate": 0.00046901135005973714,
      "loss": 7.2979,
      "step": 416
    },
    {
      "epoch": 0.12457987900515348,
      "grad_norm": 0.37522467970848083,
      "learning_rate": 0.000468936678614098,
      "loss": 7.3535,
      "step": 417
    },
    {
      "epoch": 0.1248786317125999,
      "grad_norm": 0.38580048084259033,
      "learning_rate": 0.00046886200716845876,
      "loss": 7.335,
      "step": 418
    },
    {
      "epoch": 0.1251773844200463,
      "grad_norm": 0.42477577924728394,
      "learning_rate": 0.00046878733572281963,
      "loss": 7.3984,
      "step": 419
    },
    {
      "epoch": 0.12547613712749273,
      "grad_norm": 0.369338721036911,
      "learning_rate": 0.0004687126642771804,
      "loss": 7.3545,
      "step": 420
    },
    {
      "epoch": 0.12577488983493912,
      "grad_norm": 0.28646552562713623,
      "learning_rate": 0.00046863799283154126,
      "loss": 7.3369,
      "step": 421
    },
    {
      "epoch": 0.12607364254238554,
      "grad_norm": 0.33757954835891724,
      "learning_rate": 0.000468563321385902,
      "loss": 7.4355,
      "step": 422
    },
    {
      "epoch": 0.12637239524983196,
      "grad_norm": 0.43328431248664856,
      "learning_rate": 0.0004684886499402629,
      "loss": 6.9902,
      "step": 423
    },
    {
      "epoch": 0.12667114795727835,
      "grad_norm": 0.2920494079589844,
      "learning_rate": 0.00046841397849462364,
      "loss": 7.4912,
      "step": 424
    },
    {
      "epoch": 0.12696990066472477,
      "grad_norm": 0.39742282032966614,
      "learning_rate": 0.00046833930704898445,
      "loss": 6.8477,
      "step": 425
    },
    {
      "epoch": 0.1272686533721712,
      "grad_norm": 0.3749666213989258,
      "learning_rate": 0.0004682646356033453,
      "loss": 6.9873,
      "step": 426
    },
    {
      "epoch": 0.12756740607961758,
      "grad_norm": 0.35190659761428833,
      "learning_rate": 0.0004681899641577061,
      "loss": 7.4014,
      "step": 427
    },
    {
      "epoch": 0.127866158787064,
      "grad_norm": 0.42471548914909363,
      "learning_rate": 0.00046811529271206695,
      "loss": 7.2129,
      "step": 428
    },
    {
      "epoch": 0.12816491149451043,
      "grad_norm": 0.3918190002441406,
      "learning_rate": 0.0004680406212664277,
      "loss": 7.166,
      "step": 429
    },
    {
      "epoch": 0.12846366420195682,
      "grad_norm": 0.36660444736480713,
      "learning_rate": 0.00046796594982078857,
      "loss": 7.2119,
      "step": 430
    },
    {
      "epoch": 0.12876241690940324,
      "grad_norm": 0.3479898273944855,
      "learning_rate": 0.00046789127837514933,
      "loss": 7.7314,
      "step": 431
    },
    {
      "epoch": 0.12906116961684966,
      "grad_norm": 0.3594509959220886,
      "learning_rate": 0.00046781660692951014,
      "loss": 7.3701,
      "step": 432
    },
    {
      "epoch": 0.12935992232429605,
      "grad_norm": 0.3519911468029022,
      "learning_rate": 0.00046774193548387096,
      "loss": 7.668,
      "step": 433
    },
    {
      "epoch": 0.12965867503174247,
      "grad_norm": 0.46653422713279724,
      "learning_rate": 0.00046766726403823177,
      "loss": 6.6914,
      "step": 434
    },
    {
      "epoch": 0.1299574277391889,
      "grad_norm": 0.4118472635746002,
      "learning_rate": 0.00046759259259259264,
      "loss": 7.2285,
      "step": 435
    },
    {
      "epoch": 0.1302561804466353,
      "grad_norm": 0.39476075768470764,
      "learning_rate": 0.0004675179211469534,
      "loss": 7.166,
      "step": 436
    },
    {
      "epoch": 0.1305549331540817,
      "grad_norm": 0.45234039425849915,
      "learning_rate": 0.00046744324970131426,
      "loss": 7.0479,
      "step": 437
    },
    {
      "epoch": 0.13085368586152812,
      "grad_norm": 0.3527635335922241,
      "learning_rate": 0.000467368578255675,
      "loss": 7.4775,
      "step": 438
    },
    {
      "epoch": 0.13115243856897454,
      "grad_norm": 0.4166572093963623,
      "learning_rate": 0.0004672939068100359,
      "loss": 7.4004,
      "step": 439
    },
    {
      "epoch": 0.13145119127642094,
      "grad_norm": 0.5076642632484436,
      "learning_rate": 0.00046721923536439665,
      "loss": 7.1572,
      "step": 440
    },
    {
      "epoch": 0.13174994398386736,
      "grad_norm": 0.4013204872608185,
      "learning_rate": 0.00046714456391875746,
      "loss": 7.0508,
      "step": 441
    },
    {
      "epoch": 0.13204869669131378,
      "grad_norm": 0.319581001996994,
      "learning_rate": 0.00046706989247311827,
      "loss": 7.5166,
      "step": 442
    },
    {
      "epoch": 0.13234744939876017,
      "grad_norm": 0.3800637125968933,
      "learning_rate": 0.0004669952210274791,
      "loss": 7.124,
      "step": 443
    },
    {
      "epoch": 0.1326462021062066,
      "grad_norm": 0.34482765197753906,
      "learning_rate": 0.00046692054958183995,
      "loss": 7.2949,
      "step": 444
    },
    {
      "epoch": 0.132944954813653,
      "grad_norm": 0.3178139925003052,
      "learning_rate": 0.0004668458781362007,
      "loss": 7.7031,
      "step": 445
    },
    {
      "epoch": 0.1332437075210994,
      "grad_norm": 0.6007941365242004,
      "learning_rate": 0.0004667712066905616,
      "loss": 6.5684,
      "step": 446
    },
    {
      "epoch": 0.13354246022854582,
      "grad_norm": 0.38168299198150635,
      "learning_rate": 0.00046669653524492234,
      "loss": 7.3184,
      "step": 447
    },
    {
      "epoch": 0.13384121293599224,
      "grad_norm": 0.3468656539916992,
      "learning_rate": 0.00046662186379928315,
      "loss": 7.0537,
      "step": 448
    },
    {
      "epoch": 0.13413996564343864,
      "grad_norm": 0.3573014438152313,
      "learning_rate": 0.00046654719235364396,
      "loss": 7.2559,
      "step": 449
    },
    {
      "epoch": 0.13443871835088506,
      "grad_norm": 0.37460553646087646,
      "learning_rate": 0.0004664725209080048,
      "loss": 7.3516,
      "step": 450
    },
    {
      "epoch": 0.13473747105833148,
      "grad_norm": 0.37861835956573486,
      "learning_rate": 0.0004663978494623656,
      "loss": 7.1768,
      "step": 451
    },
    {
      "epoch": 0.13503622376577787,
      "grad_norm": 0.3449447751045227,
      "learning_rate": 0.0004663231780167264,
      "loss": 7.4678,
      "step": 452
    },
    {
      "epoch": 0.1353349764732243,
      "grad_norm": 0.3241126835346222,
      "learning_rate": 0.00046624850657108727,
      "loss": 7.3457,
      "step": 453
    },
    {
      "epoch": 0.1356337291806707,
      "grad_norm": 0.3104764521121979,
      "learning_rate": 0.000466173835125448,
      "loss": 7.3584,
      "step": 454
    },
    {
      "epoch": 0.1359324818881171,
      "grad_norm": 0.4853258430957794,
      "learning_rate": 0.0004660991636798089,
      "loss": 6.4795,
      "step": 455
    },
    {
      "epoch": 0.13623123459556352,
      "grad_norm": 0.3746856451034546,
      "learning_rate": 0.00046602449223416965,
      "loss": 7.5654,
      "step": 456
    },
    {
      "epoch": 0.13652998730300994,
      "grad_norm": 0.3340097963809967,
      "learning_rate": 0.00046594982078853047,
      "loss": 7.1611,
      "step": 457
    },
    {
      "epoch": 0.13682874001045633,
      "grad_norm": 0.3395751714706421,
      "learning_rate": 0.0004658751493428913,
      "loss": 7.1582,
      "step": 458
    },
    {
      "epoch": 0.13712749271790275,
      "grad_norm": 0.3319547772407532,
      "learning_rate": 0.0004658004778972521,
      "loss": 7.3584,
      "step": 459
    },
    {
      "epoch": 0.13742624542534917,
      "grad_norm": 0.34483182430267334,
      "learning_rate": 0.0004657258064516129,
      "loss": 7.5312,
      "step": 460
    },
    {
      "epoch": 0.13772499813279557,
      "grad_norm": 0.37583398818969727,
      "learning_rate": 0.0004656511350059737,
      "loss": 7.127,
      "step": 461
    },
    {
      "epoch": 0.138023750840242,
      "grad_norm": 0.3963184952735901,
      "learning_rate": 0.0004655764635603346,
      "loss": 6.9551,
      "step": 462
    },
    {
      "epoch": 0.1383225035476884,
      "grad_norm": 0.31748080253601074,
      "learning_rate": 0.00046550179211469534,
      "loss": 7.4551,
      "step": 463
    },
    {
      "epoch": 0.1386212562551348,
      "grad_norm": 0.3935743272304535,
      "learning_rate": 0.00046542712066905616,
      "loss": 7.5693,
      "step": 464
    },
    {
      "epoch": 0.13892000896258122,
      "grad_norm": 0.3253386914730072,
      "learning_rate": 0.00046535244922341697,
      "loss": 7.0908,
      "step": 465
    },
    {
      "epoch": 0.13921876167002764,
      "grad_norm": 0.3798268735408783,
      "learning_rate": 0.0004652777777777778,
      "loss": 7.4902,
      "step": 466
    },
    {
      "epoch": 0.13951751437747403,
      "grad_norm": 0.31957101821899414,
      "learning_rate": 0.0004652031063321386,
      "loss": 7.542,
      "step": 467
    },
    {
      "epoch": 0.13981626708492045,
      "grad_norm": 0.4815508723258972,
      "learning_rate": 0.0004651284348864994,
      "loss": 6.9668,
      "step": 468
    },
    {
      "epoch": 0.14011501979236687,
      "grad_norm": 0.4467724561691284,
      "learning_rate": 0.0004650537634408602,
      "loss": 7.0322,
      "step": 469
    },
    {
      "epoch": 0.14041377249981327,
      "grad_norm": 0.489765465259552,
      "learning_rate": 0.00046497909199522103,
      "loss": 7.1465,
      "step": 470
    },
    {
      "epoch": 0.1407125252072597,
      "grad_norm": 0.3792406916618347,
      "learning_rate": 0.0004649044205495819,
      "loss": 7.0361,
      "step": 471
    },
    {
      "epoch": 0.1410112779147061,
      "grad_norm": 0.44332659244537354,
      "learning_rate": 0.00046482974910394266,
      "loss": 7.3105,
      "step": 472
    },
    {
      "epoch": 0.14131003062215253,
      "grad_norm": 0.39608466625213623,
      "learning_rate": 0.00046475507765830347,
      "loss": 7.3779,
      "step": 473
    },
    {
      "epoch": 0.14160878332959892,
      "grad_norm": 0.42770296335220337,
      "learning_rate": 0.0004646804062126643,
      "loss": 7.3154,
      "step": 474
    },
    {
      "epoch": 0.14190753603704534,
      "grad_norm": 0.36429211497306824,
      "learning_rate": 0.0004646057347670251,
      "loss": 7.459,
      "step": 475
    },
    {
      "epoch": 0.14220628874449176,
      "grad_norm": 0.37419646978378296,
      "learning_rate": 0.0004645310633213859,
      "loss": 7.5225,
      "step": 476
    },
    {
      "epoch": 0.14250504145193815,
      "grad_norm": 0.36639687418937683,
      "learning_rate": 0.0004644563918757467,
      "loss": 7.3213,
      "step": 477
    },
    {
      "epoch": 0.14280379415938457,
      "grad_norm": 0.3705472946166992,
      "learning_rate": 0.00046438172043010754,
      "loss": 7.4824,
      "step": 478
    },
    {
      "epoch": 0.143102546866831,
      "grad_norm": 0.4703329801559448,
      "learning_rate": 0.00046430704898446835,
      "loss": 7.0781,
      "step": 479
    },
    {
      "epoch": 0.14340129957427739,
      "grad_norm": 0.4923274517059326,
      "learning_rate": 0.00046423237753882916,
      "loss": 6.5352,
      "step": 480
    },
    {
      "epoch": 0.1437000522817238,
      "grad_norm": 0.4432821273803711,
      "learning_rate": 0.00046415770609319,
      "loss": 6.8555,
      "step": 481
    },
    {
      "epoch": 0.14399880498917023,
      "grad_norm": 0.4010286331176758,
      "learning_rate": 0.0004640830346475508,
      "loss": 7.2812,
      "step": 482
    },
    {
      "epoch": 0.14429755769661662,
      "grad_norm": 0.34480640292167664,
      "learning_rate": 0.0004640083632019116,
      "loss": 7.4512,
      "step": 483
    },
    {
      "epoch": 0.14459631040406304,
      "grad_norm": 0.3909822702407837,
      "learning_rate": 0.0004639336917562724,
      "loss": 6.8369,
      "step": 484
    },
    {
      "epoch": 0.14489506311150946,
      "grad_norm": 0.35184359550476074,
      "learning_rate": 0.0004638590203106332,
      "loss": 7.3018,
      "step": 485
    },
    {
      "epoch": 0.14519381581895585,
      "grad_norm": 0.47893083095550537,
      "learning_rate": 0.00046378434886499404,
      "loss": 7.0566,
      "step": 486
    },
    {
      "epoch": 0.14549256852640227,
      "grad_norm": 0.4085986614227295,
      "learning_rate": 0.00046370967741935485,
      "loss": 6.96,
      "step": 487
    },
    {
      "epoch": 0.1457913212338487,
      "grad_norm": 0.36285147070884705,
      "learning_rate": 0.00046363500597371566,
      "loss": 7.5215,
      "step": 488
    },
    {
      "epoch": 0.14609007394129508,
      "grad_norm": 0.39923295378685,
      "learning_rate": 0.0004635603345280765,
      "loss": 7.29,
      "step": 489
    },
    {
      "epoch": 0.1463888266487415,
      "grad_norm": 0.4298717677593231,
      "learning_rate": 0.0004634856630824373,
      "loss": 6.9131,
      "step": 490
    },
    {
      "epoch": 0.14668757935618792,
      "grad_norm": 0.3964043855667114,
      "learning_rate": 0.0004634109916367981,
      "loss": 7.3252,
      "step": 491
    },
    {
      "epoch": 0.14698633206363432,
      "grad_norm": 0.34743672609329224,
      "learning_rate": 0.0004633363201911589,
      "loss": 7.1904,
      "step": 492
    },
    {
      "epoch": 0.14728508477108074,
      "grad_norm": 0.3526354134082794,
      "learning_rate": 0.00046326164874551973,
      "loss": 7.5615,
      "step": 493
    },
    {
      "epoch": 0.14758383747852716,
      "grad_norm": 0.4242677688598633,
      "learning_rate": 0.00046318697729988054,
      "loss": 7.4014,
      "step": 494
    },
    {
      "epoch": 0.14788259018597355,
      "grad_norm": 0.3925628960132599,
      "learning_rate": 0.00046311230585424135,
      "loss": 7.2998,
      "step": 495
    },
    {
      "epoch": 0.14818134289341997,
      "grad_norm": 0.3802424669265747,
      "learning_rate": 0.0004630376344086021,
      "loss": 7.2988,
      "step": 496
    },
    {
      "epoch": 0.1484800956008664,
      "grad_norm": 0.3823290169239044,
      "learning_rate": 0.000462962962962963,
      "loss": 7.3926,
      "step": 497
    },
    {
      "epoch": 0.14877884830831278,
      "grad_norm": 0.3304677903652191,
      "learning_rate": 0.0004628882915173238,
      "loss": 7.6025,
      "step": 498
    },
    {
      "epoch": 0.1490776010157592,
      "grad_norm": 0.4154568314552307,
      "learning_rate": 0.0004628136200716846,
      "loss": 7.291,
      "step": 499
    },
    {
      "epoch": 0.14937635372320562,
      "grad_norm": 0.4527128338813782,
      "learning_rate": 0.0004627389486260454,
      "loss": 7.3662,
      "step": 500
    },
    {
      "epoch": 0.14967510643065202,
      "grad_norm": 0.4651779234409332,
      "learning_rate": 0.00046266427718040623,
      "loss": 7.3271,
      "step": 501
    },
    {
      "epoch": 0.14997385913809844,
      "grad_norm": 0.43253764510154724,
      "learning_rate": 0.00046258960573476705,
      "loss": 7.2441,
      "step": 502
    },
    {
      "epoch": 0.15027261184554486,
      "grad_norm": 0.41578447818756104,
      "learning_rate": 0.00046251493428912786,
      "loss": 7.3076,
      "step": 503
    },
    {
      "epoch": 0.15057136455299125,
      "grad_norm": 0.33692219853401184,
      "learning_rate": 0.00046244026284348867,
      "loss": 7.5645,
      "step": 504
    },
    {
      "epoch": 0.15087011726043767,
      "grad_norm": 0.40838131308555603,
      "learning_rate": 0.00046236559139784943,
      "loss": 7.1182,
      "step": 505
    },
    {
      "epoch": 0.1511688699678841,
      "grad_norm": 0.4092601537704468,
      "learning_rate": 0.0004622909199522103,
      "loss": 6.8379,
      "step": 506
    },
    {
      "epoch": 0.15146762267533048,
      "grad_norm": 0.4096202850341797,
      "learning_rate": 0.0004622162485065711,
      "loss": 7.0781,
      "step": 507
    },
    {
      "epoch": 0.1517663753827769,
      "grad_norm": 0.3555811941623688,
      "learning_rate": 0.0004621415770609319,
      "loss": 7.2588,
      "step": 508
    },
    {
      "epoch": 0.15206512809022332,
      "grad_norm": 0.32640114426612854,
      "learning_rate": 0.00046206690561529274,
      "loss": 7.5889,
      "step": 509
    },
    {
      "epoch": 0.15236388079766972,
      "grad_norm": 0.35323473811149597,
      "learning_rate": 0.00046199223416965355,
      "loss": 7.3721,
      "step": 510
    },
    {
      "epoch": 0.15266263350511614,
      "grad_norm": 0.3315449059009552,
      "learning_rate": 0.00046191756272401436,
      "loss": 7.4639,
      "step": 511
    },
    {
      "epoch": 0.15296138621256256,
      "grad_norm": 0.4315294921398163,
      "learning_rate": 0.0004618428912783751,
      "loss": 7.5361,
      "step": 512
    },
    {
      "epoch": 0.15326013892000898,
      "grad_norm": 0.43292179703712463,
      "learning_rate": 0.000461768219832736,
      "loss": 7.292,
      "step": 513
    },
    {
      "epoch": 0.15355889162745537,
      "grad_norm": 0.4063829779624939,
      "learning_rate": 0.00046169354838709675,
      "loss": 7.0205,
      "step": 514
    },
    {
      "epoch": 0.1538576443349018,
      "grad_norm": 0.3746349513530731,
      "learning_rate": 0.0004616188769414576,
      "loss": 7.1104,
      "step": 515
    },
    {
      "epoch": 0.1541563970423482,
      "grad_norm": 0.4410933554172516,
      "learning_rate": 0.00046154420549581837,
      "loss": 6.8438,
      "step": 516
    },
    {
      "epoch": 0.1544551497497946,
      "grad_norm": 0.3735065162181854,
      "learning_rate": 0.00046146953405017924,
      "loss": 6.873,
      "step": 517
    },
    {
      "epoch": 0.15475390245724102,
      "grad_norm": 0.38182929158210754,
      "learning_rate": 0.00046139486260454005,
      "loss": 7.4883,
      "step": 518
    },
    {
      "epoch": 0.15505265516468744,
      "grad_norm": 0.34415221214294434,
      "learning_rate": 0.00046132019115890086,
      "loss": 7.5508,
      "step": 519
    },
    {
      "epoch": 0.15535140787213383,
      "grad_norm": 0.3995930850505829,
      "learning_rate": 0.0004612455197132617,
      "loss": 7.2061,
      "step": 520
    },
    {
      "epoch": 0.15565016057958025,
      "grad_norm": 0.37925347685813904,
      "learning_rate": 0.00046117084826762244,
      "loss": 7.252,
      "step": 521
    },
    {
      "epoch": 0.15594891328702667,
      "grad_norm": 0.46988874673843384,
      "learning_rate": 0.0004610961768219833,
      "loss": 7.207,
      "step": 522
    },
    {
      "epoch": 0.15624766599447307,
      "grad_norm": 0.39252549409866333,
      "learning_rate": 0.00046102150537634406,
      "loss": 7.3164,
      "step": 523
    },
    {
      "epoch": 0.1565464187019195,
      "grad_norm": 0.4602091610431671,
      "learning_rate": 0.00046094683393070493,
      "loss": 7.0176,
      "step": 524
    },
    {
      "epoch": 0.1568451714093659,
      "grad_norm": 0.4840177595615387,
      "learning_rate": 0.0004608721624850657,
      "loss": 6.6836,
      "step": 525
    },
    {
      "epoch": 0.1571439241168123,
      "grad_norm": 0.41737475991249084,
      "learning_rate": 0.00046079749103942655,
      "loss": 7.4014,
      "step": 526
    },
    {
      "epoch": 0.15744267682425872,
      "grad_norm": 0.33526843786239624,
      "learning_rate": 0.00046072281959378737,
      "loss": 7.7256,
      "step": 527
    },
    {
      "epoch": 0.15774142953170514,
      "grad_norm": 0.5068597793579102,
      "learning_rate": 0.0004606481481481481,
      "loss": 6.2852,
      "step": 528
    },
    {
      "epoch": 0.15804018223915153,
      "grad_norm": 0.3731829524040222,
      "learning_rate": 0.000460573476702509,
      "loss": 7.2822,
      "step": 529
    },
    {
      "epoch": 0.15833893494659795,
      "grad_norm": 0.35565051436424255,
      "learning_rate": 0.00046049880525686975,
      "loss": 7.4336,
      "step": 530
    },
    {
      "epoch": 0.15863768765404437,
      "grad_norm": 0.3721979558467865,
      "learning_rate": 0.0004604241338112306,
      "loss": 7.0195,
      "step": 531
    },
    {
      "epoch": 0.15893644036149077,
      "grad_norm": 0.3453672528266907,
      "learning_rate": 0.0004603494623655914,
      "loss": 7.6523,
      "step": 532
    },
    {
      "epoch": 0.1592351930689372,
      "grad_norm": 0.3656001687049866,
      "learning_rate": 0.00046027479091995224,
      "loss": 7.6797,
      "step": 533
    },
    {
      "epoch": 0.1595339457763836,
      "grad_norm": 0.39576786756515503,
      "learning_rate": 0.000460200119474313,
      "loss": 7.418,
      "step": 534
    },
    {
      "epoch": 0.15983269848383,
      "grad_norm": 0.39797618985176086,
      "learning_rate": 0.00046012544802867387,
      "loss": 7.4248,
      "step": 535
    },
    {
      "epoch": 0.16013145119127642,
      "grad_norm": 0.4094032347202301,
      "learning_rate": 0.0004600507765830347,
      "loss": 7.4854,
      "step": 536
    },
    {
      "epoch": 0.16043020389872284,
      "grad_norm": 0.38115039467811584,
      "learning_rate": 0.00045997610513739544,
      "loss": 7.5127,
      "step": 537
    },
    {
      "epoch": 0.16072895660616923,
      "grad_norm": 0.4406241774559021,
      "learning_rate": 0.0004599014336917563,
      "loss": 7.0752,
      "step": 538
    },
    {
      "epoch": 0.16102770931361565,
      "grad_norm": 0.3622308075428009,
      "learning_rate": 0.00045982676224611707,
      "loss": 7.3613,
      "step": 539
    },
    {
      "epoch": 0.16132646202106207,
      "grad_norm": 0.49825045466423035,
      "learning_rate": 0.00045975209080047793,
      "loss": 6.6807,
      "step": 540
    },
    {
      "epoch": 0.16162521472850847,
      "grad_norm": 0.38409075140953064,
      "learning_rate": 0.0004596774193548387,
      "loss": 7.2686,
      "step": 541
    },
    {
      "epoch": 0.16192396743595489,
      "grad_norm": 0.3863038122653961,
      "learning_rate": 0.00045960274790919956,
      "loss": 7.5938,
      "step": 542
    },
    {
      "epoch": 0.1622227201434013,
      "grad_norm": 0.4190234839916229,
      "learning_rate": 0.0004595280764635603,
      "loss": 7.2627,
      "step": 543
    },
    {
      "epoch": 0.1625214728508477,
      "grad_norm": 0.3243887722492218,
      "learning_rate": 0.00045945340501792113,
      "loss": 7.5244,
      "step": 544
    },
    {
      "epoch": 0.16282022555829412,
      "grad_norm": 0.4027411937713623,
      "learning_rate": 0.000459378733572282,
      "loss": 7.3145,
      "step": 545
    },
    {
      "epoch": 0.16311897826574054,
      "grad_norm": 0.4423351585865021,
      "learning_rate": 0.00045930406212664276,
      "loss": 6.7139,
      "step": 546
    },
    {
      "epoch": 0.16341773097318693,
      "grad_norm": 0.45697882771492004,
      "learning_rate": 0.0004592293906810036,
      "loss": 7.0107,
      "step": 547
    },
    {
      "epoch": 0.16371648368063335,
      "grad_norm": 0.3535310626029968,
      "learning_rate": 0.0004591547192353644,
      "loss": 7.5635,
      "step": 548
    },
    {
      "epoch": 0.16401523638807977,
      "grad_norm": 0.40528586506843567,
      "learning_rate": 0.00045908004778972525,
      "loss": 7.2578,
      "step": 549
    },
    {
      "epoch": 0.1643139890955262,
      "grad_norm": 0.3310297429561615,
      "learning_rate": 0.000459005376344086,
      "loss": 7.6299,
      "step": 550
    },
    {
      "epoch": 0.16461274180297258,
      "grad_norm": 0.36236876249313354,
      "learning_rate": 0.0004589307048984469,
      "loss": 7.2324,
      "step": 551
    },
    {
      "epoch": 0.164911494510419,
      "grad_norm": 0.3836616277694702,
      "learning_rate": 0.00045885603345280763,
      "loss": 7.1973,
      "step": 552
    },
    {
      "epoch": 0.16521024721786542,
      "grad_norm": 0.39154064655303955,
      "learning_rate": 0.00045878136200716845,
      "loss": 7.499,
      "step": 553
    },
    {
      "epoch": 0.16550899992531182,
      "grad_norm": 0.3483290374279022,
      "learning_rate": 0.0004587066905615293,
      "loss": 7.5039,
      "step": 554
    },
    {
      "epoch": 0.16580775263275824,
      "grad_norm": 0.38793253898620605,
      "learning_rate": 0.0004586320191158901,
      "loss": 7.3457,
      "step": 555
    },
    {
      "epoch": 0.16610650534020466,
      "grad_norm": 0.3821764290332794,
      "learning_rate": 0.00045855734767025094,
      "loss": 7.3965,
      "step": 556
    },
    {
      "epoch": 0.16640525804765105,
      "grad_norm": 0.340007483959198,
      "learning_rate": 0.0004584826762246117,
      "loss": 7.5859,
      "step": 557
    },
    {
      "epoch": 0.16670401075509747,
      "grad_norm": 0.40256068110466003,
      "learning_rate": 0.00045840800477897257,
      "loss": 7.25,
      "step": 558
    },
    {
      "epoch": 0.1670027634625439,
      "grad_norm": 0.5629507899284363,
      "learning_rate": 0.0004583333333333333,
      "loss": 7.0137,
      "step": 559
    },
    {
      "epoch": 0.16730151616999028,
      "grad_norm": 0.534640908241272,
      "learning_rate": 0.00045825866188769414,
      "loss": 6.8975,
      "step": 560
    },
    {
      "epoch": 0.1676002688774367,
      "grad_norm": 0.4003329873085022,
      "learning_rate": 0.00045818399044205495,
      "loss": 7.2285,
      "step": 561
    },
    {
      "epoch": 0.16789902158488312,
      "grad_norm": 0.41849029064178467,
      "learning_rate": 0.00045810931899641576,
      "loss": 7.4932,
      "step": 562
    },
    {
      "epoch": 0.16819777429232952,
      "grad_norm": 0.43317610025405884,
      "learning_rate": 0.00045803464755077663,
      "loss": 7.1191,
      "step": 563
    },
    {
      "epoch": 0.16849652699977594,
      "grad_norm": 0.36945387721061707,
      "learning_rate": 0.0004579599761051374,
      "loss": 7.4355,
      "step": 564
    },
    {
      "epoch": 0.16879527970722236,
      "grad_norm": 0.428309828042984,
      "learning_rate": 0.00045788530465949826,
      "loss": 6.957,
      "step": 565
    },
    {
      "epoch": 0.16909403241466875,
      "grad_norm": 0.43416914343833923,
      "learning_rate": 0.000457810633213859,
      "loss": 7.3232,
      "step": 566
    },
    {
      "epoch": 0.16939278512211517,
      "grad_norm": 0.3761763274669647,
      "learning_rate": 0.0004577359617682199,
      "loss": 7.4131,
      "step": 567
    },
    {
      "epoch": 0.1696915378295616,
      "grad_norm": 0.38552385568618774,
      "learning_rate": 0.00045766129032258064,
      "loss": 6.957,
      "step": 568
    },
    {
      "epoch": 0.16999029053700798,
      "grad_norm": 0.37265893816947937,
      "learning_rate": 0.00045758661887694145,
      "loss": 7.5215,
      "step": 569
    },
    {
      "epoch": 0.1702890432444544,
      "grad_norm": 0.3897472321987152,
      "learning_rate": 0.00045751194743130227,
      "loss": 7.3398,
      "step": 570
    },
    {
      "epoch": 0.17058779595190082,
      "grad_norm": 0.3238326609134674,
      "learning_rate": 0.0004574372759856631,
      "loss": 7.7412,
      "step": 571
    },
    {
      "epoch": 0.17088654865934721,
      "grad_norm": 0.42091798782348633,
      "learning_rate": 0.00045736260454002395,
      "loss": 7.376,
      "step": 572
    },
    {
      "epoch": 0.17118530136679364,
      "grad_norm": 0.4016720950603485,
      "learning_rate": 0.0004572879330943847,
      "loss": 7.5918,
      "step": 573
    },
    {
      "epoch": 0.17148405407424006,
      "grad_norm": 0.38962221145629883,
      "learning_rate": 0.00045721326164874557,
      "loss": 7.2871,
      "step": 574
    },
    {
      "epoch": 0.17178280678168645,
      "grad_norm": 0.39090844988822937,
      "learning_rate": 0.00045713859020310633,
      "loss": 7.4746,
      "step": 575
    },
    {
      "epoch": 0.17208155948913287,
      "grad_norm": 0.40731289982795715,
      "learning_rate": 0.00045706391875746714,
      "loss": 7.0703,
      "step": 576
    },
    {
      "epoch": 0.1723803121965793,
      "grad_norm": 0.4257664680480957,
      "learning_rate": 0.00045698924731182796,
      "loss": 7.0127,
      "step": 577
    },
    {
      "epoch": 0.17267906490402568,
      "grad_norm": 0.4589594602584839,
      "learning_rate": 0.00045691457586618877,
      "loss": 6.8076,
      "step": 578
    },
    {
      "epoch": 0.1729778176114721,
      "grad_norm": 0.37811416387557983,
      "learning_rate": 0.0004568399044205496,
      "loss": 7.1123,
      "step": 579
    },
    {
      "epoch": 0.17327657031891852,
      "grad_norm": 0.401071161031723,
      "learning_rate": 0.0004567652329749104,
      "loss": 7.0098,
      "step": 580
    },
    {
      "epoch": 0.1735753230263649,
      "grad_norm": 0.4928409457206726,
      "learning_rate": 0.00045669056152927126,
      "loss": 7.0869,
      "step": 581
    },
    {
      "epoch": 0.17387407573381133,
      "grad_norm": 0.3705659806728363,
      "learning_rate": 0.000456615890083632,
      "loss": 7.2725,
      "step": 582
    },
    {
      "epoch": 0.17417282844125775,
      "grad_norm": 0.35716262459754944,
      "learning_rate": 0.0004565412186379929,
      "loss": 7.5283,
      "step": 583
    },
    {
      "epoch": 0.17447158114870415,
      "grad_norm": 0.44968387484550476,
      "learning_rate": 0.00045646654719235365,
      "loss": 6.8555,
      "step": 584
    },
    {
      "epoch": 0.17477033385615057,
      "grad_norm": 0.4309314489364624,
      "learning_rate": 0.00045639187574671446,
      "loss": 6.9453,
      "step": 585
    },
    {
      "epoch": 0.175069086563597,
      "grad_norm": 0.5305352807044983,
      "learning_rate": 0.00045631720430107527,
      "loss": 6.3955,
      "step": 586
    },
    {
      "epoch": 0.1753678392710434,
      "grad_norm": 0.36297738552093506,
      "learning_rate": 0.0004562425328554361,
      "loss": 7.1943,
      "step": 587
    },
    {
      "epoch": 0.1756665919784898,
      "grad_norm": 0.39075636863708496,
      "learning_rate": 0.0004561678614097969,
      "loss": 7.2637,
      "step": 588
    },
    {
      "epoch": 0.17596534468593622,
      "grad_norm": 0.4245891571044922,
      "learning_rate": 0.0004560931899641577,
      "loss": 6.9678,
      "step": 589
    },
    {
      "epoch": 0.17626409739338264,
      "grad_norm": 0.3536333441734314,
      "learning_rate": 0.0004560185185185186,
      "loss": 7.6035,
      "step": 590
    },
    {
      "epoch": 0.17656285010082903,
      "grad_norm": 0.37007611989974976,
      "learning_rate": 0.00045594384707287934,
      "loss": 7.542,
      "step": 591
    },
    {
      "epoch": 0.17686160280827545,
      "grad_norm": 0.37865617871284485,
      "learning_rate": 0.00045586917562724015,
      "loss": 7.1719,
      "step": 592
    },
    {
      "epoch": 0.17716035551572187,
      "grad_norm": 0.41175633668899536,
      "learning_rate": 0.00045579450418160096,
      "loss": 7.4404,
      "step": 593
    },
    {
      "epoch": 0.17745910822316827,
      "grad_norm": 0.3914108872413635,
      "learning_rate": 0.0004557198327359618,
      "loss": 7.2676,
      "step": 594
    },
    {
      "epoch": 0.1777578609306147,
      "grad_norm": 0.45254841446876526,
      "learning_rate": 0.0004556451612903226,
      "loss": 6.8359,
      "step": 595
    },
    {
      "epoch": 0.1780566136380611,
      "grad_norm": 0.36402076482772827,
      "learning_rate": 0.0004555704898446834,
      "loss": 7.5029,
      "step": 596
    },
    {
      "epoch": 0.1783553663455075,
      "grad_norm": 0.3413120210170746,
      "learning_rate": 0.0004554958183990442,
      "loss": 7.5625,
      "step": 597
    },
    {
      "epoch": 0.17865411905295392,
      "grad_norm": 0.3583661615848541,
      "learning_rate": 0.000455421146953405,
      "loss": 7.4912,
      "step": 598
    },
    {
      "epoch": 0.17895287176040034,
      "grad_norm": 0.3917255103588104,
      "learning_rate": 0.0004553464755077659,
      "loss": 7.9229,
      "step": 599
    },
    {
      "epoch": 0.17925162446784673,
      "grad_norm": 0.46217119693756104,
      "learning_rate": 0.00045527180406212665,
      "loss": 6.7598,
      "step": 600
    },
    {
      "epoch": 0.17925162446784673,
      "eval_bleu": 0.07854102719186938,
      "eval_loss": 7.11328125,
      "eval_runtime": 527.0984,
      "eval_samples_per_second": 2.673,
      "eval_steps_per_second": 0.169,
      "step": 600
    },
    {
      "epoch": 0.17955037717529315,
      "grad_norm": 0.4248005449771881,
      "learning_rate": 0.00045519713261648747,
      "loss": 6.7539,
      "step": 601
    },
    {
      "epoch": 0.17984912988273957,
      "grad_norm": 0.32401132583618164,
      "learning_rate": 0.0004551224611708483,
      "loss": 7.6738,
      "step": 602
    },
    {
      "epoch": 0.18014788259018596,
      "grad_norm": 0.43907949328422546,
      "learning_rate": 0.0004550477897252091,
      "loss": 7.3604,
      "step": 603
    },
    {
      "epoch": 0.18044663529763239,
      "grad_norm": 0.38660985231399536,
      "learning_rate": 0.0004549731182795699,
      "loss": 7.416,
      "step": 604
    },
    {
      "epoch": 0.1807453880050788,
      "grad_norm": 0.4815957844257355,
      "learning_rate": 0.0004548984468339307,
      "loss": 6.6309,
      "step": 605
    },
    {
      "epoch": 0.1810441407125252,
      "grad_norm": 0.4087294042110443,
      "learning_rate": 0.00045482377538829153,
      "loss": 7.2529,
      "step": 606
    },
    {
      "epoch": 0.18134289341997162,
      "grad_norm": 0.4295148551464081,
      "learning_rate": 0.00045474910394265234,
      "loss": 7.3789,
      "step": 607
    },
    {
      "epoch": 0.18164164612741804,
      "grad_norm": 0.386086642742157,
      "learning_rate": 0.0004546744324970131,
      "loss": 7.2842,
      "step": 608
    },
    {
      "epoch": 0.18194039883486443,
      "grad_norm": 0.3591023087501526,
      "learning_rate": 0.00045459976105137397,
      "loss": 7.4424,
      "step": 609
    },
    {
      "epoch": 0.18223915154231085,
      "grad_norm": 0.3802921772003174,
      "learning_rate": 0.0004545250896057348,
      "loss": 7.1465,
      "step": 610
    },
    {
      "epoch": 0.18253790424975727,
      "grad_norm": 0.461904376745224,
      "learning_rate": 0.0004544504181600956,
      "loss": 7.1436,
      "step": 611
    },
    {
      "epoch": 0.18283665695720366,
      "grad_norm": 0.4196985363960266,
      "learning_rate": 0.0004543757467144564,
      "loss": 7.1875,
      "step": 612
    },
    {
      "epoch": 0.18313540966465008,
      "grad_norm": 0.4085301160812378,
      "learning_rate": 0.0004543010752688172,
      "loss": 7.2158,
      "step": 613
    },
    {
      "epoch": 0.1834341623720965,
      "grad_norm": 0.37223660945892334,
      "learning_rate": 0.00045422640382317803,
      "loss": 7.4443,
      "step": 614
    },
    {
      "epoch": 0.1837329150795429,
      "grad_norm": 0.3721599876880646,
      "learning_rate": 0.0004541517323775388,
      "loss": 7.0391,
      "step": 615
    },
    {
      "epoch": 0.18403166778698932,
      "grad_norm": 0.4309301972389221,
      "learning_rate": 0.00045407706093189966,
      "loss": 7.2725,
      "step": 616
    },
    {
      "epoch": 0.18433042049443574,
      "grad_norm": 0.36106613278388977,
      "learning_rate": 0.0004540023894862604,
      "loss": 7.6318,
      "step": 617
    },
    {
      "epoch": 0.18462917320188213,
      "grad_norm": 0.47131600975990295,
      "learning_rate": 0.0004539277180406213,
      "loss": 7.5039,
      "step": 618
    },
    {
      "epoch": 0.18492792590932855,
      "grad_norm": 0.39348429441452026,
      "learning_rate": 0.0004538530465949821,
      "loss": 7.4619,
      "step": 619
    },
    {
      "epoch": 0.18522667861677497,
      "grad_norm": 0.49011680483818054,
      "learning_rate": 0.0004537783751493429,
      "loss": 6.7764,
      "step": 620
    },
    {
      "epoch": 0.18552543132422136,
      "grad_norm": 0.4355608820915222,
      "learning_rate": 0.0004537037037037037,
      "loss": 6.8281,
      "step": 621
    },
    {
      "epoch": 0.18582418403166778,
      "grad_norm": 0.42373842000961304,
      "learning_rate": 0.00045362903225806454,
      "loss": 7.4287,
      "step": 622
    },
    {
      "epoch": 0.1861229367391142,
      "grad_norm": 0.3839108943939209,
      "learning_rate": 0.00045355436081242535,
      "loss": 7.1514,
      "step": 623
    },
    {
      "epoch": 0.1864216894465606,
      "grad_norm": 0.4588606357574463,
      "learning_rate": 0.0004534796893667861,
      "loss": 7.377,
      "step": 624
    },
    {
      "epoch": 0.18672044215400702,
      "grad_norm": 0.38743725419044495,
      "learning_rate": 0.000453405017921147,
      "loss": 7.2119,
      "step": 625
    },
    {
      "epoch": 0.18701919486145344,
      "grad_norm": 0.34786054491996765,
      "learning_rate": 0.00045333034647550773,
      "loss": 7.2021,
      "step": 626
    },
    {
      "epoch": 0.18731794756889986,
      "grad_norm": 0.5188442468643188,
      "learning_rate": 0.0004532556750298686,
      "loss": 6.8203,
      "step": 627
    },
    {
      "epoch": 0.18761670027634625,
      "grad_norm": 0.34678980708122253,
      "learning_rate": 0.0004531810035842294,
      "loss": 7.7139,
      "step": 628
    },
    {
      "epoch": 0.18791545298379267,
      "grad_norm": 0.38118764758110046,
      "learning_rate": 0.0004531063321385902,
      "loss": 7.3252,
      "step": 629
    },
    {
      "epoch": 0.1882142056912391,
      "grad_norm": 0.357207715511322,
      "learning_rate": 0.00045303166069295104,
      "loss": 7.6729,
      "step": 630
    },
    {
      "epoch": 0.18851295839868548,
      "grad_norm": 0.3754153251647949,
      "learning_rate": 0.0004529569892473118,
      "loss": 7.4785,
      "step": 631
    },
    {
      "epoch": 0.1888117111061319,
      "grad_norm": 0.3679674565792084,
      "learning_rate": 0.00045288231780167266,
      "loss": 7.1943,
      "step": 632
    },
    {
      "epoch": 0.18911046381357832,
      "grad_norm": 0.5033873319625854,
      "learning_rate": 0.0004528076463560334,
      "loss": 7.3643,
      "step": 633
    },
    {
      "epoch": 0.18940921652102471,
      "grad_norm": 0.38266322016716003,
      "learning_rate": 0.0004527329749103943,
      "loss": 7.5508,
      "step": 634
    },
    {
      "epoch": 0.18970796922847113,
      "grad_norm": 0.39511701464653015,
      "learning_rate": 0.00045265830346475505,
      "loss": 6.8467,
      "step": 635
    },
    {
      "epoch": 0.19000672193591756,
      "grad_norm": 0.4473613202571869,
      "learning_rate": 0.0004525836320191159,
      "loss": 7.0176,
      "step": 636
    },
    {
      "epoch": 0.19030547464336395,
      "grad_norm": 0.4341128170490265,
      "learning_rate": 0.00045250896057347673,
      "loss": 7.2852,
      "step": 637
    },
    {
      "epoch": 0.19060422735081037,
      "grad_norm": 0.3157351016998291,
      "learning_rate": 0.00045243428912783754,
      "loss": 7.6836,
      "step": 638
    },
    {
      "epoch": 0.1909029800582568,
      "grad_norm": 0.3980732858181,
      "learning_rate": 0.00045235961768219835,
      "loss": 7.1055,
      "step": 639
    },
    {
      "epoch": 0.19120173276570318,
      "grad_norm": 0.3441426157951355,
      "learning_rate": 0.0004522849462365591,
      "loss": 7.6016,
      "step": 640
    },
    {
      "epoch": 0.1915004854731496,
      "grad_norm": 0.3617788553237915,
      "learning_rate": 0.00045221027479092,
      "loss": 7.4297,
      "step": 641
    },
    {
      "epoch": 0.19179923818059602,
      "grad_norm": 0.4115547239780426,
      "learning_rate": 0.00045213560334528074,
      "loss": 6.957,
      "step": 642
    },
    {
      "epoch": 0.1920979908880424,
      "grad_norm": 0.3798518180847168,
      "learning_rate": 0.0004520609318996416,
      "loss": 7.5117,
      "step": 643
    },
    {
      "epoch": 0.19239674359548883,
      "grad_norm": 0.38557571172714233,
      "learning_rate": 0.00045198626045400237,
      "loss": 7.0547,
      "step": 644
    },
    {
      "epoch": 0.19269549630293525,
      "grad_norm": 0.5245217680931091,
      "learning_rate": 0.00045191158900836323,
      "loss": 6.6494,
      "step": 645
    },
    {
      "epoch": 0.19299424901038165,
      "grad_norm": 0.3415203094482422,
      "learning_rate": 0.00045183691756272405,
      "loss": 7.7441,
      "step": 646
    },
    {
      "epoch": 0.19329300171782807,
      "grad_norm": 0.4497454762458801,
      "learning_rate": 0.0004517622461170848,
      "loss": 7.4971,
      "step": 647
    },
    {
      "epoch": 0.1935917544252745,
      "grad_norm": 0.40038663148880005,
      "learning_rate": 0.00045168757467144567,
      "loss": 7.7158,
      "step": 648
    },
    {
      "epoch": 0.19389050713272088,
      "grad_norm": 0.38530421257019043,
      "learning_rate": 0.00045161290322580643,
      "loss": 7.3926,
      "step": 649
    },
    {
      "epoch": 0.1941892598401673,
      "grad_norm": 0.39906278252601624,
      "learning_rate": 0.0004515382317801673,
      "loss": 6.8848,
      "step": 650
    },
    {
      "epoch": 0.19448801254761372,
      "grad_norm": 0.4303952753543854,
      "learning_rate": 0.00045146356033452806,
      "loss": 7.4951,
      "step": 651
    },
    {
      "epoch": 0.1947867652550601,
      "grad_norm": 0.3416970372200012,
      "learning_rate": 0.0004513888888888889,
      "loss": 7.5947,
      "step": 652
    },
    {
      "epoch": 0.19508551796250653,
      "grad_norm": 0.3872196674346924,
      "learning_rate": 0.0004513142174432497,
      "loss": 7.1836,
      "step": 653
    },
    {
      "epoch": 0.19538427066995295,
      "grad_norm": 0.4784843623638153,
      "learning_rate": 0.00045123954599761055,
      "loss": 7.3574,
      "step": 654
    },
    {
      "epoch": 0.19568302337739935,
      "grad_norm": 0.3752526640892029,
      "learning_rate": 0.00045116487455197136,
      "loss": 7.666,
      "step": 655
    },
    {
      "epoch": 0.19598177608484577,
      "grad_norm": 0.3630761504173279,
      "learning_rate": 0.0004510902031063321,
      "loss": 7.5322,
      "step": 656
    },
    {
      "epoch": 0.19628052879229219,
      "grad_norm": 0.36428409814834595,
      "learning_rate": 0.000451015531660693,
      "loss": 7.4971,
      "step": 657
    },
    {
      "epoch": 0.19657928149973858,
      "grad_norm": 0.4428221583366394,
      "learning_rate": 0.00045094086021505375,
      "loss": 7.2744,
      "step": 658
    },
    {
      "epoch": 0.196878034207185,
      "grad_norm": 0.4339991509914398,
      "learning_rate": 0.0004508661887694146,
      "loss": 7.127,
      "step": 659
    },
    {
      "epoch": 0.19717678691463142,
      "grad_norm": 0.4080313444137573,
      "learning_rate": 0.00045079151732377537,
      "loss": 7.0605,
      "step": 660
    },
    {
      "epoch": 0.1974755396220778,
      "grad_norm": 0.35533300042152405,
      "learning_rate": 0.00045071684587813624,
      "loss": 7.7754,
      "step": 661
    },
    {
      "epoch": 0.19777429232952423,
      "grad_norm": 0.3933696746826172,
      "learning_rate": 0.000450642174432497,
      "loss": 7.1836,
      "step": 662
    },
    {
      "epoch": 0.19807304503697065,
      "grad_norm": 0.4357900321483612,
      "learning_rate": 0.0004505675029868578,
      "loss": 6.9248,
      "step": 663
    },
    {
      "epoch": 0.19837179774441707,
      "grad_norm": 0.35669243335723877,
      "learning_rate": 0.0004504928315412187,
      "loss": 7.4629,
      "step": 664
    },
    {
      "epoch": 0.19867055045186346,
      "grad_norm": 0.40406301617622375,
      "learning_rate": 0.00045041816009557944,
      "loss": 7.5498,
      "step": 665
    },
    {
      "epoch": 0.19896930315930988,
      "grad_norm": 0.37862446904182434,
      "learning_rate": 0.0004503434886499403,
      "loss": 7.1182,
      "step": 666
    },
    {
      "epoch": 0.1992680558667563,
      "grad_norm": 0.31080374121665955,
      "learning_rate": 0.00045026881720430106,
      "loss": 7.8965,
      "step": 667
    },
    {
      "epoch": 0.1995668085742027,
      "grad_norm": 0.3831155300140381,
      "learning_rate": 0.00045019414575866193,
      "loss": 7.127,
      "step": 668
    },
    {
      "epoch": 0.19986556128164912,
      "grad_norm": 0.35943517088890076,
      "learning_rate": 0.0004501194743130227,
      "loss": 7.6123,
      "step": 669
    },
    {
      "epoch": 0.20016431398909554,
      "grad_norm": 0.39862382411956787,
      "learning_rate": 0.00045004480286738355,
      "loss": 6.8672,
      "step": 670
    },
    {
      "epoch": 0.20046306669654193,
      "grad_norm": 0.3977946639060974,
      "learning_rate": 0.0004499701314217443,
      "loss": 7.2354,
      "step": 671
    },
    {
      "epoch": 0.20076181940398835,
      "grad_norm": 0.4058443009853363,
      "learning_rate": 0.0004498954599761051,
      "loss": 7.1074,
      "step": 672
    },
    {
      "epoch": 0.20106057211143477,
      "grad_norm": 0.4078497290611267,
      "learning_rate": 0.000449820788530466,
      "loss": 7.1729,
      "step": 673
    },
    {
      "epoch": 0.20135932481888116,
      "grad_norm": 0.42287686467170715,
      "learning_rate": 0.00044974611708482675,
      "loss": 7.375,
      "step": 674
    },
    {
      "epoch": 0.20165807752632758,
      "grad_norm": 0.4304882287979126,
      "learning_rate": 0.0004496714456391876,
      "loss": 7.0117,
      "step": 675
    },
    {
      "epoch": 0.201956830233774,
      "grad_norm": 0.42946475744247437,
      "learning_rate": 0.0004495967741935484,
      "loss": 7.2471,
      "step": 676
    },
    {
      "epoch": 0.2022555829412204,
      "grad_norm": 0.4023226499557495,
      "learning_rate": 0.00044952210274790924,
      "loss": 7.4561,
      "step": 677
    },
    {
      "epoch": 0.20255433564866682,
      "grad_norm": 0.4913168251514435,
      "learning_rate": 0.00044944743130227,
      "loss": 6.6797,
      "step": 678
    },
    {
      "epoch": 0.20285308835611324,
      "grad_norm": 0.47047677636146545,
      "learning_rate": 0.0004493727598566308,
      "loss": 7.1924,
      "step": 679
    },
    {
      "epoch": 0.20315184106355963,
      "grad_norm": 0.31947970390319824,
      "learning_rate": 0.00044929808841099163,
      "loss": 7.4912,
      "step": 680
    },
    {
      "epoch": 0.20345059377100605,
      "grad_norm": 0.3915490210056305,
      "learning_rate": 0.00044922341696535244,
      "loss": 7.1162,
      "step": 681
    },
    {
      "epoch": 0.20374934647845247,
      "grad_norm": 0.6536716222763062,
      "learning_rate": 0.0004491487455197133,
      "loss": 6.3047,
      "step": 682
    },
    {
      "epoch": 0.20404809918589886,
      "grad_norm": 0.4451655149459839,
      "learning_rate": 0.00044907407407407407,
      "loss": 7.1592,
      "step": 683
    },
    {
      "epoch": 0.20434685189334528,
      "grad_norm": 0.35187673568725586,
      "learning_rate": 0.00044899940262843493,
      "loss": 7.4609,
      "step": 684
    },
    {
      "epoch": 0.2046456046007917,
      "grad_norm": 0.3187737464904785,
      "learning_rate": 0.0004489247311827957,
      "loss": 7.8906,
      "step": 685
    },
    {
      "epoch": 0.2049443573082381,
      "grad_norm": 0.40084370970726013,
      "learning_rate": 0.00044885005973715656,
      "loss": 7.0869,
      "step": 686
    },
    {
      "epoch": 0.20524311001568452,
      "grad_norm": 0.42563003301620483,
      "learning_rate": 0.0004487753882915173,
      "loss": 7.374,
      "step": 687
    },
    {
      "epoch": 0.20554186272313094,
      "grad_norm": 0.41075465083122253,
      "learning_rate": 0.00044870071684587813,
      "loss": 7.1553,
      "step": 688
    },
    {
      "epoch": 0.20584061543057733,
      "grad_norm": 0.35735753178596497,
      "learning_rate": 0.00044862604540023894,
      "loss": 7.5332,
      "step": 689
    },
    {
      "epoch": 0.20613936813802375,
      "grad_norm": 0.337489515542984,
      "learning_rate": 0.00044855137395459976,
      "loss": 7.2363,
      "step": 690
    },
    {
      "epoch": 0.20643812084547017,
      "grad_norm": 0.3457932770252228,
      "learning_rate": 0.0004484767025089606,
      "loss": 7.5195,
      "step": 691
    },
    {
      "epoch": 0.20673687355291656,
      "grad_norm": 0.4107029139995575,
      "learning_rate": 0.0004484020310633214,
      "loss": 7.2256,
      "step": 692
    },
    {
      "epoch": 0.20703562626036298,
      "grad_norm": 0.39478421211242676,
      "learning_rate": 0.00044832735961768225,
      "loss": 7.5059,
      "step": 693
    },
    {
      "epoch": 0.2073343789678094,
      "grad_norm": 0.37058475613594055,
      "learning_rate": 0.000448252688172043,
      "loss": 7.248,
      "step": 694
    },
    {
      "epoch": 0.2076331316752558,
      "grad_norm": 0.4061013460159302,
      "learning_rate": 0.0004481780167264038,
      "loss": 7.3711,
      "step": 695
    },
    {
      "epoch": 0.20793188438270221,
      "grad_norm": 0.3810076117515564,
      "learning_rate": 0.00044810334528076463,
      "loss": 7.3574,
      "step": 696
    },
    {
      "epoch": 0.20823063709014863,
      "grad_norm": 0.4121871888637543,
      "learning_rate": 0.00044802867383512545,
      "loss": 7.043,
      "step": 697
    },
    {
      "epoch": 0.20852938979759503,
      "grad_norm": 0.4459388852119446,
      "learning_rate": 0.00044795400238948626,
      "loss": 7.2451,
      "step": 698
    },
    {
      "epoch": 0.20882814250504145,
      "grad_norm": 0.43536895513534546,
      "learning_rate": 0.0004478793309438471,
      "loss": 7.5674,
      "step": 699
    },
    {
      "epoch": 0.20912689521248787,
      "grad_norm": 0.46629413962364197,
      "learning_rate": 0.0004478046594982079,
      "loss": 7.043,
      "step": 700
    },
    {
      "epoch": 0.2094256479199343,
      "grad_norm": 0.5301429033279419,
      "learning_rate": 0.0004477299880525687,
      "loss": 6.2744,
      "step": 701
    },
    {
      "epoch": 0.20972440062738068,
      "grad_norm": 0.4304063022136688,
      "learning_rate": 0.00044765531660692957,
      "loss": 6.7617,
      "step": 702
    },
    {
      "epoch": 0.2100231533348271,
      "grad_norm": 0.4079777002334595,
      "learning_rate": 0.0004475806451612903,
      "loss": 7.5693,
      "step": 703
    },
    {
      "epoch": 0.21032190604227352,
      "grad_norm": 0.38183334469795227,
      "learning_rate": 0.00044750597371565114,
      "loss": 6.9434,
      "step": 704
    },
    {
      "epoch": 0.2106206587497199,
      "grad_norm": 0.36060017347335815,
      "learning_rate": 0.00044743130227001195,
      "loss": 7.583,
      "step": 705
    },
    {
      "epoch": 0.21091941145716633,
      "grad_norm": 0.36223557591438293,
      "learning_rate": 0.00044735663082437276,
      "loss": 7.4258,
      "step": 706
    },
    {
      "epoch": 0.21121816416461275,
      "grad_norm": 0.3928808271884918,
      "learning_rate": 0.0004472819593787336,
      "loss": 7.6377,
      "step": 707
    },
    {
      "epoch": 0.21151691687205915,
      "grad_norm": 0.4357273280620575,
      "learning_rate": 0.0004472072879330944,
      "loss": 7.3408,
      "step": 708
    },
    {
      "epoch": 0.21181566957950557,
      "grad_norm": 0.3856928050518036,
      "learning_rate": 0.0004471326164874552,
      "loss": 7.6719,
      "step": 709
    },
    {
      "epoch": 0.212114422286952,
      "grad_norm": 0.3722672760486603,
      "learning_rate": 0.000447057945041816,
      "loss": 7.7949,
      "step": 710
    },
    {
      "epoch": 0.21241317499439838,
      "grad_norm": 0.46537214517593384,
      "learning_rate": 0.00044698327359617683,
      "loss": 7.0049,
      "step": 711
    },
    {
      "epoch": 0.2127119277018448,
      "grad_norm": 0.4096795618534088,
      "learning_rate": 0.00044690860215053764,
      "loss": 7.2275,
      "step": 712
    },
    {
      "epoch": 0.21301068040929122,
      "grad_norm": 0.3321285545825958,
      "learning_rate": 0.00044683393070489845,
      "loss": 7.791,
      "step": 713
    },
    {
      "epoch": 0.2133094331167376,
      "grad_norm": 0.4175223112106323,
      "learning_rate": 0.00044675925925925927,
      "loss": 7.2383,
      "step": 714
    },
    {
      "epoch": 0.21360818582418403,
      "grad_norm": 0.554729700088501,
      "learning_rate": 0.0004466845878136201,
      "loss": 6.957,
      "step": 715
    },
    {
      "epoch": 0.21390693853163045,
      "grad_norm": 0.40223103761672974,
      "learning_rate": 0.0004466099163679809,
      "loss": 7.4307,
      "step": 716
    },
    {
      "epoch": 0.21420569123907685,
      "grad_norm": 0.4515427350997925,
      "learning_rate": 0.0004465352449223417,
      "loss": 7.3398,
      "step": 717
    },
    {
      "epoch": 0.21450444394652327,
      "grad_norm": 0.42579343914985657,
      "learning_rate": 0.0004464605734767025,
      "loss": 7.1123,
      "step": 718
    },
    {
      "epoch": 0.21480319665396969,
      "grad_norm": 0.3857142925262451,
      "learning_rate": 0.00044638590203106333,
      "loss": 7.0029,
      "step": 719
    },
    {
      "epoch": 0.21510194936141608,
      "grad_norm": 0.43541309237480164,
      "learning_rate": 0.00044631123058542414,
      "loss": 7.1973,
      "step": 720
    },
    {
      "epoch": 0.2154007020688625,
      "grad_norm": 0.41649940609931946,
      "learning_rate": 0.00044623655913978496,
      "loss": 7.0547,
      "step": 721
    },
    {
      "epoch": 0.21569945477630892,
      "grad_norm": 0.46136072278022766,
      "learning_rate": 0.00044616188769414577,
      "loss": 7.0449,
      "step": 722
    },
    {
      "epoch": 0.2159982074837553,
      "grad_norm": 0.4854053556919098,
      "learning_rate": 0.0004460872162485066,
      "loss": 6.6396,
      "step": 723
    },
    {
      "epoch": 0.21629696019120173,
      "grad_norm": 0.47906750440597534,
      "learning_rate": 0.0004460125448028674,
      "loss": 7.6074,
      "step": 724
    },
    {
      "epoch": 0.21659571289864815,
      "grad_norm": 0.41345587372779846,
      "learning_rate": 0.0004459378733572282,
      "loss": 6.9346,
      "step": 725
    },
    {
      "epoch": 0.21689446560609454,
      "grad_norm": 0.3388844132423401,
      "learning_rate": 0.000445863201911589,
      "loss": 7.6631,
      "step": 726
    },
    {
      "epoch": 0.21719321831354096,
      "grad_norm": 0.513288140296936,
      "learning_rate": 0.0004457885304659498,
      "loss": 6.4883,
      "step": 727
    },
    {
      "epoch": 0.21749197102098738,
      "grad_norm": 0.45356428623199463,
      "learning_rate": 0.00044571385902031065,
      "loss": 7.2061,
      "step": 728
    },
    {
      "epoch": 0.21779072372843378,
      "grad_norm": 0.32497695088386536,
      "learning_rate": 0.00044563918757467146,
      "loss": 7.5879,
      "step": 729
    },
    {
      "epoch": 0.2180894764358802,
      "grad_norm": 0.4006805419921875,
      "learning_rate": 0.00044556451612903227,
      "loss": 7.6592,
      "step": 730
    },
    {
      "epoch": 0.21838822914332662,
      "grad_norm": 0.5001377463340759,
      "learning_rate": 0.0004454898446833931,
      "loss": 6.6309,
      "step": 731
    },
    {
      "epoch": 0.218686981850773,
      "grad_norm": 0.40575700998306274,
      "learning_rate": 0.0004454151732377539,
      "loss": 7.1797,
      "step": 732
    },
    {
      "epoch": 0.21898573455821943,
      "grad_norm": 0.43372267484664917,
      "learning_rate": 0.0004453405017921147,
      "loss": 7.1787,
      "step": 733
    },
    {
      "epoch": 0.21928448726566585,
      "grad_norm": 0.3778757154941559,
      "learning_rate": 0.0004452658303464755,
      "loss": 7.3379,
      "step": 734
    },
    {
      "epoch": 0.21958323997311224,
      "grad_norm": 0.4057849645614624,
      "learning_rate": 0.00044519115890083634,
      "loss": 7.2871,
      "step": 735
    },
    {
      "epoch": 0.21988199268055866,
      "grad_norm": 0.3847145736217499,
      "learning_rate": 0.0004451164874551971,
      "loss": 7.5518,
      "step": 736
    },
    {
      "epoch": 0.22018074538800508,
      "grad_norm": 0.47657546401023865,
      "learning_rate": 0.00044504181600955796,
      "loss": 6.9531,
      "step": 737
    },
    {
      "epoch": 0.2204794980954515,
      "grad_norm": 0.49452465772628784,
      "learning_rate": 0.0004449671445639188,
      "loss": 6.9648,
      "step": 738
    },
    {
      "epoch": 0.2207782508028979,
      "grad_norm": 0.39857950806617737,
      "learning_rate": 0.0004448924731182796,
      "loss": 7.3818,
      "step": 739
    },
    {
      "epoch": 0.22107700351034432,
      "grad_norm": 0.5226014852523804,
      "learning_rate": 0.0004448178016726404,
      "loss": 6.9971,
      "step": 740
    },
    {
      "epoch": 0.22137575621779074,
      "grad_norm": 0.4519363045692444,
      "learning_rate": 0.0004447431302270012,
      "loss": 6.8096,
      "step": 741
    },
    {
      "epoch": 0.22167450892523713,
      "grad_norm": 0.43581196665763855,
      "learning_rate": 0.00044466845878136203,
      "loss": 7.5605,
      "step": 742
    },
    {
      "epoch": 0.22197326163268355,
      "grad_norm": 0.4889175295829773,
      "learning_rate": 0.0004445937873357228,
      "loss": 6.9033,
      "step": 743
    },
    {
      "epoch": 0.22227201434012997,
      "grad_norm": 0.486364483833313,
      "learning_rate": 0.00044451911589008365,
      "loss": 6.6016,
      "step": 744
    },
    {
      "epoch": 0.22257076704757636,
      "grad_norm": 0.41740578413009644,
      "learning_rate": 0.0004444444444444444,
      "loss": 7.1426,
      "step": 745
    },
    {
      "epoch": 0.22286951975502278,
      "grad_norm": 0.3550545871257782,
      "learning_rate": 0.0004443697729988053,
      "loss": 7.4805,
      "step": 746
    },
    {
      "epoch": 0.2231682724624692,
      "grad_norm": 0.3622879385948181,
      "learning_rate": 0.0004442951015531661,
      "loss": 7.2256,
      "step": 747
    },
    {
      "epoch": 0.2234670251699156,
      "grad_norm": 0.3899250626564026,
      "learning_rate": 0.0004442204301075269,
      "loss": 7.5566,
      "step": 748
    },
    {
      "epoch": 0.22376577787736202,
      "grad_norm": 0.48321664333343506,
      "learning_rate": 0.0004441457586618877,
      "loss": 6.832,
      "step": 749
    },
    {
      "epoch": 0.22406453058480844,
      "grad_norm": 0.3857758343219757,
      "learning_rate": 0.00044407108721624853,
      "loss": 7.2617,
      "step": 750
    },
    {
      "epoch": 0.22436328329225483,
      "grad_norm": 0.4406209886074066,
      "learning_rate": 0.00044399641577060934,
      "loss": 7.3379,
      "step": 751
    },
    {
      "epoch": 0.22466203599970125,
      "grad_norm": 0.5260152816772461,
      "learning_rate": 0.0004439217443249701,
      "loss": 6.6836,
      "step": 752
    },
    {
      "epoch": 0.22496078870714767,
      "grad_norm": 0.46181535720825195,
      "learning_rate": 0.00044384707287933097,
      "loss": 6.7881,
      "step": 753
    },
    {
      "epoch": 0.22525954141459406,
      "grad_norm": 0.6058606505393982,
      "learning_rate": 0.00044377240143369173,
      "loss": 6.416,
      "step": 754
    },
    {
      "epoch": 0.22555829412204048,
      "grad_norm": 0.4534761309623718,
      "learning_rate": 0.0004436977299880526,
      "loss": 7.2461,
      "step": 755
    },
    {
      "epoch": 0.2258570468294869,
      "grad_norm": 0.42217007279396057,
      "learning_rate": 0.0004436230585424134,
      "loss": 7.3545,
      "step": 756
    },
    {
      "epoch": 0.2261557995369333,
      "grad_norm": 0.36713290214538574,
      "learning_rate": 0.0004435483870967742,
      "loss": 7.8701,
      "step": 757
    },
    {
      "epoch": 0.22645455224437971,
      "grad_norm": 0.3474491238594055,
      "learning_rate": 0.00044347371565113503,
      "loss": 7.5928,
      "step": 758
    },
    {
      "epoch": 0.22675330495182613,
      "grad_norm": 0.3256765902042389,
      "learning_rate": 0.0004433990442054958,
      "loss": 7.7773,
      "step": 759
    },
    {
      "epoch": 0.22705205765927253,
      "grad_norm": 0.5174461007118225,
      "learning_rate": 0.00044332437275985666,
      "loss": 6.7617,
      "step": 760
    },
    {
      "epoch": 0.22735081036671895,
      "grad_norm": 0.3373779058456421,
      "learning_rate": 0.0004432497013142174,
      "loss": 7.6387,
      "step": 761
    },
    {
      "epoch": 0.22764956307416537,
      "grad_norm": 0.39637190103530884,
      "learning_rate": 0.0004431750298685783,
      "loss": 7.3125,
      "step": 762
    },
    {
      "epoch": 0.22794831578161176,
      "grad_norm": 0.42789018154144287,
      "learning_rate": 0.00044310035842293904,
      "loss": 7.2324,
      "step": 763
    },
    {
      "epoch": 0.22824706848905818,
      "grad_norm": 0.3285674750804901,
      "learning_rate": 0.0004430256869772999,
      "loss": 7.4385,
      "step": 764
    },
    {
      "epoch": 0.2285458211965046,
      "grad_norm": 0.4036949574947357,
      "learning_rate": 0.0004429510155316607,
      "loss": 7.5088,
      "step": 765
    },
    {
      "epoch": 0.228844573903951,
      "grad_norm": 0.4450298547744751,
      "learning_rate": 0.00044287634408602154,
      "loss": 7.5752,
      "step": 766
    },
    {
      "epoch": 0.2291433266113974,
      "grad_norm": 0.42466631531715393,
      "learning_rate": 0.00044280167264038235,
      "loss": 7.0586,
      "step": 767
    },
    {
      "epoch": 0.22944207931884383,
      "grad_norm": 0.37189003825187683,
      "learning_rate": 0.0004427270011947431,
      "loss": 7.2451,
      "step": 768
    },
    {
      "epoch": 0.22974083202629023,
      "grad_norm": 0.34833380579948425,
      "learning_rate": 0.000442652329749104,
      "loss": 7.8906,
      "step": 769
    },
    {
      "epoch": 0.23003958473373665,
      "grad_norm": 0.5377634167671204,
      "learning_rate": 0.00044257765830346473,
      "loss": 6.748,
      "step": 770
    },
    {
      "epoch": 0.23033833744118307,
      "grad_norm": 0.4270717203617096,
      "learning_rate": 0.0004425029868578256,
      "loss": 7.3311,
      "step": 771
    },
    {
      "epoch": 0.23063709014862946,
      "grad_norm": 0.4251841902732849,
      "learning_rate": 0.00044242831541218636,
      "loss": 7.3564,
      "step": 772
    },
    {
      "epoch": 0.23093584285607588,
      "grad_norm": 0.42088013887405396,
      "learning_rate": 0.0004423536439665472,
      "loss": 7.4062,
      "step": 773
    },
    {
      "epoch": 0.2312345955635223,
      "grad_norm": 0.4633598029613495,
      "learning_rate": 0.00044227897252090804,
      "loss": 7.458,
      "step": 774
    },
    {
      "epoch": 0.2315333482709687,
      "grad_norm": 0.43119463324546814,
      "learning_rate": 0.0004422043010752688,
      "loss": 6.8721,
      "step": 775
    },
    {
      "epoch": 0.2318321009784151,
      "grad_norm": 0.4418625235557556,
      "learning_rate": 0.00044212962962962966,
      "loss": 7.2168,
      "step": 776
    },
    {
      "epoch": 0.23213085368586153,
      "grad_norm": 0.46492862701416016,
      "learning_rate": 0.0004420549581839904,
      "loss": 6.9004,
      "step": 777
    },
    {
      "epoch": 0.23242960639330795,
      "grad_norm": 0.3611902892589569,
      "learning_rate": 0.0004419802867383513,
      "loss": 7.3301,
      "step": 778
    },
    {
      "epoch": 0.23272835910075435,
      "grad_norm": 0.32795652747154236,
      "learning_rate": 0.00044190561529271205,
      "loss": 7.8711,
      "step": 779
    },
    {
      "epoch": 0.23302711180820077,
      "grad_norm": 0.4667639434337616,
      "learning_rate": 0.0004418309438470729,
      "loss": 6.9668,
      "step": 780
    },
    {
      "epoch": 0.23332586451564719,
      "grad_norm": 0.43481895327568054,
      "learning_rate": 0.0004417562724014337,
      "loss": 7.0312,
      "step": 781
    },
    {
      "epoch": 0.23362461722309358,
      "grad_norm": 0.35526904463768005,
      "learning_rate": 0.00044168160095579454,
      "loss": 7.7061,
      "step": 782
    },
    {
      "epoch": 0.23392336993054,
      "grad_norm": 0.45083528757095337,
      "learning_rate": 0.00044160692951015536,
      "loss": 6.8311,
      "step": 783
    },
    {
      "epoch": 0.23422212263798642,
      "grad_norm": 0.45404699444770813,
      "learning_rate": 0.0004415322580645161,
      "loss": 6.9971,
      "step": 784
    },
    {
      "epoch": 0.2345208753454328,
      "grad_norm": 0.4249763786792755,
      "learning_rate": 0.000441457586618877,
      "loss": 7.0723,
      "step": 785
    },
    {
      "epoch": 0.23481962805287923,
      "grad_norm": 0.38304606080055237,
      "learning_rate": 0.00044138291517323774,
      "loss": 7.5439,
      "step": 786
    },
    {
      "epoch": 0.23511838076032565,
      "grad_norm": 0.3788539171218872,
      "learning_rate": 0.0004413082437275986,
      "loss": 7.5654,
      "step": 787
    },
    {
      "epoch": 0.23541713346777204,
      "grad_norm": 0.3718545138835907,
      "learning_rate": 0.00044123357228195937,
      "loss": 7.6309,
      "step": 788
    },
    {
      "epoch": 0.23571588617521846,
      "grad_norm": 0.39574339985847473,
      "learning_rate": 0.00044115890083632023,
      "loss": 7.208,
      "step": 789
    },
    {
      "epoch": 0.23601463888266488,
      "grad_norm": 0.3946608603000641,
      "learning_rate": 0.000441084229390681,
      "loss": 7.1035,
      "step": 790
    },
    {
      "epoch": 0.23631339159011128,
      "grad_norm": 0.6183930039405823,
      "learning_rate": 0.0004410095579450418,
      "loss": 6.5049,
      "step": 791
    },
    {
      "epoch": 0.2366121442975577,
      "grad_norm": 0.43061065673828125,
      "learning_rate": 0.0004409348864994026,
      "loss": 7.6357,
      "step": 792
    },
    {
      "epoch": 0.23691089700500412,
      "grad_norm": 0.3741077184677124,
      "learning_rate": 0.00044086021505376343,
      "loss": 7.2139,
      "step": 793
    },
    {
      "epoch": 0.2372096497124505,
      "grad_norm": 0.3502742350101471,
      "learning_rate": 0.0004407855436081243,
      "loss": 7.8447,
      "step": 794
    },
    {
      "epoch": 0.23750840241989693,
      "grad_norm": 0.4088904559612274,
      "learning_rate": 0.00044071087216248506,
      "loss": 7.1035,
      "step": 795
    },
    {
      "epoch": 0.23780715512734335,
      "grad_norm": 0.49940183758735657,
      "learning_rate": 0.0004406362007168459,
      "loss": 7.5742,
      "step": 796
    },
    {
      "epoch": 0.23810590783478974,
      "grad_norm": 0.4954121708869934,
      "learning_rate": 0.0004405615292712067,
      "loss": 7.3447,
      "step": 797
    },
    {
      "epoch": 0.23840466054223616,
      "grad_norm": 0.4448952078819275,
      "learning_rate": 0.00044048685782556755,
      "loss": 7.5518,
      "step": 798
    },
    {
      "epoch": 0.23870341324968258,
      "grad_norm": 0.4292686879634857,
      "learning_rate": 0.0004404121863799283,
      "loss": 6.8701,
      "step": 799
    },
    {
      "epoch": 0.23900216595712898,
      "grad_norm": 0.4051790237426758,
      "learning_rate": 0.0004403375149342891,
      "loss": 7.0977,
      "step": 800
    },
    {
      "epoch": 0.23900216595712898,
      "eval_bleu": 0.09268497379540871,
      "eval_loss": 7.11328125,
      "eval_runtime": 482.1033,
      "eval_samples_per_second": 2.923,
      "eval_steps_per_second": 0.185,
      "step": 800
    },
    {
      "epoch": 0.2393009186645754,
      "grad_norm": 0.4723067283630371,
      "learning_rate": 0.00044026284348864993,
      "loss": 6.8682,
      "step": 801
    },
    {
      "epoch": 0.23959967137202182,
      "grad_norm": 0.4082038402557373,
      "learning_rate": 0.00044018817204301075,
      "loss": 7.1895,
      "step": 802
    },
    {
      "epoch": 0.2398984240794682,
      "grad_norm": 0.496094286441803,
      "learning_rate": 0.0004401135005973716,
      "loss": 7.0098,
      "step": 803
    },
    {
      "epoch": 0.24019717678691463,
      "grad_norm": 0.49288639426231384,
      "learning_rate": 0.00044003882915173237,
      "loss": 7.1094,
      "step": 804
    },
    {
      "epoch": 0.24049592949436105,
      "grad_norm": 0.3998868465423584,
      "learning_rate": 0.00043996415770609324,
      "loss": 7.8066,
      "step": 805
    },
    {
      "epoch": 0.24079468220180744,
      "grad_norm": 0.42048683762550354,
      "learning_rate": 0.000439889486260454,
      "loss": 7.2666,
      "step": 806
    },
    {
      "epoch": 0.24109343490925386,
      "grad_norm": 0.5373194813728333,
      "learning_rate": 0.0004398148148148148,
      "loss": 6.9785,
      "step": 807
    },
    {
      "epoch": 0.24139218761670028,
      "grad_norm": 0.49598559737205505,
      "learning_rate": 0.0004397401433691756,
      "loss": 7.3652,
      "step": 808
    },
    {
      "epoch": 0.24169094032414667,
      "grad_norm": 0.3324628174304962,
      "learning_rate": 0.00043966547192353644,
      "loss": 8.1182,
      "step": 809
    },
    {
      "epoch": 0.2419896930315931,
      "grad_norm": 0.36435815691947937,
      "learning_rate": 0.00043959080047789725,
      "loss": 7.5576,
      "step": 810
    },
    {
      "epoch": 0.24228844573903952,
      "grad_norm": 0.38541746139526367,
      "learning_rate": 0.00043951612903225806,
      "loss": 7.4707,
      "step": 811
    },
    {
      "epoch": 0.2425871984464859,
      "grad_norm": 0.38939177989959717,
      "learning_rate": 0.00043944145758661893,
      "loss": 7.4492,
      "step": 812
    },
    {
      "epoch": 0.24288595115393233,
      "grad_norm": 0.7057454586029053,
      "learning_rate": 0.0004393667861409797,
      "loss": 7.1445,
      "step": 813
    },
    {
      "epoch": 0.24318470386137875,
      "grad_norm": 0.32446718215942383,
      "learning_rate": 0.00043929211469534055,
      "loss": 8.0498,
      "step": 814
    },
    {
      "epoch": 0.24348345656882517,
      "grad_norm": 0.5168993473052979,
      "learning_rate": 0.0004392174432497013,
      "loss": 6.957,
      "step": 815
    },
    {
      "epoch": 0.24378220927627156,
      "grad_norm": 0.41026151180267334,
      "learning_rate": 0.0004391427718040621,
      "loss": 7.291,
      "step": 816
    },
    {
      "epoch": 0.24408096198371798,
      "grad_norm": 0.4517073631286621,
      "learning_rate": 0.00043906810035842294,
      "loss": 7.1455,
      "step": 817
    },
    {
      "epoch": 0.2443797146911644,
      "grad_norm": 0.414474755525589,
      "learning_rate": 0.00043899342891278375,
      "loss": 7.3086,
      "step": 818
    },
    {
      "epoch": 0.2446784673986108,
      "grad_norm": 0.5586822628974915,
      "learning_rate": 0.00043891875746714456,
      "loss": 6.5918,
      "step": 819
    },
    {
      "epoch": 0.24497722010605721,
      "grad_norm": 0.46947115659713745,
      "learning_rate": 0.0004388440860215054,
      "loss": 7.2402,
      "step": 820
    },
    {
      "epoch": 0.24527597281350363,
      "grad_norm": 0.3789454698562622,
      "learning_rate": 0.00043876941457586624,
      "loss": 7.6982,
      "step": 821
    },
    {
      "epoch": 0.24557472552095003,
      "grad_norm": 0.4131413698196411,
      "learning_rate": 0.000438694743130227,
      "loss": 7.1621,
      "step": 822
    },
    {
      "epoch": 0.24587347822839645,
      "grad_norm": 0.382409930229187,
      "learning_rate": 0.0004386200716845878,
      "loss": 7.2578,
      "step": 823
    },
    {
      "epoch": 0.24617223093584287,
      "grad_norm": 0.5812870860099792,
      "learning_rate": 0.00043854540023894863,
      "loss": 6.4443,
      "step": 824
    },
    {
      "epoch": 0.24647098364328926,
      "grad_norm": 0.4468601942062378,
      "learning_rate": 0.00043847072879330944,
      "loss": 7.0801,
      "step": 825
    },
    {
      "epoch": 0.24676973635073568,
      "grad_norm": 0.3774029016494751,
      "learning_rate": 0.00043839605734767025,
      "loss": 7.3457,
      "step": 826
    },
    {
      "epoch": 0.2470684890581821,
      "grad_norm": 0.4216448962688446,
      "learning_rate": 0.00043832138590203107,
      "loss": 7.6172,
      "step": 827
    },
    {
      "epoch": 0.2473672417656285,
      "grad_norm": 0.3918052017688751,
      "learning_rate": 0.0004382467144563919,
      "loss": 7.2012,
      "step": 828
    },
    {
      "epoch": 0.2476659944730749,
      "grad_norm": 0.44214770197868347,
      "learning_rate": 0.0004381720430107527,
      "loss": 7.0391,
      "step": 829
    },
    {
      "epoch": 0.24796474718052133,
      "grad_norm": 0.38930222392082214,
      "learning_rate": 0.00043809737156511356,
      "loss": 7.375,
      "step": 830
    },
    {
      "epoch": 0.24826349988796773,
      "grad_norm": 0.3546565771102905,
      "learning_rate": 0.0004380227001194743,
      "loss": 7.7666,
      "step": 831
    },
    {
      "epoch": 0.24856225259541415,
      "grad_norm": 0.47644031047821045,
      "learning_rate": 0.00043794802867383513,
      "loss": 6.8301,
      "step": 832
    },
    {
      "epoch": 0.24886100530286057,
      "grad_norm": 0.3499388098716736,
      "learning_rate": 0.00043787335722819594,
      "loss": 7.418,
      "step": 833
    },
    {
      "epoch": 0.24915975801030696,
      "grad_norm": 0.4086293578147888,
      "learning_rate": 0.00043779868578255676,
      "loss": 7.0137,
      "step": 834
    },
    {
      "epoch": 0.24945851071775338,
      "grad_norm": 0.4585743546485901,
      "learning_rate": 0.00043772401433691757,
      "loss": 6.9082,
      "step": 835
    },
    {
      "epoch": 0.2497572634251998,
      "grad_norm": 0.4046449065208435,
      "learning_rate": 0.0004376493428912784,
      "loss": 7.3057,
      "step": 836
    },
    {
      "epoch": 0.2500560161326462,
      "grad_norm": 0.6411067247390747,
      "learning_rate": 0.0004375746714456392,
      "loss": 6.7051,
      "step": 837
    },
    {
      "epoch": 0.2503547688400926,
      "grad_norm": 0.36595621705055237,
      "learning_rate": 0.0004375,
      "loss": 7.2129,
      "step": 838
    },
    {
      "epoch": 0.250653521547539,
      "grad_norm": 0.4720175266265869,
      "learning_rate": 0.0004374253285543608,
      "loss": 7.0781,
      "step": 839
    },
    {
      "epoch": 0.25095227425498545,
      "grad_norm": 0.42469558119773865,
      "learning_rate": 0.00043735065710872163,
      "loss": 7.043,
      "step": 840
    },
    {
      "epoch": 0.25125102696243184,
      "grad_norm": 0.44197142124176025,
      "learning_rate": 0.00043727598566308245,
      "loss": 7.1924,
      "step": 841
    },
    {
      "epoch": 0.25154977966987824,
      "grad_norm": 0.4659299850463867,
      "learning_rate": 0.00043720131421744326,
      "loss": 7.1729,
      "step": 842
    },
    {
      "epoch": 0.2518485323773247,
      "grad_norm": 0.3998686671257019,
      "learning_rate": 0.0004371266427718041,
      "loss": 7.4912,
      "step": 843
    },
    {
      "epoch": 0.2521472850847711,
      "grad_norm": 0.3724229335784912,
      "learning_rate": 0.0004370519713261649,
      "loss": 7.9717,
      "step": 844
    },
    {
      "epoch": 0.25244603779221747,
      "grad_norm": 0.37791314721107483,
      "learning_rate": 0.0004369772998805257,
      "loss": 7.7568,
      "step": 845
    },
    {
      "epoch": 0.2527447904996639,
      "grad_norm": 0.48041200637817383,
      "learning_rate": 0.00043690262843488646,
      "loss": 6.8857,
      "step": 846
    },
    {
      "epoch": 0.2530435432071103,
      "grad_norm": 0.39764827489852905,
      "learning_rate": 0.0004368279569892473,
      "loss": 6.9229,
      "step": 847
    },
    {
      "epoch": 0.2533422959145567,
      "grad_norm": 0.40556612610816956,
      "learning_rate": 0.00043675328554360814,
      "loss": 7.082,
      "step": 848
    },
    {
      "epoch": 0.25364104862200315,
      "grad_norm": 0.36819037795066833,
      "learning_rate": 0.00043667861409796895,
      "loss": 7.6426,
      "step": 849
    },
    {
      "epoch": 0.25393980132944954,
      "grad_norm": 0.3755166232585907,
      "learning_rate": 0.00043660394265232976,
      "loss": 7.5801,
      "step": 850
    },
    {
      "epoch": 0.25423855403689594,
      "grad_norm": 0.33136412501335144,
      "learning_rate": 0.0004365292712066906,
      "loss": 7.8838,
      "step": 851
    },
    {
      "epoch": 0.2545373067443424,
      "grad_norm": 0.45201340317726135,
      "learning_rate": 0.0004364545997610514,
      "loss": 7.3086,
      "step": 852
    },
    {
      "epoch": 0.2548360594517888,
      "grad_norm": 0.39920032024383545,
      "learning_rate": 0.0004363799283154122,
      "loss": 7.2324,
      "step": 853
    },
    {
      "epoch": 0.25513481215923517,
      "grad_norm": 0.39989739656448364,
      "learning_rate": 0.000436305256869773,
      "loss": 7.6963,
      "step": 854
    },
    {
      "epoch": 0.2554335648666816,
      "grad_norm": 0.3279734253883362,
      "learning_rate": 0.0004362305854241338,
      "loss": 7.7969,
      "step": 855
    },
    {
      "epoch": 0.255732317574128,
      "grad_norm": 0.37907564640045166,
      "learning_rate": 0.00043615591397849464,
      "loss": 7.0273,
      "step": 856
    },
    {
      "epoch": 0.2560310702815744,
      "grad_norm": 0.38861390948295593,
      "learning_rate": 0.00043608124253285545,
      "loss": 7.5273,
      "step": 857
    },
    {
      "epoch": 0.25632982298902085,
      "grad_norm": 0.5566051602363586,
      "learning_rate": 0.00043600657108721627,
      "loss": 6.585,
      "step": 858
    },
    {
      "epoch": 0.25662857569646724,
      "grad_norm": 0.46195581555366516,
      "learning_rate": 0.0004359318996415771,
      "loss": 7.1416,
      "step": 859
    },
    {
      "epoch": 0.25692732840391364,
      "grad_norm": 0.4175141155719757,
      "learning_rate": 0.0004358572281959379,
      "loss": 6.9648,
      "step": 860
    },
    {
      "epoch": 0.2572260811113601,
      "grad_norm": 0.35858628153800964,
      "learning_rate": 0.0004357825567502987,
      "loss": 7.584,
      "step": 861
    },
    {
      "epoch": 0.2575248338188065,
      "grad_norm": 0.4719788432121277,
      "learning_rate": 0.00043570788530465946,
      "loss": 7.4131,
      "step": 862
    },
    {
      "epoch": 0.25782358652625287,
      "grad_norm": 0.42543765902519226,
      "learning_rate": 0.00043563321385902033,
      "loss": 7.5186,
      "step": 863
    },
    {
      "epoch": 0.2581223392336993,
      "grad_norm": 0.4605262279510498,
      "learning_rate": 0.0004355585424133811,
      "loss": 6.915,
      "step": 864
    },
    {
      "epoch": 0.2584210919411457,
      "grad_norm": 0.427592396736145,
      "learning_rate": 0.00043548387096774196,
      "loss": 7.2627,
      "step": 865
    },
    {
      "epoch": 0.2587198446485921,
      "grad_norm": 0.45586714148521423,
      "learning_rate": 0.00043540919952210277,
      "loss": 7.1318,
      "step": 866
    },
    {
      "epoch": 0.25901859735603855,
      "grad_norm": 0.44765621423721313,
      "learning_rate": 0.0004353345280764636,
      "loss": 6.625,
      "step": 867
    },
    {
      "epoch": 0.25931735006348494,
      "grad_norm": 0.37651553750038147,
      "learning_rate": 0.0004352598566308244,
      "loss": 7.5098,
      "step": 868
    },
    {
      "epoch": 0.25961610277093133,
      "grad_norm": 0.4246636629104614,
      "learning_rate": 0.0004351851851851852,
      "loss": 6.8955,
      "step": 869
    },
    {
      "epoch": 0.2599148554783778,
      "grad_norm": 0.3331461250782013,
      "learning_rate": 0.000435110513739546,
      "loss": 7.7314,
      "step": 870
    },
    {
      "epoch": 0.2602136081858242,
      "grad_norm": 0.34687379002571106,
      "learning_rate": 0.0004350358422939068,
      "loss": 7.7461,
      "step": 871
    },
    {
      "epoch": 0.2605123608932706,
      "grad_norm": 0.45022693276405334,
      "learning_rate": 0.00043496117084826765,
      "loss": 7.0283,
      "step": 872
    },
    {
      "epoch": 0.260811113600717,
      "grad_norm": 0.3949999213218689,
      "learning_rate": 0.0004348864994026284,
      "loss": 7.46,
      "step": 873
    },
    {
      "epoch": 0.2611098663081634,
      "grad_norm": 0.3527705669403076,
      "learning_rate": 0.00043481182795698927,
      "loss": 7.748,
      "step": 874
    },
    {
      "epoch": 0.26140861901560986,
      "grad_norm": 0.39960646629333496,
      "learning_rate": 0.0004347371565113501,
      "loss": 7.0283,
      "step": 875
    },
    {
      "epoch": 0.26170737172305625,
      "grad_norm": 0.45411694049835205,
      "learning_rate": 0.0004346624850657109,
      "loss": 7.3623,
      "step": 876
    },
    {
      "epoch": 0.26200612443050264,
      "grad_norm": 0.3744768798351288,
      "learning_rate": 0.0004345878136200717,
      "loss": 7.6484,
      "step": 877
    },
    {
      "epoch": 0.2623048771379491,
      "grad_norm": 0.3516564667224884,
      "learning_rate": 0.00043451314217443247,
      "loss": 7.4297,
      "step": 878
    },
    {
      "epoch": 0.2626036298453955,
      "grad_norm": 0.41869622468948364,
      "learning_rate": 0.00043443847072879334,
      "loss": 7.0605,
      "step": 879
    },
    {
      "epoch": 0.2629023825528419,
      "grad_norm": 0.4702286124229431,
      "learning_rate": 0.0004343637992831541,
      "loss": 6.999,
      "step": 880
    },
    {
      "epoch": 0.2632011352602883,
      "grad_norm": 0.39602258801460266,
      "learning_rate": 0.00043428912783751496,
      "loss": 7.5547,
      "step": 881
    },
    {
      "epoch": 0.2634998879677347,
      "grad_norm": 0.41770872473716736,
      "learning_rate": 0.0004342144563918757,
      "loss": 7.2539,
      "step": 882
    },
    {
      "epoch": 0.2637986406751811,
      "grad_norm": 0.37051114439964294,
      "learning_rate": 0.0004341397849462366,
      "loss": 7.751,
      "step": 883
    },
    {
      "epoch": 0.26409739338262755,
      "grad_norm": 0.5518862009048462,
      "learning_rate": 0.0004340651135005974,
      "loss": 6.8984,
      "step": 884
    },
    {
      "epoch": 0.26439614609007395,
      "grad_norm": 0.38869529962539673,
      "learning_rate": 0.0004339904420549582,
      "loss": 7.5859,
      "step": 885
    },
    {
      "epoch": 0.26469489879752034,
      "grad_norm": 0.5049735307693481,
      "learning_rate": 0.00043391577060931903,
      "loss": 6.9932,
      "step": 886
    },
    {
      "epoch": 0.2649936515049668,
      "grad_norm": 0.4229329526424408,
      "learning_rate": 0.0004338410991636798,
      "loss": 7.2998,
      "step": 887
    },
    {
      "epoch": 0.2652924042124132,
      "grad_norm": 0.44961467385292053,
      "learning_rate": 0.00043376642771804065,
      "loss": 7.0498,
      "step": 888
    },
    {
      "epoch": 0.2655911569198596,
      "grad_norm": 0.347825288772583,
      "learning_rate": 0.0004336917562724014,
      "loss": 7.8828,
      "step": 889
    },
    {
      "epoch": 0.265889909627306,
      "grad_norm": 0.4737858176231384,
      "learning_rate": 0.0004336170848267623,
      "loss": 7.1729,
      "step": 890
    },
    {
      "epoch": 0.2661886623347524,
      "grad_norm": 0.46223488450050354,
      "learning_rate": 0.00043354241338112304,
      "loss": 7.1943,
      "step": 891
    },
    {
      "epoch": 0.2664874150421988,
      "grad_norm": 0.434112548828125,
      "learning_rate": 0.0004334677419354839,
      "loss": 7.2539,
      "step": 892
    },
    {
      "epoch": 0.26678616774964525,
      "grad_norm": 0.41467055678367615,
      "learning_rate": 0.00043339307048984466,
      "loss": 7.4287,
      "step": 893
    },
    {
      "epoch": 0.26708492045709165,
      "grad_norm": 0.33384230732917786,
      "learning_rate": 0.0004333183990442055,
      "loss": 7.5605,
      "step": 894
    },
    {
      "epoch": 0.26738367316453804,
      "grad_norm": 0.4823230803012848,
      "learning_rate": 0.00043324372759856634,
      "loss": 7.3311,
      "step": 895
    },
    {
      "epoch": 0.2676824258719845,
      "grad_norm": 0.4141921401023865,
      "learning_rate": 0.0004331690561529271,
      "loss": 7.1357,
      "step": 896
    },
    {
      "epoch": 0.2679811785794309,
      "grad_norm": 0.449584037065506,
      "learning_rate": 0.00043309438470728797,
      "loss": 7.2646,
      "step": 897
    },
    {
      "epoch": 0.26827993128687727,
      "grad_norm": 0.4164203703403473,
      "learning_rate": 0.00043301971326164873,
      "loss": 7.1309,
      "step": 898
    },
    {
      "epoch": 0.2685786839943237,
      "grad_norm": 0.3806099593639374,
      "learning_rate": 0.0004329450418160096,
      "loss": 7.6475,
      "step": 899
    },
    {
      "epoch": 0.2688774367017701,
      "grad_norm": 0.4499588906764984,
      "learning_rate": 0.00043287037037037035,
      "loss": 6.8301,
      "step": 900
    },
    {
      "epoch": 0.2691761894092165,
      "grad_norm": 0.40806183218955994,
      "learning_rate": 0.0004327956989247312,
      "loss": 7.2188,
      "step": 901
    },
    {
      "epoch": 0.26947494211666295,
      "grad_norm": 0.4790497124195099,
      "learning_rate": 0.000432721027479092,
      "loss": 6.8428,
      "step": 902
    },
    {
      "epoch": 0.26977369482410934,
      "grad_norm": 0.3520163893699646,
      "learning_rate": 0.0004326463560334528,
      "loss": 7.6104,
      "step": 903
    },
    {
      "epoch": 0.27007244753155574,
      "grad_norm": 0.4220838248729706,
      "learning_rate": 0.00043257168458781366,
      "loss": 7.418,
      "step": 904
    },
    {
      "epoch": 0.2703712002390022,
      "grad_norm": 0.3603399097919464,
      "learning_rate": 0.0004324970131421744,
      "loss": 7.5361,
      "step": 905
    },
    {
      "epoch": 0.2706699529464486,
      "grad_norm": 0.4377238154411316,
      "learning_rate": 0.0004324223416965353,
      "loss": 7.2451,
      "step": 906
    },
    {
      "epoch": 0.27096870565389497,
      "grad_norm": 0.39507755637168884,
      "learning_rate": 0.00043234767025089604,
      "loss": 7.4395,
      "step": 907
    },
    {
      "epoch": 0.2712674583613414,
      "grad_norm": 0.42503637075424194,
      "learning_rate": 0.0004322729988052569,
      "loss": 7.3936,
      "step": 908
    },
    {
      "epoch": 0.2715662110687878,
      "grad_norm": 0.5348562002182007,
      "learning_rate": 0.00043219832735961767,
      "loss": 7.2559,
      "step": 909
    },
    {
      "epoch": 0.2718649637762342,
      "grad_norm": 0.39749211072921753,
      "learning_rate": 0.0004321236559139785,
      "loss": 7.3105,
      "step": 910
    },
    {
      "epoch": 0.27216371648368065,
      "grad_norm": 0.36132773756980896,
      "learning_rate": 0.0004320489844683393,
      "loss": 7.627,
      "step": 911
    },
    {
      "epoch": 0.27246246919112704,
      "grad_norm": 0.45232710242271423,
      "learning_rate": 0.0004319743130227001,
      "loss": 6.6787,
      "step": 912
    },
    {
      "epoch": 0.27276122189857344,
      "grad_norm": 0.427321195602417,
      "learning_rate": 0.000431899641577061,
      "loss": 7.2676,
      "step": 913
    },
    {
      "epoch": 0.2730599746060199,
      "grad_norm": 0.39951008558273315,
      "learning_rate": 0.00043182497013142173,
      "loss": 7.5352,
      "step": 914
    },
    {
      "epoch": 0.2733587273134663,
      "grad_norm": 0.4677272439002991,
      "learning_rate": 0.0004317502986857826,
      "loss": 7.1943,
      "step": 915
    },
    {
      "epoch": 0.27365748002091267,
      "grad_norm": 0.4327559173107147,
      "learning_rate": 0.00043167562724014336,
      "loss": 7.4766,
      "step": 916
    },
    {
      "epoch": 0.2739562327283591,
      "grad_norm": 0.5182842016220093,
      "learning_rate": 0.0004316009557945042,
      "loss": 7.2559,
      "step": 917
    },
    {
      "epoch": 0.2742549854358055,
      "grad_norm": 0.41771912574768066,
      "learning_rate": 0.000431526284348865,
      "loss": 7.3994,
      "step": 918
    },
    {
      "epoch": 0.2745537381432519,
      "grad_norm": 0.3690473735332489,
      "learning_rate": 0.0004314516129032258,
      "loss": 7.3652,
      "step": 919
    },
    {
      "epoch": 0.27485249085069835,
      "grad_norm": 0.43988996744155884,
      "learning_rate": 0.0004313769414575866,
      "loss": 7.0537,
      "step": 920
    },
    {
      "epoch": 0.27515124355814474,
      "grad_norm": 0.4160076975822449,
      "learning_rate": 0.0004313022700119474,
      "loss": 7.4883,
      "step": 921
    },
    {
      "epoch": 0.27544999626559113,
      "grad_norm": 0.40717995166778564,
      "learning_rate": 0.0004312275985663083,
      "loss": 7.501,
      "step": 922
    },
    {
      "epoch": 0.2757487489730376,
      "grad_norm": 0.35410597920417786,
      "learning_rate": 0.00043115292712066905,
      "loss": 7.4473,
      "step": 923
    },
    {
      "epoch": 0.276047501680484,
      "grad_norm": 0.41300350427627563,
      "learning_rate": 0.0004310782556750299,
      "loss": 6.9473,
      "step": 924
    },
    {
      "epoch": 0.27634625438793037,
      "grad_norm": 0.4854552149772644,
      "learning_rate": 0.0004310035842293907,
      "loss": 6.8984,
      "step": 925
    },
    {
      "epoch": 0.2766450070953768,
      "grad_norm": 0.4599839448928833,
      "learning_rate": 0.0004309289127837515,
      "loss": 7.1152,
      "step": 926
    },
    {
      "epoch": 0.2769437598028232,
      "grad_norm": 0.38379576802253723,
      "learning_rate": 0.0004308542413381123,
      "loss": 7.1592,
      "step": 927
    },
    {
      "epoch": 0.2772425125102696,
      "grad_norm": 0.47607821226119995,
      "learning_rate": 0.0004307795698924731,
      "loss": 6.7227,
      "step": 928
    },
    {
      "epoch": 0.27754126521771605,
      "grad_norm": 0.40904954075813293,
      "learning_rate": 0.0004307048984468339,
      "loss": 7.3203,
      "step": 929
    },
    {
      "epoch": 0.27784001792516244,
      "grad_norm": 0.37295886874198914,
      "learning_rate": 0.00043063022700119474,
      "loss": 7.9307,
      "step": 930
    },
    {
      "epoch": 0.27813877063260883,
      "grad_norm": 0.4743909239768982,
      "learning_rate": 0.0004305555555555556,
      "loss": 7.3359,
      "step": 931
    },
    {
      "epoch": 0.2784375233400553,
      "grad_norm": 0.41805747151374817,
      "learning_rate": 0.00043048088410991637,
      "loss": 7.4531,
      "step": 932
    },
    {
      "epoch": 0.2787362760475017,
      "grad_norm": 0.41753989458084106,
      "learning_rate": 0.00043040621266427723,
      "loss": 7.1289,
      "step": 933
    },
    {
      "epoch": 0.27903502875494807,
      "grad_norm": 0.3981003165245056,
      "learning_rate": 0.000430331541218638,
      "loss": 7.2656,
      "step": 934
    },
    {
      "epoch": 0.2793337814623945,
      "grad_norm": 0.5286235213279724,
      "learning_rate": 0.0004302568697729988,
      "loss": 6.6787,
      "step": 935
    },
    {
      "epoch": 0.2796325341698409,
      "grad_norm": 0.42176806926727295,
      "learning_rate": 0.0004301821983273596,
      "loss": 7.3369,
      "step": 936
    },
    {
      "epoch": 0.2799312868772873,
      "grad_norm": 0.3891243040561676,
      "learning_rate": 0.00043010752688172043,
      "loss": 7.1787,
      "step": 937
    },
    {
      "epoch": 0.28023003958473375,
      "grad_norm": 0.4196974039077759,
      "learning_rate": 0.00043003285543608124,
      "loss": 7.0068,
      "step": 938
    },
    {
      "epoch": 0.28052879229218014,
      "grad_norm": 0.3838033974170685,
      "learning_rate": 0.00042995818399044206,
      "loss": 7.2344,
      "step": 939
    },
    {
      "epoch": 0.28082754499962653,
      "grad_norm": 0.36370956897735596,
      "learning_rate": 0.0004298835125448029,
      "loss": 7.001,
      "step": 940
    },
    {
      "epoch": 0.281126297707073,
      "grad_norm": 0.4557884633541107,
      "learning_rate": 0.0004298088410991637,
      "loss": 7.2988,
      "step": 941
    },
    {
      "epoch": 0.2814250504145194,
      "grad_norm": 0.4014793634414673,
      "learning_rate": 0.0004297341696535245,
      "loss": 7.6152,
      "step": 942
    },
    {
      "epoch": 0.28172380312196577,
      "grad_norm": 0.4336600601673126,
      "learning_rate": 0.0004296594982078853,
      "loss": 7.1885,
      "step": 943
    },
    {
      "epoch": 0.2820225558294122,
      "grad_norm": 0.4273433983325958,
      "learning_rate": 0.0004295848267622461,
      "loss": 7.2988,
      "step": 944
    },
    {
      "epoch": 0.2823213085368586,
      "grad_norm": 0.41110217571258545,
      "learning_rate": 0.00042951015531660693,
      "loss": 7.4561,
      "step": 945
    },
    {
      "epoch": 0.28262006124430505,
      "grad_norm": 0.3183257579803467,
      "learning_rate": 0.00042943548387096775,
      "loss": 7.6865,
      "step": 946
    },
    {
      "epoch": 0.28291881395175145,
      "grad_norm": 0.5694592595100403,
      "learning_rate": 0.00042936081242532856,
      "loss": 6.9375,
      "step": 947
    },
    {
      "epoch": 0.28321756665919784,
      "grad_norm": 0.48623138666152954,
      "learning_rate": 0.00042928614097968937,
      "loss": 6.7793,
      "step": 948
    },
    {
      "epoch": 0.2835163193666443,
      "grad_norm": 0.30896005034446716,
      "learning_rate": 0.00042921146953405024,
      "loss": 7.8906,
      "step": 949
    },
    {
      "epoch": 0.2838150720740907,
      "grad_norm": 0.3366028964519501,
      "learning_rate": 0.000429136798088411,
      "loss": 7.6016,
      "step": 950
    },
    {
      "epoch": 0.28411382478153707,
      "grad_norm": 0.460205078125,
      "learning_rate": 0.0004290621266427718,
      "loss": 7.1436,
      "step": 951
    },
    {
      "epoch": 0.2844125774889835,
      "grad_norm": 0.3499848246574402,
      "learning_rate": 0.0004289874551971326,
      "loss": 7.501,
      "step": 952
    },
    {
      "epoch": 0.2847113301964299,
      "grad_norm": 0.40299978852272034,
      "learning_rate": 0.00042891278375149344,
      "loss": 7.6494,
      "step": 953
    },
    {
      "epoch": 0.2850100829038763,
      "grad_norm": 0.43178945779800415,
      "learning_rate": 0.00042883811230585425,
      "loss": 7.1963,
      "step": 954
    },
    {
      "epoch": 0.28530883561132275,
      "grad_norm": 0.3649863004684448,
      "learning_rate": 0.00042876344086021506,
      "loss": 7.6279,
      "step": 955
    },
    {
      "epoch": 0.28560758831876915,
      "grad_norm": 0.43270859122276306,
      "learning_rate": 0.0004286887694145759,
      "loss": 7.1582,
      "step": 956
    },
    {
      "epoch": 0.28590634102621554,
      "grad_norm": 0.4331178069114685,
      "learning_rate": 0.0004286140979689367,
      "loss": 7.165,
      "step": 957
    },
    {
      "epoch": 0.286205093733662,
      "grad_norm": 0.37868112325668335,
      "learning_rate": 0.0004285394265232975,
      "loss": 7.4316,
      "step": 958
    },
    {
      "epoch": 0.2865038464411084,
      "grad_norm": 0.3806072473526001,
      "learning_rate": 0.0004284647550776583,
      "loss": 7.3516,
      "step": 959
    },
    {
      "epoch": 0.28680259914855477,
      "grad_norm": 0.3835466206073761,
      "learning_rate": 0.0004283900836320191,
      "loss": 7.3818,
      "step": 960
    },
    {
      "epoch": 0.2871013518560012,
      "grad_norm": 0.45750972628593445,
      "learning_rate": 0.00042831541218637994,
      "loss": 7.1133,
      "step": 961
    },
    {
      "epoch": 0.2874001045634476,
      "grad_norm": 0.5838074684143066,
      "learning_rate": 0.00042824074074074075,
      "loss": 6.3604,
      "step": 962
    },
    {
      "epoch": 0.287698857270894,
      "grad_norm": 0.486063152551651,
      "learning_rate": 0.00042816606929510156,
      "loss": 6.7471,
      "step": 963
    },
    {
      "epoch": 0.28799760997834045,
      "grad_norm": 0.46097901463508606,
      "learning_rate": 0.0004280913978494624,
      "loss": 7.1152,
      "step": 964
    },
    {
      "epoch": 0.28829636268578684,
      "grad_norm": 0.4228213131427765,
      "learning_rate": 0.0004280167264038232,
      "loss": 7.4141,
      "step": 965
    },
    {
      "epoch": 0.28859511539323324,
      "grad_norm": 0.5176370143890381,
      "learning_rate": 0.000427942054958184,
      "loss": 7.0469,
      "step": 966
    },
    {
      "epoch": 0.2888938681006797,
      "grad_norm": 0.4281718134880066,
      "learning_rate": 0.0004278673835125448,
      "loss": 7.4385,
      "step": 967
    },
    {
      "epoch": 0.2891926208081261,
      "grad_norm": 0.4047592878341675,
      "learning_rate": 0.00042779271206690563,
      "loss": 7.8604,
      "step": 968
    },
    {
      "epoch": 0.28949137351557247,
      "grad_norm": 0.442004919052124,
      "learning_rate": 0.00042771804062126644,
      "loss": 7.042,
      "step": 969
    },
    {
      "epoch": 0.2897901262230189,
      "grad_norm": 0.36630216240882874,
      "learning_rate": 0.00042764336917562725,
      "loss": 7.3594,
      "step": 970
    },
    {
      "epoch": 0.2900888789304653,
      "grad_norm": 0.37489184737205505,
      "learning_rate": 0.00042756869772998807,
      "loss": 7.5254,
      "step": 971
    },
    {
      "epoch": 0.2903876316379117,
      "grad_norm": 0.4115922749042511,
      "learning_rate": 0.0004274940262843489,
      "loss": 7.3984,
      "step": 972
    },
    {
      "epoch": 0.29068638434535815,
      "grad_norm": 0.4399450421333313,
      "learning_rate": 0.0004274193548387097,
      "loss": 6.9121,
      "step": 973
    },
    {
      "epoch": 0.29098513705280454,
      "grad_norm": 0.45114240050315857,
      "learning_rate": 0.00042734468339307045,
      "loss": 7.624,
      "step": 974
    },
    {
      "epoch": 0.29128388976025094,
      "grad_norm": 0.4363076686859131,
      "learning_rate": 0.0004272700119474313,
      "loss": 7.125,
      "step": 975
    },
    {
      "epoch": 0.2915826424676974,
      "grad_norm": 0.47879043221473694,
      "learning_rate": 0.00042719534050179213,
      "loss": 7.0078,
      "step": 976
    },
    {
      "epoch": 0.2918813951751438,
      "grad_norm": 0.44835165143013,
      "learning_rate": 0.00042712066905615294,
      "loss": 6.9941,
      "step": 977
    },
    {
      "epoch": 0.29218014788259017,
      "grad_norm": 0.5064224004745483,
      "learning_rate": 0.00042704599761051376,
      "loss": 7.1309,
      "step": 978
    },
    {
      "epoch": 0.2924789005900366,
      "grad_norm": 0.4334956109523773,
      "learning_rate": 0.00042697132616487457,
      "loss": 6.915,
      "step": 979
    },
    {
      "epoch": 0.292777653297483,
      "grad_norm": 0.4073925316333771,
      "learning_rate": 0.0004268966547192354,
      "loss": 7.625,
      "step": 980
    },
    {
      "epoch": 0.2930764060049294,
      "grad_norm": 0.419558048248291,
      "learning_rate": 0.0004268219832735962,
      "loss": 7.2295,
      "step": 981
    },
    {
      "epoch": 0.29337515871237585,
      "grad_norm": 0.37737566232681274,
      "learning_rate": 0.000426747311827957,
      "loss": 7.3828,
      "step": 982
    },
    {
      "epoch": 0.29367391141982224,
      "grad_norm": 0.42369920015335083,
      "learning_rate": 0.00042667264038231777,
      "loss": 7.4023,
      "step": 983
    },
    {
      "epoch": 0.29397266412726863,
      "grad_norm": 0.5877148509025574,
      "learning_rate": 0.00042659796893667863,
      "loss": 7.0264,
      "step": 984
    },
    {
      "epoch": 0.2942714168347151,
      "grad_norm": 0.4579118490219116,
      "learning_rate": 0.0004265232974910394,
      "loss": 7.2715,
      "step": 985
    },
    {
      "epoch": 0.2945701695421615,
      "grad_norm": 0.43164610862731934,
      "learning_rate": 0.00042644862604540026,
      "loss": 7.542,
      "step": 986
    },
    {
      "epoch": 0.29486892224960787,
      "grad_norm": 0.4240492284297943,
      "learning_rate": 0.0004263739545997611,
      "loss": 7.5625,
      "step": 987
    },
    {
      "epoch": 0.2951676749570543,
      "grad_norm": 0.5499091148376465,
      "learning_rate": 0.0004262992831541219,
      "loss": 7.0068,
      "step": 988
    },
    {
      "epoch": 0.2954664276645007,
      "grad_norm": 0.49844327569007874,
      "learning_rate": 0.0004262246117084827,
      "loss": 6.4561,
      "step": 989
    },
    {
      "epoch": 0.2957651803719471,
      "grad_norm": 0.3727582097053528,
      "learning_rate": 0.00042614994026284346,
      "loss": 7.3701,
      "step": 990
    },
    {
      "epoch": 0.29606393307939355,
      "grad_norm": 0.5158160924911499,
      "learning_rate": 0.0004260752688172043,
      "loss": 6.9229,
      "step": 991
    },
    {
      "epoch": 0.29636268578683994,
      "grad_norm": 0.8223013877868652,
      "learning_rate": 0.0004260005973715651,
      "loss": 6.585,
      "step": 992
    },
    {
      "epoch": 0.29666143849428633,
      "grad_norm": 0.44930702447891235,
      "learning_rate": 0.00042592592592592595,
      "loss": 7.3584,
      "step": 993
    },
    {
      "epoch": 0.2969601912017328,
      "grad_norm": 0.44352638721466064,
      "learning_rate": 0.0004258512544802867,
      "loss": 6.8203,
      "step": 994
    },
    {
      "epoch": 0.2972589439091792,
      "grad_norm": 0.47012829780578613,
      "learning_rate": 0.0004257765830346476,
      "loss": 6.8076,
      "step": 995
    },
    {
      "epoch": 0.29755769661662557,
      "grad_norm": 0.5783194899559021,
      "learning_rate": 0.0004257019115890084,
      "loss": 6.4414,
      "step": 996
    },
    {
      "epoch": 0.297856449324072,
      "grad_norm": 0.456613689661026,
      "learning_rate": 0.0004256272401433692,
      "loss": 7.1377,
      "step": 997
    },
    {
      "epoch": 0.2981552020315184,
      "grad_norm": 0.4158826172351837,
      "learning_rate": 0.00042555256869773,
      "loss": 7.2705,
      "step": 998
    },
    {
      "epoch": 0.2984539547389648,
      "grad_norm": 0.4144705832004547,
      "learning_rate": 0.0004254778972520908,
      "loss": 7.4863,
      "step": 999
    },
    {
      "epoch": 0.29875270744641125,
      "grad_norm": 0.4350469410419464,
      "learning_rate": 0.00042540322580645164,
      "loss": 7.1953,
      "step": 1000
    },
    {
      "epoch": 0.29875270744641125,
      "eval_bleu": 0.0852744815912739,
      "eval_loss": 7.0859375,
      "eval_runtime": 569.3102,
      "eval_samples_per_second": 2.475,
      "eval_steps_per_second": 0.156,
      "step": 1000
    },
    {
      "epoch": 0.29905146015385764,
      "grad_norm": 0.36561650037765503,
      "learning_rate": 0.0004253285543608124,
      "loss": 7.2129,
      "step": 1001
    },
    {
      "epoch": 0.29935021286130403,
      "grad_norm": 0.3895508348941803,
      "learning_rate": 0.00042525388291517327,
      "loss": 7.252,
      "step": 1002
    },
    {
      "epoch": 0.2996489655687505,
      "grad_norm": 0.45288950204849243,
      "learning_rate": 0.000425179211469534,
      "loss": 7.6357,
      "step": 1003
    },
    {
      "epoch": 0.2999477182761969,
      "grad_norm": 0.45206037163734436,
      "learning_rate": 0.0004251045400238949,
      "loss": 6.8604,
      "step": 1004
    },
    {
      "epoch": 0.30024647098364327,
      "grad_norm": 0.4485305845737457,
      "learning_rate": 0.0004250298685782557,
      "loss": 6.5996,
      "step": 1005
    },
    {
      "epoch": 0.3005452236910897,
      "grad_norm": 0.3345312774181366,
      "learning_rate": 0.00042495519713261646,
      "loss": 7.4072,
      "step": 1006
    },
    {
      "epoch": 0.3008439763985361,
      "grad_norm": 0.5029211044311523,
      "learning_rate": 0.00042488052568697733,
      "loss": 7.1133,
      "step": 1007
    },
    {
      "epoch": 0.3011427291059825,
      "grad_norm": 0.5049786567687988,
      "learning_rate": 0.0004248058542413381,
      "loss": 6.8262,
      "step": 1008
    },
    {
      "epoch": 0.30144148181342895,
      "grad_norm": 0.4324814975261688,
      "learning_rate": 0.00042473118279569896,
      "loss": 7.1533,
      "step": 1009
    },
    {
      "epoch": 0.30174023452087534,
      "grad_norm": 0.4080350995063782,
      "learning_rate": 0.0004246565113500597,
      "loss": 7.4619,
      "step": 1010
    },
    {
      "epoch": 0.30203898722832173,
      "grad_norm": 0.37531614303588867,
      "learning_rate": 0.0004245818399044206,
      "loss": 7.0762,
      "step": 1011
    },
    {
      "epoch": 0.3023377399357682,
      "grad_norm": 0.40993693470954895,
      "learning_rate": 0.00042450716845878134,
      "loss": 7.4434,
      "step": 1012
    },
    {
      "epoch": 0.30263649264321457,
      "grad_norm": 0.4353625476360321,
      "learning_rate": 0.0004244324970131422,
      "loss": 7.0479,
      "step": 1013
    },
    {
      "epoch": 0.30293524535066096,
      "grad_norm": 0.4394516348838806,
      "learning_rate": 0.000424357825567503,
      "loss": 7.0205,
      "step": 1014
    },
    {
      "epoch": 0.3032339980581074,
      "grad_norm": 0.35799428820610046,
      "learning_rate": 0.0004242831541218638,
      "loss": 7.3789,
      "step": 1015
    },
    {
      "epoch": 0.3035327507655538,
      "grad_norm": 0.40728357434272766,
      "learning_rate": 0.00042420848267622465,
      "loss": 7.0654,
      "step": 1016
    },
    {
      "epoch": 0.3038315034730002,
      "grad_norm": 0.45761191844940186,
      "learning_rate": 0.0004241338112305854,
      "loss": 7.1104,
      "step": 1017
    },
    {
      "epoch": 0.30413025618044665,
      "grad_norm": 0.48977231979370117,
      "learning_rate": 0.00042405913978494627,
      "loss": 6.7881,
      "step": 1018
    },
    {
      "epoch": 0.30442900888789304,
      "grad_norm": 0.42500731348991394,
      "learning_rate": 0.00042398446833930703,
      "loss": 7.208,
      "step": 1019
    },
    {
      "epoch": 0.30472776159533943,
      "grad_norm": 0.4306623339653015,
      "learning_rate": 0.0004239097968936679,
      "loss": 7.5918,
      "step": 1020
    },
    {
      "epoch": 0.3050265143027859,
      "grad_norm": 0.5146358609199524,
      "learning_rate": 0.00042383512544802866,
      "loss": 6.9805,
      "step": 1021
    },
    {
      "epoch": 0.30532526701023227,
      "grad_norm": 0.39700937271118164,
      "learning_rate": 0.00042376045400238947,
      "loss": 7.4883,
      "step": 1022
    },
    {
      "epoch": 0.3056240197176787,
      "grad_norm": 0.4184378385543823,
      "learning_rate": 0.00042368578255675034,
      "loss": 7.2793,
      "step": 1023
    },
    {
      "epoch": 0.3059227724251251,
      "grad_norm": 0.39305630326271057,
      "learning_rate": 0.0004236111111111111,
      "loss": 7.5068,
      "step": 1024
    },
    {
      "epoch": 0.3062215251325715,
      "grad_norm": 0.44654279947280884,
      "learning_rate": 0.00042353643966547196,
      "loss": 6.8379,
      "step": 1025
    },
    {
      "epoch": 0.30652027784001795,
      "grad_norm": 0.29086920619010925,
      "learning_rate": 0.0004234617682198327,
      "loss": 7.9746,
      "step": 1026
    },
    {
      "epoch": 0.30681903054746434,
      "grad_norm": 0.4487977921962738,
      "learning_rate": 0.0004233870967741936,
      "loss": 6.9561,
      "step": 1027
    },
    {
      "epoch": 0.30711778325491074,
      "grad_norm": 0.38400325179100037,
      "learning_rate": 0.00042331242532855435,
      "loss": 7.874,
      "step": 1028
    },
    {
      "epoch": 0.3074165359623572,
      "grad_norm": 0.5237321257591248,
      "learning_rate": 0.0004232377538829152,
      "loss": 6.9717,
      "step": 1029
    },
    {
      "epoch": 0.3077152886698036,
      "grad_norm": 0.4198107421398163,
      "learning_rate": 0.000423163082437276,
      "loss": 7.6621,
      "step": 1030
    },
    {
      "epoch": 0.30801404137724997,
      "grad_norm": 0.4032559394836426,
      "learning_rate": 0.0004230884109916368,
      "loss": 7.5762,
      "step": 1031
    },
    {
      "epoch": 0.3083127940846964,
      "grad_norm": 0.35650745034217834,
      "learning_rate": 0.00042301373954599765,
      "loss": 7.7373,
      "step": 1032
    },
    {
      "epoch": 0.3086115467921428,
      "grad_norm": 0.4206409752368927,
      "learning_rate": 0.0004229390681003584,
      "loss": 7.3008,
      "step": 1033
    },
    {
      "epoch": 0.3089102994995892,
      "grad_norm": 0.4199439585208893,
      "learning_rate": 0.0004228643966547193,
      "loss": 7.2432,
      "step": 1034
    },
    {
      "epoch": 0.30920905220703565,
      "grad_norm": 0.43988296389579773,
      "learning_rate": 0.00042278972520908004,
      "loss": 7.4736,
      "step": 1035
    },
    {
      "epoch": 0.30950780491448204,
      "grad_norm": 0.38467374444007874,
      "learning_rate": 0.0004227150537634409,
      "loss": 7.4219,
      "step": 1036
    },
    {
      "epoch": 0.30980655762192844,
      "grad_norm": 0.44611185789108276,
      "learning_rate": 0.00042264038231780166,
      "loss": 7.1689,
      "step": 1037
    },
    {
      "epoch": 0.3101053103293749,
      "grad_norm": 0.4784609079360962,
      "learning_rate": 0.0004225657108721625,
      "loss": 7.0791,
      "step": 1038
    },
    {
      "epoch": 0.3104040630368213,
      "grad_norm": 0.44399359822273254,
      "learning_rate": 0.0004224910394265233,
      "loss": 7.1221,
      "step": 1039
    },
    {
      "epoch": 0.31070281574426767,
      "grad_norm": 0.43030330538749695,
      "learning_rate": 0.0004224163679808841,
      "loss": 7.123,
      "step": 1040
    },
    {
      "epoch": 0.3110015684517141,
      "grad_norm": 0.41098731756210327,
      "learning_rate": 0.00042234169653524497,
      "loss": 7.1465,
      "step": 1041
    },
    {
      "epoch": 0.3113003211591605,
      "grad_norm": 0.40267035365104675,
      "learning_rate": 0.00042226702508960573,
      "loss": 7.3945,
      "step": 1042
    },
    {
      "epoch": 0.3115990738666069,
      "grad_norm": 0.4354916214942932,
      "learning_rate": 0.0004221923536439666,
      "loss": 6.8838,
      "step": 1043
    },
    {
      "epoch": 0.31189782657405335,
      "grad_norm": 0.390175461769104,
      "learning_rate": 0.00042211768219832735,
      "loss": 7.4346,
      "step": 1044
    },
    {
      "epoch": 0.31219657928149974,
      "grad_norm": 0.42380452156066895,
      "learning_rate": 0.0004220430107526882,
      "loss": 7.5732,
      "step": 1045
    },
    {
      "epoch": 0.31249533198894613,
      "grad_norm": 0.3856702148914337,
      "learning_rate": 0.000421968339307049,
      "loss": 7.5078,
      "step": 1046
    },
    {
      "epoch": 0.3127940846963926,
      "grad_norm": 0.3865894675254822,
      "learning_rate": 0.0004218936678614098,
      "loss": 7.4785,
      "step": 1047
    },
    {
      "epoch": 0.313092837403839,
      "grad_norm": 0.4473107159137726,
      "learning_rate": 0.0004218189964157706,
      "loss": 7.4834,
      "step": 1048
    },
    {
      "epoch": 0.31339159011128537,
      "grad_norm": 0.42065873742103577,
      "learning_rate": 0.0004217443249701314,
      "loss": 7.3008,
      "step": 1049
    },
    {
      "epoch": 0.3136903428187318,
      "grad_norm": 0.4139236807823181,
      "learning_rate": 0.0004216696535244923,
      "loss": 7.5684,
      "step": 1050
    },
    {
      "epoch": 0.3139890955261782,
      "grad_norm": 0.5097725987434387,
      "learning_rate": 0.00042159498207885304,
      "loss": 6.8262,
      "step": 1051
    },
    {
      "epoch": 0.3142878482336246,
      "grad_norm": 0.49422988295555115,
      "learning_rate": 0.0004215203106332139,
      "loss": 7.1826,
      "step": 1052
    },
    {
      "epoch": 0.31458660094107105,
      "grad_norm": 0.35531729459762573,
      "learning_rate": 0.00042144563918757467,
      "loss": 7.7021,
      "step": 1053
    },
    {
      "epoch": 0.31488535364851744,
      "grad_norm": 0.4253123700618744,
      "learning_rate": 0.0004213709677419355,
      "loss": 7.332,
      "step": 1054
    },
    {
      "epoch": 0.31518410635596383,
      "grad_norm": 0.37399572134017944,
      "learning_rate": 0.0004212962962962963,
      "loss": 7.3076,
      "step": 1055
    },
    {
      "epoch": 0.3154828590634103,
      "grad_norm": 0.35754701495170593,
      "learning_rate": 0.0004212216248506571,
      "loss": 7.8271,
      "step": 1056
    },
    {
      "epoch": 0.3157816117708567,
      "grad_norm": 0.38742172718048096,
      "learning_rate": 0.0004211469534050179,
      "loss": 7.5771,
      "step": 1057
    },
    {
      "epoch": 0.31608036447830307,
      "grad_norm": 0.5507731437683105,
      "learning_rate": 0.00042107228195937873,
      "loss": 6.5117,
      "step": 1058
    },
    {
      "epoch": 0.3163791171857495,
      "grad_norm": 0.420071005821228,
      "learning_rate": 0.0004209976105137396,
      "loss": 7.1768,
      "step": 1059
    },
    {
      "epoch": 0.3166778698931959,
      "grad_norm": 0.39536240696907043,
      "learning_rate": 0.00042092293906810036,
      "loss": 7.2031,
      "step": 1060
    },
    {
      "epoch": 0.3169766226006423,
      "grad_norm": 0.452719122171402,
      "learning_rate": 0.0004208482676224612,
      "loss": 7.2637,
      "step": 1061
    },
    {
      "epoch": 0.31727537530808875,
      "grad_norm": 0.40127918124198914,
      "learning_rate": 0.000420773596176822,
      "loss": 7.3857,
      "step": 1062
    },
    {
      "epoch": 0.31757412801553514,
      "grad_norm": 0.4113979935646057,
      "learning_rate": 0.0004206989247311828,
      "loss": 7.1787,
      "step": 1063
    },
    {
      "epoch": 0.31787288072298153,
      "grad_norm": 0.3689039647579193,
      "learning_rate": 0.0004206242532855436,
      "loss": 7.3633,
      "step": 1064
    },
    {
      "epoch": 0.318171633430428,
      "grad_norm": 0.35421326756477356,
      "learning_rate": 0.0004205495818399044,
      "loss": 7.5059,
      "step": 1065
    },
    {
      "epoch": 0.3184703861378744,
      "grad_norm": 0.46921083331108093,
      "learning_rate": 0.00042047491039426524,
      "loss": 7.2822,
      "step": 1066
    },
    {
      "epoch": 0.31876913884532077,
      "grad_norm": 0.4237115681171417,
      "learning_rate": 0.00042040023894862605,
      "loss": 7.0098,
      "step": 1067
    },
    {
      "epoch": 0.3190678915527672,
      "grad_norm": 0.45442110300064087,
      "learning_rate": 0.0004203255675029869,
      "loss": 7.2881,
      "step": 1068
    },
    {
      "epoch": 0.3193666442602136,
      "grad_norm": 0.4490732252597809,
      "learning_rate": 0.0004202508960573477,
      "loss": 7.1074,
      "step": 1069
    },
    {
      "epoch": 0.31966539696766,
      "grad_norm": 0.3271319568157196,
      "learning_rate": 0.0004201762246117085,
      "loss": 7.832,
      "step": 1070
    },
    {
      "epoch": 0.31996414967510645,
      "grad_norm": 0.41599899530410767,
      "learning_rate": 0.0004201015531660693,
      "loss": 7.4219,
      "step": 1071
    },
    {
      "epoch": 0.32026290238255284,
      "grad_norm": 0.45124301314353943,
      "learning_rate": 0.0004200268817204301,
      "loss": 7.3955,
      "step": 1072
    },
    {
      "epoch": 0.32056165508999923,
      "grad_norm": 0.3407822251319885,
      "learning_rate": 0.0004199522102747909,
      "loss": 7.9688,
      "step": 1073
    },
    {
      "epoch": 0.3208604077974457,
      "grad_norm": 0.36545518040657043,
      "learning_rate": 0.00041987753882915174,
      "loss": 7.4639,
      "step": 1074
    },
    {
      "epoch": 0.32115916050489207,
      "grad_norm": 0.3354385793209076,
      "learning_rate": 0.00041980286738351255,
      "loss": 7.8682,
      "step": 1075
    },
    {
      "epoch": 0.32145791321233846,
      "grad_norm": 0.4444710314273834,
      "learning_rate": 0.00041972819593787337,
      "loss": 6.9795,
      "step": 1076
    },
    {
      "epoch": 0.3217566659197849,
      "grad_norm": 0.45291373133659363,
      "learning_rate": 0.0004196535244922341,
      "loss": 7.1318,
      "step": 1077
    },
    {
      "epoch": 0.3220554186272313,
      "grad_norm": 0.5061197280883789,
      "learning_rate": 0.000419578853046595,
      "loss": 7.042,
      "step": 1078
    },
    {
      "epoch": 0.3223541713346777,
      "grad_norm": 0.46306324005126953,
      "learning_rate": 0.0004195041816009558,
      "loss": 7.373,
      "step": 1079
    },
    {
      "epoch": 0.32265292404212415,
      "grad_norm": 0.36244407296180725,
      "learning_rate": 0.0004194295101553166,
      "loss": 7.6348,
      "step": 1080
    },
    {
      "epoch": 0.32295167674957054,
      "grad_norm": 0.430561900138855,
      "learning_rate": 0.00041935483870967743,
      "loss": 7.418,
      "step": 1081
    },
    {
      "epoch": 0.32325042945701693,
      "grad_norm": 0.43227821588516235,
      "learning_rate": 0.00041928016726403824,
      "loss": 7.1846,
      "step": 1082
    },
    {
      "epoch": 0.3235491821644634,
      "grad_norm": 0.4208293855190277,
      "learning_rate": 0.00041920549581839906,
      "loss": 7.6465,
      "step": 1083
    },
    {
      "epoch": 0.32384793487190977,
      "grad_norm": 0.39111223816871643,
      "learning_rate": 0.00041913082437275987,
      "loss": 7.6377,
      "step": 1084
    },
    {
      "epoch": 0.32414668757935616,
      "grad_norm": 0.4090277850627899,
      "learning_rate": 0.0004190561529271207,
      "loss": 7.1055,
      "step": 1085
    },
    {
      "epoch": 0.3244454402868026,
      "grad_norm": 0.3865348994731903,
      "learning_rate": 0.00041898148148148144,
      "loss": 7.3691,
      "step": 1086
    },
    {
      "epoch": 0.324744192994249,
      "grad_norm": 0.4322788417339325,
      "learning_rate": 0.0004189068100358423,
      "loss": 7.1475,
      "step": 1087
    },
    {
      "epoch": 0.3250429457016954,
      "grad_norm": 0.35430046916007996,
      "learning_rate": 0.0004188321385902031,
      "loss": 7.4434,
      "step": 1088
    },
    {
      "epoch": 0.32534169840914184,
      "grad_norm": 0.4211747646331787,
      "learning_rate": 0.00041875746714456393,
      "loss": 7.4053,
      "step": 1089
    },
    {
      "epoch": 0.32564045111658824,
      "grad_norm": 0.39679154753685,
      "learning_rate": 0.00041868279569892475,
      "loss": 7.6865,
      "step": 1090
    },
    {
      "epoch": 0.32593920382403463,
      "grad_norm": 0.34410324692726135,
      "learning_rate": 0.00041860812425328556,
      "loss": 7.8857,
      "step": 1091
    },
    {
      "epoch": 0.3262379565314811,
      "grad_norm": 0.4657333195209503,
      "learning_rate": 0.00041853345280764637,
      "loss": 6.833,
      "step": 1092
    },
    {
      "epoch": 0.32653670923892747,
      "grad_norm": 0.32042425870895386,
      "learning_rate": 0.00041845878136200713,
      "loss": 7.8672,
      "step": 1093
    },
    {
      "epoch": 0.32683546194637386,
      "grad_norm": 0.3664456009864807,
      "learning_rate": 0.000418384109916368,
      "loss": 7.46,
      "step": 1094
    },
    {
      "epoch": 0.3271342146538203,
      "grad_norm": 0.5140801668167114,
      "learning_rate": 0.00041830943847072876,
      "loss": 7.1807,
      "step": 1095
    },
    {
      "epoch": 0.3274329673612667,
      "grad_norm": 0.3706851601600647,
      "learning_rate": 0.0004182347670250896,
      "loss": 7.793,
      "step": 1096
    },
    {
      "epoch": 0.32773172006871315,
      "grad_norm": 0.41141581535339355,
      "learning_rate": 0.00041816009557945044,
      "loss": 7.4648,
      "step": 1097
    },
    {
      "epoch": 0.32803047277615954,
      "grad_norm": 0.3611508309841156,
      "learning_rate": 0.00041808542413381125,
      "loss": 7.584,
      "step": 1098
    },
    {
      "epoch": 0.32832922548360594,
      "grad_norm": 0.4680461287498474,
      "learning_rate": 0.00041801075268817206,
      "loss": 7.1572,
      "step": 1099
    },
    {
      "epoch": 0.3286279781910524,
      "grad_norm": 0.4653686285018921,
      "learning_rate": 0.0004179360812425329,
      "loss": 6.7275,
      "step": 1100
    },
    {
      "epoch": 0.3289267308984988,
      "grad_norm": 0.4809640347957611,
      "learning_rate": 0.0004178614097968937,
      "loss": 6.9189,
      "step": 1101
    },
    {
      "epoch": 0.32922548360594517,
      "grad_norm": 0.46765419840812683,
      "learning_rate": 0.00041778673835125445,
      "loss": 7.0791,
      "step": 1102
    },
    {
      "epoch": 0.3295242363133916,
      "grad_norm": 0.42379406094551086,
      "learning_rate": 0.0004177120669056153,
      "loss": 7.5078,
      "step": 1103
    },
    {
      "epoch": 0.329822989020838,
      "grad_norm": 0.36727920174598694,
      "learning_rate": 0.00041763739545997607,
      "loss": 7.6504,
      "step": 1104
    },
    {
      "epoch": 0.3301217417282844,
      "grad_norm": 0.46467524766921997,
      "learning_rate": 0.00041756272401433694,
      "loss": 7.3955,
      "step": 1105
    },
    {
      "epoch": 0.33042049443573085,
      "grad_norm": 0.4351022243499756,
      "learning_rate": 0.00041748805256869775,
      "loss": 7.1455,
      "step": 1106
    },
    {
      "epoch": 0.33071924714317724,
      "grad_norm": 0.5749213099479675,
      "learning_rate": 0.00041741338112305856,
      "loss": 6.9209,
      "step": 1107
    },
    {
      "epoch": 0.33101799985062363,
      "grad_norm": 0.539630651473999,
      "learning_rate": 0.0004173387096774194,
      "loss": 6.8828,
      "step": 1108
    },
    {
      "epoch": 0.3313167525580701,
      "grad_norm": 0.4563591480255127,
      "learning_rate": 0.00041726403823178014,
      "loss": 6.9453,
      "step": 1109
    },
    {
      "epoch": 0.3316155052655165,
      "grad_norm": 0.5736159682273865,
      "learning_rate": 0.000417189366786141,
      "loss": 7.1055,
      "step": 1110
    },
    {
      "epoch": 0.33191425797296287,
      "grad_norm": 0.42450249195098877,
      "learning_rate": 0.00041711469534050176,
      "loss": 7.4746,
      "step": 1111
    },
    {
      "epoch": 0.3322130106804093,
      "grad_norm": 0.40547314286231995,
      "learning_rate": 0.00041704002389486263,
      "loss": 7.1455,
      "step": 1112
    },
    {
      "epoch": 0.3325117633878557,
      "grad_norm": 0.37994298338890076,
      "learning_rate": 0.0004169653524492234,
      "loss": 7.4209,
      "step": 1113
    },
    {
      "epoch": 0.3328105160953021,
      "grad_norm": 0.4664217233657837,
      "learning_rate": 0.00041689068100358425,
      "loss": 7.7168,
      "step": 1114
    },
    {
      "epoch": 0.33310926880274855,
      "grad_norm": 0.3890913128852844,
      "learning_rate": 0.00041681600955794507,
      "loss": 7.6094,
      "step": 1115
    },
    {
      "epoch": 0.33340802151019494,
      "grad_norm": 0.3892643451690674,
      "learning_rate": 0.0004167413381123059,
      "loss": 7.0322,
      "step": 1116
    },
    {
      "epoch": 0.33370677421764133,
      "grad_norm": 0.5475536584854126,
      "learning_rate": 0.0004166666666666667,
      "loss": 6.9141,
      "step": 1117
    },
    {
      "epoch": 0.3340055269250878,
      "grad_norm": 0.40577182173728943,
      "learning_rate": 0.00041659199522102745,
      "loss": 7.1914,
      "step": 1118
    },
    {
      "epoch": 0.3343042796325342,
      "grad_norm": 0.3819344639778137,
      "learning_rate": 0.0004165173237753883,
      "loss": 7.3027,
      "step": 1119
    },
    {
      "epoch": 0.33460303233998057,
      "grad_norm": 0.42910104990005493,
      "learning_rate": 0.0004164426523297491,
      "loss": 7.1895,
      "step": 1120
    },
    {
      "epoch": 0.334901785047427,
      "grad_norm": 0.37336793541908264,
      "learning_rate": 0.00041636798088410994,
      "loss": 7.2764,
      "step": 1121
    },
    {
      "epoch": 0.3352005377548734,
      "grad_norm": 0.44528958201408386,
      "learning_rate": 0.0004162933094384707,
      "loss": 7.4395,
      "step": 1122
    },
    {
      "epoch": 0.3354992904623198,
      "grad_norm": 0.399472177028656,
      "learning_rate": 0.00041621863799283157,
      "loss": 7.3965,
      "step": 1123
    },
    {
      "epoch": 0.33579804316976625,
      "grad_norm": 0.45034775137901306,
      "learning_rate": 0.0004161439665471924,
      "loss": 6.9238,
      "step": 1124
    },
    {
      "epoch": 0.33609679587721264,
      "grad_norm": 0.33687517046928406,
      "learning_rate": 0.00041606929510155314,
      "loss": 7.5479,
      "step": 1125
    },
    {
      "epoch": 0.33639554858465903,
      "grad_norm": 0.42495161294937134,
      "learning_rate": 0.000415994623655914,
      "loss": 7.0088,
      "step": 1126
    },
    {
      "epoch": 0.3366943012921055,
      "grad_norm": 0.44581952691078186,
      "learning_rate": 0.00041591995221027477,
      "loss": 7.2061,
      "step": 1127
    },
    {
      "epoch": 0.3369930539995519,
      "grad_norm": 0.4555799067020416,
      "learning_rate": 0.00041584528076463564,
      "loss": 7.0127,
      "step": 1128
    },
    {
      "epoch": 0.33729180670699827,
      "grad_norm": 0.4673580825328827,
      "learning_rate": 0.0004157706093189964,
      "loss": 7.4307,
      "step": 1129
    },
    {
      "epoch": 0.3375905594144447,
      "grad_norm": 0.35866519808769226,
      "learning_rate": 0.00041569593787335726,
      "loss": 7.5967,
      "step": 1130
    },
    {
      "epoch": 0.3378893121218911,
      "grad_norm": 0.4254395067691803,
      "learning_rate": 0.000415621266427718,
      "loss": 7.1182,
      "step": 1131
    },
    {
      "epoch": 0.3381880648293375,
      "grad_norm": 0.39071184396743774,
      "learning_rate": 0.0004155465949820789,
      "loss": 7.8086,
      "step": 1132
    },
    {
      "epoch": 0.33848681753678395,
      "grad_norm": 0.4330123960971832,
      "learning_rate": 0.0004154719235364397,
      "loss": 7.2852,
      "step": 1133
    },
    {
      "epoch": 0.33878557024423034,
      "grad_norm": 0.5372425317764282,
      "learning_rate": 0.00041539725209080046,
      "loss": 6.6484,
      "step": 1134
    },
    {
      "epoch": 0.33908432295167673,
      "grad_norm": 0.45645973086357117,
      "learning_rate": 0.0004153225806451613,
      "loss": 7.2041,
      "step": 1135
    },
    {
      "epoch": 0.3393830756591232,
      "grad_norm": 0.3294666111469269,
      "learning_rate": 0.0004152479091995221,
      "loss": 7.5527,
      "step": 1136
    },
    {
      "epoch": 0.33968182836656957,
      "grad_norm": 0.4823763370513916,
      "learning_rate": 0.00041517323775388295,
      "loss": 7.4375,
      "step": 1137
    },
    {
      "epoch": 0.33998058107401596,
      "grad_norm": 0.5532384514808655,
      "learning_rate": 0.0004150985663082437,
      "loss": 6.5391,
      "step": 1138
    },
    {
      "epoch": 0.3402793337814624,
      "grad_norm": 0.557755172252655,
      "learning_rate": 0.0004150238948626046,
      "loss": 6.9922,
      "step": 1139
    },
    {
      "epoch": 0.3405780864889088,
      "grad_norm": 0.4971378445625305,
      "learning_rate": 0.00041494922341696534,
      "loss": 7.1543,
      "step": 1140
    },
    {
      "epoch": 0.3408768391963552,
      "grad_norm": 0.2950311303138733,
      "learning_rate": 0.00041487455197132615,
      "loss": 7.7617,
      "step": 1141
    },
    {
      "epoch": 0.34117559190380164,
      "grad_norm": 0.35271570086479187,
      "learning_rate": 0.000414799880525687,
      "loss": 7.3711,
      "step": 1142
    },
    {
      "epoch": 0.34147434461124804,
      "grad_norm": 0.38431084156036377,
      "learning_rate": 0.0004147252090800478,
      "loss": 7.4404,
      "step": 1143
    },
    {
      "epoch": 0.34177309731869443,
      "grad_norm": 0.48284491896629333,
      "learning_rate": 0.00041465053763440864,
      "loss": 6.8359,
      "step": 1144
    },
    {
      "epoch": 0.3420718500261409,
      "grad_norm": 0.41182276606559753,
      "learning_rate": 0.0004145758661887694,
      "loss": 7.3369,
      "step": 1145
    },
    {
      "epoch": 0.34237060273358727,
      "grad_norm": 0.4213884770870209,
      "learning_rate": 0.00041450119474313027,
      "loss": 7.2598,
      "step": 1146
    },
    {
      "epoch": 0.34266935544103366,
      "grad_norm": 0.45883122086524963,
      "learning_rate": 0.000414426523297491,
      "loss": 7.3926,
      "step": 1147
    },
    {
      "epoch": 0.3429681081484801,
      "grad_norm": 0.45671749114990234,
      "learning_rate": 0.0004143518518518519,
      "loss": 7.1357,
      "step": 1148
    },
    {
      "epoch": 0.3432668608559265,
      "grad_norm": 0.4320181906223297,
      "learning_rate": 0.00041427718040621265,
      "loss": 7.6631,
      "step": 1149
    },
    {
      "epoch": 0.3435656135633729,
      "grad_norm": 0.4781048893928528,
      "learning_rate": 0.00041420250896057346,
      "loss": 7.1143,
      "step": 1150
    },
    {
      "epoch": 0.34386436627081934,
      "grad_norm": 0.35291406512260437,
      "learning_rate": 0.00041412783751493433,
      "loss": 7.4746,
      "step": 1151
    },
    {
      "epoch": 0.34416311897826574,
      "grad_norm": 0.447512686252594,
      "learning_rate": 0.0004140531660692951,
      "loss": 7.0127,
      "step": 1152
    },
    {
      "epoch": 0.34446187168571213,
      "grad_norm": 0.3887690603733063,
      "learning_rate": 0.00041397849462365596,
      "loss": 7.4551,
      "step": 1153
    },
    {
      "epoch": 0.3447606243931586,
      "grad_norm": 0.4299403131008148,
      "learning_rate": 0.0004139038231780167,
      "loss": 7.5117,
      "step": 1154
    },
    {
      "epoch": 0.34505937710060497,
      "grad_norm": 0.41844820976257324,
      "learning_rate": 0.0004138291517323776,
      "loss": 7.3887,
      "step": 1155
    },
    {
      "epoch": 0.34535812980805136,
      "grad_norm": 0.5585400462150574,
      "learning_rate": 0.00041375448028673834,
      "loss": 6.7627,
      "step": 1156
    },
    {
      "epoch": 0.3456568825154978,
      "grad_norm": 0.39746180176734924,
      "learning_rate": 0.00041367980884109915,
      "loss": 7.4424,
      "step": 1157
    },
    {
      "epoch": 0.3459556352229442,
      "grad_norm": 0.44526970386505127,
      "learning_rate": 0.00041360513739545997,
      "loss": 7.2939,
      "step": 1158
    },
    {
      "epoch": 0.3462543879303906,
      "grad_norm": 0.4681883454322815,
      "learning_rate": 0.0004135304659498208,
      "loss": 7.2197,
      "step": 1159
    },
    {
      "epoch": 0.34655314063783704,
      "grad_norm": 0.3992255628108978,
      "learning_rate": 0.00041345579450418165,
      "loss": 7.3076,
      "step": 1160
    },
    {
      "epoch": 0.34685189334528344,
      "grad_norm": 0.39443346858024597,
      "learning_rate": 0.0004133811230585424,
      "loss": 7.5547,
      "step": 1161
    },
    {
      "epoch": 0.3471506460527298,
      "grad_norm": 0.43411120772361755,
      "learning_rate": 0.00041330645161290327,
      "loss": 6.7852,
      "step": 1162
    },
    {
      "epoch": 0.3474493987601763,
      "grad_norm": 0.4030504524707794,
      "learning_rate": 0.00041323178016726403,
      "loss": 6.9648,
      "step": 1163
    },
    {
      "epoch": 0.34774815146762267,
      "grad_norm": 0.3786243796348572,
      "learning_rate": 0.0004131571087216249,
      "loss": 7.3066,
      "step": 1164
    },
    {
      "epoch": 0.34804690417506906,
      "grad_norm": 0.39617952704429626,
      "learning_rate": 0.00041308243727598566,
      "loss": 7.5703,
      "step": 1165
    },
    {
      "epoch": 0.3483456568825155,
      "grad_norm": 0.5793432593345642,
      "learning_rate": 0.00041300776583034647,
      "loss": 6.7637,
      "step": 1166
    },
    {
      "epoch": 0.3486444095899619,
      "grad_norm": 0.5101607441902161,
      "learning_rate": 0.0004129330943847073,
      "loss": 7.0479,
      "step": 1167
    },
    {
      "epoch": 0.3489431622974083,
      "grad_norm": 0.5196777582168579,
      "learning_rate": 0.0004128584229390681,
      "loss": 7.0762,
      "step": 1168
    },
    {
      "epoch": 0.34924191500485474,
      "grad_norm": 0.40564852952957153,
      "learning_rate": 0.0004127837514934289,
      "loss": 7.5732,
      "step": 1169
    },
    {
      "epoch": 0.34954066771230113,
      "grad_norm": 0.4312015175819397,
      "learning_rate": 0.0004127090800477897,
      "loss": 6.9189,
      "step": 1170
    },
    {
      "epoch": 0.3498394204197475,
      "grad_norm": 0.4284078776836395,
      "learning_rate": 0.0004126344086021506,
      "loss": 7.4395,
      "step": 1171
    },
    {
      "epoch": 0.350138173127194,
      "grad_norm": 0.38022831082344055,
      "learning_rate": 0.00041255973715651135,
      "loss": 7.4209,
      "step": 1172
    },
    {
      "epoch": 0.35043692583464037,
      "grad_norm": 0.36888185143470764,
      "learning_rate": 0.00041248506571087216,
      "loss": 7.5293,
      "step": 1173
    },
    {
      "epoch": 0.3507356785420868,
      "grad_norm": 0.4749763309955597,
      "learning_rate": 0.000412410394265233,
      "loss": 7.1055,
      "step": 1174
    },
    {
      "epoch": 0.3510344312495332,
      "grad_norm": 0.3912242650985718,
      "learning_rate": 0.0004123357228195938,
      "loss": 7.209,
      "step": 1175
    },
    {
      "epoch": 0.3513331839569796,
      "grad_norm": 0.4033750593662262,
      "learning_rate": 0.0004122610513739546,
      "loss": 7.3994,
      "step": 1176
    },
    {
      "epoch": 0.35163193666442605,
      "grad_norm": 0.472583144903183,
      "learning_rate": 0.0004121863799283154,
      "loss": 7.0225,
      "step": 1177
    },
    {
      "epoch": 0.35193068937187244,
      "grad_norm": 0.4219514727592468,
      "learning_rate": 0.0004121117084826762,
      "loss": 6.9229,
      "step": 1178
    },
    {
      "epoch": 0.35222944207931883,
      "grad_norm": 0.5255980491638184,
      "learning_rate": 0.00041203703703703704,
      "loss": 6.6982,
      "step": 1179
    },
    {
      "epoch": 0.3525281947867653,
      "grad_norm": 0.5178106427192688,
      "learning_rate": 0.0004119623655913979,
      "loss": 6.7852,
      "step": 1180
    },
    {
      "epoch": 0.3528269474942117,
      "grad_norm": 0.40459030866622925,
      "learning_rate": 0.00041188769414575866,
      "loss": 7.667,
      "step": 1181
    },
    {
      "epoch": 0.35312570020165807,
      "grad_norm": 0.41773372888565063,
      "learning_rate": 0.0004118130227001195,
      "loss": 7.2959,
      "step": 1182
    },
    {
      "epoch": 0.3534244529091045,
      "grad_norm": 0.5378082990646362,
      "learning_rate": 0.0004117383512544803,
      "loss": 6.7588,
      "step": 1183
    },
    {
      "epoch": 0.3537232056165509,
      "grad_norm": 0.4928402900695801,
      "learning_rate": 0.0004116636798088411,
      "loss": 6.959,
      "step": 1184
    },
    {
      "epoch": 0.3540219583239973,
      "grad_norm": 0.42796409130096436,
      "learning_rate": 0.0004115890083632019,
      "loss": 7.1572,
      "step": 1185
    },
    {
      "epoch": 0.35432071103144375,
      "grad_norm": 0.476517915725708,
      "learning_rate": 0.00041151433691756273,
      "loss": 7.2139,
      "step": 1186
    },
    {
      "epoch": 0.35461946373889014,
      "grad_norm": 0.37012097239494324,
      "learning_rate": 0.00041143966547192354,
      "loss": 7.249,
      "step": 1187
    },
    {
      "epoch": 0.35491821644633653,
      "grad_norm": 0.46436527371406555,
      "learning_rate": 0.00041136499402628435,
      "loss": 7.2373,
      "step": 1188
    },
    {
      "epoch": 0.355216969153783,
      "grad_norm": 0.4384196698665619,
      "learning_rate": 0.00041129032258064517,
      "loss": 7.1592,
      "step": 1189
    },
    {
      "epoch": 0.3555157218612294,
      "grad_norm": 0.38760754466056824,
      "learning_rate": 0.000411215651135006,
      "loss": 7.5557,
      "step": 1190
    },
    {
      "epoch": 0.35581447456867576,
      "grad_norm": 0.541159451007843,
      "learning_rate": 0.0004111409796893668,
      "loss": 7.3984,
      "step": 1191
    },
    {
      "epoch": 0.3561132272761222,
      "grad_norm": 0.42061081528663635,
      "learning_rate": 0.0004110663082437276,
      "loss": 6.8076,
      "step": 1192
    },
    {
      "epoch": 0.3564119799835686,
      "grad_norm": 0.43756312131881714,
      "learning_rate": 0.0004109916367980884,
      "loss": 7.3623,
      "step": 1193
    },
    {
      "epoch": 0.356710732691015,
      "grad_norm": 0.42711007595062256,
      "learning_rate": 0.00041091696535244923,
      "loss": 7.501,
      "step": 1194
    },
    {
      "epoch": 0.35700948539846145,
      "grad_norm": 0.4982943832874298,
      "learning_rate": 0.00041084229390681004,
      "loss": 7.0596,
      "step": 1195
    },
    {
      "epoch": 0.35730823810590784,
      "grad_norm": 0.43600234389305115,
      "learning_rate": 0.00041076762246117086,
      "loss": 7.4717,
      "step": 1196
    },
    {
      "epoch": 0.35760699081335423,
      "grad_norm": 0.46614736318588257,
      "learning_rate": 0.00041069295101553167,
      "loss": 7.1045,
      "step": 1197
    },
    {
      "epoch": 0.3579057435208007,
      "grad_norm": 0.3765042722225189,
      "learning_rate": 0.0004106182795698925,
      "loss": 7.626,
      "step": 1198
    },
    {
      "epoch": 0.35820449622824707,
      "grad_norm": 0.342729777097702,
      "learning_rate": 0.0004105436081242533,
      "loss": 7.7578,
      "step": 1199
    },
    {
      "epoch": 0.35850324893569346,
      "grad_norm": 0.396738201379776,
      "learning_rate": 0.0004104689366786141,
      "loss": 7.6025,
      "step": 1200
    },
    {
      "epoch": 0.35850324893569346,
      "eval_bleu": 0.09957964562434878,
      "eval_loss": 7.05859375,
      "eval_runtime": 536.1864,
      "eval_samples_per_second": 2.628,
      "eval_steps_per_second": 0.166,
      "step": 1200
    },
    {
      "epoch": 0.3588020016431399,
      "grad_norm": 0.4535461366176605,
      "learning_rate": 0.0004103942652329749,
      "loss": 7.0723,
      "step": 1201
    },
    {
      "epoch": 0.3591007543505863,
      "grad_norm": 0.4690917730331421,
      "learning_rate": 0.00041031959378733573,
      "loss": 7.0166,
      "step": 1202
    },
    {
      "epoch": 0.3593995070580327,
      "grad_norm": 0.46025314927101135,
      "learning_rate": 0.00041024492234169655,
      "loss": 7.0312,
      "step": 1203
    },
    {
      "epoch": 0.35969825976547914,
      "grad_norm": 0.5319485068321228,
      "learning_rate": 0.00041017025089605736,
      "loss": 7.1094,
      "step": 1204
    },
    {
      "epoch": 0.35999701247292554,
      "grad_norm": 0.4770788252353668,
      "learning_rate": 0.0004100955794504181,
      "loss": 6.793,
      "step": 1205
    },
    {
      "epoch": 0.36029576518037193,
      "grad_norm": 0.45234036445617676,
      "learning_rate": 0.000410020908004779,
      "loss": 7.0449,
      "step": 1206
    },
    {
      "epoch": 0.3605945178878184,
      "grad_norm": 0.462645024061203,
      "learning_rate": 0.0004099462365591398,
      "loss": 7.083,
      "step": 1207
    },
    {
      "epoch": 0.36089327059526477,
      "grad_norm": 0.34814587235450745,
      "learning_rate": 0.0004098715651135006,
      "loss": 7.6074,
      "step": 1208
    },
    {
      "epoch": 0.36119202330271116,
      "grad_norm": 0.4286554455757141,
      "learning_rate": 0.0004097968936678614,
      "loss": 6.9717,
      "step": 1209
    },
    {
      "epoch": 0.3614907760101576,
      "grad_norm": 0.46662402153015137,
      "learning_rate": 0.00040972222222222224,
      "loss": 6.9395,
      "step": 1210
    },
    {
      "epoch": 0.361789528717604,
      "grad_norm": 0.3659725785255432,
      "learning_rate": 0.00040964755077658305,
      "loss": 7.6621,
      "step": 1211
    },
    {
      "epoch": 0.3620882814250504,
      "grad_norm": 0.38391178846359253,
      "learning_rate": 0.00040957287933094386,
      "loss": 7.5254,
      "step": 1212
    },
    {
      "epoch": 0.36238703413249684,
      "grad_norm": 0.36423176527023315,
      "learning_rate": 0.0004094982078853047,
      "loss": 7.4727,
      "step": 1213
    },
    {
      "epoch": 0.36268578683994324,
      "grad_norm": 0.470901757478714,
      "learning_rate": 0.00040942353643966543,
      "loss": 6.7119,
      "step": 1214
    },
    {
      "epoch": 0.36298453954738963,
      "grad_norm": 0.418977290391922,
      "learning_rate": 0.0004093488649940263,
      "loss": 7.1006,
      "step": 1215
    },
    {
      "epoch": 0.3632832922548361,
      "grad_norm": 0.35365229845046997,
      "learning_rate": 0.0004092741935483871,
      "loss": 7.6094,
      "step": 1216
    },
    {
      "epoch": 0.36358204496228247,
      "grad_norm": 0.42185428738594055,
      "learning_rate": 0.0004091995221027479,
      "loss": 7.1006,
      "step": 1217
    },
    {
      "epoch": 0.36388079766972886,
      "grad_norm": 0.42102837562561035,
      "learning_rate": 0.00040912485065710874,
      "loss": 7.0137,
      "step": 1218
    },
    {
      "epoch": 0.3641795503771753,
      "grad_norm": 0.40738722681999207,
      "learning_rate": 0.00040905017921146955,
      "loss": 7.3848,
      "step": 1219
    },
    {
      "epoch": 0.3644783030846217,
      "grad_norm": 0.3942583203315735,
      "learning_rate": 0.00040897550776583037,
      "loss": 7.1016,
      "step": 1220
    },
    {
      "epoch": 0.3647770557920681,
      "grad_norm": 0.5242077112197876,
      "learning_rate": 0.0004089008363201911,
      "loss": 6.1992,
      "step": 1221
    },
    {
      "epoch": 0.36507580849951454,
      "grad_norm": 0.39056196808815,
      "learning_rate": 0.000408826164874552,
      "loss": 7.3682,
      "step": 1222
    },
    {
      "epoch": 0.36537456120696093,
      "grad_norm": 0.4657384157180786,
      "learning_rate": 0.00040875149342891275,
      "loss": 7.4365,
      "step": 1223
    },
    {
      "epoch": 0.3656733139144073,
      "grad_norm": 0.3703705072402954,
      "learning_rate": 0.0004086768219832736,
      "loss": 7.0771,
      "step": 1224
    },
    {
      "epoch": 0.3659720666218538,
      "grad_norm": 0.3990827202796936,
      "learning_rate": 0.00040860215053763443,
      "loss": 7.6914,
      "step": 1225
    },
    {
      "epoch": 0.36627081932930017,
      "grad_norm": 0.4852645695209503,
      "learning_rate": 0.00040852747909199524,
      "loss": 7.3174,
      "step": 1226
    },
    {
      "epoch": 0.36656957203674656,
      "grad_norm": 0.3915609121322632,
      "learning_rate": 0.00040845280764635606,
      "loss": 7.168,
      "step": 1227
    },
    {
      "epoch": 0.366868324744193,
      "grad_norm": 0.3578599691390991,
      "learning_rate": 0.00040837813620071687,
      "loss": 7.3574,
      "step": 1228
    },
    {
      "epoch": 0.3671670774516394,
      "grad_norm": 0.33268851041793823,
      "learning_rate": 0.0004083034647550777,
      "loss": 7.8438,
      "step": 1229
    },
    {
      "epoch": 0.3674658301590858,
      "grad_norm": 0.5153006911277771,
      "learning_rate": 0.00040822879330943844,
      "loss": 6.4248,
      "step": 1230
    },
    {
      "epoch": 0.36776458286653224,
      "grad_norm": 0.4173300266265869,
      "learning_rate": 0.0004081541218637993,
      "loss": 7.5537,
      "step": 1231
    },
    {
      "epoch": 0.36806333557397863,
      "grad_norm": 0.7996116876602173,
      "learning_rate": 0.00040807945041816007,
      "loss": 7.0811,
      "step": 1232
    },
    {
      "epoch": 0.368362088281425,
      "grad_norm": 0.40460270643234253,
      "learning_rate": 0.00040800477897252093,
      "loss": 7.4697,
      "step": 1233
    },
    {
      "epoch": 0.3686608409888715,
      "grad_norm": 0.388881117105484,
      "learning_rate": 0.00040793010752688175,
      "loss": 7.5977,
      "step": 1234
    },
    {
      "epoch": 0.36895959369631787,
      "grad_norm": 0.5645081996917725,
      "learning_rate": 0.00040785543608124256,
      "loss": 6.6699,
      "step": 1235
    },
    {
      "epoch": 0.36925834640376426,
      "grad_norm": 0.3587989807128906,
      "learning_rate": 0.00040778076463560337,
      "loss": 7.5693,
      "step": 1236
    },
    {
      "epoch": 0.3695570991112107,
      "grad_norm": 0.4457625448703766,
      "learning_rate": 0.00040770609318996413,
      "loss": 7.1895,
      "step": 1237
    },
    {
      "epoch": 0.3698558518186571,
      "grad_norm": 0.39566147327423096,
      "learning_rate": 0.000407631421744325,
      "loss": 7.2549,
      "step": 1238
    },
    {
      "epoch": 0.3701546045261035,
      "grad_norm": 0.38304874300956726,
      "learning_rate": 0.00040755675029868576,
      "loss": 7.2744,
      "step": 1239
    },
    {
      "epoch": 0.37045335723354994,
      "grad_norm": 0.35393014550209045,
      "learning_rate": 0.0004074820788530466,
      "loss": 7.7246,
      "step": 1240
    },
    {
      "epoch": 0.37075210994099633,
      "grad_norm": 0.44617149233818054,
      "learning_rate": 0.0004074074074074074,
      "loss": 6.9346,
      "step": 1241
    },
    {
      "epoch": 0.3710508626484427,
      "grad_norm": 0.35063305497169495,
      "learning_rate": 0.00040733273596176825,
      "loss": 7.3965,
      "step": 1242
    },
    {
      "epoch": 0.3713496153558892,
      "grad_norm": 0.40441280603408813,
      "learning_rate": 0.00040725806451612906,
      "loss": 7.0293,
      "step": 1243
    },
    {
      "epoch": 0.37164836806333557,
      "grad_norm": 0.43735045194625854,
      "learning_rate": 0.0004071833930704899,
      "loss": 7.29,
      "step": 1244
    },
    {
      "epoch": 0.37194712077078196,
      "grad_norm": 0.39322859048843384,
      "learning_rate": 0.0004071087216248507,
      "loss": 7.4727,
      "step": 1245
    },
    {
      "epoch": 0.3722458734782284,
      "grad_norm": 0.3413228988647461,
      "learning_rate": 0.00040703405017921145,
      "loss": 7.8809,
      "step": 1246
    },
    {
      "epoch": 0.3725446261856748,
      "grad_norm": 0.44855207204818726,
      "learning_rate": 0.0004069593787335723,
      "loss": 6.8701,
      "step": 1247
    },
    {
      "epoch": 0.3728433788931212,
      "grad_norm": 0.37545856833457947,
      "learning_rate": 0.00040688470728793307,
      "loss": 7.6055,
      "step": 1248
    },
    {
      "epoch": 0.37314213160056764,
      "grad_norm": 0.5158642530441284,
      "learning_rate": 0.00040681003584229394,
      "loss": 6.8008,
      "step": 1249
    },
    {
      "epoch": 0.37344088430801403,
      "grad_norm": 0.46901288628578186,
      "learning_rate": 0.0004067353643966547,
      "loss": 7.6367,
      "step": 1250
    },
    {
      "epoch": 0.3737396370154605,
      "grad_norm": 0.4072977304458618,
      "learning_rate": 0.00040666069295101556,
      "loss": 7.374,
      "step": 1251
    },
    {
      "epoch": 0.37403838972290687,
      "grad_norm": 0.3788970410823822,
      "learning_rate": 0.0004065860215053764,
      "loss": 7.6416,
      "step": 1252
    },
    {
      "epoch": 0.37433714243035326,
      "grad_norm": 0.4254037141799927,
      "learning_rate": 0.00040651135005973714,
      "loss": 7.1738,
      "step": 1253
    },
    {
      "epoch": 0.3746358951377997,
      "grad_norm": 0.38370802998542786,
      "learning_rate": 0.000406436678614098,
      "loss": 7.2568,
      "step": 1254
    },
    {
      "epoch": 0.3749346478452461,
      "grad_norm": 0.5000751614570618,
      "learning_rate": 0.00040636200716845876,
      "loss": 6.7842,
      "step": 1255
    },
    {
      "epoch": 0.3752334005526925,
      "grad_norm": 0.42839884757995605,
      "learning_rate": 0.00040628733572281963,
      "loss": 6.9023,
      "step": 1256
    },
    {
      "epoch": 0.37553215326013895,
      "grad_norm": 0.4234052300453186,
      "learning_rate": 0.0004062126642771804,
      "loss": 7.2412,
      "step": 1257
    },
    {
      "epoch": 0.37583090596758534,
      "grad_norm": 0.44725269079208374,
      "learning_rate": 0.00040613799283154125,
      "loss": 6.7627,
      "step": 1258
    },
    {
      "epoch": 0.37612965867503173,
      "grad_norm": 0.3868129551410675,
      "learning_rate": 0.000406063321385902,
      "loss": 7.1943,
      "step": 1259
    },
    {
      "epoch": 0.3764284113824782,
      "grad_norm": 0.4219895899295807,
      "learning_rate": 0.0004059886499402629,
      "loss": 7.0674,
      "step": 1260
    },
    {
      "epoch": 0.37672716408992457,
      "grad_norm": 0.5258481502532959,
      "learning_rate": 0.00040591397849462364,
      "loss": 6.6855,
      "step": 1261
    },
    {
      "epoch": 0.37702591679737096,
      "grad_norm": 0.4593670666217804,
      "learning_rate": 0.00040583930704898445,
      "loss": 7.4365,
      "step": 1262
    },
    {
      "epoch": 0.3773246695048174,
      "grad_norm": 0.44170162081718445,
      "learning_rate": 0.0004057646356033453,
      "loss": 7.1719,
      "step": 1263
    },
    {
      "epoch": 0.3776234222122638,
      "grad_norm": 0.4755409359931946,
      "learning_rate": 0.0004056899641577061,
      "loss": 6.7842,
      "step": 1264
    },
    {
      "epoch": 0.3779221749197102,
      "grad_norm": 0.33215969800949097,
      "learning_rate": 0.00040561529271206695,
      "loss": 7.8789,
      "step": 1265
    },
    {
      "epoch": 0.37822092762715664,
      "grad_norm": 0.5002089738845825,
      "learning_rate": 0.0004055406212664277,
      "loss": 6.9922,
      "step": 1266
    },
    {
      "epoch": 0.37851968033460304,
      "grad_norm": 0.45952701568603516,
      "learning_rate": 0.00040546594982078857,
      "loss": 7.376,
      "step": 1267
    },
    {
      "epoch": 0.37881843304204943,
      "grad_norm": 0.422726571559906,
      "learning_rate": 0.00040539127837514933,
      "loss": 7.1562,
      "step": 1268
    },
    {
      "epoch": 0.3791171857494959,
      "grad_norm": 0.3700873553752899,
      "learning_rate": 0.00040531660692951014,
      "loss": 7.4639,
      "step": 1269
    },
    {
      "epoch": 0.37941593845694227,
      "grad_norm": 0.47478920221328735,
      "learning_rate": 0.00040524193548387096,
      "loss": 7.1934,
      "step": 1270
    },
    {
      "epoch": 0.37971469116438866,
      "grad_norm": 0.4316105842590332,
      "learning_rate": 0.00040516726403823177,
      "loss": 6.8086,
      "step": 1271
    },
    {
      "epoch": 0.3800134438718351,
      "grad_norm": 0.37591585516929626,
      "learning_rate": 0.00040509259259259264,
      "loss": 7.1211,
      "step": 1272
    },
    {
      "epoch": 0.3803121965792815,
      "grad_norm": 0.4020209312438965,
      "learning_rate": 0.0004050179211469534,
      "loss": 7.3271,
      "step": 1273
    },
    {
      "epoch": 0.3806109492867279,
      "grad_norm": 0.46013426780700684,
      "learning_rate": 0.00040494324970131426,
      "loss": 6.7715,
      "step": 1274
    },
    {
      "epoch": 0.38090970199417434,
      "grad_norm": 0.41175949573516846,
      "learning_rate": 0.000404868578255675,
      "loss": 7.2119,
      "step": 1275
    },
    {
      "epoch": 0.38120845470162074,
      "grad_norm": 0.44741788506507874,
      "learning_rate": 0.0004047939068100359,
      "loss": 6.5898,
      "step": 1276
    },
    {
      "epoch": 0.38150720740906713,
      "grad_norm": 0.5059889554977417,
      "learning_rate": 0.00040471923536439665,
      "loss": 6.2715,
      "step": 1277
    },
    {
      "epoch": 0.3818059601165136,
      "grad_norm": 0.3805628716945648,
      "learning_rate": 0.00040464456391875746,
      "loss": 7.3359,
      "step": 1278
    },
    {
      "epoch": 0.38210471282395997,
      "grad_norm": 0.3913379907608032,
      "learning_rate": 0.00040456989247311827,
      "loss": 7.4951,
      "step": 1279
    },
    {
      "epoch": 0.38240346553140636,
      "grad_norm": 0.50736403465271,
      "learning_rate": 0.0004044952210274791,
      "loss": 6.7881,
      "step": 1280
    },
    {
      "epoch": 0.3827022182388528,
      "grad_norm": 0.4632420241832733,
      "learning_rate": 0.00040442054958183995,
      "loss": 7.4248,
      "step": 1281
    },
    {
      "epoch": 0.3830009709462992,
      "grad_norm": 0.509141743183136,
      "learning_rate": 0.0004043458781362007,
      "loss": 7.2266,
      "step": 1282
    },
    {
      "epoch": 0.3832997236537456,
      "grad_norm": 0.4201907217502594,
      "learning_rate": 0.0004042712066905616,
      "loss": 7.2373,
      "step": 1283
    },
    {
      "epoch": 0.38359847636119204,
      "grad_norm": 0.4298637807369232,
      "learning_rate": 0.00040419653524492234,
      "loss": 7.2402,
      "step": 1284
    },
    {
      "epoch": 0.38389722906863843,
      "grad_norm": 0.4859130382537842,
      "learning_rate": 0.00040412186379928315,
      "loss": 6.8516,
      "step": 1285
    },
    {
      "epoch": 0.3841959817760848,
      "grad_norm": 0.45485153794288635,
      "learning_rate": 0.00040404719235364396,
      "loss": 7.5576,
      "step": 1286
    },
    {
      "epoch": 0.3844947344835313,
      "grad_norm": 0.5605916380882263,
      "learning_rate": 0.0004039725209080048,
      "loss": 7.2344,
      "step": 1287
    },
    {
      "epoch": 0.38479348719097767,
      "grad_norm": 0.3534426987171173,
      "learning_rate": 0.0004038978494623656,
      "loss": 7.5381,
      "step": 1288
    },
    {
      "epoch": 0.38509223989842406,
      "grad_norm": 0.3818137049674988,
      "learning_rate": 0.0004038231780167264,
      "loss": 7.3857,
      "step": 1289
    },
    {
      "epoch": 0.3853909926058705,
      "grad_norm": 0.45477986335754395,
      "learning_rate": 0.00040374850657108727,
      "loss": 6.9775,
      "step": 1290
    },
    {
      "epoch": 0.3856897453133169,
      "grad_norm": 0.49868372082710266,
      "learning_rate": 0.000403673835125448,
      "loss": 7.1729,
      "step": 1291
    },
    {
      "epoch": 0.3859884980207633,
      "grad_norm": 0.4422799050807953,
      "learning_rate": 0.0004035991636798089,
      "loss": 6.876,
      "step": 1292
    },
    {
      "epoch": 0.38628725072820974,
      "grad_norm": 0.4107644855976105,
      "learning_rate": 0.00040352449223416965,
      "loss": 6.8789,
      "step": 1293
    },
    {
      "epoch": 0.38658600343565613,
      "grad_norm": 0.3871323764324188,
      "learning_rate": 0.00040344982078853046,
      "loss": 7.4941,
      "step": 1294
    },
    {
      "epoch": 0.3868847561431025,
      "grad_norm": 0.38297879695892334,
      "learning_rate": 0.0004033751493428913,
      "loss": 7.5391,
      "step": 1295
    },
    {
      "epoch": 0.387183508850549,
      "grad_norm": 0.4868331849575043,
      "learning_rate": 0.0004033004778972521,
      "loss": 6.5713,
      "step": 1296
    },
    {
      "epoch": 0.38748226155799537,
      "grad_norm": 0.4602886438369751,
      "learning_rate": 0.0004032258064516129,
      "loss": 7.2021,
      "step": 1297
    },
    {
      "epoch": 0.38778101426544176,
      "grad_norm": 0.4704173505306244,
      "learning_rate": 0.0004031511350059737,
      "loss": 7.1123,
      "step": 1298
    },
    {
      "epoch": 0.3880797669728882,
      "grad_norm": 0.42659884691238403,
      "learning_rate": 0.0004030764635603346,
      "loss": 7.3271,
      "step": 1299
    },
    {
      "epoch": 0.3883785196803346,
      "grad_norm": 0.39889708161354065,
      "learning_rate": 0.00040300179211469534,
      "loss": 7.4795,
      "step": 1300
    },
    {
      "epoch": 0.388677272387781,
      "grad_norm": 0.32731691002845764,
      "learning_rate": 0.00040292712066905615,
      "loss": 7.959,
      "step": 1301
    },
    {
      "epoch": 0.38897602509522744,
      "grad_norm": 0.4377132058143616,
      "learning_rate": 0.00040285244922341697,
      "loss": 7.1455,
      "step": 1302
    },
    {
      "epoch": 0.38927477780267383,
      "grad_norm": 0.5914089679718018,
      "learning_rate": 0.0004027777777777778,
      "loss": 7.3555,
      "step": 1303
    },
    {
      "epoch": 0.3895735305101202,
      "grad_norm": 0.4718615412712097,
      "learning_rate": 0.0004027031063321386,
      "loss": 7.1504,
      "step": 1304
    },
    {
      "epoch": 0.3898722832175667,
      "grad_norm": 0.3611397445201874,
      "learning_rate": 0.0004026284348864994,
      "loss": 7.8145,
      "step": 1305
    },
    {
      "epoch": 0.39017103592501307,
      "grad_norm": 0.4058666229248047,
      "learning_rate": 0.0004025537634408602,
      "loss": 7.3154,
      "step": 1306
    },
    {
      "epoch": 0.39046978863245946,
      "grad_norm": 0.3440166711807251,
      "learning_rate": 0.00040247909199522103,
      "loss": 7.8584,
      "step": 1307
    },
    {
      "epoch": 0.3907685413399059,
      "grad_norm": 0.47709521651268005,
      "learning_rate": 0.0004024044205495819,
      "loss": 7.248,
      "step": 1308
    },
    {
      "epoch": 0.3910672940473523,
      "grad_norm": 0.3559253513813019,
      "learning_rate": 0.00040232974910394266,
      "loss": 7.7441,
      "step": 1309
    },
    {
      "epoch": 0.3913660467547987,
      "grad_norm": 0.38514524698257446,
      "learning_rate": 0.00040225507765830347,
      "loss": 7.1475,
      "step": 1310
    },
    {
      "epoch": 0.39166479946224514,
      "grad_norm": 0.3566279113292694,
      "learning_rate": 0.0004021804062126643,
      "loss": 7.3672,
      "step": 1311
    },
    {
      "epoch": 0.39196355216969153,
      "grad_norm": 0.4191759526729584,
      "learning_rate": 0.0004021057347670251,
      "loss": 7.3203,
      "step": 1312
    },
    {
      "epoch": 0.3922623048771379,
      "grad_norm": 0.4021904170513153,
      "learning_rate": 0.0004020310633213859,
      "loss": 7.4287,
      "step": 1313
    },
    {
      "epoch": 0.39256105758458437,
      "grad_norm": 0.4226718842983246,
      "learning_rate": 0.0004019563918757467,
      "loss": 7.0361,
      "step": 1314
    },
    {
      "epoch": 0.39285981029203076,
      "grad_norm": 0.39375734329223633,
      "learning_rate": 0.00040188172043010753,
      "loss": 7.5264,
      "step": 1315
    },
    {
      "epoch": 0.39315856299947716,
      "grad_norm": 0.3949388861656189,
      "learning_rate": 0.00040180704898446835,
      "loss": 7.3955,
      "step": 1316
    },
    {
      "epoch": 0.3934573157069236,
      "grad_norm": 0.37904563546180725,
      "learning_rate": 0.00040173237753882916,
      "loss": 7.377,
      "step": 1317
    },
    {
      "epoch": 0.39375606841437,
      "grad_norm": 0.45754674077033997,
      "learning_rate": 0.00040165770609319,
      "loss": 6.9434,
      "step": 1318
    },
    {
      "epoch": 0.3940548211218164,
      "grad_norm": 0.35402339696884155,
      "learning_rate": 0.0004015830346475508,
      "loss": 7.3818,
      "step": 1319
    },
    {
      "epoch": 0.39435357382926284,
      "grad_norm": 0.4285252094268799,
      "learning_rate": 0.0004015083632019116,
      "loss": 7.3291,
      "step": 1320
    },
    {
      "epoch": 0.39465232653670923,
      "grad_norm": 0.4927235543727875,
      "learning_rate": 0.0004014336917562724,
      "loss": 6.7354,
      "step": 1321
    },
    {
      "epoch": 0.3949510792441556,
      "grad_norm": 0.44210943579673767,
      "learning_rate": 0.0004013590203106332,
      "loss": 7.3525,
      "step": 1322
    },
    {
      "epoch": 0.39524983195160207,
      "grad_norm": 0.3810325562953949,
      "learning_rate": 0.00040128434886499404,
      "loss": 7.5098,
      "step": 1323
    },
    {
      "epoch": 0.39554858465904846,
      "grad_norm": 0.555059015750885,
      "learning_rate": 0.00040120967741935485,
      "loss": 6.1357,
      "step": 1324
    },
    {
      "epoch": 0.3958473373664949,
      "grad_norm": 0.32264047861099243,
      "learning_rate": 0.00040113500597371566,
      "loss": 7.9697,
      "step": 1325
    },
    {
      "epoch": 0.3961460900739413,
      "grad_norm": 0.47967639565467834,
      "learning_rate": 0.0004010603345280765,
      "loss": 7.2236,
      "step": 1326
    },
    {
      "epoch": 0.3964448427813877,
      "grad_norm": 0.3806140720844269,
      "learning_rate": 0.0004009856630824373,
      "loss": 7.3457,
      "step": 1327
    },
    {
      "epoch": 0.39674359548883414,
      "grad_norm": 0.39954668283462524,
      "learning_rate": 0.0004009109916367981,
      "loss": 7.5098,
      "step": 1328
    },
    {
      "epoch": 0.39704234819628054,
      "grad_norm": 0.561843991279602,
      "learning_rate": 0.0004008363201911589,
      "loss": 6.3154,
      "step": 1329
    },
    {
      "epoch": 0.39734110090372693,
      "grad_norm": 0.4274856448173523,
      "learning_rate": 0.00040076164874551973,
      "loss": 7.0205,
      "step": 1330
    },
    {
      "epoch": 0.3976398536111734,
      "grad_norm": 0.4248376488685608,
      "learning_rate": 0.00040068697729988054,
      "loss": 7.1123,
      "step": 1331
    },
    {
      "epoch": 0.39793860631861977,
      "grad_norm": 0.326907217502594,
      "learning_rate": 0.00040061230585424135,
      "loss": 7.7168,
      "step": 1332
    },
    {
      "epoch": 0.39823735902606616,
      "grad_norm": 0.4475764036178589,
      "learning_rate": 0.0004005376344086021,
      "loss": 7.3125,
      "step": 1333
    },
    {
      "epoch": 0.3985361117335126,
      "grad_norm": 0.46090999245643616,
      "learning_rate": 0.000400462962962963,
      "loss": 7.0557,
      "step": 1334
    },
    {
      "epoch": 0.398834864440959,
      "grad_norm": 0.4872890114784241,
      "learning_rate": 0.0004003882915173238,
      "loss": 6.9736,
      "step": 1335
    },
    {
      "epoch": 0.3991336171484054,
      "grad_norm": 0.5997264981269836,
      "learning_rate": 0.0004003136200716846,
      "loss": 6.5508,
      "step": 1336
    },
    {
      "epoch": 0.39943236985585184,
      "grad_norm": 0.4190281629562378,
      "learning_rate": 0.0004002389486260454,
      "loss": 7.0723,
      "step": 1337
    },
    {
      "epoch": 0.39973112256329824,
      "grad_norm": 0.6724498271942139,
      "learning_rate": 0.00040016427718040623,
      "loss": 6.4873,
      "step": 1338
    },
    {
      "epoch": 0.40002987527074463,
      "grad_norm": 0.37512078881263733,
      "learning_rate": 0.00040008960573476704,
      "loss": 7.749,
      "step": 1339
    },
    {
      "epoch": 0.4003286279781911,
      "grad_norm": 0.43471014499664307,
      "learning_rate": 0.00040001493428912786,
      "loss": 6.959,
      "step": 1340
    },
    {
      "epoch": 0.40062738068563747,
      "grad_norm": 0.35706111788749695,
      "learning_rate": 0.00039994026284348867,
      "loss": 7.3916,
      "step": 1341
    },
    {
      "epoch": 0.40092613339308386,
      "grad_norm": 0.46811357140541077,
      "learning_rate": 0.00039986559139784943,
      "loss": 7.1846,
      "step": 1342
    },
    {
      "epoch": 0.4012248861005303,
      "grad_norm": 0.46734628081321716,
      "learning_rate": 0.0003997909199522103,
      "loss": 7.0674,
      "step": 1343
    },
    {
      "epoch": 0.4015236388079767,
      "grad_norm": 0.4335964322090149,
      "learning_rate": 0.0003997162485065711,
      "loss": 7.3281,
      "step": 1344
    },
    {
      "epoch": 0.4018223915154231,
      "grad_norm": 0.3685634136199951,
      "learning_rate": 0.0003996415770609319,
      "loss": 7.3633,
      "step": 1345
    },
    {
      "epoch": 0.40212114422286954,
      "grad_norm": 0.4093611538410187,
      "learning_rate": 0.00039956690561529273,
      "loss": 7.0645,
      "step": 1346
    },
    {
      "epoch": 0.40241989693031593,
      "grad_norm": 0.4455241560935974,
      "learning_rate": 0.00039949223416965355,
      "loss": 7.25,
      "step": 1347
    },
    {
      "epoch": 0.4027186496377623,
      "grad_norm": 0.3848409950733185,
      "learning_rate": 0.00039941756272401436,
      "loss": 7.4502,
      "step": 1348
    },
    {
      "epoch": 0.4030174023452088,
      "grad_norm": 0.5473566651344299,
      "learning_rate": 0.0003993428912783751,
      "loss": 6.9189,
      "step": 1349
    },
    {
      "epoch": 0.40331615505265517,
      "grad_norm": 0.5039893984794617,
      "learning_rate": 0.000399268219832736,
      "loss": 7.3633,
      "step": 1350
    },
    {
      "epoch": 0.40361490776010156,
      "grad_norm": 0.3977317810058594,
      "learning_rate": 0.00039919354838709674,
      "loss": 7.5449,
      "step": 1351
    },
    {
      "epoch": 0.403913660467548,
      "grad_norm": 0.40589627623558044,
      "learning_rate": 0.0003991188769414576,
      "loss": 7.1777,
      "step": 1352
    },
    {
      "epoch": 0.4042124131749944,
      "grad_norm": 0.4793681502342224,
      "learning_rate": 0.00039904420549581837,
      "loss": 6.998,
      "step": 1353
    },
    {
      "epoch": 0.4045111658824408,
      "grad_norm": 0.41409528255462646,
      "learning_rate": 0.00039896953405017924,
      "loss": 7.1094,
      "step": 1354
    },
    {
      "epoch": 0.40480991858988724,
      "grad_norm": 0.4545752704143524,
      "learning_rate": 0.00039889486260454005,
      "loss": 7.1152,
      "step": 1355
    },
    {
      "epoch": 0.40510867129733363,
      "grad_norm": 0.4737352430820465,
      "learning_rate": 0.00039882019115890086,
      "loss": 7.1484,
      "step": 1356
    },
    {
      "epoch": 0.40540742400478,
      "grad_norm": 0.4616076350212097,
      "learning_rate": 0.0003987455197132617,
      "loss": 7.1885,
      "step": 1357
    },
    {
      "epoch": 0.4057061767122265,
      "grad_norm": 0.5174769759178162,
      "learning_rate": 0.00039867084826762243,
      "loss": 7.3711,
      "step": 1358
    },
    {
      "epoch": 0.40600492941967287,
      "grad_norm": 0.9302103519439697,
      "learning_rate": 0.0003985961768219833,
      "loss": 7.5518,
      "step": 1359
    },
    {
      "epoch": 0.40630368212711926,
      "grad_norm": 0.41146960854530334,
      "learning_rate": 0.00039852150537634406,
      "loss": 7.0127,
      "step": 1360
    },
    {
      "epoch": 0.4066024348345657,
      "grad_norm": 0.48491713404655457,
      "learning_rate": 0.0003984468339307049,
      "loss": 6.6348,
      "step": 1361
    },
    {
      "epoch": 0.4069011875420121,
      "grad_norm": 0.4004208743572235,
      "learning_rate": 0.0003983721624850657,
      "loss": 7.2295,
      "step": 1362
    },
    {
      "epoch": 0.4071999402494585,
      "grad_norm": 0.4112577438354492,
      "learning_rate": 0.00039829749103942655,
      "loss": 7.2393,
      "step": 1363
    },
    {
      "epoch": 0.40749869295690494,
      "grad_norm": 0.44645434617996216,
      "learning_rate": 0.00039822281959378737,
      "loss": 7.0498,
      "step": 1364
    },
    {
      "epoch": 0.40779744566435133,
      "grad_norm": 0.34510186314582825,
      "learning_rate": 0.0003981481481481481,
      "loss": 7.6406,
      "step": 1365
    },
    {
      "epoch": 0.4080961983717977,
      "grad_norm": 0.3635798394680023,
      "learning_rate": 0.000398073476702509,
      "loss": 7.2959,
      "step": 1366
    },
    {
      "epoch": 0.4083949510792442,
      "grad_norm": 0.42905423045158386,
      "learning_rate": 0.00039799880525686975,
      "loss": 7.5518,
      "step": 1367
    },
    {
      "epoch": 0.40869370378669057,
      "grad_norm": 0.40726009011268616,
      "learning_rate": 0.0003979241338112306,
      "loss": 7.3691,
      "step": 1368
    },
    {
      "epoch": 0.40899245649413696,
      "grad_norm": 0.38802382349967957,
      "learning_rate": 0.0003978494623655914,
      "loss": 7.2246,
      "step": 1369
    },
    {
      "epoch": 0.4092912092015834,
      "grad_norm": 0.4276600480079651,
      "learning_rate": 0.00039777479091995224,
      "loss": 7.3037,
      "step": 1370
    },
    {
      "epoch": 0.4095899619090298,
      "grad_norm": 0.5946275591850281,
      "learning_rate": 0.000397700119474313,
      "loss": 7.0117,
      "step": 1371
    },
    {
      "epoch": 0.4098887146164762,
      "grad_norm": 0.42682212591171265,
      "learning_rate": 0.00039762544802867387,
      "loss": 7.4971,
      "step": 1372
    },
    {
      "epoch": 0.41018746732392264,
      "grad_norm": 0.5076514482498169,
      "learning_rate": 0.0003975507765830347,
      "loss": 6.5869,
      "step": 1373
    },
    {
      "epoch": 0.41048622003136903,
      "grad_norm": 0.4785946309566498,
      "learning_rate": 0.00039747610513739544,
      "loss": 6.8467,
      "step": 1374
    },
    {
      "epoch": 0.4107849727388154,
      "grad_norm": 0.45732253789901733,
      "learning_rate": 0.0003974014336917563,
      "loss": 7.2354,
      "step": 1375
    },
    {
      "epoch": 0.41108372544626187,
      "grad_norm": 0.3913795053958893,
      "learning_rate": 0.00039732676224611707,
      "loss": 7.4395,
      "step": 1376
    },
    {
      "epoch": 0.41138247815370826,
      "grad_norm": 0.4227626919746399,
      "learning_rate": 0.00039725209080047793,
      "loss": 7.4932,
      "step": 1377
    },
    {
      "epoch": 0.41168123086115466,
      "grad_norm": 0.4357970952987671,
      "learning_rate": 0.0003971774193548387,
      "loss": 6.9922,
      "step": 1378
    },
    {
      "epoch": 0.4119799835686011,
      "grad_norm": 0.48738133907318115,
      "learning_rate": 0.00039710274790919956,
      "loss": 7.2588,
      "step": 1379
    },
    {
      "epoch": 0.4122787362760475,
      "grad_norm": 0.37713372707366943,
      "learning_rate": 0.0003970280764635603,
      "loss": 7.3613,
      "step": 1380
    },
    {
      "epoch": 0.4125774889834939,
      "grad_norm": 0.34404146671295166,
      "learning_rate": 0.00039695340501792113,
      "loss": 7.5693,
      "step": 1381
    },
    {
      "epoch": 0.41287624169094034,
      "grad_norm": 0.372784823179245,
      "learning_rate": 0.000396878733572282,
      "loss": 7.4463,
      "step": 1382
    },
    {
      "epoch": 0.41317499439838673,
      "grad_norm": 0.441914439201355,
      "learning_rate": 0.00039680406212664276,
      "loss": 7.4346,
      "step": 1383
    },
    {
      "epoch": 0.4134737471058331,
      "grad_norm": 0.5862779021263123,
      "learning_rate": 0.0003967293906810036,
      "loss": 6.3184,
      "step": 1384
    },
    {
      "epoch": 0.41377249981327957,
      "grad_norm": 0.5438957810401917,
      "learning_rate": 0.0003966547192353644,
      "loss": 6.5625,
      "step": 1385
    },
    {
      "epoch": 0.41407125252072596,
      "grad_norm": 0.4764567017555237,
      "learning_rate": 0.00039658004778972525,
      "loss": 7.0889,
      "step": 1386
    },
    {
      "epoch": 0.41437000522817236,
      "grad_norm": 0.43036162853240967,
      "learning_rate": 0.000396505376344086,
      "loss": 7.1484,
      "step": 1387
    },
    {
      "epoch": 0.4146687579356188,
      "grad_norm": 0.5068739652633667,
      "learning_rate": 0.0003964307048984469,
      "loss": 7.333,
      "step": 1388
    },
    {
      "epoch": 0.4149675106430652,
      "grad_norm": 0.42902594804763794,
      "learning_rate": 0.00039635603345280763,
      "loss": 7.0908,
      "step": 1389
    },
    {
      "epoch": 0.4152662633505116,
      "grad_norm": 0.5386912822723389,
      "learning_rate": 0.00039628136200716845,
      "loss": 6.9756,
      "step": 1390
    },
    {
      "epoch": 0.41556501605795804,
      "grad_norm": 0.4063040316104889,
      "learning_rate": 0.0003962066905615293,
      "loss": 6.9092,
      "step": 1391
    },
    {
      "epoch": 0.41586376876540443,
      "grad_norm": 0.4669172465801239,
      "learning_rate": 0.00039613201911589007,
      "loss": 6.8789,
      "step": 1392
    },
    {
      "epoch": 0.4161625214728508,
      "grad_norm": 0.45136263966560364,
      "learning_rate": 0.00039605734767025094,
      "loss": 7.2637,
      "step": 1393
    },
    {
      "epoch": 0.41646127418029727,
      "grad_norm": 0.4837387502193451,
      "learning_rate": 0.0003959826762246117,
      "loss": 7.1143,
      "step": 1394
    },
    {
      "epoch": 0.41676002688774366,
      "grad_norm": 0.5211097002029419,
      "learning_rate": 0.00039590800477897256,
      "loss": 7.1338,
      "step": 1395
    },
    {
      "epoch": 0.41705877959519005,
      "grad_norm": 0.48205679655075073,
      "learning_rate": 0.0003958333333333333,
      "loss": 7.2842,
      "step": 1396
    },
    {
      "epoch": 0.4173575323026365,
      "grad_norm": 0.4614178240299225,
      "learning_rate": 0.00039575866188769414,
      "loss": 7.251,
      "step": 1397
    },
    {
      "epoch": 0.4176562850100829,
      "grad_norm": 0.5243740677833557,
      "learning_rate": 0.00039568399044205495,
      "loss": 6.6172,
      "step": 1398
    },
    {
      "epoch": 0.4179550377175293,
      "grad_norm": 0.5515962243080139,
      "learning_rate": 0.00039560931899641576,
      "loss": 7.1748,
      "step": 1399
    },
    {
      "epoch": 0.41825379042497574,
      "grad_norm": 0.3449063301086426,
      "learning_rate": 0.00039553464755077663,
      "loss": 7.7109,
      "step": 1400
    },
    {
      "epoch": 0.41825379042497574,
      "eval_bleu": 0.10192604878975882,
      "eval_loss": 7.0625,
      "eval_runtime": 580.6345,
      "eval_samples_per_second": 2.427,
      "eval_steps_per_second": 0.153,
      "step": 1400
    },
    {
      "epoch": 0.41855254313242213,
      "grad_norm": 0.29618170857429504,
      "learning_rate": 0.0003954599761051374,
      "loss": 7.9424,
      "step": 1401
    },
    {
      "epoch": 0.4188512958398686,
      "grad_norm": 0.47894224524497986,
      "learning_rate": 0.00039538530465949825,
      "loss": 7.1123,
      "step": 1402
    },
    {
      "epoch": 0.41915004854731497,
      "grad_norm": 0.5301428437232971,
      "learning_rate": 0.000395310633213859,
      "loss": 6.8223,
      "step": 1403
    },
    {
      "epoch": 0.41944880125476136,
      "grad_norm": 0.4700724482536316,
      "learning_rate": 0.0003952359617682199,
      "loss": 6.958,
      "step": 1404
    },
    {
      "epoch": 0.4197475539622078,
      "grad_norm": 0.38836222887039185,
      "learning_rate": 0.00039516129032258064,
      "loss": 7.3467,
      "step": 1405
    },
    {
      "epoch": 0.4200463066696542,
      "grad_norm": 0.484891414642334,
      "learning_rate": 0.00039508661887694145,
      "loss": 6.9355,
      "step": 1406
    },
    {
      "epoch": 0.4203450593771006,
      "grad_norm": 0.44520285725593567,
      "learning_rate": 0.00039501194743130227,
      "loss": 6.8867,
      "step": 1407
    },
    {
      "epoch": 0.42064381208454704,
      "grad_norm": 0.47022169828414917,
      "learning_rate": 0.0003949372759856631,
      "loss": 7.3936,
      "step": 1408
    },
    {
      "epoch": 0.42094256479199343,
      "grad_norm": 0.43711212277412415,
      "learning_rate": 0.00039486260454002395,
      "loss": 7.0361,
      "step": 1409
    },
    {
      "epoch": 0.4212413174994398,
      "grad_norm": 0.4393630623817444,
      "learning_rate": 0.0003947879330943847,
      "loss": 7.415,
      "step": 1410
    },
    {
      "epoch": 0.4215400702068863,
      "grad_norm": 0.4215008616447449,
      "learning_rate": 0.00039471326164874557,
      "loss": 7.2227,
      "step": 1411
    },
    {
      "epoch": 0.42183882291433267,
      "grad_norm": 0.4265718162059784,
      "learning_rate": 0.00039463859020310633,
      "loss": 7.1836,
      "step": 1412
    },
    {
      "epoch": 0.42213757562177906,
      "grad_norm": 0.3926502466201782,
      "learning_rate": 0.00039456391875746714,
      "loss": 7.3721,
      "step": 1413
    },
    {
      "epoch": 0.4224363283292255,
      "grad_norm": 0.4213932454586029,
      "learning_rate": 0.00039448924731182796,
      "loss": 7.1289,
      "step": 1414
    },
    {
      "epoch": 0.4227350810366719,
      "grad_norm": 0.4301188588142395,
      "learning_rate": 0.00039441457586618877,
      "loss": 7.4014,
      "step": 1415
    },
    {
      "epoch": 0.4230338337441183,
      "grad_norm": 0.371794730424881,
      "learning_rate": 0.0003943399044205496,
      "loss": 7.2754,
      "step": 1416
    },
    {
      "epoch": 0.42333258645156474,
      "grad_norm": 0.6214315891265869,
      "learning_rate": 0.0003942652329749104,
      "loss": 6.8594,
      "step": 1417
    },
    {
      "epoch": 0.42363133915901113,
      "grad_norm": 0.44963952898979187,
      "learning_rate": 0.00039419056152927126,
      "loss": 7.084,
      "step": 1418
    },
    {
      "epoch": 0.4239300918664575,
      "grad_norm": 0.4310494661331177,
      "learning_rate": 0.000394115890083632,
      "loss": 6.8223,
      "step": 1419
    },
    {
      "epoch": 0.424228844573904,
      "grad_norm": 0.47870320081710815,
      "learning_rate": 0.0003940412186379929,
      "loss": 6.9307,
      "step": 1420
    },
    {
      "epoch": 0.42452759728135037,
      "grad_norm": 0.4860629439353943,
      "learning_rate": 0.00039396654719235365,
      "loss": 7.1289,
      "step": 1421
    },
    {
      "epoch": 0.42482634998879676,
      "grad_norm": 0.5410910248756409,
      "learning_rate": 0.00039389187574671446,
      "loss": 6.7949,
      "step": 1422
    },
    {
      "epoch": 0.4251251026962432,
      "grad_norm": 0.4135812222957611,
      "learning_rate": 0.00039381720430107527,
      "loss": 7.3281,
      "step": 1423
    },
    {
      "epoch": 0.4254238554036896,
      "grad_norm": 0.38667285442352295,
      "learning_rate": 0.0003937425328554361,
      "loss": 7.6709,
      "step": 1424
    },
    {
      "epoch": 0.425722608111136,
      "grad_norm": 0.4448122978210449,
      "learning_rate": 0.0003936678614097969,
      "loss": 7.459,
      "step": 1425
    },
    {
      "epoch": 0.42602136081858244,
      "grad_norm": 0.4631573557853699,
      "learning_rate": 0.0003935931899641577,
      "loss": 7.1768,
      "step": 1426
    },
    {
      "epoch": 0.42632011352602883,
      "grad_norm": 0.41764986515045166,
      "learning_rate": 0.0003935185185185186,
      "loss": 7.3408,
      "step": 1427
    },
    {
      "epoch": 0.4266188662334752,
      "grad_norm": 0.4320964515209198,
      "learning_rate": 0.00039344384707287934,
      "loss": 7.3252,
      "step": 1428
    },
    {
      "epoch": 0.4269176189409217,
      "grad_norm": 0.415304958820343,
      "learning_rate": 0.00039336917562724015,
      "loss": 7.5439,
      "step": 1429
    },
    {
      "epoch": 0.42721637164836807,
      "grad_norm": 0.4885118007659912,
      "learning_rate": 0.00039329450418160096,
      "loss": 6.9395,
      "step": 1430
    },
    {
      "epoch": 0.42751512435581446,
      "grad_norm": 0.4191299378871918,
      "learning_rate": 0.0003932198327359618,
      "loss": 7.084,
      "step": 1431
    },
    {
      "epoch": 0.4278138770632609,
      "grad_norm": 0.5862101912498474,
      "learning_rate": 0.0003931451612903226,
      "loss": 6.6201,
      "step": 1432
    },
    {
      "epoch": 0.4281126297707073,
      "grad_norm": 0.41920557618141174,
      "learning_rate": 0.0003930704898446834,
      "loss": 7.5469,
      "step": 1433
    },
    {
      "epoch": 0.4284113824781537,
      "grad_norm": 0.41364428400993347,
      "learning_rate": 0.0003929958183990442,
      "loss": 7.4619,
      "step": 1434
    },
    {
      "epoch": 0.42871013518560014,
      "grad_norm": 0.5411252975463867,
      "learning_rate": 0.000392921146953405,
      "loss": 6.3047,
      "step": 1435
    },
    {
      "epoch": 0.42900888789304653,
      "grad_norm": 0.4260885417461395,
      "learning_rate": 0.0003928464755077659,
      "loss": 7.0703,
      "step": 1436
    },
    {
      "epoch": 0.4293076406004929,
      "grad_norm": 0.4201105833053589,
      "learning_rate": 0.00039277180406212665,
      "loss": 7.6582,
      "step": 1437
    },
    {
      "epoch": 0.42960639330793937,
      "grad_norm": 0.4451907277107239,
      "learning_rate": 0.00039269713261648746,
      "loss": 7.0469,
      "step": 1438
    },
    {
      "epoch": 0.42990514601538576,
      "grad_norm": 0.395119845867157,
      "learning_rate": 0.0003926224611708483,
      "loss": 7.3975,
      "step": 1439
    },
    {
      "epoch": 0.43020389872283216,
      "grad_norm": 0.4948483109474182,
      "learning_rate": 0.0003925477897252091,
      "loss": 6.8301,
      "step": 1440
    },
    {
      "epoch": 0.4305026514302786,
      "grad_norm": 0.7064464688301086,
      "learning_rate": 0.0003924731182795699,
      "loss": 6.5498,
      "step": 1441
    },
    {
      "epoch": 0.430801404137725,
      "grad_norm": 0.43144655227661133,
      "learning_rate": 0.0003923984468339307,
      "loss": 7.3545,
      "step": 1442
    },
    {
      "epoch": 0.4311001568451714,
      "grad_norm": 0.34789925813674927,
      "learning_rate": 0.00039232377538829153,
      "loss": 7.875,
      "step": 1443
    },
    {
      "epoch": 0.43139890955261784,
      "grad_norm": 0.5104612112045288,
      "learning_rate": 0.00039224910394265234,
      "loss": 6.6914,
      "step": 1444
    },
    {
      "epoch": 0.43169766226006423,
      "grad_norm": 0.4577682316303253,
      "learning_rate": 0.0003921744324970131,
      "loss": 7.124,
      "step": 1445
    },
    {
      "epoch": 0.4319964149675106,
      "grad_norm": 0.5320565700531006,
      "learning_rate": 0.00039209976105137397,
      "loss": 7.0469,
      "step": 1446
    },
    {
      "epoch": 0.43229516767495707,
      "grad_norm": 0.46176111698150635,
      "learning_rate": 0.0003920250896057348,
      "loss": 7.248,
      "step": 1447
    },
    {
      "epoch": 0.43259392038240346,
      "grad_norm": 0.4152930676937103,
      "learning_rate": 0.0003919504181600956,
      "loss": 7.4053,
      "step": 1448
    },
    {
      "epoch": 0.43289267308984986,
      "grad_norm": 0.4140941798686981,
      "learning_rate": 0.0003918757467144564,
      "loss": 7.0186,
      "step": 1449
    },
    {
      "epoch": 0.4331914257972963,
      "grad_norm": 0.41071006655693054,
      "learning_rate": 0.0003918010752688172,
      "loss": 7.0977,
      "step": 1450
    },
    {
      "epoch": 0.4334901785047427,
      "grad_norm": 0.48309236764907837,
      "learning_rate": 0.00039172640382317803,
      "loss": 7.3096,
      "step": 1451
    },
    {
      "epoch": 0.4337889312121891,
      "grad_norm": 0.5486384034156799,
      "learning_rate": 0.0003916517323775388,
      "loss": 6.6719,
      "step": 1452
    },
    {
      "epoch": 0.43408768391963554,
      "grad_norm": 0.36855727434158325,
      "learning_rate": 0.00039157706093189966,
      "loss": 7.8008,
      "step": 1453
    },
    {
      "epoch": 0.43438643662708193,
      "grad_norm": 0.39983585476875305,
      "learning_rate": 0.0003915023894862604,
      "loss": 7.6367,
      "step": 1454
    },
    {
      "epoch": 0.4346851893345283,
      "grad_norm": 0.4070429801940918,
      "learning_rate": 0.0003914277180406213,
      "loss": 6.916,
      "step": 1455
    },
    {
      "epoch": 0.43498394204197477,
      "grad_norm": 0.4203869700431824,
      "learning_rate": 0.0003913530465949821,
      "loss": 7.1865,
      "step": 1456
    },
    {
      "epoch": 0.43528269474942116,
      "grad_norm": 0.43473830819129944,
      "learning_rate": 0.0003912783751493429,
      "loss": 7.2969,
      "step": 1457
    },
    {
      "epoch": 0.43558144745686755,
      "grad_norm": 0.4258636236190796,
      "learning_rate": 0.0003912037037037037,
      "loss": 6.9053,
      "step": 1458
    },
    {
      "epoch": 0.435880200164314,
      "grad_norm": 0.356250524520874,
      "learning_rate": 0.00039112903225806453,
      "loss": 7.9023,
      "step": 1459
    },
    {
      "epoch": 0.4361789528717604,
      "grad_norm": 0.38025471568107605,
      "learning_rate": 0.00039105436081242535,
      "loss": 7.5059,
      "step": 1460
    },
    {
      "epoch": 0.4364777055792068,
      "grad_norm": 0.45830583572387695,
      "learning_rate": 0.0003909796893667861,
      "loss": 7.0801,
      "step": 1461
    },
    {
      "epoch": 0.43677645828665324,
      "grad_norm": 0.4040442407131195,
      "learning_rate": 0.000390905017921147,
      "loss": 7.582,
      "step": 1462
    },
    {
      "epoch": 0.4370752109940996,
      "grad_norm": 0.417721152305603,
      "learning_rate": 0.00039083034647550773,
      "loss": 7.2656,
      "step": 1463
    },
    {
      "epoch": 0.437373963701546,
      "grad_norm": 0.392659068107605,
      "learning_rate": 0.0003907556750298686,
      "loss": 7.0557,
      "step": 1464
    },
    {
      "epoch": 0.43767271640899247,
      "grad_norm": 0.4259890019893646,
      "learning_rate": 0.0003906810035842294,
      "loss": 6.9326,
      "step": 1465
    },
    {
      "epoch": 0.43797146911643886,
      "grad_norm": 0.4372883439064026,
      "learning_rate": 0.0003906063321385902,
      "loss": 7.4209,
      "step": 1466
    },
    {
      "epoch": 0.43827022182388525,
      "grad_norm": 0.42887693643569946,
      "learning_rate": 0.00039053166069295104,
      "loss": 7.2559,
      "step": 1467
    },
    {
      "epoch": 0.4385689745313317,
      "grad_norm": 0.5071214437484741,
      "learning_rate": 0.0003904569892473118,
      "loss": 6.96,
      "step": 1468
    },
    {
      "epoch": 0.4388677272387781,
      "grad_norm": 0.459905207157135,
      "learning_rate": 0.00039038231780167266,
      "loss": 7.1504,
      "step": 1469
    },
    {
      "epoch": 0.4391664799462245,
      "grad_norm": 0.3931758403778076,
      "learning_rate": 0.0003903076463560334,
      "loss": 7.4209,
      "step": 1470
    },
    {
      "epoch": 0.43946523265367093,
      "grad_norm": 0.4178850054740906,
      "learning_rate": 0.0003902329749103943,
      "loss": 7.335,
      "step": 1471
    },
    {
      "epoch": 0.4397639853611173,
      "grad_norm": 0.41972818970680237,
      "learning_rate": 0.00039015830346475505,
      "loss": 7.4014,
      "step": 1472
    },
    {
      "epoch": 0.4400627380685637,
      "grad_norm": 0.47802576422691345,
      "learning_rate": 0.0003900836320191159,
      "loss": 6.6436,
      "step": 1473
    },
    {
      "epoch": 0.44036149077601017,
      "grad_norm": 0.5412635207176208,
      "learning_rate": 0.00039000896057347673,
      "loss": 7.0889,
      "step": 1474
    },
    {
      "epoch": 0.44066024348345656,
      "grad_norm": 0.42846620082855225,
      "learning_rate": 0.00038993428912783754,
      "loss": 7.499,
      "step": 1475
    },
    {
      "epoch": 0.440958996190903,
      "grad_norm": 0.5721824765205383,
      "learning_rate": 0.00038985961768219835,
      "loss": 6.418,
      "step": 1476
    },
    {
      "epoch": 0.4412577488983494,
      "grad_norm": 0.4607917368412018,
      "learning_rate": 0.0003897849462365591,
      "loss": 7.1562,
      "step": 1477
    },
    {
      "epoch": 0.4415565016057958,
      "grad_norm": 0.4025934338569641,
      "learning_rate": 0.00038971027479092,
      "loss": 7.0049,
      "step": 1478
    },
    {
      "epoch": 0.44185525431324224,
      "grad_norm": 0.4372842013835907,
      "learning_rate": 0.00038963560334528074,
      "loss": 7.126,
      "step": 1479
    },
    {
      "epoch": 0.44215400702068863,
      "grad_norm": 0.3944029211997986,
      "learning_rate": 0.0003895609318996416,
      "loss": 7.0342,
      "step": 1480
    },
    {
      "epoch": 0.442452759728135,
      "grad_norm": 0.4079132676124573,
      "learning_rate": 0.00038948626045400236,
      "loss": 7.2969,
      "step": 1481
    },
    {
      "epoch": 0.4427515124355815,
      "grad_norm": 0.4577992856502533,
      "learning_rate": 0.00038941158900836323,
      "loss": 6.9082,
      "step": 1482
    },
    {
      "epoch": 0.44305026514302787,
      "grad_norm": 0.4519190788269043,
      "learning_rate": 0.00038933691756272404,
      "loss": 7.2207,
      "step": 1483
    },
    {
      "epoch": 0.44334901785047426,
      "grad_norm": 0.44804441928863525,
      "learning_rate": 0.0003892622461170848,
      "loss": 7.1738,
      "step": 1484
    },
    {
      "epoch": 0.4436477705579207,
      "grad_norm": 0.39885643124580383,
      "learning_rate": 0.00038918757467144567,
      "loss": 7.4561,
      "step": 1485
    },
    {
      "epoch": 0.4439465232653671,
      "grad_norm": 0.38609951734542847,
      "learning_rate": 0.00038911290322580643,
      "loss": 7.4102,
      "step": 1486
    },
    {
      "epoch": 0.4442452759728135,
      "grad_norm": 0.4150185286998749,
      "learning_rate": 0.0003890382317801673,
      "loss": 7.4277,
      "step": 1487
    },
    {
      "epoch": 0.44454402868025994,
      "grad_norm": 0.4368774890899658,
      "learning_rate": 0.00038896356033452805,
      "loss": 7.1982,
      "step": 1488
    },
    {
      "epoch": 0.44484278138770633,
      "grad_norm": 0.4504484236240387,
      "learning_rate": 0.0003888888888888889,
      "loss": 7.0254,
      "step": 1489
    },
    {
      "epoch": 0.4451415340951527,
      "grad_norm": 0.40636709332466125,
      "learning_rate": 0.0003888142174432497,
      "loss": 7.5166,
      "step": 1490
    },
    {
      "epoch": 0.4454402868025992,
      "grad_norm": 0.4728342890739441,
      "learning_rate": 0.00038873954599761055,
      "loss": 6.9961,
      "step": 1491
    },
    {
      "epoch": 0.44573903951004556,
      "grad_norm": 0.3699270486831665,
      "learning_rate": 0.00038866487455197136,
      "loss": 7.2422,
      "step": 1492
    },
    {
      "epoch": 0.44603779221749196,
      "grad_norm": 0.46431079506874084,
      "learning_rate": 0.0003885902031063321,
      "loss": 7.25,
      "step": 1493
    },
    {
      "epoch": 0.4463365449249384,
      "grad_norm": 0.46854496002197266,
      "learning_rate": 0.000388515531660693,
      "loss": 7.2285,
      "step": 1494
    },
    {
      "epoch": 0.4466352976323848,
      "grad_norm": 0.44297081232070923,
      "learning_rate": 0.00038844086021505374,
      "loss": 7.4873,
      "step": 1495
    },
    {
      "epoch": 0.4469340503398312,
      "grad_norm": 0.4125458300113678,
      "learning_rate": 0.0003883661887694146,
      "loss": 7.3418,
      "step": 1496
    },
    {
      "epoch": 0.44723280304727764,
      "grad_norm": 0.5128099918365479,
      "learning_rate": 0.00038829151732377537,
      "loss": 7.6797,
      "step": 1497
    },
    {
      "epoch": 0.44753155575472403,
      "grad_norm": 0.43388837575912476,
      "learning_rate": 0.00038821684587813624,
      "loss": 7.3955,
      "step": 1498
    },
    {
      "epoch": 0.4478303084621704,
      "grad_norm": 0.4634524881839752,
      "learning_rate": 0.000388142174432497,
      "loss": 6.6748,
      "step": 1499
    },
    {
      "epoch": 0.44812906116961687,
      "grad_norm": 0.39982128143310547,
      "learning_rate": 0.0003880675029868578,
      "loss": 7.7998,
      "step": 1500
    },
    {
      "epoch": 0.44842781387706326,
      "grad_norm": 0.5062081217765808,
      "learning_rate": 0.0003879928315412187,
      "loss": 6.8584,
      "step": 1501
    },
    {
      "epoch": 0.44872656658450966,
      "grad_norm": 0.47585707902908325,
      "learning_rate": 0.00038791816009557943,
      "loss": 7.0098,
      "step": 1502
    },
    {
      "epoch": 0.4490253192919561,
      "grad_norm": 1.420627236366272,
      "learning_rate": 0.0003878434886499403,
      "loss": 7.6436,
      "step": 1503
    },
    {
      "epoch": 0.4493240719994025,
      "grad_norm": 0.47976231575012207,
      "learning_rate": 0.00038776881720430106,
      "loss": 6.8135,
      "step": 1504
    },
    {
      "epoch": 0.4496228247068489,
      "grad_norm": 0.8474008440971375,
      "learning_rate": 0.00038769414575866193,
      "loss": 7.3418,
      "step": 1505
    },
    {
      "epoch": 0.44992157741429534,
      "grad_norm": 0.6583009958267212,
      "learning_rate": 0.0003876194743130227,
      "loss": 7.5088,
      "step": 1506
    },
    {
      "epoch": 0.45022033012174173,
      "grad_norm": 0.42941582202911377,
      "learning_rate": 0.00038754480286738355,
      "loss": 7.3857,
      "step": 1507
    },
    {
      "epoch": 0.4505190828291881,
      "grad_norm": 0.47223004698753357,
      "learning_rate": 0.0003874701314217443,
      "loss": 7.0225,
      "step": 1508
    },
    {
      "epoch": 0.45081783553663457,
      "grad_norm": 0.4547000527381897,
      "learning_rate": 0.0003873954599761051,
      "loss": 7.5049,
      "step": 1509
    },
    {
      "epoch": 0.45111658824408096,
      "grad_norm": 0.3728136420249939,
      "learning_rate": 0.000387320788530466,
      "loss": 7.5859,
      "step": 1510
    },
    {
      "epoch": 0.45141534095152736,
      "grad_norm": 0.5134193897247314,
      "learning_rate": 0.00038724611708482675,
      "loss": 7.3125,
      "step": 1511
    },
    {
      "epoch": 0.4517140936589738,
      "grad_norm": 0.4077126681804657,
      "learning_rate": 0.0003871714456391876,
      "loss": 6.9385,
      "step": 1512
    },
    {
      "epoch": 0.4520128463664202,
      "grad_norm": 0.44684311747550964,
      "learning_rate": 0.0003870967741935484,
      "loss": 6.9766,
      "step": 1513
    },
    {
      "epoch": 0.4523115990738666,
      "grad_norm": 0.36017173528671265,
      "learning_rate": 0.00038702210274790924,
      "loss": 7.4189,
      "step": 1514
    },
    {
      "epoch": 0.45261035178131304,
      "grad_norm": 0.4248254597187042,
      "learning_rate": 0.00038694743130227,
      "loss": 7.626,
      "step": 1515
    },
    {
      "epoch": 0.45290910448875943,
      "grad_norm": 0.48187077045440674,
      "learning_rate": 0.0003868727598566308,
      "loss": 7.1973,
      "step": 1516
    },
    {
      "epoch": 0.4532078571962058,
      "grad_norm": 2.935636281967163,
      "learning_rate": 0.00038679808841099163,
      "loss": 7.5361,
      "step": 1517
    },
    {
      "epoch": 0.45350660990365227,
      "grad_norm": 0.43481627106666565,
      "learning_rate": 0.00038672341696535244,
      "loss": 7.3496,
      "step": 1518
    },
    {
      "epoch": 0.45380536261109866,
      "grad_norm": 0.44335079193115234,
      "learning_rate": 0.0003866487455197133,
      "loss": 7.1279,
      "step": 1519
    },
    {
      "epoch": 0.45410411531854505,
      "grad_norm": 0.5495507717132568,
      "learning_rate": 0.00038657407407407407,
      "loss": 6.8936,
      "step": 1520
    },
    {
      "epoch": 0.4544028680259915,
      "grad_norm": 0.47012919187545776,
      "learning_rate": 0.00038649940262843493,
      "loss": 6.9209,
      "step": 1521
    },
    {
      "epoch": 0.4547016207334379,
      "grad_norm": 0.47491782903671265,
      "learning_rate": 0.0003864247311827957,
      "loss": 6.8428,
      "step": 1522
    },
    {
      "epoch": 0.4550003734408843,
      "grad_norm": 0.5752939581871033,
      "learning_rate": 0.00038635005973715656,
      "loss": 6.9219,
      "step": 1523
    },
    {
      "epoch": 0.45529912614833074,
      "grad_norm": 0.41006824374198914,
      "learning_rate": 0.0003862753882915173,
      "loss": 7.3184,
      "step": 1524
    },
    {
      "epoch": 0.4555978788557771,
      "grad_norm": 0.5327703356742859,
      "learning_rate": 0.00038620071684587813,
      "loss": 7.0781,
      "step": 1525
    },
    {
      "epoch": 0.4558966315632235,
      "grad_norm": 0.4031495153903961,
      "learning_rate": 0.00038612604540023894,
      "loss": 7.3115,
      "step": 1526
    },
    {
      "epoch": 0.45619538427066997,
      "grad_norm": 0.4971637427806854,
      "learning_rate": 0.00038605137395459976,
      "loss": 7.3564,
      "step": 1527
    },
    {
      "epoch": 0.45649413697811636,
      "grad_norm": 0.4081481397151947,
      "learning_rate": 0.0003859767025089606,
      "loss": 7.2217,
      "step": 1528
    },
    {
      "epoch": 0.45679288968556275,
      "grad_norm": 0.4533267915248871,
      "learning_rate": 0.0003859020310633214,
      "loss": 6.9922,
      "step": 1529
    },
    {
      "epoch": 0.4570916423930092,
      "grad_norm": 0.40585553646087646,
      "learning_rate": 0.00038582735961768225,
      "loss": 7.0586,
      "step": 1530
    },
    {
      "epoch": 0.4573903951004556,
      "grad_norm": 0.4111849069595337,
      "learning_rate": 0.000385752688172043,
      "loss": 7.4912,
      "step": 1531
    },
    {
      "epoch": 0.457689147807902,
      "grad_norm": 0.4559098482131958,
      "learning_rate": 0.0003856780167264038,
      "loss": 7.0,
      "step": 1532
    },
    {
      "epoch": 0.45798790051534843,
      "grad_norm": 0.46705034375190735,
      "learning_rate": 0.00038560334528076463,
      "loss": 6.8965,
      "step": 1533
    },
    {
      "epoch": 0.4582866532227948,
      "grad_norm": 0.375968337059021,
      "learning_rate": 0.00038552867383512545,
      "loss": 7.3066,
      "step": 1534
    },
    {
      "epoch": 0.4585854059302412,
      "grad_norm": 0.4177662432193756,
      "learning_rate": 0.00038545400238948626,
      "loss": 7.1875,
      "step": 1535
    },
    {
      "epoch": 0.45888415863768767,
      "grad_norm": 0.4532710015773773,
      "learning_rate": 0.00038537933094384707,
      "loss": 7.3604,
      "step": 1536
    },
    {
      "epoch": 0.45918291134513406,
      "grad_norm": 0.4226815104484558,
      "learning_rate": 0.0003853046594982079,
      "loss": 7.3857,
      "step": 1537
    },
    {
      "epoch": 0.45948166405258045,
      "grad_norm": 0.4782011806964874,
      "learning_rate": 0.0003852299880525687,
      "loss": 7.2686,
      "step": 1538
    },
    {
      "epoch": 0.4597804167600269,
      "grad_norm": 0.5404102802276611,
      "learning_rate": 0.00038515531660692956,
      "loss": 6.6885,
      "step": 1539
    },
    {
      "epoch": 0.4600791694674733,
      "grad_norm": 0.48159152269363403,
      "learning_rate": 0.0003850806451612903,
      "loss": 7.1201,
      "step": 1540
    },
    {
      "epoch": 0.4603779221749197,
      "grad_norm": 0.49900856614112854,
      "learning_rate": 0.00038500597371565114,
      "loss": 7.0068,
      "step": 1541
    },
    {
      "epoch": 0.46067667488236613,
      "grad_norm": 0.41682982444763184,
      "learning_rate": 0.00038493130227001195,
      "loss": 7.2432,
      "step": 1542
    },
    {
      "epoch": 0.4609754275898125,
      "grad_norm": 0.3852193355560303,
      "learning_rate": 0.00038485663082437276,
      "loss": 7.3633,
      "step": 1543
    },
    {
      "epoch": 0.4612741802972589,
      "grad_norm": 0.45564955472946167,
      "learning_rate": 0.0003847819593787336,
      "loss": 7.3809,
      "step": 1544
    },
    {
      "epoch": 0.46157293300470537,
      "grad_norm": 0.4794396460056305,
      "learning_rate": 0.0003847072879330944,
      "loss": 7.0928,
      "step": 1545
    },
    {
      "epoch": 0.46187168571215176,
      "grad_norm": 0.4758355915546417,
      "learning_rate": 0.0003846326164874552,
      "loss": 7.166,
      "step": 1546
    },
    {
      "epoch": 0.46217043841959815,
      "grad_norm": 0.4977599084377289,
      "learning_rate": 0.000384557945041816,
      "loss": 7.3652,
      "step": 1547
    },
    {
      "epoch": 0.4624691911270446,
      "grad_norm": 0.4405752420425415,
      "learning_rate": 0.0003844832735961768,
      "loss": 7.1064,
      "step": 1548
    },
    {
      "epoch": 0.462767943834491,
      "grad_norm": 0.41783133149147034,
      "learning_rate": 0.00038440860215053764,
      "loss": 7.1553,
      "step": 1549
    },
    {
      "epoch": 0.4630666965419374,
      "grad_norm": 0.4130675494670868,
      "learning_rate": 0.00038433393070489845,
      "loss": 7.4248,
      "step": 1550
    },
    {
      "epoch": 0.46336544924938383,
      "grad_norm": 0.33401474356651306,
      "learning_rate": 0.00038425925925925927,
      "loss": 7.8047,
      "step": 1551
    },
    {
      "epoch": 0.4636642019568302,
      "grad_norm": 0.4267752170562744,
      "learning_rate": 0.0003841845878136201,
      "loss": 7.1846,
      "step": 1552
    },
    {
      "epoch": 0.46396295466427667,
      "grad_norm": 0.37683624029159546,
      "learning_rate": 0.0003841099163679809,
      "loss": 7.6699,
      "step": 1553
    },
    {
      "epoch": 0.46426170737172306,
      "grad_norm": 0.49098363518714905,
      "learning_rate": 0.0003840352449223417,
      "loss": 6.9229,
      "step": 1554
    },
    {
      "epoch": 0.46456046007916946,
      "grad_norm": 1.7839118242263794,
      "learning_rate": 0.0003839605734767025,
      "loss": 7.085,
      "step": 1555
    },
    {
      "epoch": 0.4648592127866159,
      "grad_norm": 0.43354126811027527,
      "learning_rate": 0.00038388590203106333,
      "loss": 7.1406,
      "step": 1556
    },
    {
      "epoch": 0.4651579654940623,
      "grad_norm": 0.5115739703178406,
      "learning_rate": 0.00038381123058542414,
      "loss": 6.667,
      "step": 1557
    },
    {
      "epoch": 0.4654567182015087,
      "grad_norm": 0.43596702814102173,
      "learning_rate": 0.00038373655913978496,
      "loss": 6.9033,
      "step": 1558
    },
    {
      "epoch": 0.46575547090895514,
      "grad_norm": 0.4076865017414093,
      "learning_rate": 0.00038366188769414577,
      "loss": 7.0498,
      "step": 1559
    },
    {
      "epoch": 0.46605422361640153,
      "grad_norm": 0.44397029280662537,
      "learning_rate": 0.0003835872162485066,
      "loss": 7.2275,
      "step": 1560
    },
    {
      "epoch": 0.4663529763238479,
      "grad_norm": 0.4232134521007538,
      "learning_rate": 0.0003835125448028674,
      "loss": 7.1035,
      "step": 1561
    },
    {
      "epoch": 0.46665172903129437,
      "grad_norm": 0.3880419135093689,
      "learning_rate": 0.0003834378733572282,
      "loss": 7.3623,
      "step": 1562
    },
    {
      "epoch": 0.46695048173874076,
      "grad_norm": 0.4763181209564209,
      "learning_rate": 0.000383363201911589,
      "loss": 7.0146,
      "step": 1563
    },
    {
      "epoch": 0.46724923444618716,
      "grad_norm": 0.4966956377029419,
      "learning_rate": 0.0003832885304659498,
      "loss": 7.1055,
      "step": 1564
    },
    {
      "epoch": 0.4675479871536336,
      "grad_norm": 0.4368298649787903,
      "learning_rate": 0.00038321385902031065,
      "loss": 6.9629,
      "step": 1565
    },
    {
      "epoch": 0.46784673986108,
      "grad_norm": 0.402526319026947,
      "learning_rate": 0.00038313918757467146,
      "loss": 7.4209,
      "step": 1566
    },
    {
      "epoch": 0.4681454925685264,
      "grad_norm": 0.5572559833526611,
      "learning_rate": 0.00038306451612903227,
      "loss": 7.1885,
      "step": 1567
    },
    {
      "epoch": 0.46844424527597284,
      "grad_norm": 0.4194570481777191,
      "learning_rate": 0.0003829898446833931,
      "loss": 7.4424,
      "step": 1568
    },
    {
      "epoch": 0.46874299798341923,
      "grad_norm": 0.45749661326408386,
      "learning_rate": 0.0003829151732377539,
      "loss": 7.0029,
      "step": 1569
    },
    {
      "epoch": 0.4690417506908656,
      "grad_norm": 0.4213704764842987,
      "learning_rate": 0.0003828405017921147,
      "loss": 7.9043,
      "step": 1570
    },
    {
      "epoch": 0.46934050339831207,
      "grad_norm": 0.5370138883590698,
      "learning_rate": 0.0003827658303464755,
      "loss": 6.4551,
      "step": 1571
    },
    {
      "epoch": 0.46963925610575846,
      "grad_norm": 0.4237613081932068,
      "learning_rate": 0.00038269115890083634,
      "loss": 7.1172,
      "step": 1572
    },
    {
      "epoch": 0.46993800881320485,
      "grad_norm": 0.4011322557926178,
      "learning_rate": 0.0003826164874551971,
      "loss": 7.5107,
      "step": 1573
    },
    {
      "epoch": 0.4702367615206513,
      "grad_norm": 0.40216371417045593,
      "learning_rate": 0.00038254181600955796,
      "loss": 7.4482,
      "step": 1574
    },
    {
      "epoch": 0.4705355142280977,
      "grad_norm": 0.4055023789405823,
      "learning_rate": 0.0003824671445639188,
      "loss": 7.4473,
      "step": 1575
    },
    {
      "epoch": 0.4708342669355441,
      "grad_norm": 0.4075641632080078,
      "learning_rate": 0.0003823924731182796,
      "loss": 7.0342,
      "step": 1576
    },
    {
      "epoch": 0.47113301964299054,
      "grad_norm": 0.4685882031917572,
      "learning_rate": 0.0003823178016726404,
      "loss": 6.8643,
      "step": 1577
    },
    {
      "epoch": 0.47143177235043693,
      "grad_norm": 0.3861036002635956,
      "learning_rate": 0.0003822431302270012,
      "loss": 7.7178,
      "step": 1578
    },
    {
      "epoch": 0.4717305250578833,
      "grad_norm": 0.4381156265735626,
      "learning_rate": 0.000382168458781362,
      "loss": 7.1016,
      "step": 1579
    },
    {
      "epoch": 0.47202927776532977,
      "grad_norm": 0.4354434013366699,
      "learning_rate": 0.0003820937873357228,
      "loss": 7.3584,
      "step": 1580
    },
    {
      "epoch": 0.47232803047277616,
      "grad_norm": 0.5318011045455933,
      "learning_rate": 0.00038201911589008365,
      "loss": 7.248,
      "step": 1581
    },
    {
      "epoch": 0.47262678318022255,
      "grad_norm": 0.45766961574554443,
      "learning_rate": 0.0003819444444444444,
      "loss": 6.6904,
      "step": 1582
    },
    {
      "epoch": 0.472925535887669,
      "grad_norm": 0.4763125479221344,
      "learning_rate": 0.0003818697729988053,
      "loss": 7.0547,
      "step": 1583
    },
    {
      "epoch": 0.4732242885951154,
      "grad_norm": 0.5579553842544556,
      "learning_rate": 0.0003817951015531661,
      "loss": 6.7578,
      "step": 1584
    },
    {
      "epoch": 0.4735230413025618,
      "grad_norm": 0.3823830187320709,
      "learning_rate": 0.0003817204301075269,
      "loss": 7.1504,
      "step": 1585
    },
    {
      "epoch": 0.47382179401000823,
      "grad_norm": 0.4992998540401459,
      "learning_rate": 0.0003816457586618877,
      "loss": 7.1543,
      "step": 1586
    },
    {
      "epoch": 0.4741205467174546,
      "grad_norm": 0.3813839256763458,
      "learning_rate": 0.00038157108721624853,
      "loss": 7.6768,
      "step": 1587
    },
    {
      "epoch": 0.474419299424901,
      "grad_norm": 0.48052138090133667,
      "learning_rate": 0.00038149641577060934,
      "loss": 7.5488,
      "step": 1588
    },
    {
      "epoch": 0.47471805213234747,
      "grad_norm": 0.4030061960220337,
      "learning_rate": 0.0003814217443249701,
      "loss": 7.3096,
      "step": 1589
    },
    {
      "epoch": 0.47501680483979386,
      "grad_norm": 0.45032036304473877,
      "learning_rate": 0.00038134707287933097,
      "loss": 7.4072,
      "step": 1590
    },
    {
      "epoch": 0.47531555754724025,
      "grad_norm": 2.397012948989868,
      "learning_rate": 0.0003812724014336917,
      "loss": 7.3496,
      "step": 1591
    },
    {
      "epoch": 0.4756143102546867,
      "grad_norm": 0.4579375386238098,
      "learning_rate": 0.0003811977299880526,
      "loss": 7.6504,
      "step": 1592
    },
    {
      "epoch": 0.4759130629621331,
      "grad_norm": 0.503897488117218,
      "learning_rate": 0.0003811230585424134,
      "loss": 6.96,
      "step": 1593
    },
    {
      "epoch": 0.4762118156695795,
      "grad_norm": 0.3505367040634155,
      "learning_rate": 0.0003810483870967742,
      "loss": 7.8223,
      "step": 1594
    },
    {
      "epoch": 0.47651056837702593,
      "grad_norm": 0.5027017593383789,
      "learning_rate": 0.00038097371565113503,
      "loss": 7.3535,
      "step": 1595
    },
    {
      "epoch": 0.4768093210844723,
      "grad_norm": 0.38443219661712646,
      "learning_rate": 0.0003808990442054958,
      "loss": 7.2354,
      "step": 1596
    },
    {
      "epoch": 0.4771080737919187,
      "grad_norm": 0.4328257441520691,
      "learning_rate": 0.00038082437275985666,
      "loss": 7.2881,
      "step": 1597
    },
    {
      "epoch": 0.47740682649936517,
      "grad_norm": 0.3895472288131714,
      "learning_rate": 0.0003807497013142174,
      "loss": 7.3994,
      "step": 1598
    },
    {
      "epoch": 0.47770557920681156,
      "grad_norm": 0.4991438388824463,
      "learning_rate": 0.0003806750298685783,
      "loss": 7.0303,
      "step": 1599
    },
    {
      "epoch": 0.47800433191425795,
      "grad_norm": 0.41270527243614197,
      "learning_rate": 0.00038060035842293904,
      "loss": 7.2773,
      "step": 1600
    },
    {
      "epoch": 0.47800433191425795,
      "eval_bleu": 0.1126648493790043,
      "eval_loss": 7.04296875,
      "eval_runtime": 521.5956,
      "eval_samples_per_second": 2.701,
      "eval_steps_per_second": 0.171,
      "step": 1600
    },
    {
      "epoch": 0.4783030846217044,
      "grad_norm": 0.44417113065719604,
      "learning_rate": 0.0003805256869772999,
      "loss": 7.3398,
      "step": 1601
    },
    {
      "epoch": 0.4786018373291508,
      "grad_norm": 0.42542850971221924,
      "learning_rate": 0.0003804510155316607,
      "loss": 6.9424,
      "step": 1602
    },
    {
      "epoch": 0.4789005900365972,
      "grad_norm": 0.40450915694236755,
      "learning_rate": 0.00038037634408602153,
      "loss": 7.6035,
      "step": 1603
    },
    {
      "epoch": 0.47919934274404363,
      "grad_norm": 0.44619157910346985,
      "learning_rate": 0.00038030167264038235,
      "loss": 6.8809,
      "step": 1604
    },
    {
      "epoch": 0.47949809545149,
      "grad_norm": 0.3695848286151886,
      "learning_rate": 0.0003802270011947431,
      "loss": 7.6201,
      "step": 1605
    },
    {
      "epoch": 0.4797968481589364,
      "grad_norm": 0.38081616163253784,
      "learning_rate": 0.000380152329749104,
      "loss": 7.3926,
      "step": 1606
    },
    {
      "epoch": 0.48009560086638287,
      "grad_norm": 0.42770037055015564,
      "learning_rate": 0.00038007765830346473,
      "loss": 6.9697,
      "step": 1607
    },
    {
      "epoch": 0.48039435357382926,
      "grad_norm": 0.4359980523586273,
      "learning_rate": 0.0003800029868578256,
      "loss": 7.6006,
      "step": 1608
    },
    {
      "epoch": 0.48069310628127565,
      "grad_norm": 0.4881608486175537,
      "learning_rate": 0.00037992831541218636,
      "loss": 6.9814,
      "step": 1609
    },
    {
      "epoch": 0.4809918589887221,
      "grad_norm": 0.49886202812194824,
      "learning_rate": 0.0003798536439665472,
      "loss": 7.0918,
      "step": 1610
    },
    {
      "epoch": 0.4812906116961685,
      "grad_norm": 0.3478994369506836,
      "learning_rate": 0.00037977897252090804,
      "loss": 7.7148,
      "step": 1611
    },
    {
      "epoch": 0.4815893644036149,
      "grad_norm": 0.382284939289093,
      "learning_rate": 0.0003797043010752688,
      "loss": 7.6865,
      "step": 1612
    },
    {
      "epoch": 0.48188811711106133,
      "grad_norm": 0.4275393784046173,
      "learning_rate": 0.00037962962962962966,
      "loss": 7.2969,
      "step": 1613
    },
    {
      "epoch": 0.4821868698185077,
      "grad_norm": 0.4360906779766083,
      "learning_rate": 0.0003795549581839904,
      "loss": 6.8779,
      "step": 1614
    },
    {
      "epoch": 0.4824856225259541,
      "grad_norm": 0.5956497192382812,
      "learning_rate": 0.0003794802867383513,
      "loss": 6.8447,
      "step": 1615
    },
    {
      "epoch": 0.48278437523340056,
      "grad_norm": 0.434543639421463,
      "learning_rate": 0.00037940561529271205,
      "loss": 6.9326,
      "step": 1616
    },
    {
      "epoch": 0.48308312794084696,
      "grad_norm": 0.4199983477592468,
      "learning_rate": 0.0003793309438470729,
      "loss": 7.1299,
      "step": 1617
    },
    {
      "epoch": 0.48338188064829335,
      "grad_norm": 0.3461011052131653,
      "learning_rate": 0.0003792562724014337,
      "loss": 7.4775,
      "step": 1618
    },
    {
      "epoch": 0.4836806333557398,
      "grad_norm": 0.4382019639015198,
      "learning_rate": 0.00037918160095579454,
      "loss": 7.2529,
      "step": 1619
    },
    {
      "epoch": 0.4839793860631862,
      "grad_norm": 0.40294602513313293,
      "learning_rate": 0.00037910692951015535,
      "loss": 7.3467,
      "step": 1620
    },
    {
      "epoch": 0.4842781387706326,
      "grad_norm": 0.37087351083755493,
      "learning_rate": 0.0003790322580645161,
      "loss": 7.7295,
      "step": 1621
    },
    {
      "epoch": 0.48457689147807903,
      "grad_norm": 0.3472111225128174,
      "learning_rate": 0.000378957586618877,
      "loss": 7.6436,
      "step": 1622
    },
    {
      "epoch": 0.4848756441855254,
      "grad_norm": 0.5030783414840698,
      "learning_rate": 0.00037888291517323774,
      "loss": 6.9824,
      "step": 1623
    },
    {
      "epoch": 0.4851743968929718,
      "grad_norm": 0.4157377779483795,
      "learning_rate": 0.0003788082437275986,
      "loss": 7.5322,
      "step": 1624
    },
    {
      "epoch": 0.48547314960041826,
      "grad_norm": 0.4651932120323181,
      "learning_rate": 0.00037873357228195936,
      "loss": 7.3594,
      "step": 1625
    },
    {
      "epoch": 0.48577190230786466,
      "grad_norm": 0.37564340233802795,
      "learning_rate": 0.00037865890083632023,
      "loss": 7.4814,
      "step": 1626
    },
    {
      "epoch": 0.4860706550153111,
      "grad_norm": 0.4041016697883606,
      "learning_rate": 0.000378584229390681,
      "loss": 7.7119,
      "step": 1627
    },
    {
      "epoch": 0.4863694077227575,
      "grad_norm": 0.4492062032222748,
      "learning_rate": 0.0003785095579450418,
      "loss": 7.0254,
      "step": 1628
    },
    {
      "epoch": 0.4866681604302039,
      "grad_norm": 0.504016637802124,
      "learning_rate": 0.0003784348864994026,
      "loss": 6.7109,
      "step": 1629
    },
    {
      "epoch": 0.48696691313765034,
      "grad_norm": 0.49563169479370117,
      "learning_rate": 0.00037836021505376343,
      "loss": 7.4092,
      "step": 1630
    },
    {
      "epoch": 0.48726566584509673,
      "grad_norm": 0.3838041126728058,
      "learning_rate": 0.0003782855436081243,
      "loss": 7.6045,
      "step": 1631
    },
    {
      "epoch": 0.4875644185525431,
      "grad_norm": 0.46986427903175354,
      "learning_rate": 0.00037821087216248505,
      "loss": 6.9336,
      "step": 1632
    },
    {
      "epoch": 0.48786317125998957,
      "grad_norm": 0.4426565170288086,
      "learning_rate": 0.0003781362007168459,
      "loss": 6.9629,
      "step": 1633
    },
    {
      "epoch": 0.48816192396743596,
      "grad_norm": 0.4039163589477539,
      "learning_rate": 0.0003780615292712067,
      "loss": 7.0879,
      "step": 1634
    },
    {
      "epoch": 0.48846067667488235,
      "grad_norm": 0.4283905327320099,
      "learning_rate": 0.00037798685782556755,
      "loss": 7.6494,
      "step": 1635
    },
    {
      "epoch": 0.4887594293823288,
      "grad_norm": 0.4513199031352997,
      "learning_rate": 0.0003779121863799283,
      "loss": 7.1377,
      "step": 1636
    },
    {
      "epoch": 0.4890581820897752,
      "grad_norm": 0.4653087854385376,
      "learning_rate": 0.0003778375149342891,
      "loss": 7.3145,
      "step": 1637
    },
    {
      "epoch": 0.4893569347972216,
      "grad_norm": 0.4321863055229187,
      "learning_rate": 0.00037776284348864993,
      "loss": 7.4473,
      "step": 1638
    },
    {
      "epoch": 0.48965568750466804,
      "grad_norm": 0.42243555188179016,
      "learning_rate": 0.00037768817204301074,
      "loss": 7.1533,
      "step": 1639
    },
    {
      "epoch": 0.48995444021211443,
      "grad_norm": 0.4735368490219116,
      "learning_rate": 0.0003776135005973716,
      "loss": 7.123,
      "step": 1640
    },
    {
      "epoch": 0.4902531929195608,
      "grad_norm": 0.36344707012176514,
      "learning_rate": 0.00037753882915173237,
      "loss": 7.8145,
      "step": 1641
    },
    {
      "epoch": 0.49055194562700727,
      "grad_norm": 0.5829116702079773,
      "learning_rate": 0.00037746415770609324,
      "loss": 6.6436,
      "step": 1642
    },
    {
      "epoch": 0.49085069833445366,
      "grad_norm": 0.38576146960258484,
      "learning_rate": 0.000377389486260454,
      "loss": 7.4492,
      "step": 1643
    },
    {
      "epoch": 0.49114945104190005,
      "grad_norm": 0.5396251082420349,
      "learning_rate": 0.0003773148148148148,
      "loss": 6.5732,
      "step": 1644
    },
    {
      "epoch": 0.4914482037493465,
      "grad_norm": 0.596579909324646,
      "learning_rate": 0.0003772401433691756,
      "loss": 6.457,
      "step": 1645
    },
    {
      "epoch": 0.4917469564567929,
      "grad_norm": 0.5249083638191223,
      "learning_rate": 0.00037716547192353643,
      "loss": 7.3955,
      "step": 1646
    },
    {
      "epoch": 0.4920457091642393,
      "grad_norm": 0.5927354097366333,
      "learning_rate": 0.00037709080047789725,
      "loss": 6.8379,
      "step": 1647
    },
    {
      "epoch": 0.49234446187168573,
      "grad_norm": 0.42792344093322754,
      "learning_rate": 0.00037701612903225806,
      "loss": 7.2344,
      "step": 1648
    },
    {
      "epoch": 0.4926432145791321,
      "grad_norm": 0.35381805896759033,
      "learning_rate": 0.00037694145758661893,
      "loss": 7.6709,
      "step": 1649
    },
    {
      "epoch": 0.4929419672865785,
      "grad_norm": 0.41630011796951294,
      "learning_rate": 0.0003768667861409797,
      "loss": 7.4814,
      "step": 1650
    },
    {
      "epoch": 0.49324071999402497,
      "grad_norm": 0.4449090361595154,
      "learning_rate": 0.00037679211469534055,
      "loss": 7.0479,
      "step": 1651
    },
    {
      "epoch": 0.49353947270147136,
      "grad_norm": 0.3648960292339325,
      "learning_rate": 0.0003767174432497013,
      "loss": 7.3213,
      "step": 1652
    },
    {
      "epoch": 0.49383822540891775,
      "grad_norm": 0.34373652935028076,
      "learning_rate": 0.0003766427718040621,
      "loss": 7.6846,
      "step": 1653
    },
    {
      "epoch": 0.4941369781163642,
      "grad_norm": 0.48176833987236023,
      "learning_rate": 0.00037656810035842294,
      "loss": 7.0117,
      "step": 1654
    },
    {
      "epoch": 0.4944357308238106,
      "grad_norm": 0.4425264596939087,
      "learning_rate": 0.00037649342891278375,
      "loss": 7.2314,
      "step": 1655
    },
    {
      "epoch": 0.494734483531257,
      "grad_norm": 0.42168691754341125,
      "learning_rate": 0.00037641875746714456,
      "loss": 7.1514,
      "step": 1656
    },
    {
      "epoch": 0.49503323623870343,
      "grad_norm": 0.4607039988040924,
      "learning_rate": 0.0003763440860215054,
      "loss": 7.417,
      "step": 1657
    },
    {
      "epoch": 0.4953319889461498,
      "grad_norm": 0.4009475111961365,
      "learning_rate": 0.00037626941457586624,
      "loss": 7.5859,
      "step": 1658
    },
    {
      "epoch": 0.4956307416535962,
      "grad_norm": 0.3007049858570099,
      "learning_rate": 0.000376194743130227,
      "loss": 7.7725,
      "step": 1659
    },
    {
      "epoch": 0.49592949436104267,
      "grad_norm": 0.5069277882575989,
      "learning_rate": 0.0003761200716845878,
      "loss": 6.9824,
      "step": 1660
    },
    {
      "epoch": 0.49622824706848906,
      "grad_norm": 0.410268098115921,
      "learning_rate": 0.00037604540023894863,
      "loss": 6.9209,
      "step": 1661
    },
    {
      "epoch": 0.49652699977593545,
      "grad_norm": 0.4473022222518921,
      "learning_rate": 0.00037597072879330944,
      "loss": 7.208,
      "step": 1662
    },
    {
      "epoch": 0.4968257524833819,
      "grad_norm": 0.4040369391441345,
      "learning_rate": 0.00037589605734767025,
      "loss": 7.0459,
      "step": 1663
    },
    {
      "epoch": 0.4971245051908283,
      "grad_norm": 0.4795704483985901,
      "learning_rate": 0.00037582138590203107,
      "loss": 6.7471,
      "step": 1664
    },
    {
      "epoch": 0.4974232578982747,
      "grad_norm": 0.36049559712409973,
      "learning_rate": 0.0003757467144563919,
      "loss": 7.8018,
      "step": 1665
    },
    {
      "epoch": 0.49772201060572113,
      "grad_norm": 0.44148868322372437,
      "learning_rate": 0.0003756720430107527,
      "loss": 7.0596,
      "step": 1666
    },
    {
      "epoch": 0.4980207633131675,
      "grad_norm": 0.4577416479587555,
      "learning_rate": 0.00037559737156511356,
      "loss": 7.1582,
      "step": 1667
    },
    {
      "epoch": 0.4983195160206139,
      "grad_norm": 0.3790927827358246,
      "learning_rate": 0.0003755227001194743,
      "loss": 7.2754,
      "step": 1668
    },
    {
      "epoch": 0.49861826872806037,
      "grad_norm": 0.5053640604019165,
      "learning_rate": 0.00037544802867383513,
      "loss": 6.6475,
      "step": 1669
    },
    {
      "epoch": 0.49891702143550676,
      "grad_norm": 0.4830523431301117,
      "learning_rate": 0.00037537335722819594,
      "loss": 6.7305,
      "step": 1670
    },
    {
      "epoch": 0.49921577414295315,
      "grad_norm": 0.5369777083396912,
      "learning_rate": 0.00037529868578255676,
      "loss": 7.042,
      "step": 1671
    },
    {
      "epoch": 0.4995145268503996,
      "grad_norm": 0.40661001205444336,
      "learning_rate": 0.00037522401433691757,
      "loss": 7.4658,
      "step": 1672
    },
    {
      "epoch": 0.499813279557846,
      "grad_norm": 0.5267220139503479,
      "learning_rate": 0.0003751493428912784,
      "loss": 7.6426,
      "step": 1673
    },
    {
      "epoch": 0.5001120322652924,
      "grad_norm": 0.4594343900680542,
      "learning_rate": 0.0003750746714456392,
      "loss": 6.9482,
      "step": 1674
    },
    {
      "epoch": 0.5004107849727388,
      "grad_norm": 0.4897546172142029,
      "learning_rate": 0.000375,
      "loss": 7.1338,
      "step": 1675
    },
    {
      "epoch": 0.5007095376801852,
      "grad_norm": 0.489618182182312,
      "learning_rate": 0.0003749253285543608,
      "loss": 7.2393,
      "step": 1676
    },
    {
      "epoch": 0.5010082903876316,
      "grad_norm": 0.4082314968109131,
      "learning_rate": 0.00037485065710872163,
      "loss": 7.0508,
      "step": 1677
    },
    {
      "epoch": 0.501307043095078,
      "grad_norm": 0.382503479719162,
      "learning_rate": 0.00037477598566308245,
      "loss": 6.9219,
      "step": 1678
    },
    {
      "epoch": 0.5016057958025245,
      "grad_norm": 0.3947944939136505,
      "learning_rate": 0.00037470131421744326,
      "loss": 7.4199,
      "step": 1679
    },
    {
      "epoch": 0.5019045485099709,
      "grad_norm": 0.5183405876159668,
      "learning_rate": 0.00037462664277180407,
      "loss": 7.2188,
      "step": 1680
    },
    {
      "epoch": 0.5022033012174173,
      "grad_norm": 0.43446287512779236,
      "learning_rate": 0.0003745519713261649,
      "loss": 7.5527,
      "step": 1681
    },
    {
      "epoch": 0.5025020539248637,
      "grad_norm": 0.43083640933036804,
      "learning_rate": 0.0003744772998805257,
      "loss": 7.123,
      "step": 1682
    },
    {
      "epoch": 0.5028008066323101,
      "grad_norm": 0.4055100381374359,
      "learning_rate": 0.00037440262843488646,
      "loss": 7.1318,
      "step": 1683
    },
    {
      "epoch": 0.5030995593397565,
      "grad_norm": 0.43135982751846313,
      "learning_rate": 0.0003743279569892473,
      "loss": 7.0732,
      "step": 1684
    },
    {
      "epoch": 0.503398312047203,
      "grad_norm": 0.41497135162353516,
      "learning_rate": 0.00037425328554360814,
      "loss": 6.9668,
      "step": 1685
    },
    {
      "epoch": 0.5036970647546494,
      "grad_norm": 0.5851547718048096,
      "learning_rate": 0.00037417861409796895,
      "loss": 6.7783,
      "step": 1686
    },
    {
      "epoch": 0.5039958174620958,
      "grad_norm": 0.45623525977134705,
      "learning_rate": 0.00037410394265232976,
      "loss": 7.1377,
      "step": 1687
    },
    {
      "epoch": 0.5042945701695422,
      "grad_norm": 0.4278165400028229,
      "learning_rate": 0.0003740292712066906,
      "loss": 7.6094,
      "step": 1688
    },
    {
      "epoch": 0.5045933228769885,
      "grad_norm": 0.4835047125816345,
      "learning_rate": 0.0003739545997610514,
      "loss": 6.8896,
      "step": 1689
    },
    {
      "epoch": 0.5048920755844349,
      "grad_norm": 0.4370006024837494,
      "learning_rate": 0.0003738799283154122,
      "loss": 7.4639,
      "step": 1690
    },
    {
      "epoch": 0.5051908282918814,
      "grad_norm": 0.45454421639442444,
      "learning_rate": 0.000373805256869773,
      "loss": 7.2236,
      "step": 1691
    },
    {
      "epoch": 0.5054895809993278,
      "grad_norm": 0.3482648432254791,
      "learning_rate": 0.00037373058542413377,
      "loss": 7.6953,
      "step": 1692
    },
    {
      "epoch": 0.5057883337067742,
      "grad_norm": 0.5499765872955322,
      "learning_rate": 0.00037365591397849464,
      "loss": 7.0391,
      "step": 1693
    },
    {
      "epoch": 0.5060870864142206,
      "grad_norm": 0.43015819787979126,
      "learning_rate": 0.00037358124253285545,
      "loss": 7.5205,
      "step": 1694
    },
    {
      "epoch": 0.506385839121667,
      "grad_norm": 0.45197927951812744,
      "learning_rate": 0.00037350657108721627,
      "loss": 7.0117,
      "step": 1695
    },
    {
      "epoch": 0.5066845918291134,
      "grad_norm": 0.44043800234794617,
      "learning_rate": 0.0003734318996415771,
      "loss": 7.0742,
      "step": 1696
    },
    {
      "epoch": 0.5069833445365599,
      "grad_norm": 0.6030011773109436,
      "learning_rate": 0.0003733572281959379,
      "loss": 6.9219,
      "step": 1697
    },
    {
      "epoch": 0.5072820972440063,
      "grad_norm": 0.45337316393852234,
      "learning_rate": 0.0003732825567502987,
      "loss": 7.3887,
      "step": 1698
    },
    {
      "epoch": 0.5075808499514527,
      "grad_norm": 0.5659255385398865,
      "learning_rate": 0.00037320788530465946,
      "loss": 6.8232,
      "step": 1699
    },
    {
      "epoch": 0.5078796026588991,
      "grad_norm": 0.3919174075126648,
      "learning_rate": 0.00037313321385902033,
      "loss": 7.3965,
      "step": 1700
    },
    {
      "epoch": 0.5081783553663455,
      "grad_norm": 0.39638903737068176,
      "learning_rate": 0.0003730585424133811,
      "loss": 7.5977,
      "step": 1701
    },
    {
      "epoch": 0.5084771080737919,
      "grad_norm": 0.44675129652023315,
      "learning_rate": 0.00037298387096774196,
      "loss": 7.5625,
      "step": 1702
    },
    {
      "epoch": 0.5087758607812384,
      "grad_norm": 0.535296618938446,
      "learning_rate": 0.00037290919952210277,
      "loss": 6.7041,
      "step": 1703
    },
    {
      "epoch": 0.5090746134886848,
      "grad_norm": 0.3738793134689331,
      "learning_rate": 0.0003728345280764636,
      "loss": 7.375,
      "step": 1704
    },
    {
      "epoch": 0.5093733661961312,
      "grad_norm": 0.45613130927085876,
      "learning_rate": 0.0003727598566308244,
      "loss": 7.04,
      "step": 1705
    },
    {
      "epoch": 0.5096721189035776,
      "grad_norm": 0.4040818214416504,
      "learning_rate": 0.0003726851851851852,
      "loss": 7.3037,
      "step": 1706
    },
    {
      "epoch": 0.509970871611024,
      "grad_norm": 0.4533347487449646,
      "learning_rate": 0.000372610513739546,
      "loss": 6.9883,
      "step": 1707
    },
    {
      "epoch": 0.5102696243184703,
      "grad_norm": 0.5309017896652222,
      "learning_rate": 0.0003725358422939068,
      "loss": 7.0273,
      "step": 1708
    },
    {
      "epoch": 0.5105683770259168,
      "grad_norm": 0.3588685393333435,
      "learning_rate": 0.00037246117084826765,
      "loss": 7.7793,
      "step": 1709
    },
    {
      "epoch": 0.5108671297333632,
      "grad_norm": 0.6909194588661194,
      "learning_rate": 0.0003723864994026284,
      "loss": 6.9473,
      "step": 1710
    },
    {
      "epoch": 0.5111658824408096,
      "grad_norm": 0.49202749133110046,
      "learning_rate": 0.00037231182795698927,
      "loss": 7.3574,
      "step": 1711
    },
    {
      "epoch": 0.511464635148256,
      "grad_norm": 0.5249351859092712,
      "learning_rate": 0.0003722371565113501,
      "loss": 6.4971,
      "step": 1712
    },
    {
      "epoch": 0.5117633878557024,
      "grad_norm": 0.4267426133155823,
      "learning_rate": 0.0003721624850657109,
      "loss": 6.9395,
      "step": 1713
    },
    {
      "epoch": 0.5120621405631488,
      "grad_norm": 0.43718409538269043,
      "learning_rate": 0.0003720878136200717,
      "loss": 7.0156,
      "step": 1714
    },
    {
      "epoch": 0.5123608932705953,
      "grad_norm": 0.4516166150569916,
      "learning_rate": 0.00037201314217443247,
      "loss": 6.9863,
      "step": 1715
    },
    {
      "epoch": 0.5126596459780417,
      "grad_norm": 0.42357736825942993,
      "learning_rate": 0.00037193847072879334,
      "loss": 7.0215,
      "step": 1716
    },
    {
      "epoch": 0.5129583986854881,
      "grad_norm": 0.46084704995155334,
      "learning_rate": 0.0003718637992831541,
      "loss": 7.0811,
      "step": 1717
    },
    {
      "epoch": 0.5132571513929345,
      "grad_norm": 0.4689125716686249,
      "learning_rate": 0.00037178912783751496,
      "loss": 7.0107,
      "step": 1718
    },
    {
      "epoch": 0.5135559041003809,
      "grad_norm": 0.4669073820114136,
      "learning_rate": 0.0003717144563918757,
      "loss": 6.9512,
      "step": 1719
    },
    {
      "epoch": 0.5138546568078273,
      "grad_norm": 0.5000540614128113,
      "learning_rate": 0.0003716397849462366,
      "loss": 7.0361,
      "step": 1720
    },
    {
      "epoch": 0.5141534095152738,
      "grad_norm": 0.47200870513916016,
      "learning_rate": 0.0003715651135005974,
      "loss": 7.1055,
      "step": 1721
    },
    {
      "epoch": 0.5144521622227202,
      "grad_norm": 0.4281380772590637,
      "learning_rate": 0.0003714904420549582,
      "loss": 7.3271,
      "step": 1722
    },
    {
      "epoch": 0.5147509149301666,
      "grad_norm": 0.45114371180534363,
      "learning_rate": 0.000371415770609319,
      "loss": 7.625,
      "step": 1723
    },
    {
      "epoch": 0.515049667637613,
      "grad_norm": 0.3489571511745453,
      "learning_rate": 0.0003713410991636798,
      "loss": 7.5332,
      "step": 1724
    },
    {
      "epoch": 0.5153484203450593,
      "grad_norm": 0.455884724855423,
      "learning_rate": 0.00037126642771804065,
      "loss": 6.874,
      "step": 1725
    },
    {
      "epoch": 0.5156471730525057,
      "grad_norm": 0.4463188052177429,
      "learning_rate": 0.0003711917562724014,
      "loss": 7.0518,
      "step": 1726
    },
    {
      "epoch": 0.5159459257599522,
      "grad_norm": 0.5482686758041382,
      "learning_rate": 0.0003711170848267623,
      "loss": 6.6777,
      "step": 1727
    },
    {
      "epoch": 0.5162446784673986,
      "grad_norm": 0.46086958050727844,
      "learning_rate": 0.00037104241338112304,
      "loss": 7.2773,
      "step": 1728
    },
    {
      "epoch": 0.516543431174845,
      "grad_norm": 0.4424954056739807,
      "learning_rate": 0.0003709677419354839,
      "loss": 7.208,
      "step": 1729
    },
    {
      "epoch": 0.5168421838822914,
      "grad_norm": 0.36620014905929565,
      "learning_rate": 0.00037089307048984466,
      "loss": 7.7119,
      "step": 1730
    },
    {
      "epoch": 0.5171409365897378,
      "grad_norm": 0.44831058382987976,
      "learning_rate": 0.0003708183990442055,
      "loss": 7.1426,
      "step": 1731
    },
    {
      "epoch": 0.5174396892971842,
      "grad_norm": 0.5190175175666809,
      "learning_rate": 0.00037074372759856634,
      "loss": 6.7471,
      "step": 1732
    },
    {
      "epoch": 0.5177384420046307,
      "grad_norm": 0.5416143536567688,
      "learning_rate": 0.0003706690561529271,
      "loss": 6.9385,
      "step": 1733
    },
    {
      "epoch": 0.5180371947120771,
      "grad_norm": 0.5267763137817383,
      "learning_rate": 0.00037059438470728797,
      "loss": 6.8184,
      "step": 1734
    },
    {
      "epoch": 0.5183359474195235,
      "grad_norm": 0.4359271228313446,
      "learning_rate": 0.0003705197132616487,
      "loss": 7.6533,
      "step": 1735
    },
    {
      "epoch": 0.5186347001269699,
      "grad_norm": 0.6271741986274719,
      "learning_rate": 0.0003704450418160096,
      "loss": 6.1416,
      "step": 1736
    },
    {
      "epoch": 0.5189334528344163,
      "grad_norm": 0.44820931553840637,
      "learning_rate": 0.00037037037037037035,
      "loss": 7.1348,
      "step": 1737
    },
    {
      "epoch": 0.5192322055418627,
      "grad_norm": 0.5357819199562073,
      "learning_rate": 0.0003702956989247312,
      "loss": 7.2686,
      "step": 1738
    },
    {
      "epoch": 0.5195309582493092,
      "grad_norm": 0.4111812114715576,
      "learning_rate": 0.000370221027479092,
      "loss": 7.7793,
      "step": 1739
    },
    {
      "epoch": 0.5198297109567556,
      "grad_norm": 0.4060628414154053,
      "learning_rate": 0.0003701463560334528,
      "loss": 7.2646,
      "step": 1740
    },
    {
      "epoch": 0.520128463664202,
      "grad_norm": 0.39134275913238525,
      "learning_rate": 0.00037007168458781366,
      "loss": 7.5273,
      "step": 1741
    },
    {
      "epoch": 0.5204272163716483,
      "grad_norm": 0.4165923595428467,
      "learning_rate": 0.0003699970131421744,
      "loss": 7.4512,
      "step": 1742
    },
    {
      "epoch": 0.5207259690790947,
      "grad_norm": 0.5725052356719971,
      "learning_rate": 0.0003699223416965353,
      "loss": 6.8252,
      "step": 1743
    },
    {
      "epoch": 0.5210247217865412,
      "grad_norm": 0.4103209972381592,
      "learning_rate": 0.00036984767025089604,
      "loss": 6.8242,
      "step": 1744
    },
    {
      "epoch": 0.5213234744939876,
      "grad_norm": 0.44425320625305176,
      "learning_rate": 0.0003697729988052569,
      "loss": 7.3096,
      "step": 1745
    },
    {
      "epoch": 0.521622227201434,
      "grad_norm": 0.5696417689323425,
      "learning_rate": 0.00036969832735961767,
      "loss": 6.3818,
      "step": 1746
    },
    {
      "epoch": 0.5219209799088804,
      "grad_norm": 0.376056969165802,
      "learning_rate": 0.0003696236559139785,
      "loss": 7.5342,
      "step": 1747
    },
    {
      "epoch": 0.5222197326163268,
      "grad_norm": 0.4267451763153076,
      "learning_rate": 0.0003695489844683393,
      "loss": 7.0977,
      "step": 1748
    },
    {
      "epoch": 0.5225184853237732,
      "grad_norm": 0.42918357253074646,
      "learning_rate": 0.0003694743130227001,
      "loss": 6.9951,
      "step": 1749
    },
    {
      "epoch": 0.5228172380312197,
      "grad_norm": 0.5346568822860718,
      "learning_rate": 0.000369399641577061,
      "loss": 6.7871,
      "step": 1750
    },
    {
      "epoch": 0.5231159907386661,
      "grad_norm": 0.4624533951282501,
      "learning_rate": 0.00036932497013142173,
      "loss": 7.6094,
      "step": 1751
    },
    {
      "epoch": 0.5234147434461125,
      "grad_norm": 0.4096202254295349,
      "learning_rate": 0.0003692502986857826,
      "loss": 7.4668,
      "step": 1752
    },
    {
      "epoch": 0.5237134961535589,
      "grad_norm": 0.48780012130737305,
      "learning_rate": 0.00036917562724014336,
      "loss": 7.5361,
      "step": 1753
    },
    {
      "epoch": 0.5240122488610053,
      "grad_norm": 0.44860485196113586,
      "learning_rate": 0.0003691009557945042,
      "loss": 6.8848,
      "step": 1754
    },
    {
      "epoch": 0.5243110015684517,
      "grad_norm": 0.4155891239643097,
      "learning_rate": 0.000369026284348865,
      "loss": 7.4629,
      "step": 1755
    },
    {
      "epoch": 0.5246097542758982,
      "grad_norm": 0.4344923496246338,
      "learning_rate": 0.0003689516129032258,
      "loss": 7.5605,
      "step": 1756
    },
    {
      "epoch": 0.5249085069833446,
      "grad_norm": 0.3818672299385071,
      "learning_rate": 0.0003688769414575866,
      "loss": 7.043,
      "step": 1757
    },
    {
      "epoch": 0.525207259690791,
      "grad_norm": 0.42192110419273376,
      "learning_rate": 0.0003688022700119474,
      "loss": 7.1143,
      "step": 1758
    },
    {
      "epoch": 0.5255060123982374,
      "grad_norm": 0.4506534934043884,
      "learning_rate": 0.0003687275985663083,
      "loss": 7.3867,
      "step": 1759
    },
    {
      "epoch": 0.5258047651056837,
      "grad_norm": 0.4215080142021179,
      "learning_rate": 0.00036865292712066905,
      "loss": 7.5527,
      "step": 1760
    },
    {
      "epoch": 0.5261035178131301,
      "grad_norm": 0.5141822099685669,
      "learning_rate": 0.0003685782556750299,
      "loss": 7.0703,
      "step": 1761
    },
    {
      "epoch": 0.5264022705205766,
      "grad_norm": 0.445200651884079,
      "learning_rate": 0.0003685035842293907,
      "loss": 6.8721,
      "step": 1762
    },
    {
      "epoch": 0.526701023228023,
      "grad_norm": 0.3742438554763794,
      "learning_rate": 0.0003684289127837515,
      "loss": 7.7529,
      "step": 1763
    },
    {
      "epoch": 0.5269997759354694,
      "grad_norm": 0.4356149137020111,
      "learning_rate": 0.0003683542413381123,
      "loss": 7.4941,
      "step": 1764
    },
    {
      "epoch": 0.5272985286429158,
      "grad_norm": 0.42085301876068115,
      "learning_rate": 0.0003682795698924731,
      "loss": 6.9678,
      "step": 1765
    },
    {
      "epoch": 0.5275972813503622,
      "grad_norm": 0.5002272725105286,
      "learning_rate": 0.0003682048984468339,
      "loss": 7.2373,
      "step": 1766
    },
    {
      "epoch": 0.5278960340578086,
      "grad_norm": 0.5166744589805603,
      "learning_rate": 0.00036813022700119474,
      "loss": 6.8262,
      "step": 1767
    },
    {
      "epoch": 0.5281947867652551,
      "grad_norm": 0.40453603863716125,
      "learning_rate": 0.0003680555555555556,
      "loss": 7.1494,
      "step": 1768
    },
    {
      "epoch": 0.5284935394727015,
      "grad_norm": 0.5204369425773621,
      "learning_rate": 0.00036798088410991636,
      "loss": 6.9053,
      "step": 1769
    },
    {
      "epoch": 0.5287922921801479,
      "grad_norm": 0.5766379833221436,
      "learning_rate": 0.00036790621266427723,
      "loss": 7.0234,
      "step": 1770
    },
    {
      "epoch": 0.5290910448875943,
      "grad_norm": 0.4769459068775177,
      "learning_rate": 0.000367831541218638,
      "loss": 6.7383,
      "step": 1771
    },
    {
      "epoch": 0.5293897975950407,
      "grad_norm": 0.5289496779441833,
      "learning_rate": 0.0003677568697729988,
      "loss": 7.1221,
      "step": 1772
    },
    {
      "epoch": 0.5296885503024871,
      "grad_norm": 0.40201056003570557,
      "learning_rate": 0.0003676821983273596,
      "loss": 7.1406,
      "step": 1773
    },
    {
      "epoch": 0.5299873030099336,
      "grad_norm": 0.4197743535041809,
      "learning_rate": 0.00036760752688172043,
      "loss": 7.2803,
      "step": 1774
    },
    {
      "epoch": 0.53028605571738,
      "grad_norm": 0.4716838598251343,
      "learning_rate": 0.00036753285543608124,
      "loss": 7.1211,
      "step": 1775
    },
    {
      "epoch": 0.5305848084248264,
      "grad_norm": 0.45334115624427795,
      "learning_rate": 0.00036745818399044205,
      "loss": 7.2197,
      "step": 1776
    },
    {
      "epoch": 0.5308835611322728,
      "grad_norm": 0.44493338465690613,
      "learning_rate": 0.0003673835125448029,
      "loss": 6.957,
      "step": 1777
    },
    {
      "epoch": 0.5311823138397191,
      "grad_norm": 0.39251792430877686,
      "learning_rate": 0.0003673088410991637,
      "loss": 7.4297,
      "step": 1778
    },
    {
      "epoch": 0.5314810665471655,
      "grad_norm": 0.4328767955303192,
      "learning_rate": 0.0003672341696535245,
      "loss": 7.6816,
      "step": 1779
    },
    {
      "epoch": 0.531779819254612,
      "grad_norm": 0.4476648271083832,
      "learning_rate": 0.0003671594982078853,
      "loss": 7.0029,
      "step": 1780
    },
    {
      "epoch": 0.5320785719620584,
      "grad_norm": 0.4966125190258026,
      "learning_rate": 0.0003670848267622461,
      "loss": 6.9004,
      "step": 1781
    },
    {
      "epoch": 0.5323773246695048,
      "grad_norm": 0.36078405380249023,
      "learning_rate": 0.00036701015531660693,
      "loss": 7.6338,
      "step": 1782
    },
    {
      "epoch": 0.5326760773769512,
      "grad_norm": 0.515585720539093,
      "learning_rate": 0.00036693548387096774,
      "loss": 6.6182,
      "step": 1783
    },
    {
      "epoch": 0.5329748300843976,
      "grad_norm": 0.3595338463783264,
      "learning_rate": 0.00036686081242532856,
      "loss": 7.7002,
      "step": 1784
    },
    {
      "epoch": 0.533273582791844,
      "grad_norm": 0.4050963222980499,
      "learning_rate": 0.00036678614097968937,
      "loss": 7.6914,
      "step": 1785
    },
    {
      "epoch": 0.5335723354992905,
      "grad_norm": 0.39025774598121643,
      "learning_rate": 0.00036671146953405024,
      "loss": 7.6211,
      "step": 1786
    },
    {
      "epoch": 0.5338710882067369,
      "grad_norm": 0.41511914134025574,
      "learning_rate": 0.000366636798088411,
      "loss": 7.2549,
      "step": 1787
    },
    {
      "epoch": 0.5341698409141833,
      "grad_norm": 0.5071979761123657,
      "learning_rate": 0.0003665621266427718,
      "loss": 6.7051,
      "step": 1788
    },
    {
      "epoch": 0.5344685936216297,
      "grad_norm": 0.593086302280426,
      "learning_rate": 0.0003664874551971326,
      "loss": 6.7578,
      "step": 1789
    },
    {
      "epoch": 0.5347673463290761,
      "grad_norm": 0.4553339183330536,
      "learning_rate": 0.00036641278375149343,
      "loss": 7.1104,
      "step": 1790
    },
    {
      "epoch": 0.5350660990365225,
      "grad_norm": 0.41731828451156616,
      "learning_rate": 0.00036633811230585425,
      "loss": 7.0693,
      "step": 1791
    },
    {
      "epoch": 0.535364851743969,
      "grad_norm": 0.4171659052371979,
      "learning_rate": 0.00036626344086021506,
      "loss": 7.2295,
      "step": 1792
    },
    {
      "epoch": 0.5356636044514154,
      "grad_norm": 0.46239224076271057,
      "learning_rate": 0.0003661887694145759,
      "loss": 7.2705,
      "step": 1793
    },
    {
      "epoch": 0.5359623571588618,
      "grad_norm": 0.39590707421302795,
      "learning_rate": 0.0003661140979689367,
      "loss": 7.3105,
      "step": 1794
    },
    {
      "epoch": 0.5362611098663081,
      "grad_norm": 0.4476425349712372,
      "learning_rate": 0.0003660394265232975,
      "loss": 7.0361,
      "step": 1795
    },
    {
      "epoch": 0.5365598625737545,
      "grad_norm": 0.42073723673820496,
      "learning_rate": 0.0003659647550776583,
      "loss": 7.0732,
      "step": 1796
    },
    {
      "epoch": 0.5368586152812009,
      "grad_norm": 0.44485944509506226,
      "learning_rate": 0.0003658900836320191,
      "loss": 7.8311,
      "step": 1797
    },
    {
      "epoch": 0.5371573679886474,
      "grad_norm": 0.4307780861854553,
      "learning_rate": 0.00036581541218637994,
      "loss": 7.3555,
      "step": 1798
    },
    {
      "epoch": 0.5374561206960938,
      "grad_norm": 0.4892483949661255,
      "learning_rate": 0.00036574074074074075,
      "loss": 6.6016,
      "step": 1799
    },
    {
      "epoch": 0.5377548734035402,
      "grad_norm": 0.43709927797317505,
      "learning_rate": 0.00036566606929510156,
      "loss": 7.6035,
      "step": 1800
    },
    {
      "epoch": 0.5377548734035402,
      "eval_bleu": 0.11200152069157863,
      "eval_loss": 7.03125,
      "eval_runtime": 535.6512,
      "eval_samples_per_second": 2.63,
      "eval_steps_per_second": 0.166,
      "step": 1800
    },
    {
      "epoch": 0.5380536261109866,
      "grad_norm": 0.3769877254962921,
      "learning_rate": 0.0003655913978494624,
      "loss": 7.958,
      "step": 1801
    },
    {
      "epoch": 0.538352378818433,
      "grad_norm": 0.3463749587535858,
      "learning_rate": 0.0003655167264038232,
      "loss": 7.6797,
      "step": 1802
    },
    {
      "epoch": 0.5386511315258794,
      "grad_norm": 0.3249629735946655,
      "learning_rate": 0.000365442054958184,
      "loss": 7.7617,
      "step": 1803
    },
    {
      "epoch": 0.5389498842333259,
      "grad_norm": 0.46065160632133484,
      "learning_rate": 0.0003653673835125448,
      "loss": 7.0459,
      "step": 1804
    },
    {
      "epoch": 0.5392486369407723,
      "grad_norm": 0.40083834528923035,
      "learning_rate": 0.00036529271206690563,
      "loss": 7.6113,
      "step": 1805
    },
    {
      "epoch": 0.5395473896482187,
      "grad_norm": 0.4890677034854889,
      "learning_rate": 0.00036521804062126644,
      "loss": 7.2021,
      "step": 1806
    },
    {
      "epoch": 0.5398461423556651,
      "grad_norm": 0.5099648237228394,
      "learning_rate": 0.00036514336917562725,
      "loss": 7.04,
      "step": 1807
    },
    {
      "epoch": 0.5401448950631115,
      "grad_norm": 0.42745155096054077,
      "learning_rate": 0.00036506869772998807,
      "loss": 7.042,
      "step": 1808
    },
    {
      "epoch": 0.5404436477705579,
      "grad_norm": 0.4279111325740814,
      "learning_rate": 0.0003649940262843489,
      "loss": 7.3037,
      "step": 1809
    },
    {
      "epoch": 0.5407424004780044,
      "grad_norm": 0.3628852963447571,
      "learning_rate": 0.0003649193548387097,
      "loss": 7.5605,
      "step": 1810
    },
    {
      "epoch": 0.5410411531854508,
      "grad_norm": 0.4299941956996918,
      "learning_rate": 0.00036484468339307045,
      "loss": 6.9521,
      "step": 1811
    },
    {
      "epoch": 0.5413399058928972,
      "grad_norm": 0.43412163853645325,
      "learning_rate": 0.0003647700119474313,
      "loss": 7.2188,
      "step": 1812
    },
    {
      "epoch": 0.5416386586003435,
      "grad_norm": 0.4459388256072998,
      "learning_rate": 0.00036469534050179213,
      "loss": 7.2451,
      "step": 1813
    },
    {
      "epoch": 0.5419374113077899,
      "grad_norm": 0.4845440089702606,
      "learning_rate": 0.00036462066905615294,
      "loss": 7.333,
      "step": 1814
    },
    {
      "epoch": 0.5422361640152363,
      "grad_norm": 0.42027685046195984,
      "learning_rate": 0.00036454599761051376,
      "loss": 7.5293,
      "step": 1815
    },
    {
      "epoch": 0.5425349167226828,
      "grad_norm": 0.41822442412376404,
      "learning_rate": 0.00036447132616487457,
      "loss": 7.8359,
      "step": 1816
    },
    {
      "epoch": 0.5428336694301292,
      "grad_norm": 0.3803103566169739,
      "learning_rate": 0.0003643966547192354,
      "loss": 7.4424,
      "step": 1817
    },
    {
      "epoch": 0.5431324221375756,
      "grad_norm": 0.4687531590461731,
      "learning_rate": 0.0003643219832735962,
      "loss": 7.2451,
      "step": 1818
    },
    {
      "epoch": 0.543431174845022,
      "grad_norm": 0.44610264897346497,
      "learning_rate": 0.000364247311827957,
      "loss": 7.583,
      "step": 1819
    },
    {
      "epoch": 0.5437299275524684,
      "grad_norm": 0.41222304105758667,
      "learning_rate": 0.00036417264038231777,
      "loss": 7.3262,
      "step": 1820
    },
    {
      "epoch": 0.5440286802599149,
      "grad_norm": 0.4808083474636078,
      "learning_rate": 0.00036409796893667863,
      "loss": 6.7949,
      "step": 1821
    },
    {
      "epoch": 0.5443274329673613,
      "grad_norm": 0.4226115942001343,
      "learning_rate": 0.0003640232974910394,
      "loss": 7.3535,
      "step": 1822
    },
    {
      "epoch": 0.5446261856748077,
      "grad_norm": 0.3178735375404358,
      "learning_rate": 0.00036394862604540026,
      "loss": 7.6436,
      "step": 1823
    },
    {
      "epoch": 0.5449249383822541,
      "grad_norm": 0.3476685583591461,
      "learning_rate": 0.00036387395459976107,
      "loss": 7.5801,
      "step": 1824
    },
    {
      "epoch": 0.5452236910897005,
      "grad_norm": 0.4379502534866333,
      "learning_rate": 0.0003637992831541219,
      "loss": 7.3682,
      "step": 1825
    },
    {
      "epoch": 0.5455224437971469,
      "grad_norm": 0.4735087752342224,
      "learning_rate": 0.0003637246117084827,
      "loss": 7.04,
      "step": 1826
    },
    {
      "epoch": 0.5458211965045934,
      "grad_norm": 0.5032477378845215,
      "learning_rate": 0.00036364994026284346,
      "loss": 7.5195,
      "step": 1827
    },
    {
      "epoch": 0.5461199492120398,
      "grad_norm": 0.5984776020050049,
      "learning_rate": 0.0003635752688172043,
      "loss": 7.0742,
      "step": 1828
    },
    {
      "epoch": 0.5464187019194862,
      "grad_norm": 0.487301230430603,
      "learning_rate": 0.0003635005973715651,
      "loss": 7.3262,
      "step": 1829
    },
    {
      "epoch": 0.5467174546269326,
      "grad_norm": 0.40835824608802795,
      "learning_rate": 0.00036342592592592595,
      "loss": 7.3047,
      "step": 1830
    },
    {
      "epoch": 0.547016207334379,
      "grad_norm": 0.37940993905067444,
      "learning_rate": 0.0003633512544802867,
      "loss": 7.5107,
      "step": 1831
    },
    {
      "epoch": 0.5473149600418253,
      "grad_norm": 0.4357999861240387,
      "learning_rate": 0.0003632765830346476,
      "loss": 7.2588,
      "step": 1832
    },
    {
      "epoch": 0.5476137127492718,
      "grad_norm": 0.4739578068256378,
      "learning_rate": 0.0003632019115890084,
      "loss": 6.8799,
      "step": 1833
    },
    {
      "epoch": 0.5479124654567182,
      "grad_norm": 0.422121524810791,
      "learning_rate": 0.0003631272401433692,
      "loss": 7.4492,
      "step": 1834
    },
    {
      "epoch": 0.5482112181641646,
      "grad_norm": 0.5361121296882629,
      "learning_rate": 0.00036305256869773,
      "loss": 6.877,
      "step": 1835
    },
    {
      "epoch": 0.548509970871611,
      "grad_norm": 0.40251627564430237,
      "learning_rate": 0.00036297789725209077,
      "loss": 7.2549,
      "step": 1836
    },
    {
      "epoch": 0.5488087235790574,
      "grad_norm": 0.3986588418483734,
      "learning_rate": 0.00036290322580645164,
      "loss": 7.7822,
      "step": 1837
    },
    {
      "epoch": 0.5491074762865038,
      "grad_norm": 0.4440116584300995,
      "learning_rate": 0.0003628285543608124,
      "loss": 6.8779,
      "step": 1838
    },
    {
      "epoch": 0.5494062289939503,
      "grad_norm": 0.34190452098846436,
      "learning_rate": 0.00036275388291517327,
      "loss": 7.498,
      "step": 1839
    },
    {
      "epoch": 0.5497049817013967,
      "grad_norm": 0.4390486180782318,
      "learning_rate": 0.000362679211469534,
      "loss": 7.1084,
      "step": 1840
    },
    {
      "epoch": 0.5500037344088431,
      "grad_norm": 0.46566808223724365,
      "learning_rate": 0.0003626045400238949,
      "loss": 7.1709,
      "step": 1841
    },
    {
      "epoch": 0.5503024871162895,
      "grad_norm": 0.3714979887008667,
      "learning_rate": 0.0003625298685782557,
      "loss": 7.5391,
      "step": 1842
    },
    {
      "epoch": 0.5506012398237359,
      "grad_norm": 0.4267682731151581,
      "learning_rate": 0.00036245519713261646,
      "loss": 7.4414,
      "step": 1843
    },
    {
      "epoch": 0.5508999925311823,
      "grad_norm": 0.453889399766922,
      "learning_rate": 0.00036238052568697733,
      "loss": 7.2637,
      "step": 1844
    },
    {
      "epoch": 0.5511987452386288,
      "grad_norm": 0.533337414264679,
      "learning_rate": 0.0003623058542413381,
      "loss": 6.9258,
      "step": 1845
    },
    {
      "epoch": 0.5514974979460752,
      "grad_norm": 0.39616870880126953,
      "learning_rate": 0.00036223118279569896,
      "loss": 7.1865,
      "step": 1846
    },
    {
      "epoch": 0.5517962506535216,
      "grad_norm": 0.4717317819595337,
      "learning_rate": 0.0003621565113500597,
      "loss": 7.1396,
      "step": 1847
    },
    {
      "epoch": 0.552095003360968,
      "grad_norm": 0.42816033959388733,
      "learning_rate": 0.0003620818399044206,
      "loss": 7.1914,
      "step": 1848
    },
    {
      "epoch": 0.5523937560684143,
      "grad_norm": 0.547660768032074,
      "learning_rate": 0.00036200716845878134,
      "loss": 6.8447,
      "step": 1849
    },
    {
      "epoch": 0.5526925087758607,
      "grad_norm": 0.4329815208911896,
      "learning_rate": 0.0003619324970131422,
      "loss": 7.5498,
      "step": 1850
    },
    {
      "epoch": 0.5529912614833072,
      "grad_norm": 0.42650899291038513,
      "learning_rate": 0.000361857825567503,
      "loss": 6.7568,
      "step": 1851
    },
    {
      "epoch": 0.5532900141907536,
      "grad_norm": 0.48277515172958374,
      "learning_rate": 0.0003617831541218638,
      "loss": 7.0186,
      "step": 1852
    },
    {
      "epoch": 0.5535887668982,
      "grad_norm": 0.4820873439311981,
      "learning_rate": 0.00036170848267622465,
      "loss": 6.9268,
      "step": 1853
    },
    {
      "epoch": 0.5538875196056464,
      "grad_norm": 0.4493240416049957,
      "learning_rate": 0.0003616338112305854,
      "loss": 7.1719,
      "step": 1854
    },
    {
      "epoch": 0.5541862723130928,
      "grad_norm": 0.43493640422821045,
      "learning_rate": 0.00036155913978494627,
      "loss": 7.293,
      "step": 1855
    },
    {
      "epoch": 0.5544850250205392,
      "grad_norm": 0.3967174291610718,
      "learning_rate": 0.00036148446833930703,
      "loss": 7.6191,
      "step": 1856
    },
    {
      "epoch": 0.5547837777279857,
      "grad_norm": 0.518828272819519,
      "learning_rate": 0.0003614097968936679,
      "loss": 6.8799,
      "step": 1857
    },
    {
      "epoch": 0.5550825304354321,
      "grad_norm": 4.103127956390381,
      "learning_rate": 0.00036133512544802866,
      "loss": 7.335,
      "step": 1858
    },
    {
      "epoch": 0.5553812831428785,
      "grad_norm": 0.3585701584815979,
      "learning_rate": 0.00036126045400238947,
      "loss": 7.6045,
      "step": 1859
    },
    {
      "epoch": 0.5556800358503249,
      "grad_norm": 0.4748585820198059,
      "learning_rate": 0.00036118578255675034,
      "loss": 7.2842,
      "step": 1860
    },
    {
      "epoch": 0.5559787885577713,
      "grad_norm": 0.4154168665409088,
      "learning_rate": 0.0003611111111111111,
      "loss": 7.0195,
      "step": 1861
    },
    {
      "epoch": 0.5562775412652177,
      "grad_norm": 0.4059871733188629,
      "learning_rate": 0.00036103643966547196,
      "loss": 7.1846,
      "step": 1862
    },
    {
      "epoch": 0.5565762939726642,
      "grad_norm": 0.5374539494514465,
      "learning_rate": 0.0003609617682198327,
      "loss": 6.9795,
      "step": 1863
    },
    {
      "epoch": 0.5568750466801106,
      "grad_norm": 0.4308192729949951,
      "learning_rate": 0.0003608870967741936,
      "loss": 7.4082,
      "step": 1864
    },
    {
      "epoch": 0.557173799387557,
      "grad_norm": 0.4446411728858948,
      "learning_rate": 0.00036081242532855435,
      "loss": 7.1172,
      "step": 1865
    },
    {
      "epoch": 0.5574725520950033,
      "grad_norm": 0.39201319217681885,
      "learning_rate": 0.0003607377538829152,
      "loss": 7.1826,
      "step": 1866
    },
    {
      "epoch": 0.5577713048024497,
      "grad_norm": 0.3626995086669922,
      "learning_rate": 0.00036066308243727597,
      "loss": 7.8115,
      "step": 1867
    },
    {
      "epoch": 0.5580700575098961,
      "grad_norm": 0.4786704480648041,
      "learning_rate": 0.0003605884109916368,
      "loss": 7.083,
      "step": 1868
    },
    {
      "epoch": 0.5583688102173426,
      "grad_norm": 0.3507809340953827,
      "learning_rate": 0.00036051373954599765,
      "loss": 7.6113,
      "step": 1869
    },
    {
      "epoch": 0.558667562924789,
      "grad_norm": 0.3810882568359375,
      "learning_rate": 0.0003604390681003584,
      "loss": 7.7119,
      "step": 1870
    },
    {
      "epoch": 0.5589663156322354,
      "grad_norm": 0.5036351084709167,
      "learning_rate": 0.0003603643966547193,
      "loss": 7.0361,
      "step": 1871
    },
    {
      "epoch": 0.5592650683396818,
      "grad_norm": 0.3997707962989807,
      "learning_rate": 0.00036028972520908004,
      "loss": 7.1846,
      "step": 1872
    },
    {
      "epoch": 0.5595638210471282,
      "grad_norm": 0.3768296241760254,
      "learning_rate": 0.0003602150537634409,
      "loss": 7.6367,
      "step": 1873
    },
    {
      "epoch": 0.5598625737545746,
      "grad_norm": 0.4304940104484558,
      "learning_rate": 0.00036014038231780166,
      "loss": 7.2725,
      "step": 1874
    },
    {
      "epoch": 0.5601613264620211,
      "grad_norm": 0.43988555669784546,
      "learning_rate": 0.0003600657108721625,
      "loss": 7.4648,
      "step": 1875
    },
    {
      "epoch": 0.5604600791694675,
      "grad_norm": 0.38436073064804077,
      "learning_rate": 0.0003599910394265233,
      "loss": 7.3662,
      "step": 1876
    },
    {
      "epoch": 0.5607588318769139,
      "grad_norm": 0.37261873483657837,
      "learning_rate": 0.0003599163679808841,
      "loss": 7.4678,
      "step": 1877
    },
    {
      "epoch": 0.5610575845843603,
      "grad_norm": 0.4239116311073303,
      "learning_rate": 0.00035984169653524497,
      "loss": 7.3311,
      "step": 1878
    },
    {
      "epoch": 0.5613563372918067,
      "grad_norm": 0.3910449743270874,
      "learning_rate": 0.0003597670250896057,
      "loss": 7.5254,
      "step": 1879
    },
    {
      "epoch": 0.5616550899992531,
      "grad_norm": 0.47066521644592285,
      "learning_rate": 0.0003596923536439666,
      "loss": 7.2393,
      "step": 1880
    },
    {
      "epoch": 0.5619538427066996,
      "grad_norm": 0.43041133880615234,
      "learning_rate": 0.00035961768219832735,
      "loss": 7.0889,
      "step": 1881
    },
    {
      "epoch": 0.562252595414146,
      "grad_norm": 0.40710926055908203,
      "learning_rate": 0.0003595430107526882,
      "loss": 7.1123,
      "step": 1882
    },
    {
      "epoch": 0.5625513481215924,
      "grad_norm": 0.418027400970459,
      "learning_rate": 0.000359468339307049,
      "loss": 7.3174,
      "step": 1883
    },
    {
      "epoch": 0.5628501008290387,
      "grad_norm": 0.48321840167045593,
      "learning_rate": 0.0003593936678614098,
      "loss": 7.209,
      "step": 1884
    },
    {
      "epoch": 0.5631488535364851,
      "grad_norm": 0.4216794967651367,
      "learning_rate": 0.0003593189964157706,
      "loss": 7.46,
      "step": 1885
    },
    {
      "epoch": 0.5634476062439315,
      "grad_norm": 0.420259565114975,
      "learning_rate": 0.0003592443249701314,
      "loss": 7.458,
      "step": 1886
    },
    {
      "epoch": 0.563746358951378,
      "grad_norm": 0.4139153063297272,
      "learning_rate": 0.0003591696535244923,
      "loss": 7.3252,
      "step": 1887
    },
    {
      "epoch": 0.5640451116588244,
      "grad_norm": 0.41245418787002563,
      "learning_rate": 0.00035909498207885304,
      "loss": 7.3438,
      "step": 1888
    },
    {
      "epoch": 0.5643438643662708,
      "grad_norm": 0.4641612470149994,
      "learning_rate": 0.0003590203106332139,
      "loss": 6.3975,
      "step": 1889
    },
    {
      "epoch": 0.5646426170737172,
      "grad_norm": 0.47636356949806213,
      "learning_rate": 0.00035894563918757467,
      "loss": 6.9297,
      "step": 1890
    },
    {
      "epoch": 0.5649413697811636,
      "grad_norm": 0.4787176251411438,
      "learning_rate": 0.0003588709677419355,
      "loss": 6.9717,
      "step": 1891
    },
    {
      "epoch": 0.5652401224886101,
      "grad_norm": 0.4445871114730835,
      "learning_rate": 0.0003587962962962963,
      "loss": 7.0352,
      "step": 1892
    },
    {
      "epoch": 0.5655388751960565,
      "grad_norm": 0.40166711807250977,
      "learning_rate": 0.0003587216248506571,
      "loss": 7.209,
      "step": 1893
    },
    {
      "epoch": 0.5658376279035029,
      "grad_norm": 0.4817529618740082,
      "learning_rate": 0.0003586469534050179,
      "loss": 7.0889,
      "step": 1894
    },
    {
      "epoch": 0.5661363806109493,
      "grad_norm": 0.48484256863594055,
      "learning_rate": 0.00035857228195937873,
      "loss": 7.2168,
      "step": 1895
    },
    {
      "epoch": 0.5664351333183957,
      "grad_norm": 0.46850407123565674,
      "learning_rate": 0.0003584976105137396,
      "loss": 7.1924,
      "step": 1896
    },
    {
      "epoch": 0.5667338860258421,
      "grad_norm": 0.5761783123016357,
      "learning_rate": 0.00035842293906810036,
      "loss": 6.8242,
      "step": 1897
    },
    {
      "epoch": 0.5670326387332886,
      "grad_norm": 0.44336825609207153,
      "learning_rate": 0.0003583482676224612,
      "loss": 7.4912,
      "step": 1898
    },
    {
      "epoch": 0.567331391440735,
      "grad_norm": 0.3638671040534973,
      "learning_rate": 0.000358273596176822,
      "loss": 7.46,
      "step": 1899
    },
    {
      "epoch": 0.5676301441481814,
      "grad_norm": 0.392203688621521,
      "learning_rate": 0.0003581989247311828,
      "loss": 7.4531,
      "step": 1900
    },
    {
      "epoch": 0.5679288968556278,
      "grad_norm": 0.45037129521369934,
      "learning_rate": 0.0003581242532855436,
      "loss": 7.042,
      "step": 1901
    },
    {
      "epoch": 0.5682276495630741,
      "grad_norm": 0.4074626863002777,
      "learning_rate": 0.0003580495818399044,
      "loss": 7.4092,
      "step": 1902
    },
    {
      "epoch": 0.5685264022705205,
      "grad_norm": 0.37348872423171997,
      "learning_rate": 0.00035797491039426524,
      "loss": 7.7422,
      "step": 1903
    },
    {
      "epoch": 0.568825154977967,
      "grad_norm": 0.44802048802375793,
      "learning_rate": 0.00035790023894862605,
      "loss": 7.5723,
      "step": 1904
    },
    {
      "epoch": 0.5691239076854134,
      "grad_norm": 0.4002765715122223,
      "learning_rate": 0.0003578255675029869,
      "loss": 6.9209,
      "step": 1905
    },
    {
      "epoch": 0.5694226603928598,
      "grad_norm": 0.45525676012039185,
      "learning_rate": 0.0003577508960573477,
      "loss": 7.1631,
      "step": 1906
    },
    {
      "epoch": 0.5697214131003062,
      "grad_norm": 0.4796808063983917,
      "learning_rate": 0.0003576762246117085,
      "loss": 6.7002,
      "step": 1907
    },
    {
      "epoch": 0.5700201658077526,
      "grad_norm": 0.42957693338394165,
      "learning_rate": 0.0003576015531660693,
      "loss": 7.4414,
      "step": 1908
    },
    {
      "epoch": 0.570318918515199,
      "grad_norm": 0.5027180314064026,
      "learning_rate": 0.0003575268817204301,
      "loss": 6.9648,
      "step": 1909
    },
    {
      "epoch": 0.5706176712226455,
      "grad_norm": 0.531443178653717,
      "learning_rate": 0.0003574522102747909,
      "loss": 6.5439,
      "step": 1910
    },
    {
      "epoch": 0.5709164239300919,
      "grad_norm": 0.43260499835014343,
      "learning_rate": 0.00035737753882915174,
      "loss": 7.1094,
      "step": 1911
    },
    {
      "epoch": 0.5712151766375383,
      "grad_norm": 0.39811941981315613,
      "learning_rate": 0.00035730286738351255,
      "loss": 7.6104,
      "step": 1912
    },
    {
      "epoch": 0.5715139293449847,
      "grad_norm": 0.4760710895061493,
      "learning_rate": 0.00035722819593787336,
      "loss": 6.9961,
      "step": 1913
    },
    {
      "epoch": 0.5718126820524311,
      "grad_norm": 0.4175036549568176,
      "learning_rate": 0.0003571535244922341,
      "loss": 7.4268,
      "step": 1914
    },
    {
      "epoch": 0.5721114347598775,
      "grad_norm": 0.45248517394065857,
      "learning_rate": 0.000357078853046595,
      "loss": 7.1143,
      "step": 1915
    },
    {
      "epoch": 0.572410187467324,
      "grad_norm": 0.3563169836997986,
      "learning_rate": 0.0003570041816009558,
      "loss": 7.667,
      "step": 1916
    },
    {
      "epoch": 0.5727089401747704,
      "grad_norm": 0.4768933355808258,
      "learning_rate": 0.0003569295101553166,
      "loss": 7.2812,
      "step": 1917
    },
    {
      "epoch": 0.5730076928822168,
      "grad_norm": 0.44211888313293457,
      "learning_rate": 0.00035685483870967743,
      "loss": 7.0986,
      "step": 1918
    },
    {
      "epoch": 0.5733064455896631,
      "grad_norm": 0.3830799162387848,
      "learning_rate": 0.00035678016726403824,
      "loss": 7.5107,
      "step": 1919
    },
    {
      "epoch": 0.5736051982971095,
      "grad_norm": 0.47476711869239807,
      "learning_rate": 0.00035670549581839905,
      "loss": 7.3076,
      "step": 1920
    },
    {
      "epoch": 0.5739039510045559,
      "grad_norm": 0.4367532730102539,
      "learning_rate": 0.00035663082437275987,
      "loss": 6.9619,
      "step": 1921
    },
    {
      "epoch": 0.5742027037120024,
      "grad_norm": 0.4412977695465088,
      "learning_rate": 0.0003565561529271207,
      "loss": 7.1094,
      "step": 1922
    },
    {
      "epoch": 0.5745014564194488,
      "grad_norm": 0.4635726809501648,
      "learning_rate": 0.00035648148148148144,
      "loss": 7.123,
      "step": 1923
    },
    {
      "epoch": 0.5748002091268952,
      "grad_norm": 0.4324605166912079,
      "learning_rate": 0.0003564068100358423,
      "loss": 7.3564,
      "step": 1924
    },
    {
      "epoch": 0.5750989618343416,
      "grad_norm": 0.38577431440353394,
      "learning_rate": 0.0003563321385902031,
      "loss": 7.127,
      "step": 1925
    },
    {
      "epoch": 0.575397714541788,
      "grad_norm": 0.41258475184440613,
      "learning_rate": 0.00035625746714456393,
      "loss": 7.1865,
      "step": 1926
    },
    {
      "epoch": 0.5756964672492344,
      "grad_norm": 0.47219306230545044,
      "learning_rate": 0.00035618279569892474,
      "loss": 6.9062,
      "step": 1927
    },
    {
      "epoch": 0.5759952199566809,
      "grad_norm": 0.40738627314567566,
      "learning_rate": 0.00035610812425328556,
      "loss": 7.4512,
      "step": 1928
    },
    {
      "epoch": 0.5762939726641273,
      "grad_norm": 0.4446709156036377,
      "learning_rate": 0.00035603345280764637,
      "loss": 6.9834,
      "step": 1929
    },
    {
      "epoch": 0.5765927253715737,
      "grad_norm": 0.5081677436828613,
      "learning_rate": 0.00035595878136200713,
      "loss": 7.0166,
      "step": 1930
    },
    {
      "epoch": 0.5768914780790201,
      "grad_norm": 0.42281973361968994,
      "learning_rate": 0.000355884109916368,
      "loss": 7.377,
      "step": 1931
    },
    {
      "epoch": 0.5771902307864665,
      "grad_norm": 0.45531147718429565,
      "learning_rate": 0.00035580943847072875,
      "loss": 7.1562,
      "step": 1932
    },
    {
      "epoch": 0.5774889834939129,
      "grad_norm": 0.3787435293197632,
      "learning_rate": 0.0003557347670250896,
      "loss": 7.6914,
      "step": 1933
    },
    {
      "epoch": 0.5777877362013594,
      "grad_norm": 0.46622052788734436,
      "learning_rate": 0.00035566009557945043,
      "loss": 7.3682,
      "step": 1934
    },
    {
      "epoch": 0.5780864889088058,
      "grad_norm": 0.49817898869514465,
      "learning_rate": 0.00035558542413381125,
      "loss": 7.0234,
      "step": 1935
    },
    {
      "epoch": 0.5783852416162522,
      "grad_norm": 0.35708343982696533,
      "learning_rate": 0.00035551075268817206,
      "loss": 7.4336,
      "step": 1936
    },
    {
      "epoch": 0.5786839943236985,
      "grad_norm": 0.5559229850769043,
      "learning_rate": 0.0003554360812425329,
      "loss": 6.6338,
      "step": 1937
    },
    {
      "epoch": 0.5789827470311449,
      "grad_norm": 0.4425787031650543,
      "learning_rate": 0.0003553614097968937,
      "loss": 7.0059,
      "step": 1938
    },
    {
      "epoch": 0.5792814997385913,
      "grad_norm": 0.4389673173427582,
      "learning_rate": 0.00035528673835125444,
      "loss": 7.4346,
      "step": 1939
    },
    {
      "epoch": 0.5795802524460378,
      "grad_norm": 0.4227897822856903,
      "learning_rate": 0.0003552120669056153,
      "loss": 7.5303,
      "step": 1940
    },
    {
      "epoch": 0.5798790051534842,
      "grad_norm": 0.43230223655700684,
      "learning_rate": 0.00035513739545997607,
      "loss": 7.6816,
      "step": 1941
    },
    {
      "epoch": 0.5801777578609306,
      "grad_norm": 0.6601218581199646,
      "learning_rate": 0.00035506272401433694,
      "loss": 6.8457,
      "step": 1942
    },
    {
      "epoch": 0.580476510568377,
      "grad_norm": 0.44688180088996887,
      "learning_rate": 0.00035498805256869775,
      "loss": 7.4922,
      "step": 1943
    },
    {
      "epoch": 0.5807752632758234,
      "grad_norm": 0.4997798800468445,
      "learning_rate": 0.00035491338112305856,
      "loss": 6.8994,
      "step": 1944
    },
    {
      "epoch": 0.5810740159832698,
      "grad_norm": 0.5072457194328308,
      "learning_rate": 0.0003548387096774194,
      "loss": 7.1826,
      "step": 1945
    },
    {
      "epoch": 0.5813727686907163,
      "grad_norm": 0.6380172967910767,
      "learning_rate": 0.00035476403823178013,
      "loss": 7.3691,
      "step": 1946
    },
    {
      "epoch": 0.5816715213981627,
      "grad_norm": 0.4315758943557739,
      "learning_rate": 0.000354689366786141,
      "loss": 6.915,
      "step": 1947
    },
    {
      "epoch": 0.5819702741056091,
      "grad_norm": 0.4053134620189667,
      "learning_rate": 0.00035461469534050176,
      "loss": 7.29,
      "step": 1948
    },
    {
      "epoch": 0.5822690268130555,
      "grad_norm": 0.42219915986061096,
      "learning_rate": 0.00035454002389486263,
      "loss": 7.5205,
      "step": 1949
    },
    {
      "epoch": 0.5825677795205019,
      "grad_norm": 0.5013216733932495,
      "learning_rate": 0.0003544653524492234,
      "loss": 6.7568,
      "step": 1950
    },
    {
      "epoch": 0.5828665322279483,
      "grad_norm": 0.47292274236679077,
      "learning_rate": 0.00035439068100358425,
      "loss": 6.5615,
      "step": 1951
    },
    {
      "epoch": 0.5831652849353948,
      "grad_norm": 0.4753882884979248,
      "learning_rate": 0.00035431600955794507,
      "loss": 7.2344,
      "step": 1952
    },
    {
      "epoch": 0.5834640376428412,
      "grad_norm": 0.49393439292907715,
      "learning_rate": 0.0003542413381123059,
      "loss": 6.8691,
      "step": 1953
    },
    {
      "epoch": 0.5837627903502876,
      "grad_norm": 0.4827612042427063,
      "learning_rate": 0.0003541666666666667,
      "loss": 6.7949,
      "step": 1954
    },
    {
      "epoch": 0.584061543057734,
      "grad_norm": 0.46224725246429443,
      "learning_rate": 0.00035409199522102745,
      "loss": 7.165,
      "step": 1955
    },
    {
      "epoch": 0.5843602957651803,
      "grad_norm": 0.43688541650772095,
      "learning_rate": 0.0003540173237753883,
      "loss": 7.3945,
      "step": 1956
    },
    {
      "epoch": 0.5846590484726267,
      "grad_norm": 0.4470222592353821,
      "learning_rate": 0.0003539426523297491,
      "loss": 7.1562,
      "step": 1957
    },
    {
      "epoch": 0.5849578011800732,
      "grad_norm": 0.5149625539779663,
      "learning_rate": 0.00035386798088410994,
      "loss": 7.0391,
      "step": 1958
    },
    {
      "epoch": 0.5852565538875196,
      "grad_norm": 0.5525931119918823,
      "learning_rate": 0.0003537933094384707,
      "loss": 6.75,
      "step": 1959
    },
    {
      "epoch": 0.585555306594966,
      "grad_norm": 0.4698519706726074,
      "learning_rate": 0.00035371863799283157,
      "loss": 6.665,
      "step": 1960
    },
    {
      "epoch": 0.5858540593024124,
      "grad_norm": 0.46401816606521606,
      "learning_rate": 0.0003536439665471924,
      "loss": 6.9443,
      "step": 1961
    },
    {
      "epoch": 0.5861528120098588,
      "grad_norm": 0.4299846887588501,
      "learning_rate": 0.00035356929510155314,
      "loss": 7.5283,
      "step": 1962
    },
    {
      "epoch": 0.5864515647173052,
      "grad_norm": 0.42563384771347046,
      "learning_rate": 0.000353494623655914,
      "loss": 7.6143,
      "step": 1963
    },
    {
      "epoch": 0.5867503174247517,
      "grad_norm": 0.4661365747451782,
      "learning_rate": 0.00035341995221027477,
      "loss": 6.873,
      "step": 1964
    },
    {
      "epoch": 0.5870490701321981,
      "grad_norm": 0.38576602935791016,
      "learning_rate": 0.00035334528076463563,
      "loss": 7.7168,
      "step": 1965
    },
    {
      "epoch": 0.5873478228396445,
      "grad_norm": 0.4159693121910095,
      "learning_rate": 0.0003532706093189964,
      "loss": 7.3457,
      "step": 1966
    },
    {
      "epoch": 0.5876465755470909,
      "grad_norm": 0.6560734510421753,
      "learning_rate": 0.00035319593787335726,
      "loss": 6.5146,
      "step": 1967
    },
    {
      "epoch": 0.5879453282545373,
      "grad_norm": 0.4424792528152466,
      "learning_rate": 0.000353121266427718,
      "loss": 7.3037,
      "step": 1968
    },
    {
      "epoch": 0.5882440809619838,
      "grad_norm": 0.43898528814315796,
      "learning_rate": 0.0003530465949820789,
      "loss": 6.9062,
      "step": 1969
    },
    {
      "epoch": 0.5885428336694302,
      "grad_norm": 0.42146167159080505,
      "learning_rate": 0.0003529719235364397,
      "loss": 7.3711,
      "step": 1970
    },
    {
      "epoch": 0.5888415863768766,
      "grad_norm": 0.4390244781970978,
      "learning_rate": 0.00035289725209080046,
      "loss": 7.5742,
      "step": 1971
    },
    {
      "epoch": 0.589140339084323,
      "grad_norm": 0.3848218023777008,
      "learning_rate": 0.0003528225806451613,
      "loss": 7.7627,
      "step": 1972
    },
    {
      "epoch": 0.5894390917917693,
      "grad_norm": 0.3908710181713104,
      "learning_rate": 0.0003527479091995221,
      "loss": 7.2324,
      "step": 1973
    },
    {
      "epoch": 0.5897378444992157,
      "grad_norm": 0.5010510683059692,
      "learning_rate": 0.00035267323775388295,
      "loss": 6.8193,
      "step": 1974
    },
    {
      "epoch": 0.5900365972066622,
      "grad_norm": 0.5294631719589233,
      "learning_rate": 0.0003525985663082437,
      "loss": 7.1855,
      "step": 1975
    },
    {
      "epoch": 0.5903353499141086,
      "grad_norm": 0.49677982926368713,
      "learning_rate": 0.0003525238948626046,
      "loss": 7.1143,
      "step": 1976
    },
    {
      "epoch": 0.590634102621555,
      "grad_norm": 0.4696771800518036,
      "learning_rate": 0.00035244922341696533,
      "loss": 6.5645,
      "step": 1977
    },
    {
      "epoch": 0.5909328553290014,
      "grad_norm": 0.41321828961372375,
      "learning_rate": 0.00035237455197132615,
      "loss": 7.1846,
      "step": 1978
    },
    {
      "epoch": 0.5912316080364478,
      "grad_norm": 0.39722272753715515,
      "learning_rate": 0.000352299880525687,
      "loss": 7.2129,
      "step": 1979
    },
    {
      "epoch": 0.5915303607438942,
      "grad_norm": 0.4727433919906616,
      "learning_rate": 0.00035222520908004777,
      "loss": 7.2275,
      "step": 1980
    },
    {
      "epoch": 0.5918291134513407,
      "grad_norm": 0.4232686460018158,
      "learning_rate": 0.00035215053763440864,
      "loss": 7.335,
      "step": 1981
    },
    {
      "epoch": 0.5921278661587871,
      "grad_norm": 0.3886699080467224,
      "learning_rate": 0.0003520758661887694,
      "loss": 7.4141,
      "step": 1982
    },
    {
      "epoch": 0.5924266188662335,
      "grad_norm": 0.47534745931625366,
      "learning_rate": 0.00035200119474313027,
      "loss": 7.04,
      "step": 1983
    },
    {
      "epoch": 0.5927253715736799,
      "grad_norm": 0.49704647064208984,
      "learning_rate": 0.000351926523297491,
      "loss": 6.8662,
      "step": 1984
    },
    {
      "epoch": 0.5930241242811263,
      "grad_norm": 0.42384979128837585,
      "learning_rate": 0.0003518518518518519,
      "loss": 7.2637,
      "step": 1985
    },
    {
      "epoch": 0.5933228769885727,
      "grad_norm": 0.46684929728507996,
      "learning_rate": 0.00035177718040621265,
      "loss": 7.3164,
      "step": 1986
    },
    {
      "epoch": 0.5936216296960192,
      "grad_norm": 0.47508540749549866,
      "learning_rate": 0.00035170250896057346,
      "loss": 7.2197,
      "step": 1987
    },
    {
      "epoch": 0.5939203824034656,
      "grad_norm": 0.451109915971756,
      "learning_rate": 0.00035162783751493433,
      "loss": 7.3291,
      "step": 1988
    },
    {
      "epoch": 0.594219135110912,
      "grad_norm": 0.583733320236206,
      "learning_rate": 0.0003515531660692951,
      "loss": 6.8984,
      "step": 1989
    },
    {
      "epoch": 0.5945178878183583,
      "grad_norm": 0.4255851209163666,
      "learning_rate": 0.00035147849462365596,
      "loss": 7.0508,
      "step": 1990
    },
    {
      "epoch": 0.5948166405258047,
      "grad_norm": 0.4592079520225525,
      "learning_rate": 0.0003514038231780167,
      "loss": 7.0029,
      "step": 1991
    },
    {
      "epoch": 0.5951153932332511,
      "grad_norm": 0.39769017696380615,
      "learning_rate": 0.0003513291517323776,
      "loss": 6.958,
      "step": 1992
    },
    {
      "epoch": 0.5954141459406976,
      "grad_norm": 0.35676467418670654,
      "learning_rate": 0.00035125448028673834,
      "loss": 7.5293,
      "step": 1993
    },
    {
      "epoch": 0.595712898648144,
      "grad_norm": 0.41567692160606384,
      "learning_rate": 0.00035117980884109915,
      "loss": 7.1328,
      "step": 1994
    },
    {
      "epoch": 0.5960116513555904,
      "grad_norm": 0.4591010808944702,
      "learning_rate": 0.00035110513739545997,
      "loss": 7.4404,
      "step": 1995
    },
    {
      "epoch": 0.5963104040630368,
      "grad_norm": 0.4838484823703766,
      "learning_rate": 0.0003510304659498208,
      "loss": 7.0195,
      "step": 1996
    },
    {
      "epoch": 0.5966091567704832,
      "grad_norm": 0.4309137761592865,
      "learning_rate": 0.00035095579450418165,
      "loss": 7.4619,
      "step": 1997
    },
    {
      "epoch": 0.5969079094779296,
      "grad_norm": 0.4130433201789856,
      "learning_rate": 0.0003508811230585424,
      "loss": 7.3281,
      "step": 1998
    },
    {
      "epoch": 0.5972066621853761,
      "grad_norm": 0.41272810101509094,
      "learning_rate": 0.00035080645161290327,
      "loss": 7.3828,
      "step": 1999
    },
    {
      "epoch": 0.5975054148928225,
      "grad_norm": 0.4541546404361725,
      "learning_rate": 0.00035073178016726403,
      "loss": 7.2334,
      "step": 2000
    },
    {
      "epoch": 0.5975054148928225,
      "eval_bleu": 0.12290537235092229,
      "eval_loss": 7.0390625,
      "eval_runtime": 490.8212,
      "eval_samples_per_second": 2.871,
      "eval_steps_per_second": 0.181,
      "step": 2000
    },
    {
      "epoch": 0.5978041676002689,
      "grad_norm": 0.4408903419971466,
      "learning_rate": 0.0003506571087216249,
      "loss": 7.2959,
      "step": 2001
    },
    {
      "epoch": 0.5981029203077153,
      "grad_norm": 0.3728633522987366,
      "learning_rate": 0.00035058243727598566,
      "loss": 7.4043,
      "step": 2002
    },
    {
      "epoch": 0.5984016730151617,
      "grad_norm": 0.5446397662162781,
      "learning_rate": 0.00035050776583034647,
      "loss": 6.5859,
      "step": 2003
    },
    {
      "epoch": 0.5987004257226081,
      "grad_norm": 0.43102240562438965,
      "learning_rate": 0.0003504330943847073,
      "loss": 7.3428,
      "step": 2004
    },
    {
      "epoch": 0.5989991784300546,
      "grad_norm": 0.4589422345161438,
      "learning_rate": 0.0003503584229390681,
      "loss": 7.1719,
      "step": 2005
    },
    {
      "epoch": 0.599297931137501,
      "grad_norm": 0.44982513785362244,
      "learning_rate": 0.0003502837514934289,
      "loss": 7.2666,
      "step": 2006
    },
    {
      "epoch": 0.5995966838449474,
      "grad_norm": 0.4760421812534332,
      "learning_rate": 0.0003502090800477897,
      "loss": 7.0527,
      "step": 2007
    },
    {
      "epoch": 0.5998954365523937,
      "grad_norm": 0.4347233474254608,
      "learning_rate": 0.0003501344086021506,
      "loss": 7.1016,
      "step": 2008
    },
    {
      "epoch": 0.6001941892598401,
      "grad_norm": 0.54486483335495,
      "learning_rate": 0.00035005973715651135,
      "loss": 6.8896,
      "step": 2009
    },
    {
      "epoch": 0.6004929419672865,
      "grad_norm": 0.4580966830253601,
      "learning_rate": 0.00034998506571087216,
      "loss": 7.4238,
      "step": 2010
    },
    {
      "epoch": 0.600791694674733,
      "grad_norm": 0.43037259578704834,
      "learning_rate": 0.00034991039426523297,
      "loss": 6.9668,
      "step": 2011
    },
    {
      "epoch": 0.6010904473821794,
      "grad_norm": 0.46093976497650146,
      "learning_rate": 0.0003498357228195938,
      "loss": 6.8945,
      "step": 2012
    },
    {
      "epoch": 0.6013892000896258,
      "grad_norm": 0.5165595412254333,
      "learning_rate": 0.0003497610513739546,
      "loss": 6.9473,
      "step": 2013
    },
    {
      "epoch": 0.6016879527970722,
      "grad_norm": 0.5902260541915894,
      "learning_rate": 0.0003496863799283154,
      "loss": 6.7148,
      "step": 2014
    },
    {
      "epoch": 0.6019867055045186,
      "grad_norm": 0.48872676491737366,
      "learning_rate": 0.0003496117084826762,
      "loss": 6.6162,
      "step": 2015
    },
    {
      "epoch": 0.602285458211965,
      "grad_norm": 0.45271769165992737,
      "learning_rate": 0.00034953703703703704,
      "loss": 6.7344,
      "step": 2016
    },
    {
      "epoch": 0.6025842109194115,
      "grad_norm": 0.5343185067176819,
      "learning_rate": 0.0003494623655913979,
      "loss": 6.7676,
      "step": 2017
    },
    {
      "epoch": 0.6028829636268579,
      "grad_norm": 0.36791926622390747,
      "learning_rate": 0.00034938769414575866,
      "loss": 7.6973,
      "step": 2018
    },
    {
      "epoch": 0.6031817163343043,
      "grad_norm": 0.41139617562294006,
      "learning_rate": 0.0003493130227001195,
      "loss": 7.249,
      "step": 2019
    },
    {
      "epoch": 0.6034804690417507,
      "grad_norm": 0.43142348527908325,
      "learning_rate": 0.0003492383512544803,
      "loss": 7.0791,
      "step": 2020
    },
    {
      "epoch": 0.6037792217491971,
      "grad_norm": 0.5028444528579712,
      "learning_rate": 0.0003491636798088411,
      "loss": 7.2842,
      "step": 2021
    },
    {
      "epoch": 0.6040779744566435,
      "grad_norm": 0.41644927859306335,
      "learning_rate": 0.0003490890083632019,
      "loss": 7.1729,
      "step": 2022
    },
    {
      "epoch": 0.60437672716409,
      "grad_norm": 0.40300628542900085,
      "learning_rate": 0.0003490143369175627,
      "loss": 7.541,
      "step": 2023
    },
    {
      "epoch": 0.6046754798715364,
      "grad_norm": 0.39794909954071045,
      "learning_rate": 0.00034893966547192354,
      "loss": 7.4668,
      "step": 2024
    },
    {
      "epoch": 0.6049742325789828,
      "grad_norm": 0.40703263878822327,
      "learning_rate": 0.00034886499402628435,
      "loss": 7.415,
      "step": 2025
    },
    {
      "epoch": 0.6052729852864291,
      "grad_norm": 0.48605120182037354,
      "learning_rate": 0.00034879032258064517,
      "loss": 6.7998,
      "step": 2026
    },
    {
      "epoch": 0.6055717379938755,
      "grad_norm": 0.4212060272693634,
      "learning_rate": 0.000348715651135006,
      "loss": 7.4912,
      "step": 2027
    },
    {
      "epoch": 0.6058704907013219,
      "grad_norm": 0.520315408706665,
      "learning_rate": 0.0003486409796893668,
      "loss": 6.9082,
      "step": 2028
    },
    {
      "epoch": 0.6061692434087684,
      "grad_norm": 0.47852823138237,
      "learning_rate": 0.0003485663082437276,
      "loss": 6.4463,
      "step": 2029
    },
    {
      "epoch": 0.6064679961162148,
      "grad_norm": 0.48572584986686707,
      "learning_rate": 0.0003484916367980884,
      "loss": 6.9648,
      "step": 2030
    },
    {
      "epoch": 0.6067667488236612,
      "grad_norm": 0.5304535031318665,
      "learning_rate": 0.00034841696535244923,
      "loss": 6.8115,
      "step": 2031
    },
    {
      "epoch": 0.6070655015311076,
      "grad_norm": 0.5245733857154846,
      "learning_rate": 0.00034834229390681004,
      "loss": 6.5967,
      "step": 2032
    },
    {
      "epoch": 0.607364254238554,
      "grad_norm": 0.45120254158973694,
      "learning_rate": 0.00034826762246117086,
      "loss": 7.0801,
      "step": 2033
    },
    {
      "epoch": 0.6076630069460004,
      "grad_norm": 0.4685448706150055,
      "learning_rate": 0.00034819295101553167,
      "loss": 7.2734,
      "step": 2034
    },
    {
      "epoch": 0.6079617596534469,
      "grad_norm": 0.4724712371826172,
      "learning_rate": 0.0003481182795698925,
      "loss": 7.4199,
      "step": 2035
    },
    {
      "epoch": 0.6082605123608933,
      "grad_norm": 0.399369478225708,
      "learning_rate": 0.0003480436081242533,
      "loss": 7.127,
      "step": 2036
    },
    {
      "epoch": 0.6085592650683397,
      "grad_norm": 0.4554372727870941,
      "learning_rate": 0.0003479689366786141,
      "loss": 6.9766,
      "step": 2037
    },
    {
      "epoch": 0.6088580177757861,
      "grad_norm": 0.3674052357673645,
      "learning_rate": 0.0003478942652329749,
      "loss": 7.8164,
      "step": 2038
    },
    {
      "epoch": 0.6091567704832325,
      "grad_norm": 0.3798671364784241,
      "learning_rate": 0.00034781959378733573,
      "loss": 7.6279,
      "step": 2039
    },
    {
      "epoch": 0.6094555231906789,
      "grad_norm": 0.45802244544029236,
      "learning_rate": 0.00034774492234169655,
      "loss": 6.8496,
      "step": 2040
    },
    {
      "epoch": 0.6097542758981254,
      "grad_norm": 0.48261088132858276,
      "learning_rate": 0.00034767025089605736,
      "loss": 6.8955,
      "step": 2041
    },
    {
      "epoch": 0.6100530286055718,
      "grad_norm": 0.48904502391815186,
      "learning_rate": 0.0003475955794504181,
      "loss": 6.7188,
      "step": 2042
    },
    {
      "epoch": 0.6103517813130181,
      "grad_norm": 0.42310455441474915,
      "learning_rate": 0.000347520908004779,
      "loss": 6.9277,
      "step": 2043
    },
    {
      "epoch": 0.6106505340204645,
      "grad_norm": 0.4724598526954651,
      "learning_rate": 0.0003474462365591398,
      "loss": 7.1982,
      "step": 2044
    },
    {
      "epoch": 0.6109492867279109,
      "grad_norm": 0.43471357226371765,
      "learning_rate": 0.0003473715651135006,
      "loss": 7.1201,
      "step": 2045
    },
    {
      "epoch": 0.6112480394353574,
      "grad_norm": 0.4580669105052948,
      "learning_rate": 0.0003472968936678614,
      "loss": 7.2314,
      "step": 2046
    },
    {
      "epoch": 0.6115467921428038,
      "grad_norm": 0.5483874082565308,
      "learning_rate": 0.00034722222222222224,
      "loss": 6.9043,
      "step": 2047
    },
    {
      "epoch": 0.6118455448502502,
      "grad_norm": 0.47355031967163086,
      "learning_rate": 0.00034714755077658305,
      "loss": 7.2646,
      "step": 2048
    },
    {
      "epoch": 0.6121442975576966,
      "grad_norm": 0.47629907727241516,
      "learning_rate": 0.00034707287933094386,
      "loss": 7.1084,
      "step": 2049
    },
    {
      "epoch": 0.612443050265143,
      "grad_norm": 0.564059317111969,
      "learning_rate": 0.0003469982078853047,
      "loss": 6.8633,
      "step": 2050
    },
    {
      "epoch": 0.6127418029725894,
      "grad_norm": 0.4246554672718048,
      "learning_rate": 0.00034692353643966543,
      "loss": 7.249,
      "step": 2051
    },
    {
      "epoch": 0.6130405556800359,
      "grad_norm": 0.434910386800766,
      "learning_rate": 0.0003468488649940263,
      "loss": 7.4707,
      "step": 2052
    },
    {
      "epoch": 0.6133393083874823,
      "grad_norm": 0.5171046257019043,
      "learning_rate": 0.0003467741935483871,
      "loss": 6.5195,
      "step": 2053
    },
    {
      "epoch": 0.6136380610949287,
      "grad_norm": 0.39194217324256897,
      "learning_rate": 0.0003466995221027479,
      "loss": 7.6221,
      "step": 2054
    },
    {
      "epoch": 0.6139368138023751,
      "grad_norm": 0.47705137729644775,
      "learning_rate": 0.00034662485065710874,
      "loss": 6.7734,
      "step": 2055
    },
    {
      "epoch": 0.6142355665098215,
      "grad_norm": 0.4470697045326233,
      "learning_rate": 0.00034655017921146955,
      "loss": 7.3047,
      "step": 2056
    },
    {
      "epoch": 0.6145343192172679,
      "grad_norm": 0.4777156114578247,
      "learning_rate": 0.00034647550776583036,
      "loss": 7.3311,
      "step": 2057
    },
    {
      "epoch": 0.6148330719247144,
      "grad_norm": 0.4419265687465668,
      "learning_rate": 0.0003464008363201911,
      "loss": 7.2295,
      "step": 2058
    },
    {
      "epoch": 0.6151318246321608,
      "grad_norm": 0.4624423086643219,
      "learning_rate": 0.000346326164874552,
      "loss": 7.4375,
      "step": 2059
    },
    {
      "epoch": 0.6154305773396072,
      "grad_norm": 0.5174490809440613,
      "learning_rate": 0.00034625149342891275,
      "loss": 7.0479,
      "step": 2060
    },
    {
      "epoch": 0.6157293300470535,
      "grad_norm": 0.3826654255390167,
      "learning_rate": 0.0003461768219832736,
      "loss": 7.5205,
      "step": 2061
    },
    {
      "epoch": 0.6160280827544999,
      "grad_norm": 0.39932048320770264,
      "learning_rate": 0.00034610215053763443,
      "loss": 7.7939,
      "step": 2062
    },
    {
      "epoch": 0.6163268354619463,
      "grad_norm": 0.4594382047653198,
      "learning_rate": 0.00034602747909199524,
      "loss": 7.1904,
      "step": 2063
    },
    {
      "epoch": 0.6166255881693928,
      "grad_norm": 0.45624232292175293,
      "learning_rate": 0.00034595280764635605,
      "loss": 7.376,
      "step": 2064
    },
    {
      "epoch": 0.6169243408768392,
      "grad_norm": 0.5087748765945435,
      "learning_rate": 0.00034587813620071687,
      "loss": 6.6836,
      "step": 2065
    },
    {
      "epoch": 0.6172230935842856,
      "grad_norm": 0.3723655343055725,
      "learning_rate": 0.0003458034647550777,
      "loss": 7.541,
      "step": 2066
    },
    {
      "epoch": 0.617521846291732,
      "grad_norm": 0.3811057507991791,
      "learning_rate": 0.00034572879330943844,
      "loss": 7.3701,
      "step": 2067
    },
    {
      "epoch": 0.6178205989991784,
      "grad_norm": 0.42805466055870056,
      "learning_rate": 0.0003456541218637993,
      "loss": 7.3721,
      "step": 2068
    },
    {
      "epoch": 0.6181193517066248,
      "grad_norm": 0.41953757405281067,
      "learning_rate": 0.00034557945041816006,
      "loss": 7.5088,
      "step": 2069
    },
    {
      "epoch": 0.6184181044140713,
      "grad_norm": 0.4091579020023346,
      "learning_rate": 0.00034550477897252093,
      "loss": 7.415,
      "step": 2070
    },
    {
      "epoch": 0.6187168571215177,
      "grad_norm": 0.527998149394989,
      "learning_rate": 0.00034543010752688174,
      "loss": 7.0498,
      "step": 2071
    },
    {
      "epoch": 0.6190156098289641,
      "grad_norm": 0.45671477913856506,
      "learning_rate": 0.00034535543608124256,
      "loss": 6.9414,
      "step": 2072
    },
    {
      "epoch": 0.6193143625364105,
      "grad_norm": 0.4809560775756836,
      "learning_rate": 0.00034528076463560337,
      "loss": 6.9199,
      "step": 2073
    },
    {
      "epoch": 0.6196131152438569,
      "grad_norm": 0.4881000518798828,
      "learning_rate": 0.00034520609318996413,
      "loss": 7.3584,
      "step": 2074
    },
    {
      "epoch": 0.6199118679513033,
      "grad_norm": 0.39113742113113403,
      "learning_rate": 0.000345131421744325,
      "loss": 7.3936,
      "step": 2075
    },
    {
      "epoch": 0.6202106206587498,
      "grad_norm": 0.45900076627731323,
      "learning_rate": 0.00034505675029868575,
      "loss": 7.291,
      "step": 2076
    },
    {
      "epoch": 0.6205093733661962,
      "grad_norm": 0.4460841119289398,
      "learning_rate": 0.0003449820788530466,
      "loss": 6.9668,
      "step": 2077
    },
    {
      "epoch": 0.6208081260736426,
      "grad_norm": 0.39025747776031494,
      "learning_rate": 0.0003449074074074074,
      "loss": 7.3535,
      "step": 2078
    },
    {
      "epoch": 0.621106878781089,
      "grad_norm": 0.40401238203048706,
      "learning_rate": 0.00034483273596176825,
      "loss": 7.416,
      "step": 2079
    },
    {
      "epoch": 0.6214056314885353,
      "grad_norm": 0.40984809398651123,
      "learning_rate": 0.00034475806451612906,
      "loss": 7.1104,
      "step": 2080
    },
    {
      "epoch": 0.6217043841959817,
      "grad_norm": 0.44717341661453247,
      "learning_rate": 0.0003446833930704899,
      "loss": 7.249,
      "step": 2081
    },
    {
      "epoch": 0.6220031369034282,
      "grad_norm": 0.4184079170227051,
      "learning_rate": 0.0003446087216248507,
      "loss": 7.166,
      "step": 2082
    },
    {
      "epoch": 0.6223018896108746,
      "grad_norm": 0.4305224120616913,
      "learning_rate": 0.00034453405017921144,
      "loss": 7.6943,
      "step": 2083
    },
    {
      "epoch": 0.622600642318321,
      "grad_norm": 0.4399053752422333,
      "learning_rate": 0.0003444593787335723,
      "loss": 7.25,
      "step": 2084
    },
    {
      "epoch": 0.6228993950257674,
      "grad_norm": 0.452748566865921,
      "learning_rate": 0.00034438470728793307,
      "loss": 7.1523,
      "step": 2085
    },
    {
      "epoch": 0.6231981477332138,
      "grad_norm": 0.41086408495903015,
      "learning_rate": 0.00034431003584229394,
      "loss": 7.1045,
      "step": 2086
    },
    {
      "epoch": 0.6234969004406602,
      "grad_norm": 0.34376055002212524,
      "learning_rate": 0.0003442353643966547,
      "loss": 7.1953,
      "step": 2087
    },
    {
      "epoch": 0.6237956531481067,
      "grad_norm": 0.45218420028686523,
      "learning_rate": 0.00034416069295101556,
      "loss": 7.2939,
      "step": 2088
    },
    {
      "epoch": 0.6240944058555531,
      "grad_norm": 0.39806365966796875,
      "learning_rate": 0.0003440860215053764,
      "loss": 7.2871,
      "step": 2089
    },
    {
      "epoch": 0.6243931585629995,
      "grad_norm": 0.45223894715309143,
      "learning_rate": 0.00034401135005973713,
      "loss": 6.8428,
      "step": 2090
    },
    {
      "epoch": 0.6246919112704459,
      "grad_norm": 0.5384469032287598,
      "learning_rate": 0.000343936678614098,
      "loss": 6.7344,
      "step": 2091
    },
    {
      "epoch": 0.6249906639778923,
      "grad_norm": 0.4737841486930847,
      "learning_rate": 0.00034386200716845876,
      "loss": 6.7236,
      "step": 2092
    },
    {
      "epoch": 0.6252894166853387,
      "grad_norm": 0.48520368337631226,
      "learning_rate": 0.00034378733572281963,
      "loss": 6.8721,
      "step": 2093
    },
    {
      "epoch": 0.6255881693927852,
      "grad_norm": 0.46824580430984497,
      "learning_rate": 0.0003437126642771804,
      "loss": 6.8682,
      "step": 2094
    },
    {
      "epoch": 0.6258869221002316,
      "grad_norm": 0.620871901512146,
      "learning_rate": 0.00034363799283154125,
      "loss": 6.3682,
      "step": 2095
    },
    {
      "epoch": 0.626185674807678,
      "grad_norm": 0.3715682625770569,
      "learning_rate": 0.000343563321385902,
      "loss": 7.5381,
      "step": 2096
    },
    {
      "epoch": 0.6264844275151243,
      "grad_norm": 0.5176779627799988,
      "learning_rate": 0.0003434886499402629,
      "loss": 6.8398,
      "step": 2097
    },
    {
      "epoch": 0.6267831802225707,
      "grad_norm": 0.5451622605323792,
      "learning_rate": 0.00034341397849462364,
      "loss": 6.7578,
      "step": 2098
    },
    {
      "epoch": 0.6270819329300171,
      "grad_norm": 0.4712856113910675,
      "learning_rate": 0.00034333930704898445,
      "loss": 6.9209,
      "step": 2099
    },
    {
      "epoch": 0.6273806856374636,
      "grad_norm": 0.472721666097641,
      "learning_rate": 0.0003432646356033453,
      "loss": 7.125,
      "step": 2100
    },
    {
      "epoch": 0.62767943834491,
      "grad_norm": 0.4423726201057434,
      "learning_rate": 0.0003431899641577061,
      "loss": 7.125,
      "step": 2101
    },
    {
      "epoch": 0.6279781910523564,
      "grad_norm": 0.5140547156333923,
      "learning_rate": 0.00034311529271206694,
      "loss": 7.3516,
      "step": 2102
    },
    {
      "epoch": 0.6282769437598028,
      "grad_norm": 0.4603951573371887,
      "learning_rate": 0.0003430406212664277,
      "loss": 6.6914,
      "step": 2103
    },
    {
      "epoch": 0.6285756964672492,
      "grad_norm": 0.3971847593784332,
      "learning_rate": 0.00034296594982078857,
      "loss": 7.46,
      "step": 2104
    },
    {
      "epoch": 0.6288744491746956,
      "grad_norm": 0.5088855624198914,
      "learning_rate": 0.00034289127837514933,
      "loss": 7.1377,
      "step": 2105
    },
    {
      "epoch": 0.6291732018821421,
      "grad_norm": 0.4055204689502716,
      "learning_rate": 0.00034281660692951014,
      "loss": 7.2725,
      "step": 2106
    },
    {
      "epoch": 0.6294719545895885,
      "grad_norm": 0.4062027931213379,
      "learning_rate": 0.00034274193548387095,
      "loss": 7.4707,
      "step": 2107
    },
    {
      "epoch": 0.6297707072970349,
      "grad_norm": 0.5089662671089172,
      "learning_rate": 0.00034266726403823177,
      "loss": 6.9277,
      "step": 2108
    },
    {
      "epoch": 0.6300694600044813,
      "grad_norm": 0.6759642958641052,
      "learning_rate": 0.00034259259259259263,
      "loss": 6.9385,
      "step": 2109
    },
    {
      "epoch": 0.6303682127119277,
      "grad_norm": 0.4172390103340149,
      "learning_rate": 0.0003425179211469534,
      "loss": 7.126,
      "step": 2110
    },
    {
      "epoch": 0.6306669654193741,
      "grad_norm": 0.39963647723197937,
      "learning_rate": 0.00034244324970131426,
      "loss": 7.2676,
      "step": 2111
    },
    {
      "epoch": 0.6309657181268206,
      "grad_norm": 0.41449639201164246,
      "learning_rate": 0.000342368578255675,
      "loss": 7.0684,
      "step": 2112
    },
    {
      "epoch": 0.631264470834267,
      "grad_norm": 0.47203466296195984,
      "learning_rate": 0.0003422939068100359,
      "loss": 6.7148,
      "step": 2113
    },
    {
      "epoch": 0.6315632235417133,
      "grad_norm": 0.4588898718357086,
      "learning_rate": 0.00034221923536439664,
      "loss": 6.9307,
      "step": 2114
    },
    {
      "epoch": 0.6318619762491597,
      "grad_norm": 0.4197724461555481,
      "learning_rate": 0.00034214456391875746,
      "loss": 7.6553,
      "step": 2115
    },
    {
      "epoch": 0.6321607289566061,
      "grad_norm": 0.4014405906200409,
      "learning_rate": 0.00034206989247311827,
      "loss": 7.4766,
      "step": 2116
    },
    {
      "epoch": 0.6324594816640525,
      "grad_norm": 0.4300366938114166,
      "learning_rate": 0.0003419952210274791,
      "loss": 7.4619,
      "step": 2117
    },
    {
      "epoch": 0.632758234371499,
      "grad_norm": 0.4779386520385742,
      "learning_rate": 0.00034192054958183995,
      "loss": 7.2627,
      "step": 2118
    },
    {
      "epoch": 0.6330569870789454,
      "grad_norm": 0.4852195084095001,
      "learning_rate": 0.0003418458781362007,
      "loss": 7.2383,
      "step": 2119
    },
    {
      "epoch": 0.6333557397863918,
      "grad_norm": 0.4175175726413727,
      "learning_rate": 0.0003417712066905616,
      "loss": 7.2949,
      "step": 2120
    },
    {
      "epoch": 0.6336544924938382,
      "grad_norm": 0.47934794425964355,
      "learning_rate": 0.00034169653524492233,
      "loss": 6.873,
      "step": 2121
    },
    {
      "epoch": 0.6339532452012846,
      "grad_norm": 0.4281105697154999,
      "learning_rate": 0.00034162186379928315,
      "loss": 6.957,
      "step": 2122
    },
    {
      "epoch": 0.6342519979087311,
      "grad_norm": 0.3950578272342682,
      "learning_rate": 0.00034154719235364396,
      "loss": 7.3555,
      "step": 2123
    },
    {
      "epoch": 0.6345507506161775,
      "grad_norm": 0.4739325940608978,
      "learning_rate": 0.00034147252090800477,
      "loss": 7.1025,
      "step": 2124
    },
    {
      "epoch": 0.6348495033236239,
      "grad_norm": 0.3677378296852112,
      "learning_rate": 0.0003413978494623656,
      "loss": 7.584,
      "step": 2125
    },
    {
      "epoch": 0.6351482560310703,
      "grad_norm": 0.6045296788215637,
      "learning_rate": 0.0003413231780167264,
      "loss": 6.8691,
      "step": 2126
    },
    {
      "epoch": 0.6354470087385167,
      "grad_norm": 0.42724984884262085,
      "learning_rate": 0.00034124850657108727,
      "loss": 7.373,
      "step": 2127
    },
    {
      "epoch": 0.6357457614459631,
      "grad_norm": 0.3946216404438019,
      "learning_rate": 0.000341173835125448,
      "loss": 7.3496,
      "step": 2128
    },
    {
      "epoch": 0.6360445141534096,
      "grad_norm": 0.42500245571136475,
      "learning_rate": 0.0003410991636798089,
      "loss": 7.1445,
      "step": 2129
    },
    {
      "epoch": 0.636343266860856,
      "grad_norm": 0.3536836504936218,
      "learning_rate": 0.00034102449223416965,
      "loss": 7.5977,
      "step": 2130
    },
    {
      "epoch": 0.6366420195683024,
      "grad_norm": 0.39288845658302307,
      "learning_rate": 0.00034094982078853046,
      "loss": 7.7109,
      "step": 2131
    },
    {
      "epoch": 0.6369407722757487,
      "grad_norm": 0.46469902992248535,
      "learning_rate": 0.0003408751493428913,
      "loss": 6.75,
      "step": 2132
    },
    {
      "epoch": 0.6372395249831951,
      "grad_norm": 0.44532665610313416,
      "learning_rate": 0.0003408004778972521,
      "loss": 6.7695,
      "step": 2133
    },
    {
      "epoch": 0.6375382776906415,
      "grad_norm": 0.3972446918487549,
      "learning_rate": 0.0003407258064516129,
      "loss": 7.498,
      "step": 2134
    },
    {
      "epoch": 0.637837030398088,
      "grad_norm": 0.36634451150894165,
      "learning_rate": 0.0003406511350059737,
      "loss": 7.6768,
      "step": 2135
    },
    {
      "epoch": 0.6381357831055344,
      "grad_norm": 0.39285457134246826,
      "learning_rate": 0.0003405764635603346,
      "loss": 7.2041,
      "step": 2136
    },
    {
      "epoch": 0.6384345358129808,
      "grad_norm": 0.47193947434425354,
      "learning_rate": 0.00034050179211469534,
      "loss": 7.1025,
      "step": 2137
    },
    {
      "epoch": 0.6387332885204272,
      "grad_norm": 0.413960337638855,
      "learning_rate": 0.00034042712066905615,
      "loss": 7.457,
      "step": 2138
    },
    {
      "epoch": 0.6390320412278736,
      "grad_norm": 0.4530792832374573,
      "learning_rate": 0.00034035244922341697,
      "loss": 7.3213,
      "step": 2139
    },
    {
      "epoch": 0.63933079393532,
      "grad_norm": 0.5082458853721619,
      "learning_rate": 0.0003402777777777778,
      "loss": 6.5977,
      "step": 2140
    },
    {
      "epoch": 0.6396295466427665,
      "grad_norm": 0.5013389587402344,
      "learning_rate": 0.0003402031063321386,
      "loss": 6.6562,
      "step": 2141
    },
    {
      "epoch": 0.6399282993502129,
      "grad_norm": 0.44492608308792114,
      "learning_rate": 0.0003401284348864994,
      "loss": 7.3438,
      "step": 2142
    },
    {
      "epoch": 0.6402270520576593,
      "grad_norm": 0.44961297512054443,
      "learning_rate": 0.0003400537634408602,
      "loss": 7.1621,
      "step": 2143
    },
    {
      "epoch": 0.6405258047651057,
      "grad_norm": 0.5590112805366516,
      "learning_rate": 0.00033997909199522103,
      "loss": 6.6729,
      "step": 2144
    },
    {
      "epoch": 0.6408245574725521,
      "grad_norm": 0.517207145690918,
      "learning_rate": 0.0003399044205495819,
      "loss": 6.4102,
      "step": 2145
    },
    {
      "epoch": 0.6411233101799985,
      "grad_norm": 0.4852057993412018,
      "learning_rate": 0.00033982974910394266,
      "loss": 7.5098,
      "step": 2146
    },
    {
      "epoch": 0.641422062887445,
      "grad_norm": 0.44661614298820496,
      "learning_rate": 0.00033975507765830347,
      "loss": 7.0508,
      "step": 2147
    },
    {
      "epoch": 0.6417208155948914,
      "grad_norm": 0.4711380898952484,
      "learning_rate": 0.0003396804062126643,
      "loss": 7.2158,
      "step": 2148
    },
    {
      "epoch": 0.6420195683023378,
      "grad_norm": 0.4600691795349121,
      "learning_rate": 0.0003396057347670251,
      "loss": 6.8721,
      "step": 2149
    },
    {
      "epoch": 0.6423183210097841,
      "grad_norm": 0.47319719195365906,
      "learning_rate": 0.0003395310633213859,
      "loss": 7.2715,
      "step": 2150
    },
    {
      "epoch": 0.6426170737172305,
      "grad_norm": 0.44456321001052856,
      "learning_rate": 0.0003394563918757467,
      "loss": 7.3291,
      "step": 2151
    },
    {
      "epoch": 0.6429158264246769,
      "grad_norm": 0.37274426221847534,
      "learning_rate": 0.00033938172043010753,
      "loss": 7.5908,
      "step": 2152
    },
    {
      "epoch": 0.6432145791321234,
      "grad_norm": 0.4899711310863495,
      "learning_rate": 0.00033930704898446835,
      "loss": 6.833,
      "step": 2153
    },
    {
      "epoch": 0.6435133318395698,
      "grad_norm": 0.4725845456123352,
      "learning_rate": 0.00033923237753882916,
      "loss": 7.2197,
      "step": 2154
    },
    {
      "epoch": 0.6438120845470162,
      "grad_norm": 0.46014249324798584,
      "learning_rate": 0.00033915770609318997,
      "loss": 7.0195,
      "step": 2155
    },
    {
      "epoch": 0.6441108372544626,
      "grad_norm": 0.5585758090019226,
      "learning_rate": 0.0003390830346475508,
      "loss": 6.9287,
      "step": 2156
    },
    {
      "epoch": 0.644409589961909,
      "grad_norm": 0.4151829779148102,
      "learning_rate": 0.0003390083632019116,
      "loss": 7.1533,
      "step": 2157
    },
    {
      "epoch": 0.6447083426693554,
      "grad_norm": 0.35549914836883545,
      "learning_rate": 0.0003389336917562724,
      "loss": 7.5869,
      "step": 2158
    },
    {
      "epoch": 0.6450070953768019,
      "grad_norm": 0.42318475246429443,
      "learning_rate": 0.0003388590203106332,
      "loss": 7.1055,
      "step": 2159
    },
    {
      "epoch": 0.6453058480842483,
      "grad_norm": 0.3837166428565979,
      "learning_rate": 0.00033878434886499404,
      "loss": 7.7656,
      "step": 2160
    },
    {
      "epoch": 0.6456046007916947,
      "grad_norm": 0.44955724477767944,
      "learning_rate": 0.00033870967741935485,
      "loss": 7.002,
      "step": 2161
    },
    {
      "epoch": 0.6459033534991411,
      "grad_norm": 0.4904034733772278,
      "learning_rate": 0.00033863500597371566,
      "loss": 7.334,
      "step": 2162
    },
    {
      "epoch": 0.6462021062065875,
      "grad_norm": 0.4739896357059479,
      "learning_rate": 0.0003385603345280765,
      "loss": 7.083,
      "step": 2163
    },
    {
      "epoch": 0.6465008589140339,
      "grad_norm": 0.5648462772369385,
      "learning_rate": 0.0003384856630824373,
      "loss": 7.0918,
      "step": 2164
    },
    {
      "epoch": 0.6467996116214804,
      "grad_norm": 0.44775643944740295,
      "learning_rate": 0.0003384109916367981,
      "loss": 7.0352,
      "step": 2165
    },
    {
      "epoch": 0.6470983643289268,
      "grad_norm": 0.5563214421272278,
      "learning_rate": 0.0003383363201911589,
      "loss": 6.9893,
      "step": 2166
    },
    {
      "epoch": 0.6473971170363731,
      "grad_norm": 0.41452687978744507,
      "learning_rate": 0.0003382616487455197,
      "loss": 7.4258,
      "step": 2167
    },
    {
      "epoch": 0.6476958697438195,
      "grad_norm": 0.4101472496986389,
      "learning_rate": 0.00033818697729988054,
      "loss": 7.6484,
      "step": 2168
    },
    {
      "epoch": 0.6479946224512659,
      "grad_norm": 0.3731161653995514,
      "learning_rate": 0.00033811230585424135,
      "loss": 7.5879,
      "step": 2169
    },
    {
      "epoch": 0.6482933751587123,
      "grad_norm": 0.32790154218673706,
      "learning_rate": 0.0003380376344086021,
      "loss": 7.9512,
      "step": 2170
    },
    {
      "epoch": 0.6485921278661588,
      "grad_norm": 0.4366338849067688,
      "learning_rate": 0.000337962962962963,
      "loss": 7.2842,
      "step": 2171
    },
    {
      "epoch": 0.6488908805736052,
      "grad_norm": 0.4190574884414673,
      "learning_rate": 0.0003378882915173238,
      "loss": 7.7344,
      "step": 2172
    },
    {
      "epoch": 0.6491896332810516,
      "grad_norm": 0.3906267285346985,
      "learning_rate": 0.0003378136200716846,
      "loss": 7.3301,
      "step": 2173
    },
    {
      "epoch": 0.649488385988498,
      "grad_norm": 0.4597470760345459,
      "learning_rate": 0.0003377389486260454,
      "loss": 7.0645,
      "step": 2174
    },
    {
      "epoch": 0.6497871386959444,
      "grad_norm": 0.4571896493434906,
      "learning_rate": 0.00033766427718040623,
      "loss": 7.4883,
      "step": 2175
    },
    {
      "epoch": 0.6500858914033908,
      "grad_norm": 0.5124568343162537,
      "learning_rate": 0.00033758960573476704,
      "loss": 7.0654,
      "step": 2176
    },
    {
      "epoch": 0.6503846441108373,
      "grad_norm": 0.4947362244129181,
      "learning_rate": 0.00033751493428912786,
      "loss": 6.9512,
      "step": 2177
    },
    {
      "epoch": 0.6506833968182837,
      "grad_norm": 0.4127599596977234,
      "learning_rate": 0.00033744026284348867,
      "loss": 7.0068,
      "step": 2178
    },
    {
      "epoch": 0.6509821495257301,
      "grad_norm": 0.40988513827323914,
      "learning_rate": 0.0003373655913978494,
      "loss": 7.7256,
      "step": 2179
    },
    {
      "epoch": 0.6512809022331765,
      "grad_norm": 0.35521844029426575,
      "learning_rate": 0.0003372909199522103,
      "loss": 7.7256,
      "step": 2180
    },
    {
      "epoch": 0.6515796549406229,
      "grad_norm": 0.36448994278907776,
      "learning_rate": 0.0003372162485065711,
      "loss": 7.5869,
      "step": 2181
    },
    {
      "epoch": 0.6518784076480693,
      "grad_norm": 0.4453204274177551,
      "learning_rate": 0.0003371415770609319,
      "loss": 7.4033,
      "step": 2182
    },
    {
      "epoch": 0.6521771603555158,
      "grad_norm": 0.5399232506752014,
      "learning_rate": 0.00033706690561529273,
      "loss": 6.6592,
      "step": 2183
    },
    {
      "epoch": 0.6524759130629622,
      "grad_norm": 0.36891448497772217,
      "learning_rate": 0.00033699223416965355,
      "loss": 7.2109,
      "step": 2184
    },
    {
      "epoch": 0.6527746657704085,
      "grad_norm": 0.49829328060150146,
      "learning_rate": 0.00033691756272401436,
      "loss": 7.374,
      "step": 2185
    },
    {
      "epoch": 0.6530734184778549,
      "grad_norm": 0.41165176033973694,
      "learning_rate": 0.0003368428912783751,
      "loss": 7.2637,
      "step": 2186
    },
    {
      "epoch": 0.6533721711853013,
      "grad_norm": 0.5150889158248901,
      "learning_rate": 0.000336768219832736,
      "loss": 7.1406,
      "step": 2187
    },
    {
      "epoch": 0.6536709238927477,
      "grad_norm": 0.39701980352401733,
      "learning_rate": 0.00033669354838709674,
      "loss": 7.5322,
      "step": 2188
    },
    {
      "epoch": 0.6539696766001942,
      "grad_norm": 0.4042402505874634,
      "learning_rate": 0.0003366188769414576,
      "loss": 7.4482,
      "step": 2189
    },
    {
      "epoch": 0.6542684293076406,
      "grad_norm": 0.47516942024230957,
      "learning_rate": 0.00033654420549581837,
      "loss": 6.8633,
      "step": 2190
    },
    {
      "epoch": 0.654567182015087,
      "grad_norm": 0.4496570825576782,
      "learning_rate": 0.00033646953405017924,
      "loss": 7.5576,
      "step": 2191
    },
    {
      "epoch": 0.6548659347225334,
      "grad_norm": 0.4731278419494629,
      "learning_rate": 0.00033639486260454005,
      "loss": 6.7734,
      "step": 2192
    },
    {
      "epoch": 0.6551646874299798,
      "grad_norm": 0.4289707839488983,
      "learning_rate": 0.00033632019115890086,
      "loss": 6.9961,
      "step": 2193
    },
    {
      "epoch": 0.6554634401374263,
      "grad_norm": 0.3687210977077484,
      "learning_rate": 0.0003362455197132617,
      "loss": 7.5,
      "step": 2194
    },
    {
      "epoch": 0.6557621928448727,
      "grad_norm": 0.4201013147830963,
      "learning_rate": 0.00033617084826762243,
      "loss": 7.2832,
      "step": 2195
    },
    {
      "epoch": 0.6560609455523191,
      "grad_norm": 0.3613106310367584,
      "learning_rate": 0.0003360961768219833,
      "loss": 7.4902,
      "step": 2196
    },
    {
      "epoch": 0.6563596982597655,
      "grad_norm": 0.4369504153728485,
      "learning_rate": 0.00033602150537634406,
      "loss": 6.9209,
      "step": 2197
    },
    {
      "epoch": 0.6566584509672119,
      "grad_norm": 0.4470694959163666,
      "learning_rate": 0.0003359468339307049,
      "loss": 6.8975,
      "step": 2198
    },
    {
      "epoch": 0.6569572036746583,
      "grad_norm": 0.4745953679084778,
      "learning_rate": 0.0003358721624850657,
      "loss": 6.9297,
      "step": 2199
    },
    {
      "epoch": 0.6572559563821048,
      "grad_norm": 0.45285284519195557,
      "learning_rate": 0.00033579749103942655,
      "loss": 6.8174,
      "step": 2200
    },
    {
      "epoch": 0.6572559563821048,
      "eval_bleu": 0.12388924107501391,
      "eval_loss": 7.0390625,
      "eval_runtime": 496.0085,
      "eval_samples_per_second": 2.841,
      "eval_steps_per_second": 0.179,
      "step": 2200
    },
    {
      "epoch": 0.6575547090895512,
      "grad_norm": 0.41146430373191833,
      "learning_rate": 0.00033572281959378736,
      "loss": 7.667,
      "step": 2201
    },
    {
      "epoch": 0.6578534617969976,
      "grad_norm": 0.4346786439418793,
      "learning_rate": 0.0003356481481481481,
      "loss": 6.7695,
      "step": 2202
    },
    {
      "epoch": 0.6581522145044439,
      "grad_norm": 0.38666170835494995,
      "learning_rate": 0.000335573476702509,
      "loss": 7.6016,
      "step": 2203
    },
    {
      "epoch": 0.6584509672118903,
      "grad_norm": 0.4174630045890808,
      "learning_rate": 0.00033549880525686975,
      "loss": 7.3994,
      "step": 2204
    },
    {
      "epoch": 0.6587497199193367,
      "grad_norm": 0.526978611946106,
      "learning_rate": 0.0003354241338112306,
      "loss": 6.71,
      "step": 2205
    },
    {
      "epoch": 0.6590484726267832,
      "grad_norm": 0.455472469329834,
      "learning_rate": 0.0003353494623655914,
      "loss": 7.0957,
      "step": 2206
    },
    {
      "epoch": 0.6593472253342296,
      "grad_norm": 0.523656964302063,
      "learning_rate": 0.00033527479091995224,
      "loss": 7.0479,
      "step": 2207
    },
    {
      "epoch": 0.659645978041676,
      "grad_norm": 0.4125361740589142,
      "learning_rate": 0.000335200119474313,
      "loss": 7.4307,
      "step": 2208
    },
    {
      "epoch": 0.6599447307491224,
      "grad_norm": 0.4875010848045349,
      "learning_rate": 0.00033512544802867387,
      "loss": 7.1865,
      "step": 2209
    },
    {
      "epoch": 0.6602434834565688,
      "grad_norm": 0.41761884093284607,
      "learning_rate": 0.0003350507765830347,
      "loss": 7.7979,
      "step": 2210
    },
    {
      "epoch": 0.6605422361640152,
      "grad_norm": 0.5455735921859741,
      "learning_rate": 0.00033497610513739544,
      "loss": 7.0879,
      "step": 2211
    },
    {
      "epoch": 0.6608409888714617,
      "grad_norm": 0.4693220853805542,
      "learning_rate": 0.0003349014336917563,
      "loss": 7.1982,
      "step": 2212
    },
    {
      "epoch": 0.6611397415789081,
      "grad_norm": 0.3399079740047455,
      "learning_rate": 0.00033482676224611706,
      "loss": 7.4287,
      "step": 2213
    },
    {
      "epoch": 0.6614384942863545,
      "grad_norm": 0.4244816303253174,
      "learning_rate": 0.00033475209080047793,
      "loss": 6.9248,
      "step": 2214
    },
    {
      "epoch": 0.6617372469938009,
      "grad_norm": 0.40215277671813965,
      "learning_rate": 0.0003346774193548387,
      "loss": 7.5049,
      "step": 2215
    },
    {
      "epoch": 0.6620359997012473,
      "grad_norm": 0.4298206865787506,
      "learning_rate": 0.00033460274790919956,
      "loss": 7.501,
      "step": 2216
    },
    {
      "epoch": 0.6623347524086937,
      "grad_norm": 0.46214237809181213,
      "learning_rate": 0.0003345280764635603,
      "loss": 7.0186,
      "step": 2217
    },
    {
      "epoch": 0.6626335051161402,
      "grad_norm": 0.3675558269023895,
      "learning_rate": 0.00033445340501792113,
      "loss": 7.376,
      "step": 2218
    },
    {
      "epoch": 0.6629322578235866,
      "grad_norm": 0.4161061942577362,
      "learning_rate": 0.000334378733572282,
      "loss": 7.0186,
      "step": 2219
    },
    {
      "epoch": 0.663231010531033,
      "grad_norm": 0.3941379189491272,
      "learning_rate": 0.00033430406212664275,
      "loss": 7.6777,
      "step": 2220
    },
    {
      "epoch": 0.6635297632384793,
      "grad_norm": 0.37844473123550415,
      "learning_rate": 0.0003342293906810036,
      "loss": 7.6621,
      "step": 2221
    },
    {
      "epoch": 0.6638285159459257,
      "grad_norm": 0.7952476143836975,
      "learning_rate": 0.0003341547192353644,
      "loss": 6.3018,
      "step": 2222
    },
    {
      "epoch": 0.6641272686533721,
      "grad_norm": 0.4313806891441345,
      "learning_rate": 0.00033408004778972525,
      "loss": 7.2344,
      "step": 2223
    },
    {
      "epoch": 0.6644260213608186,
      "grad_norm": 0.5948471426963806,
      "learning_rate": 0.000334005376344086,
      "loss": 6.8926,
      "step": 2224
    },
    {
      "epoch": 0.664724774068265,
      "grad_norm": 0.46765652298927307,
      "learning_rate": 0.0003339307048984469,
      "loss": 7.0469,
      "step": 2225
    },
    {
      "epoch": 0.6650235267757114,
      "grad_norm": 0.42706796526908875,
      "learning_rate": 0.00033385603345280763,
      "loss": 7.4453,
      "step": 2226
    },
    {
      "epoch": 0.6653222794831578,
      "grad_norm": 0.4025874733924866,
      "learning_rate": 0.00033378136200716844,
      "loss": 7.583,
      "step": 2227
    },
    {
      "epoch": 0.6656210321906042,
      "grad_norm": 0.3313933312892914,
      "learning_rate": 0.0003337066905615293,
      "loss": 7.6504,
      "step": 2228
    },
    {
      "epoch": 0.6659197848980506,
      "grad_norm": 0.5039166808128357,
      "learning_rate": 0.00033363201911589007,
      "loss": 6.9053,
      "step": 2229
    },
    {
      "epoch": 0.6662185376054971,
      "grad_norm": 0.3297930359840393,
      "learning_rate": 0.00033355734767025094,
      "loss": 7.6289,
      "step": 2230
    },
    {
      "epoch": 0.6665172903129435,
      "grad_norm": 0.5613890886306763,
      "learning_rate": 0.0003334826762246117,
      "loss": 6.5078,
      "step": 2231
    },
    {
      "epoch": 0.6668160430203899,
      "grad_norm": 0.41774609684944153,
      "learning_rate": 0.00033340800477897256,
      "loss": 7.4199,
      "step": 2232
    },
    {
      "epoch": 0.6671147957278363,
      "grad_norm": 0.4444144070148468,
      "learning_rate": 0.0003333333333333333,
      "loss": 7.0176,
      "step": 2233
    },
    {
      "epoch": 0.6674135484352827,
      "grad_norm": 0.43184515833854675,
      "learning_rate": 0.00033325866188769414,
      "loss": 7.5137,
      "step": 2234
    },
    {
      "epoch": 0.6677123011427291,
      "grad_norm": 0.42623621225357056,
      "learning_rate": 0.00033318399044205495,
      "loss": 7.2949,
      "step": 2235
    },
    {
      "epoch": 0.6680110538501756,
      "grad_norm": 0.4345024824142456,
      "learning_rate": 0.00033310931899641576,
      "loss": 7.458,
      "step": 2236
    },
    {
      "epoch": 0.668309806557622,
      "grad_norm": 0.44360068440437317,
      "learning_rate": 0.00033303464755077663,
      "loss": 6.9219,
      "step": 2237
    },
    {
      "epoch": 0.6686085592650683,
      "grad_norm": 0.44660067558288574,
      "learning_rate": 0.0003329599761051374,
      "loss": 7.126,
      "step": 2238
    },
    {
      "epoch": 0.6689073119725147,
      "grad_norm": 0.5510056614875793,
      "learning_rate": 0.00033288530465949825,
      "loss": 6.4414,
      "step": 2239
    },
    {
      "epoch": 0.6692060646799611,
      "grad_norm": 0.4582526981830597,
      "learning_rate": 0.000332810633213859,
      "loss": 7.2949,
      "step": 2240
    },
    {
      "epoch": 0.6695048173874075,
      "grad_norm": 0.4420579671859741,
      "learning_rate": 0.0003327359617682199,
      "loss": 7.1582,
      "step": 2241
    },
    {
      "epoch": 0.669803570094854,
      "grad_norm": 0.33090347051620483,
      "learning_rate": 0.00033266129032258064,
      "loss": 7.3838,
      "step": 2242
    },
    {
      "epoch": 0.6701023228023004,
      "grad_norm": 0.42087870836257935,
      "learning_rate": 0.00033258661887694145,
      "loss": 7.3945,
      "step": 2243
    },
    {
      "epoch": 0.6704010755097468,
      "grad_norm": 0.3834558427333832,
      "learning_rate": 0.00033251194743130226,
      "loss": 7.5527,
      "step": 2244
    },
    {
      "epoch": 0.6706998282171932,
      "grad_norm": 0.4839649200439453,
      "learning_rate": 0.0003324372759856631,
      "loss": 7.334,
      "step": 2245
    },
    {
      "epoch": 0.6709985809246396,
      "grad_norm": 0.46389368176460266,
      "learning_rate": 0.00033236260454002394,
      "loss": 7.2129,
      "step": 2246
    },
    {
      "epoch": 0.671297333632086,
      "grad_norm": 0.4464530646800995,
      "learning_rate": 0.0003322879330943847,
      "loss": 7.3975,
      "step": 2247
    },
    {
      "epoch": 0.6715960863395325,
      "grad_norm": 0.7395267486572266,
      "learning_rate": 0.00033221326164874557,
      "loss": 6.2852,
      "step": 2248
    },
    {
      "epoch": 0.6718948390469789,
      "grad_norm": 0.43897745013237,
      "learning_rate": 0.00033213859020310633,
      "loss": 7.2666,
      "step": 2249
    },
    {
      "epoch": 0.6721935917544253,
      "grad_norm": 0.4731329083442688,
      "learning_rate": 0.00033206391875746714,
      "loss": 7.458,
      "step": 2250
    },
    {
      "epoch": 0.6724923444618717,
      "grad_norm": 0.43233099579811096,
      "learning_rate": 0.00033198924731182795,
      "loss": 7.7324,
      "step": 2251
    },
    {
      "epoch": 0.6727910971693181,
      "grad_norm": 0.5223110914230347,
      "learning_rate": 0.00033191457586618877,
      "loss": 7.082,
      "step": 2252
    },
    {
      "epoch": 0.6730898498767645,
      "grad_norm": 0.46422120928764343,
      "learning_rate": 0.0003318399044205496,
      "loss": 6.876,
      "step": 2253
    },
    {
      "epoch": 0.673388602584211,
      "grad_norm": 0.6296725869178772,
      "learning_rate": 0.0003317652329749104,
      "loss": 6.8467,
      "step": 2254
    },
    {
      "epoch": 0.6736873552916574,
      "grad_norm": 0.4855220317840576,
      "learning_rate": 0.00033169056152927126,
      "loss": 6.9678,
      "step": 2255
    },
    {
      "epoch": 0.6739861079991037,
      "grad_norm": 0.5214613080024719,
      "learning_rate": 0.000331615890083632,
      "loss": 6.7002,
      "step": 2256
    },
    {
      "epoch": 0.6742848607065501,
      "grad_norm": 0.33485013246536255,
      "learning_rate": 0.0003315412186379929,
      "loss": 7.626,
      "step": 2257
    },
    {
      "epoch": 0.6745836134139965,
      "grad_norm": 0.40367114543914795,
      "learning_rate": 0.00033146654719235364,
      "loss": 7.1787,
      "step": 2258
    },
    {
      "epoch": 0.6748823661214429,
      "grad_norm": 0.4255441427230835,
      "learning_rate": 0.00033139187574671446,
      "loss": 7.3779,
      "step": 2259
    },
    {
      "epoch": 0.6751811188288894,
      "grad_norm": 0.4165725111961365,
      "learning_rate": 0.00033131720430107527,
      "loss": 7.6699,
      "step": 2260
    },
    {
      "epoch": 0.6754798715363358,
      "grad_norm": 0.45154184103012085,
      "learning_rate": 0.0003312425328554361,
      "loss": 6.916,
      "step": 2261
    },
    {
      "epoch": 0.6757786242437822,
      "grad_norm": 0.4173603057861328,
      "learning_rate": 0.0003311678614097969,
      "loss": 7.5664,
      "step": 2262
    },
    {
      "epoch": 0.6760773769512286,
      "grad_norm": 0.43226358294487,
      "learning_rate": 0.0003310931899641577,
      "loss": 7.249,
      "step": 2263
    },
    {
      "epoch": 0.676376129658675,
      "grad_norm": 0.5661295056343079,
      "learning_rate": 0.0003310185185185186,
      "loss": 6.6045,
      "step": 2264
    },
    {
      "epoch": 0.6766748823661214,
      "grad_norm": 0.48062050342559814,
      "learning_rate": 0.00033094384707287933,
      "loss": 7.1172,
      "step": 2265
    },
    {
      "epoch": 0.6769736350735679,
      "grad_norm": 0.42891132831573486,
      "learning_rate": 0.00033086917562724015,
      "loss": 7.2988,
      "step": 2266
    },
    {
      "epoch": 0.6772723877810143,
      "grad_norm": 0.4121207296848297,
      "learning_rate": 0.00033079450418160096,
      "loss": 7.6055,
      "step": 2267
    },
    {
      "epoch": 0.6775711404884607,
      "grad_norm": 0.4660576283931732,
      "learning_rate": 0.0003307198327359618,
      "loss": 7.2969,
      "step": 2268
    },
    {
      "epoch": 0.6778698931959071,
      "grad_norm": 0.41985905170440674,
      "learning_rate": 0.0003306451612903226,
      "loss": 7.6562,
      "step": 2269
    },
    {
      "epoch": 0.6781686459033535,
      "grad_norm": 0.4494779109954834,
      "learning_rate": 0.0003305704898446834,
      "loss": 7.1973,
      "step": 2270
    },
    {
      "epoch": 0.6784673986108,
      "grad_norm": 0.5264768004417419,
      "learning_rate": 0.0003304958183990442,
      "loss": 6.3945,
      "step": 2271
    },
    {
      "epoch": 0.6787661513182464,
      "grad_norm": 0.43962451815605164,
      "learning_rate": 0.000330421146953405,
      "loss": 7.1465,
      "step": 2272
    },
    {
      "epoch": 0.6790649040256928,
      "grad_norm": 0.41957706212997437,
      "learning_rate": 0.0003303464755077659,
      "loss": 7.0488,
      "step": 2273
    },
    {
      "epoch": 0.6793636567331391,
      "grad_norm": 0.46211275458335876,
      "learning_rate": 0.00033027180406212665,
      "loss": 6.9023,
      "step": 2274
    },
    {
      "epoch": 0.6796624094405855,
      "grad_norm": 0.48376476764678955,
      "learning_rate": 0.00033019713261648746,
      "loss": 6.8262,
      "step": 2275
    },
    {
      "epoch": 0.6799611621480319,
      "grad_norm": 0.5042176842689514,
      "learning_rate": 0.0003301224611708483,
      "loss": 7.0615,
      "step": 2276
    },
    {
      "epoch": 0.6802599148554784,
      "grad_norm": 0.44074851274490356,
      "learning_rate": 0.0003300477897252091,
      "loss": 7.709,
      "step": 2277
    },
    {
      "epoch": 0.6805586675629248,
      "grad_norm": 0.34828558564186096,
      "learning_rate": 0.0003299731182795699,
      "loss": 7.2852,
      "step": 2278
    },
    {
      "epoch": 0.6808574202703712,
      "grad_norm": 0.47162148356437683,
      "learning_rate": 0.0003298984468339307,
      "loss": 7.4131,
      "step": 2279
    },
    {
      "epoch": 0.6811561729778176,
      "grad_norm": 0.4479467272758484,
      "learning_rate": 0.00032982377538829153,
      "loss": 6.9365,
      "step": 2280
    },
    {
      "epoch": 0.681454925685264,
      "grad_norm": 0.531181812286377,
      "learning_rate": 0.00032974910394265234,
      "loss": 6.8027,
      "step": 2281
    },
    {
      "epoch": 0.6817536783927104,
      "grad_norm": 0.45834118127822876,
      "learning_rate": 0.0003296744324970131,
      "loss": 7.3936,
      "step": 2282
    },
    {
      "epoch": 0.6820524311001569,
      "grad_norm": 0.5532738566398621,
      "learning_rate": 0.00032959976105137397,
      "loss": 6.7812,
      "step": 2283
    },
    {
      "epoch": 0.6823511838076033,
      "grad_norm": 0.419710248708725,
      "learning_rate": 0.0003295250896057348,
      "loss": 7.3916,
      "step": 2284
    },
    {
      "epoch": 0.6826499365150497,
      "grad_norm": 0.4605238139629364,
      "learning_rate": 0.0003294504181600956,
      "loss": 7.3506,
      "step": 2285
    },
    {
      "epoch": 0.6829486892224961,
      "grad_norm": 0.5316377878189087,
      "learning_rate": 0.0003293757467144564,
      "loss": 7.1064,
      "step": 2286
    },
    {
      "epoch": 0.6832474419299425,
      "grad_norm": 0.42661434412002563,
      "learning_rate": 0.0003293010752688172,
      "loss": 7.0654,
      "step": 2287
    },
    {
      "epoch": 0.6835461946373889,
      "grad_norm": 0.5515834093093872,
      "learning_rate": 0.00032922640382317803,
      "loss": 7.1719,
      "step": 2288
    },
    {
      "epoch": 0.6838449473448354,
      "grad_norm": 0.41337522864341736,
      "learning_rate": 0.0003291517323775388,
      "loss": 7.2812,
      "step": 2289
    },
    {
      "epoch": 0.6841437000522818,
      "grad_norm": 0.43887215852737427,
      "learning_rate": 0.00032907706093189966,
      "loss": 7.0967,
      "step": 2290
    },
    {
      "epoch": 0.6844424527597281,
      "grad_norm": 0.5423800349235535,
      "learning_rate": 0.0003290023894862604,
      "loss": 7.4141,
      "step": 2291
    },
    {
      "epoch": 0.6847412054671745,
      "grad_norm": 0.41131070256233215,
      "learning_rate": 0.0003289277180406213,
      "loss": 7.6592,
      "step": 2292
    },
    {
      "epoch": 0.6850399581746209,
      "grad_norm": 0.4951474070549011,
      "learning_rate": 0.0003288530465949821,
      "loss": 7.1016,
      "step": 2293
    },
    {
      "epoch": 0.6853387108820673,
      "grad_norm": 0.44109201431274414,
      "learning_rate": 0.0003287783751493429,
      "loss": 7.2412,
      "step": 2294
    },
    {
      "epoch": 0.6856374635895138,
      "grad_norm": 0.42430442571640015,
      "learning_rate": 0.0003287037037037037,
      "loss": 7.2715,
      "step": 2295
    },
    {
      "epoch": 0.6859362162969602,
      "grad_norm": 0.47309762239456177,
      "learning_rate": 0.00032862903225806453,
      "loss": 7.5703,
      "step": 2296
    },
    {
      "epoch": 0.6862349690044066,
      "grad_norm": 0.4303103983402252,
      "learning_rate": 0.00032855436081242535,
      "loss": 7.6475,
      "step": 2297
    },
    {
      "epoch": 0.686533721711853,
      "grad_norm": 0.5226557850837708,
      "learning_rate": 0.0003284796893667861,
      "loss": 7.2529,
      "step": 2298
    },
    {
      "epoch": 0.6868324744192994,
      "grad_norm": 0.5142161250114441,
      "learning_rate": 0.00032840501792114697,
      "loss": 7.0645,
      "step": 2299
    },
    {
      "epoch": 0.6871312271267458,
      "grad_norm": 0.4557580053806305,
      "learning_rate": 0.00032833034647550773,
      "loss": 6.9824,
      "step": 2300
    },
    {
      "epoch": 0.6874299798341923,
      "grad_norm": 0.45492565631866455,
      "learning_rate": 0.0003282556750298686,
      "loss": 7.207,
      "step": 2301
    },
    {
      "epoch": 0.6877287325416387,
      "grad_norm": 0.38909226655960083,
      "learning_rate": 0.0003281810035842294,
      "loss": 7.374,
      "step": 2302
    },
    {
      "epoch": 0.6880274852490851,
      "grad_norm": 0.44891121983528137,
      "learning_rate": 0.0003281063321385902,
      "loss": 6.9893,
      "step": 2303
    },
    {
      "epoch": 0.6883262379565315,
      "grad_norm": 0.42567679286003113,
      "learning_rate": 0.00032803166069295104,
      "loss": 7.873,
      "step": 2304
    },
    {
      "epoch": 0.6886249906639779,
      "grad_norm": 0.4675540030002594,
      "learning_rate": 0.0003279569892473118,
      "loss": 7.5527,
      "step": 2305
    },
    {
      "epoch": 0.6889237433714243,
      "grad_norm": 0.48328596353530884,
      "learning_rate": 0.00032788231780167266,
      "loss": 6.5781,
      "step": 2306
    },
    {
      "epoch": 0.6892224960788708,
      "grad_norm": 0.5348363518714905,
      "learning_rate": 0.0003278076463560334,
      "loss": 6.6523,
      "step": 2307
    },
    {
      "epoch": 0.6895212487863172,
      "grad_norm": 0.3715707063674927,
      "learning_rate": 0.0003277329749103943,
      "loss": 7.2246,
      "step": 2308
    },
    {
      "epoch": 0.6898200014937635,
      "grad_norm": 0.4376160502433777,
      "learning_rate": 0.00032765830346475505,
      "loss": 7.4795,
      "step": 2309
    },
    {
      "epoch": 0.6901187542012099,
      "grad_norm": 0.4492291510105133,
      "learning_rate": 0.0003275836320191159,
      "loss": 6.999,
      "step": 2310
    },
    {
      "epoch": 0.6904175069086563,
      "grad_norm": 0.46688809990882874,
      "learning_rate": 0.0003275089605734767,
      "loss": 7.1123,
      "step": 2311
    },
    {
      "epoch": 0.6907162596161027,
      "grad_norm": 0.45158523321151733,
      "learning_rate": 0.00032743428912783754,
      "loss": 7.0176,
      "step": 2312
    },
    {
      "epoch": 0.6910150123235492,
      "grad_norm": 0.5595265030860901,
      "learning_rate": 0.00032735961768219835,
      "loss": 7.2148,
      "step": 2313
    },
    {
      "epoch": 0.6913137650309956,
      "grad_norm": 0.43572932481765747,
      "learning_rate": 0.0003272849462365591,
      "loss": 6.8691,
      "step": 2314
    },
    {
      "epoch": 0.691612517738442,
      "grad_norm": 0.5072075128555298,
      "learning_rate": 0.00032721027479092,
      "loss": 7.2549,
      "step": 2315
    },
    {
      "epoch": 0.6919112704458884,
      "grad_norm": 0.4777512550354004,
      "learning_rate": 0.00032713560334528074,
      "loss": 7.0898,
      "step": 2316
    },
    {
      "epoch": 0.6922100231533348,
      "grad_norm": 0.4700126051902771,
      "learning_rate": 0.0003270609318996416,
      "loss": 7.2256,
      "step": 2317
    },
    {
      "epoch": 0.6925087758607812,
      "grad_norm": 0.42314743995666504,
      "learning_rate": 0.00032698626045400236,
      "loss": 7.2891,
      "step": 2318
    },
    {
      "epoch": 0.6928075285682277,
      "grad_norm": 0.5129676461219788,
      "learning_rate": 0.00032691158900836323,
      "loss": 6.9561,
      "step": 2319
    },
    {
      "epoch": 0.6931062812756741,
      "grad_norm": 0.41738370060920715,
      "learning_rate": 0.00032683691756272404,
      "loss": 7.168,
      "step": 2320
    },
    {
      "epoch": 0.6934050339831205,
      "grad_norm": 0.42260679602622986,
      "learning_rate": 0.0003267622461170848,
      "loss": 7.5664,
      "step": 2321
    },
    {
      "epoch": 0.6937037866905669,
      "grad_norm": 0.4150981605052948,
      "learning_rate": 0.00032668757467144567,
      "loss": 7.6338,
      "step": 2322
    },
    {
      "epoch": 0.6940025393980133,
      "grad_norm": 0.4891993999481201,
      "learning_rate": 0.0003266129032258064,
      "loss": 7.0576,
      "step": 2323
    },
    {
      "epoch": 0.6943012921054597,
      "grad_norm": 0.4638919234275818,
      "learning_rate": 0.0003265382317801673,
      "loss": 7.1113,
      "step": 2324
    },
    {
      "epoch": 0.6946000448129062,
      "grad_norm": 0.4527246654033661,
      "learning_rate": 0.00032646356033452805,
      "loss": 6.9375,
      "step": 2325
    },
    {
      "epoch": 0.6948987975203526,
      "grad_norm": 0.45414239168167114,
      "learning_rate": 0.0003263888888888889,
      "loss": 7.0449,
      "step": 2326
    },
    {
      "epoch": 0.6951975502277989,
      "grad_norm": 0.372118204832077,
      "learning_rate": 0.0003263142174432497,
      "loss": 7.1436,
      "step": 2327
    },
    {
      "epoch": 0.6954963029352453,
      "grad_norm": 0.448059618473053,
      "learning_rate": 0.00032623954599761055,
      "loss": 7.5947,
      "step": 2328
    },
    {
      "epoch": 0.6957950556426917,
      "grad_norm": 0.4699782133102417,
      "learning_rate": 0.00032616487455197136,
      "loss": 7.1992,
      "step": 2329
    },
    {
      "epoch": 0.6960938083501381,
      "grad_norm": 0.3877904713153839,
      "learning_rate": 0.0003260902031063321,
      "loss": 7.4268,
      "step": 2330
    },
    {
      "epoch": 0.6963925610575846,
      "grad_norm": 0.5107614994049072,
      "learning_rate": 0.000326015531660693,
      "loss": 7.1924,
      "step": 2331
    },
    {
      "epoch": 0.696691313765031,
      "grad_norm": 0.48150110244750977,
      "learning_rate": 0.00032594086021505374,
      "loss": 6.999,
      "step": 2332
    },
    {
      "epoch": 0.6969900664724774,
      "grad_norm": 0.4664025604724884,
      "learning_rate": 0.0003258661887694146,
      "loss": 7.2266,
      "step": 2333
    },
    {
      "epoch": 0.6972888191799238,
      "grad_norm": 0.41135287284851074,
      "learning_rate": 0.00032579151732377537,
      "loss": 7.1611,
      "step": 2334
    },
    {
      "epoch": 0.6975875718873702,
      "grad_norm": 0.41081783175468445,
      "learning_rate": 0.00032571684587813624,
      "loss": 7.4717,
      "step": 2335
    },
    {
      "epoch": 0.6978863245948166,
      "grad_norm": 0.46510758996009827,
      "learning_rate": 0.000325642174432497,
      "loss": 7.4004,
      "step": 2336
    },
    {
      "epoch": 0.6981850773022631,
      "grad_norm": 0.5885675549507141,
      "learning_rate": 0.0003255675029868578,
      "loss": 6.6699,
      "step": 2337
    },
    {
      "epoch": 0.6984838300097095,
      "grad_norm": 0.47235241532325745,
      "learning_rate": 0.0003254928315412187,
      "loss": 6.9668,
      "step": 2338
    },
    {
      "epoch": 0.6987825827171559,
      "grad_norm": 0.46813297271728516,
      "learning_rate": 0.00032541816009557943,
      "loss": 6.8018,
      "step": 2339
    },
    {
      "epoch": 0.6990813354246023,
      "grad_norm": 0.41811054944992065,
      "learning_rate": 0.0003253434886499403,
      "loss": 7.0811,
      "step": 2340
    },
    {
      "epoch": 0.6993800881320487,
      "grad_norm": 0.34549087285995483,
      "learning_rate": 0.00032526881720430106,
      "loss": 7.4727,
      "step": 2341
    },
    {
      "epoch": 0.699678840839495,
      "grad_norm": 0.4412761628627777,
      "learning_rate": 0.0003251941457586619,
      "loss": 7.1807,
      "step": 2342
    },
    {
      "epoch": 0.6999775935469416,
      "grad_norm": 0.5636464953422546,
      "learning_rate": 0.0003251194743130227,
      "loss": 6.9385,
      "step": 2343
    },
    {
      "epoch": 0.700276346254388,
      "grad_norm": 0.5048204660415649,
      "learning_rate": 0.00032504480286738355,
      "loss": 6.8926,
      "step": 2344
    },
    {
      "epoch": 0.7005750989618343,
      "grad_norm": 0.5478748083114624,
      "learning_rate": 0.0003249701314217443,
      "loss": 6.877,
      "step": 2345
    },
    {
      "epoch": 0.7008738516692807,
      "grad_norm": 0.38074222207069397,
      "learning_rate": 0.0003248954599761051,
      "loss": 7.5977,
      "step": 2346
    },
    {
      "epoch": 0.7011726043767271,
      "grad_norm": 0.38500332832336426,
      "learning_rate": 0.000324820788530466,
      "loss": 7.3223,
      "step": 2347
    },
    {
      "epoch": 0.7014713570841736,
      "grad_norm": 0.46153831481933594,
      "learning_rate": 0.00032474611708482675,
      "loss": 7.2588,
      "step": 2348
    },
    {
      "epoch": 0.70177010979162,
      "grad_norm": 0.48554718494415283,
      "learning_rate": 0.0003246714456391876,
      "loss": 6.8662,
      "step": 2349
    },
    {
      "epoch": 0.7020688624990664,
      "grad_norm": 0.4290994703769684,
      "learning_rate": 0.0003245967741935484,
      "loss": 7.2188,
      "step": 2350
    },
    {
      "epoch": 0.7023676152065128,
      "grad_norm": 0.4313277006149292,
      "learning_rate": 0.00032452210274790924,
      "loss": 7.1641,
      "step": 2351
    },
    {
      "epoch": 0.7026663679139592,
      "grad_norm": 0.4559766352176666,
      "learning_rate": 0.00032444743130227,
      "loss": 7.4121,
      "step": 2352
    },
    {
      "epoch": 0.7029651206214056,
      "grad_norm": 0.4376521408557892,
      "learning_rate": 0.0003243727598566308,
      "loss": 7.3926,
      "step": 2353
    },
    {
      "epoch": 0.7032638733288521,
      "grad_norm": 0.6363664865493774,
      "learning_rate": 0.0003242980884109916,
      "loss": 6.8691,
      "step": 2354
    },
    {
      "epoch": 0.7035626260362985,
      "grad_norm": 0.39710986614227295,
      "learning_rate": 0.00032422341696535244,
      "loss": 7.582,
      "step": 2355
    },
    {
      "epoch": 0.7038613787437449,
      "grad_norm": 0.4773787558078766,
      "learning_rate": 0.0003241487455197133,
      "loss": 6.8672,
      "step": 2356
    },
    {
      "epoch": 0.7041601314511913,
      "grad_norm": 0.4590000808238983,
      "learning_rate": 0.00032407407407407406,
      "loss": 7.5762,
      "step": 2357
    },
    {
      "epoch": 0.7044588841586377,
      "grad_norm": 0.5175185799598694,
      "learning_rate": 0.00032399940262843493,
      "loss": 7.3682,
      "step": 2358
    },
    {
      "epoch": 0.7047576368660841,
      "grad_norm": 0.5100979208946228,
      "learning_rate": 0.0003239247311827957,
      "loss": 6.9502,
      "step": 2359
    },
    {
      "epoch": 0.7050563895735306,
      "grad_norm": 0.4798610806465149,
      "learning_rate": 0.00032385005973715656,
      "loss": 7.1162,
      "step": 2360
    },
    {
      "epoch": 0.705355142280977,
      "grad_norm": 0.4403097629547119,
      "learning_rate": 0.0003237753882915173,
      "loss": 7.2246,
      "step": 2361
    },
    {
      "epoch": 0.7056538949884233,
      "grad_norm": 0.40754398703575134,
      "learning_rate": 0.00032370071684587813,
      "loss": 7.5137,
      "step": 2362
    },
    {
      "epoch": 0.7059526476958697,
      "grad_norm": 0.5024256706237793,
      "learning_rate": 0.00032362604540023894,
      "loss": 7.208,
      "step": 2363
    },
    {
      "epoch": 0.7062514004033161,
      "grad_norm": 0.5661485195159912,
      "learning_rate": 0.00032355137395459975,
      "loss": 6.999,
      "step": 2364
    },
    {
      "epoch": 0.7065501531107625,
      "grad_norm": 0.47665080428123474,
      "learning_rate": 0.0003234767025089606,
      "loss": 6.9697,
      "step": 2365
    },
    {
      "epoch": 0.706848905818209,
      "grad_norm": 0.43246152997016907,
      "learning_rate": 0.0003234020310633214,
      "loss": 7.5342,
      "step": 2366
    },
    {
      "epoch": 0.7071476585256554,
      "grad_norm": 0.40759214758872986,
      "learning_rate": 0.00032332735961768225,
      "loss": 7.415,
      "step": 2367
    },
    {
      "epoch": 0.7074464112331018,
      "grad_norm": 0.45302096009254456,
      "learning_rate": 0.000323252688172043,
      "loss": 7.5381,
      "step": 2368
    },
    {
      "epoch": 0.7077451639405482,
      "grad_norm": 1.3652870655059814,
      "learning_rate": 0.0003231780167264038,
      "loss": 7.6982,
      "step": 2369
    },
    {
      "epoch": 0.7080439166479946,
      "grad_norm": 0.4815582036972046,
      "learning_rate": 0.00032310334528076463,
      "loss": 7.1963,
      "step": 2370
    },
    {
      "epoch": 0.708342669355441,
      "grad_norm": 0.4374658465385437,
      "learning_rate": 0.00032302867383512545,
      "loss": 7.2607,
      "step": 2371
    },
    {
      "epoch": 0.7086414220628875,
      "grad_norm": 0.3847760558128357,
      "learning_rate": 0.00032295400238948626,
      "loss": 7.958,
      "step": 2372
    },
    {
      "epoch": 0.7089401747703339,
      "grad_norm": 0.4812886714935303,
      "learning_rate": 0.00032287933094384707,
      "loss": 7.4033,
      "step": 2373
    },
    {
      "epoch": 0.7092389274777803,
      "grad_norm": 0.48059797286987305,
      "learning_rate": 0.0003228046594982079,
      "loss": 6.9453,
      "step": 2374
    },
    {
      "epoch": 0.7095376801852267,
      "grad_norm": 0.45562270283699036,
      "learning_rate": 0.0003227299880525687,
      "loss": 7.333,
      "step": 2375
    },
    {
      "epoch": 0.7098364328926731,
      "grad_norm": 0.5545848608016968,
      "learning_rate": 0.00032265531660692956,
      "loss": 6.9014,
      "step": 2376
    },
    {
      "epoch": 0.7101351856001195,
      "grad_norm": 0.48126229643821716,
      "learning_rate": 0.0003225806451612903,
      "loss": 7.0469,
      "step": 2377
    },
    {
      "epoch": 0.710433938307566,
      "grad_norm": 0.45711424946784973,
      "learning_rate": 0.00032250597371565114,
      "loss": 7.332,
      "step": 2378
    },
    {
      "epoch": 0.7107326910150124,
      "grad_norm": 0.5172871947288513,
      "learning_rate": 0.00032243130227001195,
      "loss": 6.9512,
      "step": 2379
    },
    {
      "epoch": 0.7110314437224587,
      "grad_norm": 0.40454262495040894,
      "learning_rate": 0.00032235663082437276,
      "loss": 7.2246,
      "step": 2380
    },
    {
      "epoch": 0.7113301964299051,
      "grad_norm": 0.3665471374988556,
      "learning_rate": 0.0003222819593787336,
      "loss": 7.3477,
      "step": 2381
    },
    {
      "epoch": 0.7116289491373515,
      "grad_norm": 0.4177646040916443,
      "learning_rate": 0.0003222072879330944,
      "loss": 7.4238,
      "step": 2382
    },
    {
      "epoch": 0.7119277018447979,
      "grad_norm": 0.5072565674781799,
      "learning_rate": 0.0003221326164874552,
      "loss": 7.2207,
      "step": 2383
    },
    {
      "epoch": 0.7122264545522444,
      "grad_norm": 0.49821940064430237,
      "learning_rate": 0.000322057945041816,
      "loss": 7.3643,
      "step": 2384
    },
    {
      "epoch": 0.7125252072596908,
      "grad_norm": 0.6615934371948242,
      "learning_rate": 0.0003219832735961768,
      "loss": 6.8311,
      "step": 2385
    },
    {
      "epoch": 0.7128239599671372,
      "grad_norm": 0.448798269033432,
      "learning_rate": 0.00032190860215053764,
      "loss": 7.5176,
      "step": 2386
    },
    {
      "epoch": 0.7131227126745836,
      "grad_norm": 0.4208223819732666,
      "learning_rate": 0.00032183393070489845,
      "loss": 7.3389,
      "step": 2387
    },
    {
      "epoch": 0.71342146538203,
      "grad_norm": 0.4709113836288452,
      "learning_rate": 0.00032175925925925926,
      "loss": 7.7021,
      "step": 2388
    },
    {
      "epoch": 0.7137202180894764,
      "grad_norm": 0.42858245968818665,
      "learning_rate": 0.0003216845878136201,
      "loss": 7.1709,
      "step": 2389
    },
    {
      "epoch": 0.7140189707969229,
      "grad_norm": 0.5011472105979919,
      "learning_rate": 0.0003216099163679809,
      "loss": 6.8809,
      "step": 2390
    },
    {
      "epoch": 0.7143177235043693,
      "grad_norm": 0.4309542179107666,
      "learning_rate": 0.0003215352449223417,
      "loss": 7.2129,
      "step": 2391
    },
    {
      "epoch": 0.7146164762118157,
      "grad_norm": 0.49168092012405396,
      "learning_rate": 0.0003214605734767025,
      "loss": 7.1221,
      "step": 2392
    },
    {
      "epoch": 0.7149152289192621,
      "grad_norm": 0.44288596510887146,
      "learning_rate": 0.00032138590203106333,
      "loss": 7.0635,
      "step": 2393
    },
    {
      "epoch": 0.7152139816267085,
      "grad_norm": 0.5560464859008789,
      "learning_rate": 0.00032131123058542414,
      "loss": 7.2324,
      "step": 2394
    },
    {
      "epoch": 0.7155127343341549,
      "grad_norm": 0.415097713470459,
      "learning_rate": 0.00032123655913978495,
      "loss": 7.2705,
      "step": 2395
    },
    {
      "epoch": 0.7158114870416014,
      "grad_norm": 0.416255384683609,
      "learning_rate": 0.00032116188769414577,
      "loss": 7.8525,
      "step": 2396
    },
    {
      "epoch": 0.7161102397490478,
      "grad_norm": 0.5596199631690979,
      "learning_rate": 0.0003210872162485066,
      "loss": 6.8857,
      "step": 2397
    },
    {
      "epoch": 0.7164089924564941,
      "grad_norm": 0.44279420375823975,
      "learning_rate": 0.0003210125448028674,
      "loss": 7.0566,
      "step": 2398
    },
    {
      "epoch": 0.7167077451639405,
      "grad_norm": 0.4344627261161804,
      "learning_rate": 0.0003209378733572282,
      "loss": 6.8496,
      "step": 2399
    },
    {
      "epoch": 0.7170064978713869,
      "grad_norm": 0.43424275517463684,
      "learning_rate": 0.000320863201911589,
      "loss": 6.9873,
      "step": 2400
    },
    {
      "epoch": 0.7170064978713869,
      "eval_bleu": 0.12044337736274024,
      "eval_loss": 7.02734375,
      "eval_runtime": 527.3912,
      "eval_samples_per_second": 2.672,
      "eval_steps_per_second": 0.169,
      "step": 2400
    },
    {
      "epoch": 0.7173052505788333,
      "grad_norm": 0.46290725469589233,
      "learning_rate": 0.0003207885304659498,
      "loss": 7.1064,
      "step": 2401
    },
    {
      "epoch": 0.7176040032862798,
      "grad_norm": 0.4877030849456787,
      "learning_rate": 0.00032071385902031064,
      "loss": 6.5381,
      "step": 2402
    },
    {
      "epoch": 0.7179027559937262,
      "grad_norm": 0.4038798213005066,
      "learning_rate": 0.00032063918757467146,
      "loss": 7.6211,
      "step": 2403
    },
    {
      "epoch": 0.7182015087011726,
      "grad_norm": 0.38976654410362244,
      "learning_rate": 0.00032056451612903227,
      "loss": 7.4199,
      "step": 2404
    },
    {
      "epoch": 0.718500261408619,
      "grad_norm": 0.4783181846141815,
      "learning_rate": 0.0003204898446833931,
      "loss": 7.0615,
      "step": 2405
    },
    {
      "epoch": 0.7187990141160654,
      "grad_norm": 0.3900178372859955,
      "learning_rate": 0.0003204151732377539,
      "loss": 7.6885,
      "step": 2406
    },
    {
      "epoch": 0.7190977668235118,
      "grad_norm": 0.4774911105632782,
      "learning_rate": 0.0003203405017921147,
      "loss": 7.2266,
      "step": 2407
    },
    {
      "epoch": 0.7193965195309583,
      "grad_norm": 0.4900384247303009,
      "learning_rate": 0.0003202658303464755,
      "loss": 6.9688,
      "step": 2408
    },
    {
      "epoch": 0.7196952722384047,
      "grad_norm": 0.5346188545227051,
      "learning_rate": 0.00032019115890083633,
      "loss": 6.6016,
      "step": 2409
    },
    {
      "epoch": 0.7199940249458511,
      "grad_norm": 0.47767165303230286,
      "learning_rate": 0.0003201164874551971,
      "loss": 7.0977,
      "step": 2410
    },
    {
      "epoch": 0.7202927776532975,
      "grad_norm": 0.49319377541542053,
      "learning_rate": 0.00032004181600955796,
      "loss": 7.1299,
      "step": 2411
    },
    {
      "epoch": 0.7205915303607439,
      "grad_norm": 0.5975245833396912,
      "learning_rate": 0.0003199671445639188,
      "loss": 6.7344,
      "step": 2412
    },
    {
      "epoch": 0.7208902830681903,
      "grad_norm": 0.3910267949104309,
      "learning_rate": 0.0003198924731182796,
      "loss": 7.4678,
      "step": 2413
    },
    {
      "epoch": 0.7211890357756368,
      "grad_norm": 0.40932318568229675,
      "learning_rate": 0.0003198178016726404,
      "loss": 7.6377,
      "step": 2414
    },
    {
      "epoch": 0.7214877884830831,
      "grad_norm": 0.461089551448822,
      "learning_rate": 0.0003197431302270012,
      "loss": 7.3018,
      "step": 2415
    },
    {
      "epoch": 0.7217865411905295,
      "grad_norm": 0.4952481985092163,
      "learning_rate": 0.000319668458781362,
      "loss": 6.8848,
      "step": 2416
    },
    {
      "epoch": 0.7220852938979759,
      "grad_norm": 0.38666918873786926,
      "learning_rate": 0.0003195937873357228,
      "loss": 7.4414,
      "step": 2417
    },
    {
      "epoch": 0.7223840466054223,
      "grad_norm": 0.39663809537887573,
      "learning_rate": 0.00031951911589008365,
      "loss": 7.4365,
      "step": 2418
    },
    {
      "epoch": 0.7226827993128687,
      "grad_norm": 0.5152637958526611,
      "learning_rate": 0.0003194444444444444,
      "loss": 6.3613,
      "step": 2419
    },
    {
      "epoch": 0.7229815520203152,
      "grad_norm": 0.6006479263305664,
      "learning_rate": 0.0003193697729988053,
      "loss": 6.6133,
      "step": 2420
    },
    {
      "epoch": 0.7232803047277616,
      "grad_norm": 0.425362229347229,
      "learning_rate": 0.0003192951015531661,
      "loss": 7.3555,
      "step": 2421
    },
    {
      "epoch": 0.723579057435208,
      "grad_norm": 0.4312905967235565,
      "learning_rate": 0.0003192204301075269,
      "loss": 7.335,
      "step": 2422
    },
    {
      "epoch": 0.7238778101426544,
      "grad_norm": 0.49318262934684753,
      "learning_rate": 0.0003191457586618877,
      "loss": 6.7373,
      "step": 2423
    },
    {
      "epoch": 0.7241765628501008,
      "grad_norm": 0.47769448161125183,
      "learning_rate": 0.00031907108721624853,
      "loss": 7.125,
      "step": 2424
    },
    {
      "epoch": 0.7244753155575473,
      "grad_norm": 0.42834988236427307,
      "learning_rate": 0.00031899641577060934,
      "loss": 7.3789,
      "step": 2425
    },
    {
      "epoch": 0.7247740682649937,
      "grad_norm": 0.3709925413131714,
      "learning_rate": 0.0003189217443249701,
      "loss": 7.5996,
      "step": 2426
    },
    {
      "epoch": 0.7250728209724401,
      "grad_norm": 0.4134168326854706,
      "learning_rate": 0.00031884707287933097,
      "loss": 7.3125,
      "step": 2427
    },
    {
      "epoch": 0.7253715736798865,
      "grad_norm": 0.4682566821575165,
      "learning_rate": 0.0003187724014336917,
      "loss": 7.1494,
      "step": 2428
    },
    {
      "epoch": 0.7256703263873329,
      "grad_norm": 0.42355090379714966,
      "learning_rate": 0.0003186977299880526,
      "loss": 7.2285,
      "step": 2429
    },
    {
      "epoch": 0.7259690790947793,
      "grad_norm": 0.4522647261619568,
      "learning_rate": 0.0003186230585424134,
      "loss": 7.0391,
      "step": 2430
    },
    {
      "epoch": 0.7262678318022258,
      "grad_norm": 0.4146944582462311,
      "learning_rate": 0.0003185483870967742,
      "loss": 7.2285,
      "step": 2431
    },
    {
      "epoch": 0.7265665845096722,
      "grad_norm": 0.4809489846229553,
      "learning_rate": 0.00031847371565113503,
      "loss": 7.4912,
      "step": 2432
    },
    {
      "epoch": 0.7268653372171185,
      "grad_norm": 0.47067683935165405,
      "learning_rate": 0.0003183990442054958,
      "loss": 7.498,
      "step": 2433
    },
    {
      "epoch": 0.7271640899245649,
      "grad_norm": 0.5306682586669922,
      "learning_rate": 0.00031832437275985666,
      "loss": 7.1553,
      "step": 2434
    },
    {
      "epoch": 0.7274628426320113,
      "grad_norm": 0.45483097434043884,
      "learning_rate": 0.0003182497013142174,
      "loss": 7.373,
      "step": 2435
    },
    {
      "epoch": 0.7277615953394577,
      "grad_norm": 0.49629995226860046,
      "learning_rate": 0.0003181750298685783,
      "loss": 7.0811,
      "step": 2436
    },
    {
      "epoch": 0.7280603480469042,
      "grad_norm": 0.5409778952598572,
      "learning_rate": 0.00031810035842293904,
      "loss": 7.208,
      "step": 2437
    },
    {
      "epoch": 0.7283591007543506,
      "grad_norm": 0.4401698112487793,
      "learning_rate": 0.0003180256869772999,
      "loss": 6.7217,
      "step": 2438
    },
    {
      "epoch": 0.728657853461797,
      "grad_norm": 0.6192630529403687,
      "learning_rate": 0.0003179510155316607,
      "loss": 6.3184,
      "step": 2439
    },
    {
      "epoch": 0.7289566061692434,
      "grad_norm": 0.5017337799072266,
      "learning_rate": 0.00031787634408602153,
      "loss": 7.209,
      "step": 2440
    },
    {
      "epoch": 0.7292553588766898,
      "grad_norm": 0.5518531799316406,
      "learning_rate": 0.00031780167264038235,
      "loss": 6.6436,
      "step": 2441
    },
    {
      "epoch": 0.7295541115841362,
      "grad_norm": 0.5102277994155884,
      "learning_rate": 0.0003177270011947431,
      "loss": 6.8975,
      "step": 2442
    },
    {
      "epoch": 0.7298528642915827,
      "grad_norm": 0.44835302233695984,
      "learning_rate": 0.00031765232974910397,
      "loss": 7.3672,
      "step": 2443
    },
    {
      "epoch": 0.7301516169990291,
      "grad_norm": 0.43169987201690674,
      "learning_rate": 0.00031757765830346473,
      "loss": 7.5576,
      "step": 2444
    },
    {
      "epoch": 0.7304503697064755,
      "grad_norm": 0.5438717603683472,
      "learning_rate": 0.0003175029868578256,
      "loss": 6.8428,
      "step": 2445
    },
    {
      "epoch": 0.7307491224139219,
      "grad_norm": 0.48469293117523193,
      "learning_rate": 0.00031742831541218636,
      "loss": 6.7373,
      "step": 2446
    },
    {
      "epoch": 0.7310478751213683,
      "grad_norm": 0.4145370125770569,
      "learning_rate": 0.0003173536439665472,
      "loss": 7.374,
      "step": 2447
    },
    {
      "epoch": 0.7313466278288147,
      "grad_norm": 0.3978698253631592,
      "learning_rate": 0.00031727897252090804,
      "loss": 7.3584,
      "step": 2448
    },
    {
      "epoch": 0.7316453805362612,
      "grad_norm": 0.39403846859931946,
      "learning_rate": 0.0003172043010752688,
      "loss": 7.4443,
      "step": 2449
    },
    {
      "epoch": 0.7319441332437076,
      "grad_norm": 0.4797878563404083,
      "learning_rate": 0.00031712962962962966,
      "loss": 6.7197,
      "step": 2450
    },
    {
      "epoch": 0.7322428859511539,
      "grad_norm": 0.4538998603820801,
      "learning_rate": 0.0003170549581839904,
      "loss": 7.6064,
      "step": 2451
    },
    {
      "epoch": 0.7325416386586003,
      "grad_norm": 0.44300225377082825,
      "learning_rate": 0.0003169802867383513,
      "loss": 7.0244,
      "step": 2452
    },
    {
      "epoch": 0.7328403913660467,
      "grad_norm": 0.4625772535800934,
      "learning_rate": 0.00031690561529271205,
      "loss": 7.2646,
      "step": 2453
    },
    {
      "epoch": 0.7331391440734931,
      "grad_norm": 0.4392959475517273,
      "learning_rate": 0.0003168309438470729,
      "loss": 7.334,
      "step": 2454
    },
    {
      "epoch": 0.7334378967809396,
      "grad_norm": 0.48210591077804565,
      "learning_rate": 0.00031675627240143367,
      "loss": 7.1836,
      "step": 2455
    },
    {
      "epoch": 0.733736649488386,
      "grad_norm": 0.4773869216442108,
      "learning_rate": 0.00031668160095579454,
      "loss": 7.1533,
      "step": 2456
    },
    {
      "epoch": 0.7340354021958324,
      "grad_norm": 0.4030030369758606,
      "learning_rate": 0.00031660692951015535,
      "loss": 7.0049,
      "step": 2457
    },
    {
      "epoch": 0.7343341549032788,
      "grad_norm": 0.39868807792663574,
      "learning_rate": 0.0003165322580645161,
      "loss": 7.2715,
      "step": 2458
    },
    {
      "epoch": 0.7346329076107252,
      "grad_norm": 0.4162694811820984,
      "learning_rate": 0.000316457586618877,
      "loss": 7.5166,
      "step": 2459
    },
    {
      "epoch": 0.7349316603181716,
      "grad_norm": 0.5123117566108704,
      "learning_rate": 0.00031638291517323774,
      "loss": 6.2412,
      "step": 2460
    },
    {
      "epoch": 0.7352304130256181,
      "grad_norm": 0.4974375069141388,
      "learning_rate": 0.0003163082437275986,
      "loss": 6.9014,
      "step": 2461
    },
    {
      "epoch": 0.7355291657330645,
      "grad_norm": 0.43652522563934326,
      "learning_rate": 0.00031623357228195936,
      "loss": 7.1865,
      "step": 2462
    },
    {
      "epoch": 0.7358279184405109,
      "grad_norm": 0.45850929617881775,
      "learning_rate": 0.00031615890083632023,
      "loss": 7.2373,
      "step": 2463
    },
    {
      "epoch": 0.7361266711479573,
      "grad_norm": 0.524427592754364,
      "learning_rate": 0.000316084229390681,
      "loss": 6.9932,
      "step": 2464
    },
    {
      "epoch": 0.7364254238554037,
      "grad_norm": 0.3915267586708069,
      "learning_rate": 0.0003160095579450418,
      "loss": 7.5195,
      "step": 2465
    },
    {
      "epoch": 0.73672417656285,
      "grad_norm": 0.3930119276046753,
      "learning_rate": 0.0003159348864994026,
      "loss": 7.3906,
      "step": 2466
    },
    {
      "epoch": 0.7370229292702966,
      "grad_norm": 0.5131391882896423,
      "learning_rate": 0.0003158602150537634,
      "loss": 7.1631,
      "step": 2467
    },
    {
      "epoch": 0.737321681977743,
      "grad_norm": 0.5121899247169495,
      "learning_rate": 0.0003157855436081243,
      "loss": 7.2549,
      "step": 2468
    },
    {
      "epoch": 0.7376204346851893,
      "grad_norm": 0.3928579092025757,
      "learning_rate": 0.00031571087216248505,
      "loss": 7.7744,
      "step": 2469
    },
    {
      "epoch": 0.7379191873926357,
      "grad_norm": 0.4924079179763794,
      "learning_rate": 0.0003156362007168459,
      "loss": 7.3838,
      "step": 2470
    },
    {
      "epoch": 0.7382179401000821,
      "grad_norm": 0.4664633274078369,
      "learning_rate": 0.0003155615292712067,
      "loss": 7.3848,
      "step": 2471
    },
    {
      "epoch": 0.7385166928075285,
      "grad_norm": 0.5725236535072327,
      "learning_rate": 0.00031548685782556755,
      "loss": 6.5967,
      "step": 2472
    },
    {
      "epoch": 0.738815445514975,
      "grad_norm": 0.5550546646118164,
      "learning_rate": 0.0003154121863799283,
      "loss": 6.6846,
      "step": 2473
    },
    {
      "epoch": 0.7391141982224214,
      "grad_norm": 0.43871942162513733,
      "learning_rate": 0.0003153375149342891,
      "loss": 7.1582,
      "step": 2474
    },
    {
      "epoch": 0.7394129509298678,
      "grad_norm": 0.46031486988067627,
      "learning_rate": 0.00031526284348864993,
      "loss": 7.3574,
      "step": 2475
    },
    {
      "epoch": 0.7397117036373142,
      "grad_norm": 0.39031171798706055,
      "learning_rate": 0.00031518817204301074,
      "loss": 7.5186,
      "step": 2476
    },
    {
      "epoch": 0.7400104563447606,
      "grad_norm": 0.47329720854759216,
      "learning_rate": 0.0003151135005973716,
      "loss": 7.3779,
      "step": 2477
    },
    {
      "epoch": 0.740309209052207,
      "grad_norm": 0.3831307888031006,
      "learning_rate": 0.00031503882915173237,
      "loss": 7.5801,
      "step": 2478
    },
    {
      "epoch": 0.7406079617596535,
      "grad_norm": 0.435157835483551,
      "learning_rate": 0.00031496415770609324,
      "loss": 7.2471,
      "step": 2479
    },
    {
      "epoch": 0.7409067144670999,
      "grad_norm": 0.5163206458091736,
      "learning_rate": 0.000314889486260454,
      "loss": 6.6689,
      "step": 2480
    },
    {
      "epoch": 0.7412054671745463,
      "grad_norm": 0.4673941731452942,
      "learning_rate": 0.0003148148148148148,
      "loss": 7.2705,
      "step": 2481
    },
    {
      "epoch": 0.7415042198819927,
      "grad_norm": 0.500634491443634,
      "learning_rate": 0.0003147401433691756,
      "loss": 6.8174,
      "step": 2482
    },
    {
      "epoch": 0.7418029725894391,
      "grad_norm": 0.4271180033683777,
      "learning_rate": 0.00031466547192353643,
      "loss": 7.3457,
      "step": 2483
    },
    {
      "epoch": 0.7421017252968855,
      "grad_norm": 0.4828013479709625,
      "learning_rate": 0.00031459080047789725,
      "loss": 7.3818,
      "step": 2484
    },
    {
      "epoch": 0.742400478004332,
      "grad_norm": 0.40473201870918274,
      "learning_rate": 0.00031451612903225806,
      "loss": 7.3154,
      "step": 2485
    },
    {
      "epoch": 0.7426992307117783,
      "grad_norm": 10.727764129638672,
      "learning_rate": 0.0003144414575866189,
      "loss": 7.1289,
      "step": 2486
    },
    {
      "epoch": 0.7429979834192247,
      "grad_norm": 0.44973352551460266,
      "learning_rate": 0.0003143667861409797,
      "loss": 6.9629,
      "step": 2487
    },
    {
      "epoch": 0.7432967361266711,
      "grad_norm": 0.4379320740699768,
      "learning_rate": 0.00031429211469534055,
      "loss": 7.0508,
      "step": 2488
    },
    {
      "epoch": 0.7435954888341175,
      "grad_norm": 0.42741042375564575,
      "learning_rate": 0.0003142174432497013,
      "loss": 7.4043,
      "step": 2489
    },
    {
      "epoch": 0.7438942415415639,
      "grad_norm": 0.5338515639305115,
      "learning_rate": 0.0003141427718040621,
      "loss": 6.8115,
      "step": 2490
    },
    {
      "epoch": 0.7441929942490104,
      "grad_norm": 0.4268096089363098,
      "learning_rate": 0.00031406810035842294,
      "loss": 6.918,
      "step": 2491
    },
    {
      "epoch": 0.7444917469564568,
      "grad_norm": 0.46990934014320374,
      "learning_rate": 0.00031399342891278375,
      "loss": 7.1104,
      "step": 2492
    },
    {
      "epoch": 0.7447904996639032,
      "grad_norm": 0.4671473205089569,
      "learning_rate": 0.00031391875746714456,
      "loss": 7.1074,
      "step": 2493
    },
    {
      "epoch": 0.7450892523713496,
      "grad_norm": 0.4170209765434265,
      "learning_rate": 0.0003138440860215054,
      "loss": 7.4893,
      "step": 2494
    },
    {
      "epoch": 0.745388005078796,
      "grad_norm": 0.4479294419288635,
      "learning_rate": 0.00031376941457586624,
      "loss": 7.0195,
      "step": 2495
    },
    {
      "epoch": 0.7456867577862424,
      "grad_norm": 0.485038161277771,
      "learning_rate": 0.000313694743130227,
      "loss": 6.749,
      "step": 2496
    },
    {
      "epoch": 0.7459855104936889,
      "grad_norm": 0.46051204204559326,
      "learning_rate": 0.0003136200716845878,
      "loss": 6.9141,
      "step": 2497
    },
    {
      "epoch": 0.7462842632011353,
      "grad_norm": 0.4708418548107147,
      "learning_rate": 0.0003135454002389486,
      "loss": 7.1807,
      "step": 2498
    },
    {
      "epoch": 0.7465830159085817,
      "grad_norm": 0.548369824886322,
      "learning_rate": 0.00031347072879330944,
      "loss": 6.6426,
      "step": 2499
    },
    {
      "epoch": 0.7468817686160281,
      "grad_norm": 0.4418126940727234,
      "learning_rate": 0.00031339605734767025,
      "loss": 7.4668,
      "step": 2500
    },
    {
      "epoch": 0.7471805213234745,
      "grad_norm": 0.40055593848228455,
      "learning_rate": 0.00031332138590203106,
      "loss": 7.333,
      "step": 2501
    },
    {
      "epoch": 0.747479274030921,
      "grad_norm": 0.46232134103775024,
      "learning_rate": 0.0003132467144563919,
      "loss": 7.0156,
      "step": 2502
    },
    {
      "epoch": 0.7477780267383674,
      "grad_norm": 0.5265531539916992,
      "learning_rate": 0.0003131720430107527,
      "loss": 7.0273,
      "step": 2503
    },
    {
      "epoch": 0.7480767794458137,
      "grad_norm": 0.475510835647583,
      "learning_rate": 0.00031309737156511356,
      "loss": 6.4902,
      "step": 2504
    },
    {
      "epoch": 0.7483755321532601,
      "grad_norm": 0.42841866612434387,
      "learning_rate": 0.0003130227001194743,
      "loss": 6.7363,
      "step": 2505
    },
    {
      "epoch": 0.7486742848607065,
      "grad_norm": 0.3882107436656952,
      "learning_rate": 0.00031294802867383513,
      "loss": 7.4932,
      "step": 2506
    },
    {
      "epoch": 0.7489730375681529,
      "grad_norm": 0.5385320782661438,
      "learning_rate": 0.00031287335722819594,
      "loss": 6.9727,
      "step": 2507
    },
    {
      "epoch": 0.7492717902755994,
      "grad_norm": 0.3782530426979065,
      "learning_rate": 0.00031279868578255675,
      "loss": 7.7568,
      "step": 2508
    },
    {
      "epoch": 0.7495705429830458,
      "grad_norm": 0.49057435989379883,
      "learning_rate": 0.00031272401433691757,
      "loss": 7.0312,
      "step": 2509
    },
    {
      "epoch": 0.7498692956904922,
      "grad_norm": 0.47639036178588867,
      "learning_rate": 0.0003126493428912784,
      "loss": 7.3125,
      "step": 2510
    },
    {
      "epoch": 0.7501680483979386,
      "grad_norm": 0.4615306556224823,
      "learning_rate": 0.0003125746714456392,
      "loss": 7.4033,
      "step": 2511
    },
    {
      "epoch": 0.750466801105385,
      "grad_norm": 0.45096004009246826,
      "learning_rate": 0.0003125,
      "loss": 7.0322,
      "step": 2512
    },
    {
      "epoch": 0.7507655538128314,
      "grad_norm": 0.3729296326637268,
      "learning_rate": 0.0003124253285543608,
      "loss": 7.2939,
      "step": 2513
    },
    {
      "epoch": 0.7510643065202779,
      "grad_norm": 0.4668208360671997,
      "learning_rate": 0.00031235065710872163,
      "loss": 7.0693,
      "step": 2514
    },
    {
      "epoch": 0.7513630592277243,
      "grad_norm": 0.4539394676685333,
      "learning_rate": 0.00031227598566308245,
      "loss": 7.3701,
      "step": 2515
    },
    {
      "epoch": 0.7516618119351707,
      "grad_norm": 0.34829968214035034,
      "learning_rate": 0.00031220131421744326,
      "loss": 8.0088,
      "step": 2516
    },
    {
      "epoch": 0.7519605646426171,
      "grad_norm": 0.4415362477302551,
      "learning_rate": 0.00031212664277180407,
      "loss": 7.5439,
      "step": 2517
    },
    {
      "epoch": 0.7522593173500635,
      "grad_norm": 0.39931681752204895,
      "learning_rate": 0.0003120519713261649,
      "loss": 7.8701,
      "step": 2518
    },
    {
      "epoch": 0.7525580700575099,
      "grad_norm": 0.44454625248908997,
      "learning_rate": 0.0003119772998805257,
      "loss": 7.1387,
      "step": 2519
    },
    {
      "epoch": 0.7528568227649564,
      "grad_norm": 0.4625135660171509,
      "learning_rate": 0.00031190262843488646,
      "loss": 7.332,
      "step": 2520
    },
    {
      "epoch": 0.7531555754724027,
      "grad_norm": 0.43031466007232666,
      "learning_rate": 0.0003118279569892473,
      "loss": 7.2842,
      "step": 2521
    },
    {
      "epoch": 0.7534543281798491,
      "grad_norm": 0.45029380917549133,
      "learning_rate": 0.00031175328554360814,
      "loss": 7.2783,
      "step": 2522
    },
    {
      "epoch": 0.7537530808872955,
      "grad_norm": 0.4041428565979004,
      "learning_rate": 0.00031167861409796895,
      "loss": 7.6152,
      "step": 2523
    },
    {
      "epoch": 0.7540518335947419,
      "grad_norm": 0.40165072679519653,
      "learning_rate": 0.00031160394265232976,
      "loss": 7.29,
      "step": 2524
    },
    {
      "epoch": 0.7543505863021883,
      "grad_norm": 0.5389125943183899,
      "learning_rate": 0.0003115292712066906,
      "loss": 6.7754,
      "step": 2525
    },
    {
      "epoch": 0.7546493390096348,
      "grad_norm": 0.4040021598339081,
      "learning_rate": 0.0003114545997610514,
      "loss": 7.5039,
      "step": 2526
    },
    {
      "epoch": 0.7549480917170812,
      "grad_norm": 0.453762948513031,
      "learning_rate": 0.0003113799283154122,
      "loss": 7.2578,
      "step": 2527
    },
    {
      "epoch": 0.7552468444245276,
      "grad_norm": 0.4373528063297272,
      "learning_rate": 0.000311305256869773,
      "loss": 7.207,
      "step": 2528
    },
    {
      "epoch": 0.755545597131974,
      "grad_norm": 0.5119450092315674,
      "learning_rate": 0.00031123058542413377,
      "loss": 6.9004,
      "step": 2529
    },
    {
      "epoch": 0.7558443498394204,
      "grad_norm": 0.35050201416015625,
      "learning_rate": 0.00031115591397849464,
      "loss": 7.5664,
      "step": 2530
    },
    {
      "epoch": 0.7561431025468668,
      "grad_norm": 0.4465998411178589,
      "learning_rate": 0.00031108124253285545,
      "loss": 7.4023,
      "step": 2531
    },
    {
      "epoch": 0.7564418552543133,
      "grad_norm": 0.4604286253452301,
      "learning_rate": 0.00031100657108721626,
      "loss": 7.1387,
      "step": 2532
    },
    {
      "epoch": 0.7567406079617597,
      "grad_norm": 0.4986315667629242,
      "learning_rate": 0.0003109318996415771,
      "loss": 6.8936,
      "step": 2533
    },
    {
      "epoch": 0.7570393606692061,
      "grad_norm": 0.44848304986953735,
      "learning_rate": 0.0003108572281959379,
      "loss": 7.3164,
      "step": 2534
    },
    {
      "epoch": 0.7573381133766525,
      "grad_norm": 0.4697532057762146,
      "learning_rate": 0.0003107825567502987,
      "loss": 7.0654,
      "step": 2535
    },
    {
      "epoch": 0.7576368660840989,
      "grad_norm": 0.4630412757396698,
      "learning_rate": 0.00031070788530465946,
      "loss": 7.2666,
      "step": 2536
    },
    {
      "epoch": 0.7579356187915453,
      "grad_norm": 0.37382155656814575,
      "learning_rate": 0.00031063321385902033,
      "loss": 7.8975,
      "step": 2537
    },
    {
      "epoch": 0.7582343714989918,
      "grad_norm": 0.4886372983455658,
      "learning_rate": 0.0003105585424133811,
      "loss": 7.0059,
      "step": 2538
    },
    {
      "epoch": 0.7585331242064381,
      "grad_norm": 0.40760135650634766,
      "learning_rate": 0.00031048387096774195,
      "loss": 7.2051,
      "step": 2539
    },
    {
      "epoch": 0.7588318769138845,
      "grad_norm": 0.4680858552455902,
      "learning_rate": 0.00031040919952210277,
      "loss": 6.5859,
      "step": 2540
    },
    {
      "epoch": 0.7591306296213309,
      "grad_norm": 0.40581101179122925,
      "learning_rate": 0.0003103345280764636,
      "loss": 7.8398,
      "step": 2541
    },
    {
      "epoch": 0.7594293823287773,
      "grad_norm": 0.50364750623703,
      "learning_rate": 0.0003102598566308244,
      "loss": 6.9395,
      "step": 2542
    },
    {
      "epoch": 0.7597281350362237,
      "grad_norm": 0.39069193601608276,
      "learning_rate": 0.0003101851851851852,
      "loss": 7.335,
      "step": 2543
    },
    {
      "epoch": 0.7600268877436702,
      "grad_norm": 0.45964711904525757,
      "learning_rate": 0.000310110513739546,
      "loss": 6.9902,
      "step": 2544
    },
    {
      "epoch": 0.7603256404511166,
      "grad_norm": 0.3801031708717346,
      "learning_rate": 0.0003100358422939068,
      "loss": 7.4365,
      "step": 2545
    },
    {
      "epoch": 0.760624393158563,
      "grad_norm": 0.38479262590408325,
      "learning_rate": 0.00030996117084826764,
      "loss": 7.4971,
      "step": 2546
    },
    {
      "epoch": 0.7609231458660094,
      "grad_norm": 0.43534645438194275,
      "learning_rate": 0.0003098864994026284,
      "loss": 7.7061,
      "step": 2547
    },
    {
      "epoch": 0.7612218985734558,
      "grad_norm": 0.3922795355319977,
      "learning_rate": 0.00030981182795698927,
      "loss": 7.377,
      "step": 2548
    },
    {
      "epoch": 0.7615206512809022,
      "grad_norm": 0.3912983536720276,
      "learning_rate": 0.0003097371565113501,
      "loss": 7.667,
      "step": 2549
    },
    {
      "epoch": 0.7618194039883487,
      "grad_norm": 0.5984746813774109,
      "learning_rate": 0.0003096624850657109,
      "loss": 6.6406,
      "step": 2550
    },
    {
      "epoch": 0.7621181566957951,
      "grad_norm": 0.3895493149757385,
      "learning_rate": 0.0003095878136200717,
      "loss": 7.5977,
      "step": 2551
    },
    {
      "epoch": 0.7624169094032415,
      "grad_norm": 0.41412314772605896,
      "learning_rate": 0.00030951314217443247,
      "loss": 7.7832,
      "step": 2552
    },
    {
      "epoch": 0.7627156621106879,
      "grad_norm": 0.432259202003479,
      "learning_rate": 0.00030943847072879333,
      "loss": 7.6426,
      "step": 2553
    },
    {
      "epoch": 0.7630144148181343,
      "grad_norm": 0.43694454431533813,
      "learning_rate": 0.0003093637992831541,
      "loss": 7.2061,
      "step": 2554
    },
    {
      "epoch": 0.7633131675255806,
      "grad_norm": 0.3433697819709778,
      "learning_rate": 0.00030928912783751496,
      "loss": 7.6338,
      "step": 2555
    },
    {
      "epoch": 0.7636119202330272,
      "grad_norm": 0.42642873525619507,
      "learning_rate": 0.0003092144563918757,
      "loss": 7.1172,
      "step": 2556
    },
    {
      "epoch": 0.7639106729404735,
      "grad_norm": 0.34796831011772156,
      "learning_rate": 0.0003091397849462366,
      "loss": 7.502,
      "step": 2557
    },
    {
      "epoch": 0.7642094256479199,
      "grad_norm": 0.46461963653564453,
      "learning_rate": 0.0003090651135005974,
      "loss": 7.0479,
      "step": 2558
    },
    {
      "epoch": 0.7645081783553663,
      "grad_norm": 0.44797590374946594,
      "learning_rate": 0.0003089904420549582,
      "loss": 7.0186,
      "step": 2559
    },
    {
      "epoch": 0.7648069310628127,
      "grad_norm": 0.5085062384605408,
      "learning_rate": 0.000308915770609319,
      "loss": 6.9395,
      "step": 2560
    },
    {
      "epoch": 0.7651056837702591,
      "grad_norm": 0.49935266375541687,
      "learning_rate": 0.0003088410991636798,
      "loss": 6.8438,
      "step": 2561
    },
    {
      "epoch": 0.7654044364777056,
      "grad_norm": 0.4117029011249542,
      "learning_rate": 0.00030876642771804065,
      "loss": 7.001,
      "step": 2562
    },
    {
      "epoch": 0.765703189185152,
      "grad_norm": 0.46474137902259827,
      "learning_rate": 0.0003086917562724014,
      "loss": 7.2529,
      "step": 2563
    },
    {
      "epoch": 0.7660019418925984,
      "grad_norm": 0.3932224214076996,
      "learning_rate": 0.0003086170848267623,
      "loss": 7.5488,
      "step": 2564
    },
    {
      "epoch": 0.7663006946000448,
      "grad_norm": 0.4307496249675751,
      "learning_rate": 0.00030854241338112303,
      "loss": 7.1211,
      "step": 2565
    },
    {
      "epoch": 0.7665994473074912,
      "grad_norm": 0.3427039384841919,
      "learning_rate": 0.0003084677419354839,
      "loss": 8.0098,
      "step": 2566
    },
    {
      "epoch": 0.7668982000149376,
      "grad_norm": 0.4540993571281433,
      "learning_rate": 0.00030839307048984466,
      "loss": 7.2197,
      "step": 2567
    },
    {
      "epoch": 0.7671969527223841,
      "grad_norm": 0.5255457758903503,
      "learning_rate": 0.0003083183990442055,
      "loss": 6.459,
      "step": 2568
    },
    {
      "epoch": 0.7674957054298305,
      "grad_norm": 0.4591517150402069,
      "learning_rate": 0.00030824372759856634,
      "loss": 7.0811,
      "step": 2569
    },
    {
      "epoch": 0.7677944581372769,
      "grad_norm": 0.43428754806518555,
      "learning_rate": 0.0003081690561529271,
      "loss": 7.2129,
      "step": 2570
    },
    {
      "epoch": 0.7680932108447233,
      "grad_norm": 0.45216813683509827,
      "learning_rate": 0.00030809438470728797,
      "loss": 7.4316,
      "step": 2571
    },
    {
      "epoch": 0.7683919635521697,
      "grad_norm": 0.5159900784492493,
      "learning_rate": 0.0003080197132616487,
      "loss": 6.6533,
      "step": 2572
    },
    {
      "epoch": 0.7686907162596162,
      "grad_norm": 0.3515246510505676,
      "learning_rate": 0.0003079450418160096,
      "loss": 7.7725,
      "step": 2573
    },
    {
      "epoch": 0.7689894689670626,
      "grad_norm": 0.38053223490715027,
      "learning_rate": 0.00030787037037037035,
      "loss": 7.1572,
      "step": 2574
    },
    {
      "epoch": 0.7692882216745089,
      "grad_norm": 0.5064065456390381,
      "learning_rate": 0.0003077956989247312,
      "loss": 7.0381,
      "step": 2575
    },
    {
      "epoch": 0.7695869743819553,
      "grad_norm": 0.5545828938484192,
      "learning_rate": 0.000307721027479092,
      "loss": 6.9141,
      "step": 2576
    },
    {
      "epoch": 0.7698857270894017,
      "grad_norm": 0.6444261074066162,
      "learning_rate": 0.0003076463560334528,
      "loss": 6.6113,
      "step": 2577
    },
    {
      "epoch": 0.7701844797968481,
      "grad_norm": 0.39066606760025024,
      "learning_rate": 0.00030757168458781366,
      "loss": 7.3916,
      "step": 2578
    },
    {
      "epoch": 0.7704832325042946,
      "grad_norm": 0.4336581826210022,
      "learning_rate": 0.0003074970131421744,
      "loss": 7.0703,
      "step": 2579
    },
    {
      "epoch": 0.770781985211741,
      "grad_norm": 0.43643859028816223,
      "learning_rate": 0.0003074223416965353,
      "loss": 7.2471,
      "step": 2580
    },
    {
      "epoch": 0.7710807379191874,
      "grad_norm": 0.40178239345550537,
      "learning_rate": 0.00030734767025089604,
      "loss": 6.9961,
      "step": 2581
    },
    {
      "epoch": 0.7713794906266338,
      "grad_norm": 0.5030426383018494,
      "learning_rate": 0.0003072729988052569,
      "loss": 6.7529,
      "step": 2582
    },
    {
      "epoch": 0.7716782433340802,
      "grad_norm": 0.47163644433021545,
      "learning_rate": 0.00030719832735961767,
      "loss": 7.3818,
      "step": 2583
    },
    {
      "epoch": 0.7719769960415266,
      "grad_norm": 0.5989277362823486,
      "learning_rate": 0.0003071236559139785,
      "loss": 6.4795,
      "step": 2584
    },
    {
      "epoch": 0.7722757487489731,
      "grad_norm": 0.4981294274330139,
      "learning_rate": 0.0003070489844683393,
      "loss": 7.2041,
      "step": 2585
    },
    {
      "epoch": 0.7725745014564195,
      "grad_norm": 0.46020612120628357,
      "learning_rate": 0.0003069743130227001,
      "loss": 7.3301,
      "step": 2586
    },
    {
      "epoch": 0.7728732541638659,
      "grad_norm": 0.42181506752967834,
      "learning_rate": 0.00030689964157706097,
      "loss": 7.2539,
      "step": 2587
    },
    {
      "epoch": 0.7731720068713123,
      "grad_norm": 0.5080004334449768,
      "learning_rate": 0.00030682497013142173,
      "loss": 6.7832,
      "step": 2588
    },
    {
      "epoch": 0.7734707595787587,
      "grad_norm": 0.48212891817092896,
      "learning_rate": 0.0003067502986857826,
      "loss": 7.1016,
      "step": 2589
    },
    {
      "epoch": 0.773769512286205,
      "grad_norm": 0.4420432150363922,
      "learning_rate": 0.00030667562724014336,
      "loss": 7.1934,
      "step": 2590
    },
    {
      "epoch": 0.7740682649936516,
      "grad_norm": 0.44035324454307556,
      "learning_rate": 0.0003066009557945042,
      "loss": 7.4609,
      "step": 2591
    },
    {
      "epoch": 0.774367017701098,
      "grad_norm": 0.45041733980178833,
      "learning_rate": 0.000306526284348865,
      "loss": 7.0615,
      "step": 2592
    },
    {
      "epoch": 0.7746657704085443,
      "grad_norm": 0.4507216513156891,
      "learning_rate": 0.0003064516129032258,
      "loss": 7.3174,
      "step": 2593
    },
    {
      "epoch": 0.7749645231159907,
      "grad_norm": 0.4804019033908844,
      "learning_rate": 0.0003063769414575866,
      "loss": 7.4785,
      "step": 2594
    },
    {
      "epoch": 0.7752632758234371,
      "grad_norm": 0.4195512533187866,
      "learning_rate": 0.0003063022700119474,
      "loss": 7.3555,
      "step": 2595
    },
    {
      "epoch": 0.7755620285308835,
      "grad_norm": 0.46526530385017395,
      "learning_rate": 0.0003062275985663083,
      "loss": 7.1494,
      "step": 2596
    },
    {
      "epoch": 0.77586078123833,
      "grad_norm": 0.4349091053009033,
      "learning_rate": 0.00030615292712066905,
      "loss": 7.1084,
      "step": 2597
    },
    {
      "epoch": 0.7761595339457764,
      "grad_norm": 0.340775728225708,
      "learning_rate": 0.0003060782556750299,
      "loss": 8.0059,
      "step": 2598
    },
    {
      "epoch": 0.7764582866532228,
      "grad_norm": 0.44232630729675293,
      "learning_rate": 0.00030600358422939067,
      "loss": 7.2842,
      "step": 2599
    },
    {
      "epoch": 0.7767570393606692,
      "grad_norm": 0.5106445550918579,
      "learning_rate": 0.0003059289127837515,
      "loss": 7.1436,
      "step": 2600
    },
    {
      "epoch": 0.7767570393606692,
      "eval_bleu": 0.13213323397358917,
      "eval_loss": 7.0390625,
      "eval_runtime": 466.0159,
      "eval_samples_per_second": 3.024,
      "eval_steps_per_second": 0.191,
      "step": 2600
    },
    {
      "epoch": 0.7770557920681156,
      "grad_norm": 0.38403332233428955,
      "learning_rate": 0.0003058542413381123,
      "loss": 7.3945,
      "step": 2601
    },
    {
      "epoch": 0.777354544775562,
      "grad_norm": 0.40783825516700745,
      "learning_rate": 0.0003057795698924731,
      "loss": 7.4785,
      "step": 2602
    },
    {
      "epoch": 0.7776532974830085,
      "grad_norm": 0.4470752775669098,
      "learning_rate": 0.0003057048984468339,
      "loss": 7.2314,
      "step": 2603
    },
    {
      "epoch": 0.7779520501904549,
      "grad_norm": 0.4214273691177368,
      "learning_rate": 0.00030563022700119474,
      "loss": 7.1787,
      "step": 2604
    },
    {
      "epoch": 0.7782508028979013,
      "grad_norm": 0.5322867035865784,
      "learning_rate": 0.0003055555555555556,
      "loss": 7.0049,
      "step": 2605
    },
    {
      "epoch": 0.7785495556053477,
      "grad_norm": 0.45827075839042664,
      "learning_rate": 0.00030548088410991636,
      "loss": 7.3672,
      "step": 2606
    },
    {
      "epoch": 0.7788483083127941,
      "grad_norm": 0.5031996965408325,
      "learning_rate": 0.00030540621266427723,
      "loss": 6.9873,
      "step": 2607
    },
    {
      "epoch": 0.7791470610202405,
      "grad_norm": 0.49547693133354187,
      "learning_rate": 0.000305331541218638,
      "loss": 7.0488,
      "step": 2608
    },
    {
      "epoch": 0.779445813727687,
      "grad_norm": 0.42403748631477356,
      "learning_rate": 0.0003052568697729988,
      "loss": 7.3291,
      "step": 2609
    },
    {
      "epoch": 0.7797445664351333,
      "grad_norm": 0.4739573001861572,
      "learning_rate": 0.0003051821983273596,
      "loss": 7.2148,
      "step": 2610
    },
    {
      "epoch": 0.7800433191425797,
      "grad_norm": 0.4513762295246124,
      "learning_rate": 0.00030510752688172043,
      "loss": 6.9424,
      "step": 2611
    },
    {
      "epoch": 0.7803420718500261,
      "grad_norm": 0.4268113970756531,
      "learning_rate": 0.00030503285543608124,
      "loss": 7.3213,
      "step": 2612
    },
    {
      "epoch": 0.7806408245574725,
      "grad_norm": 0.40727218985557556,
      "learning_rate": 0.00030495818399044205,
      "loss": 7.7031,
      "step": 2613
    },
    {
      "epoch": 0.7809395772649189,
      "grad_norm": 0.4604051411151886,
      "learning_rate": 0.0003048835125448029,
      "loss": 7.0986,
      "step": 2614
    },
    {
      "epoch": 0.7812383299723654,
      "grad_norm": 0.35216575860977173,
      "learning_rate": 0.0003048088410991637,
      "loss": 7.8457,
      "step": 2615
    },
    {
      "epoch": 0.7815370826798118,
      "grad_norm": 0.4444979131221771,
      "learning_rate": 0.0003047341696535245,
      "loss": 7.5654,
      "step": 2616
    },
    {
      "epoch": 0.7818358353872582,
      "grad_norm": 0.5925759673118591,
      "learning_rate": 0.0003046594982078853,
      "loss": 6.332,
      "step": 2617
    },
    {
      "epoch": 0.7821345880947046,
      "grad_norm": 0.49391624331474304,
      "learning_rate": 0.0003045848267622461,
      "loss": 6.7266,
      "step": 2618
    },
    {
      "epoch": 0.782433340802151,
      "grad_norm": 0.4246689975261688,
      "learning_rate": 0.00030451015531660693,
      "loss": 7.3301,
      "step": 2619
    },
    {
      "epoch": 0.7827320935095974,
      "grad_norm": 0.40649792551994324,
      "learning_rate": 0.00030443548387096774,
      "loss": 7.2256,
      "step": 2620
    },
    {
      "epoch": 0.7830308462170439,
      "grad_norm": 0.3732834756374359,
      "learning_rate": 0.00030436081242532856,
      "loss": 7.1445,
      "step": 2621
    },
    {
      "epoch": 0.7833295989244903,
      "grad_norm": 0.42023971676826477,
      "learning_rate": 0.00030428614097968937,
      "loss": 7.5107,
      "step": 2622
    },
    {
      "epoch": 0.7836283516319367,
      "grad_norm": 0.39419281482696533,
      "learning_rate": 0.00030421146953405024,
      "loss": 7.373,
      "step": 2623
    },
    {
      "epoch": 0.7839271043393831,
      "grad_norm": 0.4459691047668457,
      "learning_rate": 0.000304136798088411,
      "loss": 7.0615,
      "step": 2624
    },
    {
      "epoch": 0.7842258570468295,
      "grad_norm": 0.5462519526481628,
      "learning_rate": 0.0003040621266427718,
      "loss": 6.7666,
      "step": 2625
    },
    {
      "epoch": 0.7845246097542758,
      "grad_norm": 0.417119562625885,
      "learning_rate": 0.0003039874551971326,
      "loss": 7.7949,
      "step": 2626
    },
    {
      "epoch": 0.7848233624617224,
      "grad_norm": 0.37788474559783936,
      "learning_rate": 0.00030391278375149343,
      "loss": 7.2725,
      "step": 2627
    },
    {
      "epoch": 0.7851221151691687,
      "grad_norm": 0.4695953130722046,
      "learning_rate": 0.00030383811230585425,
      "loss": 7.209,
      "step": 2628
    },
    {
      "epoch": 0.7854208678766151,
      "grad_norm": 0.49597203731536865,
      "learning_rate": 0.00030376344086021506,
      "loss": 7.3184,
      "step": 2629
    },
    {
      "epoch": 0.7857196205840615,
      "grad_norm": 0.5141148567199707,
      "learning_rate": 0.00030368876941457587,
      "loss": 6.834,
      "step": 2630
    },
    {
      "epoch": 0.7860183732915079,
      "grad_norm": 0.41400814056396484,
      "learning_rate": 0.0003036140979689367,
      "loss": 7.5537,
      "step": 2631
    },
    {
      "epoch": 0.7863171259989543,
      "grad_norm": 0.3377149999141693,
      "learning_rate": 0.0003035394265232975,
      "loss": 7.9541,
      "step": 2632
    },
    {
      "epoch": 0.7866158787064008,
      "grad_norm": 0.4264136254787445,
      "learning_rate": 0.0003034647550776583,
      "loss": 7.2988,
      "step": 2633
    },
    {
      "epoch": 0.7869146314138472,
      "grad_norm": 0.4625217914581299,
      "learning_rate": 0.0003033900836320191,
      "loss": 7.0645,
      "step": 2634
    },
    {
      "epoch": 0.7872133841212936,
      "grad_norm": 0.411949098110199,
      "learning_rate": 0.00030331541218637994,
      "loss": 7.3506,
      "step": 2635
    },
    {
      "epoch": 0.78751213682874,
      "grad_norm": 0.5635335445404053,
      "learning_rate": 0.00030324074074074075,
      "loss": 6.6777,
      "step": 2636
    },
    {
      "epoch": 0.7878108895361864,
      "grad_norm": 0.42765969038009644,
      "learning_rate": 0.00030316606929510156,
      "loss": 7.0557,
      "step": 2637
    },
    {
      "epoch": 0.7881096422436328,
      "grad_norm": 0.4155837595462799,
      "learning_rate": 0.0003030913978494624,
      "loss": 7.2852,
      "step": 2638
    },
    {
      "epoch": 0.7884083949510793,
      "grad_norm": 0.6632328033447266,
      "learning_rate": 0.0003030167264038232,
      "loss": 6.2803,
      "step": 2639
    },
    {
      "epoch": 0.7887071476585257,
      "grad_norm": 0.43625694513320923,
      "learning_rate": 0.000302942054958184,
      "loss": 7.5283,
      "step": 2640
    },
    {
      "epoch": 0.7890059003659721,
      "grad_norm": 0.4300259053707123,
      "learning_rate": 0.0003028673835125448,
      "loss": 7.0664,
      "step": 2641
    },
    {
      "epoch": 0.7893046530734185,
      "grad_norm": 0.4551657736301422,
      "learning_rate": 0.0003027927120669056,
      "loss": 7.3203,
      "step": 2642
    },
    {
      "epoch": 0.7896034057808649,
      "grad_norm": 0.44690343737602234,
      "learning_rate": 0.00030271804062126644,
      "loss": 7.1133,
      "step": 2643
    },
    {
      "epoch": 0.7899021584883112,
      "grad_norm": 0.5509622693061829,
      "learning_rate": 0.00030264336917562725,
      "loss": 7.0938,
      "step": 2644
    },
    {
      "epoch": 0.7902009111957577,
      "grad_norm": 0.44250231981277466,
      "learning_rate": 0.00030256869772998806,
      "loss": 7.2529,
      "step": 2645
    },
    {
      "epoch": 0.7904996639032041,
      "grad_norm": 0.5976405143737793,
      "learning_rate": 0.0003024940262843489,
      "loss": 6.835,
      "step": 2646
    },
    {
      "epoch": 0.7907984166106505,
      "grad_norm": 0.42985934019088745,
      "learning_rate": 0.0003024193548387097,
      "loss": 7.5166,
      "step": 2647
    },
    {
      "epoch": 0.7910971693180969,
      "grad_norm": 0.46883004903793335,
      "learning_rate": 0.00030234468339307045,
      "loss": 7.085,
      "step": 2648
    },
    {
      "epoch": 0.7913959220255433,
      "grad_norm": 0.48551109433174133,
      "learning_rate": 0.0003022700119474313,
      "loss": 6.9395,
      "step": 2649
    },
    {
      "epoch": 0.7916946747329898,
      "grad_norm": 0.3903513550758362,
      "learning_rate": 0.00030219534050179213,
      "loss": 7.709,
      "step": 2650
    },
    {
      "epoch": 0.7919934274404362,
      "grad_norm": 0.44472846388816833,
      "learning_rate": 0.00030212066905615294,
      "loss": 7.6846,
      "step": 2651
    },
    {
      "epoch": 0.7922921801478826,
      "grad_norm": 0.38643407821655273,
      "learning_rate": 0.00030204599761051376,
      "loss": 7.4346,
      "step": 2652
    },
    {
      "epoch": 0.792590932855329,
      "grad_norm": 0.4769136309623718,
      "learning_rate": 0.00030197132616487457,
      "loss": 6.8174,
      "step": 2653
    },
    {
      "epoch": 0.7928896855627754,
      "grad_norm": 0.47890836000442505,
      "learning_rate": 0.0003018966547192354,
      "loss": 6.8779,
      "step": 2654
    },
    {
      "epoch": 0.7931884382702218,
      "grad_norm": 0.44024643301963806,
      "learning_rate": 0.0003018219832735962,
      "loss": 7.2451,
      "step": 2655
    },
    {
      "epoch": 0.7934871909776683,
      "grad_norm": 0.483165442943573,
      "learning_rate": 0.000301747311827957,
      "loss": 7.0195,
      "step": 2656
    },
    {
      "epoch": 0.7937859436851147,
      "grad_norm": 0.4149249196052551,
      "learning_rate": 0.00030167264038231777,
      "loss": 7.3467,
      "step": 2657
    },
    {
      "epoch": 0.7940846963925611,
      "grad_norm": 0.5020086169242859,
      "learning_rate": 0.00030159796893667863,
      "loss": 7.2178,
      "step": 2658
    },
    {
      "epoch": 0.7943834491000075,
      "grad_norm": 0.457612007856369,
      "learning_rate": 0.0003015232974910394,
      "loss": 7.1426,
      "step": 2659
    },
    {
      "epoch": 0.7946822018074539,
      "grad_norm": 0.5757554769515991,
      "learning_rate": 0.00030144862604540026,
      "loss": 6.7734,
      "step": 2660
    },
    {
      "epoch": 0.7949809545149003,
      "grad_norm": 0.5128085613250732,
      "learning_rate": 0.00030137395459976107,
      "loss": 7.2666,
      "step": 2661
    },
    {
      "epoch": 0.7952797072223468,
      "grad_norm": 0.43624216318130493,
      "learning_rate": 0.0003012992831541219,
      "loss": 7.2559,
      "step": 2662
    },
    {
      "epoch": 0.7955784599297931,
      "grad_norm": 0.48023316264152527,
      "learning_rate": 0.0003012246117084827,
      "loss": 7.1465,
      "step": 2663
    },
    {
      "epoch": 0.7958772126372395,
      "grad_norm": 0.443645179271698,
      "learning_rate": 0.00030114994026284346,
      "loss": 7.2197,
      "step": 2664
    },
    {
      "epoch": 0.7961759653446859,
      "grad_norm": 0.5473499894142151,
      "learning_rate": 0.0003010752688172043,
      "loss": 6.7451,
      "step": 2665
    },
    {
      "epoch": 0.7964747180521323,
      "grad_norm": 0.4127342104911804,
      "learning_rate": 0.0003010005973715651,
      "loss": 7.3721,
      "step": 2666
    },
    {
      "epoch": 0.7967734707595787,
      "grad_norm": 0.33128246665000916,
      "learning_rate": 0.00030092592592592595,
      "loss": 7.7305,
      "step": 2667
    },
    {
      "epoch": 0.7970722234670252,
      "grad_norm": 0.6007236838340759,
      "learning_rate": 0.0003008512544802867,
      "loss": 6.6709,
      "step": 2668
    },
    {
      "epoch": 0.7973709761744716,
      "grad_norm": 0.40361735224723816,
      "learning_rate": 0.0003007765830346476,
      "loss": 7.5664,
      "step": 2669
    },
    {
      "epoch": 0.797669728881918,
      "grad_norm": 0.45037445425987244,
      "learning_rate": 0.0003007019115890084,
      "loss": 7.4883,
      "step": 2670
    },
    {
      "epoch": 0.7979684815893644,
      "grad_norm": 0.49208390712738037,
      "learning_rate": 0.0003006272401433692,
      "loss": 7.0107,
      "step": 2671
    },
    {
      "epoch": 0.7982672342968108,
      "grad_norm": 0.4516269862651825,
      "learning_rate": 0.00030055256869773,
      "loss": 6.8652,
      "step": 2672
    },
    {
      "epoch": 0.7985659870042572,
      "grad_norm": 0.4632135331630707,
      "learning_rate": 0.00030047789725209077,
      "loss": 7.1855,
      "step": 2673
    },
    {
      "epoch": 0.7988647397117037,
      "grad_norm": 0.5033555626869202,
      "learning_rate": 0.00030040322580645164,
      "loss": 6.9971,
      "step": 2674
    },
    {
      "epoch": 0.7991634924191501,
      "grad_norm": 0.4868859648704529,
      "learning_rate": 0.0003003285543608124,
      "loss": 7.3203,
      "step": 2675
    },
    {
      "epoch": 0.7994622451265965,
      "grad_norm": 0.4631636440753937,
      "learning_rate": 0.00030025388291517326,
      "loss": 6.9287,
      "step": 2676
    },
    {
      "epoch": 0.7997609978340429,
      "grad_norm": 0.5752782821655273,
      "learning_rate": 0.000300179211469534,
      "loss": 6.3994,
      "step": 2677
    },
    {
      "epoch": 0.8000597505414893,
      "grad_norm": 0.5467042922973633,
      "learning_rate": 0.0003001045400238949,
      "loss": 6.6953,
      "step": 2678
    },
    {
      "epoch": 0.8003585032489356,
      "grad_norm": 0.542298436164856,
      "learning_rate": 0.0003000298685782557,
      "loss": 6.4941,
      "step": 2679
    },
    {
      "epoch": 0.8006572559563822,
      "grad_norm": 0.5439171195030212,
      "learning_rate": 0.00029995519713261646,
      "loss": 7.0039,
      "step": 2680
    },
    {
      "epoch": 0.8009560086638285,
      "grad_norm": 0.3327380120754242,
      "learning_rate": 0.00029988052568697733,
      "loss": 7.9756,
      "step": 2681
    },
    {
      "epoch": 0.8012547613712749,
      "grad_norm": 0.5586594343185425,
      "learning_rate": 0.0002998058542413381,
      "loss": 6.3848,
      "step": 2682
    },
    {
      "epoch": 0.8015535140787213,
      "grad_norm": 0.4539772868156433,
      "learning_rate": 0.00029973118279569895,
      "loss": 7.0508,
      "step": 2683
    },
    {
      "epoch": 0.8018522667861677,
      "grad_norm": 0.38730666041374207,
      "learning_rate": 0.0002996565113500597,
      "loss": 7.625,
      "step": 2684
    },
    {
      "epoch": 0.8021510194936141,
      "grad_norm": 0.46844545006752014,
      "learning_rate": 0.0002995818399044206,
      "loss": 7.2266,
      "step": 2685
    },
    {
      "epoch": 0.8024497722010606,
      "grad_norm": 0.43464764952659607,
      "learning_rate": 0.00029950716845878134,
      "loss": 7.2227,
      "step": 2686
    },
    {
      "epoch": 0.802748524908507,
      "grad_norm": 0.581906795501709,
      "learning_rate": 0.0002994324970131422,
      "loss": 6.7441,
      "step": 2687
    },
    {
      "epoch": 0.8030472776159534,
      "grad_norm": 0.5416211485862732,
      "learning_rate": 0.000299357825567503,
      "loss": 6.5312,
      "step": 2688
    },
    {
      "epoch": 0.8033460303233998,
      "grad_norm": 0.38994601368904114,
      "learning_rate": 0.0002992831541218638,
      "loss": 7.7324,
      "step": 2689
    },
    {
      "epoch": 0.8036447830308462,
      "grad_norm": 0.3788498044013977,
      "learning_rate": 0.00029920848267622464,
      "loss": 7.8467,
      "step": 2690
    },
    {
      "epoch": 0.8039435357382926,
      "grad_norm": 0.3791886270046234,
      "learning_rate": 0.0002991338112305854,
      "loss": 7.2803,
      "step": 2691
    },
    {
      "epoch": 0.8042422884457391,
      "grad_norm": 0.4980456233024597,
      "learning_rate": 0.00029905913978494627,
      "loss": 7.0996,
      "step": 2692
    },
    {
      "epoch": 0.8045410411531855,
      "grad_norm": 0.4388374984264374,
      "learning_rate": 0.00029898446833930703,
      "loss": 7.2402,
      "step": 2693
    },
    {
      "epoch": 0.8048397938606319,
      "grad_norm": 0.5135253667831421,
      "learning_rate": 0.0002989097968936679,
      "loss": 7.0322,
      "step": 2694
    },
    {
      "epoch": 0.8051385465680783,
      "grad_norm": 0.4735652804374695,
      "learning_rate": 0.00029883512544802865,
      "loss": 6.7373,
      "step": 2695
    },
    {
      "epoch": 0.8054372992755247,
      "grad_norm": 0.49425315856933594,
      "learning_rate": 0.00029876045400238947,
      "loss": 7.0117,
      "step": 2696
    },
    {
      "epoch": 0.805736051982971,
      "grad_norm": 0.47724202275276184,
      "learning_rate": 0.00029868578255675033,
      "loss": 6.9961,
      "step": 2697
    },
    {
      "epoch": 0.8060348046904176,
      "grad_norm": 0.4103018343448639,
      "learning_rate": 0.0002986111111111111,
      "loss": 7.2627,
      "step": 2698
    },
    {
      "epoch": 0.8063335573978639,
      "grad_norm": 0.3938697278499603,
      "learning_rate": 0.00029853643966547196,
      "loss": 7.6436,
      "step": 2699
    },
    {
      "epoch": 0.8066323101053103,
      "grad_norm": 0.4601058065891266,
      "learning_rate": 0.0002984617682198327,
      "loss": 7.2324,
      "step": 2700
    },
    {
      "epoch": 0.8069310628127567,
      "grad_norm": 0.4921930432319641,
      "learning_rate": 0.0002983870967741936,
      "loss": 7.0898,
      "step": 2701
    },
    {
      "epoch": 0.8072298155202031,
      "grad_norm": 0.4740699827671051,
      "learning_rate": 0.00029831242532855434,
      "loss": 6.7656,
      "step": 2702
    },
    {
      "epoch": 0.8075285682276495,
      "grad_norm": 0.44868505001068115,
      "learning_rate": 0.0002982377538829152,
      "loss": 7.3027,
      "step": 2703
    },
    {
      "epoch": 0.807827320935096,
      "grad_norm": 0.5387572050094604,
      "learning_rate": 0.00029816308243727597,
      "loss": 6.9131,
      "step": 2704
    },
    {
      "epoch": 0.8081260736425424,
      "grad_norm": 0.5938717722892761,
      "learning_rate": 0.0002980884109916368,
      "loss": 6.4727,
      "step": 2705
    },
    {
      "epoch": 0.8084248263499888,
      "grad_norm": 0.5115023851394653,
      "learning_rate": 0.00029801373954599765,
      "loss": 6.8467,
      "step": 2706
    },
    {
      "epoch": 0.8087235790574352,
      "grad_norm": 0.4474676847457886,
      "learning_rate": 0.0002979390681003584,
      "loss": 7.2666,
      "step": 2707
    },
    {
      "epoch": 0.8090223317648816,
      "grad_norm": 0.43349993228912354,
      "learning_rate": 0.0002978643966547193,
      "loss": 7.4375,
      "step": 2708
    },
    {
      "epoch": 0.809321084472328,
      "grad_norm": 0.5099083781242371,
      "learning_rate": 0.00029778972520908003,
      "loss": 7.0039,
      "step": 2709
    },
    {
      "epoch": 0.8096198371797745,
      "grad_norm": 0.43339091539382935,
      "learning_rate": 0.0002977150537634409,
      "loss": 7.165,
      "step": 2710
    },
    {
      "epoch": 0.8099185898872209,
      "grad_norm": 0.4520554840564728,
      "learning_rate": 0.00029764038231780166,
      "loss": 7.5322,
      "step": 2711
    },
    {
      "epoch": 0.8102173425946673,
      "grad_norm": 0.5300978422164917,
      "learning_rate": 0.0002975657108721625,
      "loss": 7.0195,
      "step": 2712
    },
    {
      "epoch": 0.8105160953021137,
      "grad_norm": 0.4775119125843048,
      "learning_rate": 0.0002974910394265233,
      "loss": 7.0781,
      "step": 2713
    },
    {
      "epoch": 0.81081484800956,
      "grad_norm": 0.4689294099807739,
      "learning_rate": 0.0002974163679808841,
      "loss": 7.0449,
      "step": 2714
    },
    {
      "epoch": 0.8111136007170064,
      "grad_norm": 0.49715250730514526,
      "learning_rate": 0.00029734169653524497,
      "loss": 7.3662,
      "step": 2715
    },
    {
      "epoch": 0.811412353424453,
      "grad_norm": 0.49279865622520447,
      "learning_rate": 0.0002972670250896057,
      "loss": 6.874,
      "step": 2716
    },
    {
      "epoch": 0.8117111061318993,
      "grad_norm": 0.44021594524383545,
      "learning_rate": 0.0002971923536439666,
      "loss": 7.3438,
      "step": 2717
    },
    {
      "epoch": 0.8120098588393457,
      "grad_norm": 0.5263257026672363,
      "learning_rate": 0.00029711768219832735,
      "loss": 7.0146,
      "step": 2718
    },
    {
      "epoch": 0.8123086115467921,
      "grad_norm": 0.4050498604774475,
      "learning_rate": 0.0002970430107526882,
      "loss": 7.3203,
      "step": 2719
    },
    {
      "epoch": 0.8126073642542385,
      "grad_norm": 0.382917195558548,
      "learning_rate": 0.000296968339307049,
      "loss": 7.8379,
      "step": 2720
    },
    {
      "epoch": 0.8129061169616849,
      "grad_norm": 0.5201869606971741,
      "learning_rate": 0.0002968936678614098,
      "loss": 6.5518,
      "step": 2721
    },
    {
      "epoch": 0.8132048696691314,
      "grad_norm": 0.4667884409427643,
      "learning_rate": 0.0002968189964157706,
      "loss": 7.2646,
      "step": 2722
    },
    {
      "epoch": 0.8135036223765778,
      "grad_norm": 0.4393201470375061,
      "learning_rate": 0.0002967443249701314,
      "loss": 7.3027,
      "step": 2723
    },
    {
      "epoch": 0.8138023750840242,
      "grad_norm": 0.38585638999938965,
      "learning_rate": 0.0002966696535244923,
      "loss": 7.4238,
      "step": 2724
    },
    {
      "epoch": 0.8141011277914706,
      "grad_norm": 0.40325525403022766,
      "learning_rate": 0.00029659498207885304,
      "loss": 7.4307,
      "step": 2725
    },
    {
      "epoch": 0.814399880498917,
      "grad_norm": 0.4623730182647705,
      "learning_rate": 0.0002965203106332139,
      "loss": 7.1572,
      "step": 2726
    },
    {
      "epoch": 0.8146986332063635,
      "grad_norm": 0.3303408622741699,
      "learning_rate": 0.00029644563918757467,
      "loss": 7.877,
      "step": 2727
    },
    {
      "epoch": 0.8149973859138099,
      "grad_norm": 0.39400988817214966,
      "learning_rate": 0.0002963709677419355,
      "loss": 7.4102,
      "step": 2728
    },
    {
      "epoch": 0.8152961386212563,
      "grad_norm": 0.4172452688217163,
      "learning_rate": 0.0002962962962962963,
      "loss": 7.6865,
      "step": 2729
    },
    {
      "epoch": 0.8155948913287027,
      "grad_norm": 0.538624107837677,
      "learning_rate": 0.0002962216248506571,
      "loss": 7.0986,
      "step": 2730
    },
    {
      "epoch": 0.8158936440361491,
      "grad_norm": 0.5311684608459473,
      "learning_rate": 0.0002961469534050179,
      "loss": 7.0713,
      "step": 2731
    },
    {
      "epoch": 0.8161923967435954,
      "grad_norm": 0.3972916007041931,
      "learning_rate": 0.00029607228195937873,
      "loss": 7.6094,
      "step": 2732
    },
    {
      "epoch": 0.816491149451042,
      "grad_norm": 0.46618443727493286,
      "learning_rate": 0.0002959976105137396,
      "loss": 7.0176,
      "step": 2733
    },
    {
      "epoch": 0.8167899021584883,
      "grad_norm": 0.4063912034034729,
      "learning_rate": 0.00029592293906810036,
      "loss": 7.4053,
      "step": 2734
    },
    {
      "epoch": 0.8170886548659347,
      "grad_norm": 0.4551185369491577,
      "learning_rate": 0.0002958482676224612,
      "loss": 7.2344,
      "step": 2735
    },
    {
      "epoch": 0.8173874075733811,
      "grad_norm": 0.5490186214447021,
      "learning_rate": 0.000295773596176822,
      "loss": 7.0293,
      "step": 2736
    },
    {
      "epoch": 0.8176861602808275,
      "grad_norm": 0.4851539731025696,
      "learning_rate": 0.0002956989247311828,
      "loss": 7.7021,
      "step": 2737
    },
    {
      "epoch": 0.8179849129882739,
      "grad_norm": 0.45636168122291565,
      "learning_rate": 0.0002956242532855436,
      "loss": 7.1289,
      "step": 2738
    },
    {
      "epoch": 0.8182836656957204,
      "grad_norm": 0.47200751304626465,
      "learning_rate": 0.0002955495818399044,
      "loss": 7.3857,
      "step": 2739
    },
    {
      "epoch": 0.8185824184031668,
      "grad_norm": 0.40828004479408264,
      "learning_rate": 0.00029547491039426523,
      "loss": 7.5801,
      "step": 2740
    },
    {
      "epoch": 0.8188811711106132,
      "grad_norm": 0.42993247509002686,
      "learning_rate": 0.00029540023894862605,
      "loss": 7.3535,
      "step": 2741
    },
    {
      "epoch": 0.8191799238180596,
      "grad_norm": 0.34438610076904297,
      "learning_rate": 0.0002953255675029869,
      "loss": 7.7012,
      "step": 2742
    },
    {
      "epoch": 0.819478676525506,
      "grad_norm": 0.5416620373725891,
      "learning_rate": 0.00029525089605734767,
      "loss": 7.0225,
      "step": 2743
    },
    {
      "epoch": 0.8197774292329524,
      "grad_norm": 0.3738478720188141,
      "learning_rate": 0.0002951762246117085,
      "loss": 7.4219,
      "step": 2744
    },
    {
      "epoch": 0.8200761819403989,
      "grad_norm": 0.3108364939689636,
      "learning_rate": 0.0002951015531660693,
      "loss": 7.7168,
      "step": 2745
    },
    {
      "epoch": 0.8203749346478453,
      "grad_norm": 0.5638368129730225,
      "learning_rate": 0.0002950268817204301,
      "loss": 7.1123,
      "step": 2746
    },
    {
      "epoch": 0.8206736873552917,
      "grad_norm": 0.5048816204071045,
      "learning_rate": 0.0002949522102747909,
      "loss": 7.0312,
      "step": 2747
    },
    {
      "epoch": 0.8209724400627381,
      "grad_norm": 0.397124320268631,
      "learning_rate": 0.00029487753882915174,
      "loss": 7.877,
      "step": 2748
    },
    {
      "epoch": 0.8212711927701845,
      "grad_norm": 0.4160412549972534,
      "learning_rate": 0.00029480286738351255,
      "loss": 7.3232,
      "step": 2749
    },
    {
      "epoch": 0.8215699454776308,
      "grad_norm": 0.6077925562858582,
      "learning_rate": 0.00029472819593787336,
      "loss": 6.4902,
      "step": 2750
    },
    {
      "epoch": 0.8218686981850774,
      "grad_norm": 0.4662589430809021,
      "learning_rate": 0.0002946535244922341,
      "loss": 7.0039,
      "step": 2751
    },
    {
      "epoch": 0.8221674508925237,
      "grad_norm": 0.4891437888145447,
      "learning_rate": 0.000294578853046595,
      "loss": 7.0586,
      "step": 2752
    },
    {
      "epoch": 0.8224662035999701,
      "grad_norm": 0.488322913646698,
      "learning_rate": 0.0002945041816009558,
      "loss": 6.8516,
      "step": 2753
    },
    {
      "epoch": 0.8227649563074165,
      "grad_norm": 0.42977675795555115,
      "learning_rate": 0.0002944295101553166,
      "loss": 7.4551,
      "step": 2754
    },
    {
      "epoch": 0.8230637090148629,
      "grad_norm": 0.48707467317581177,
      "learning_rate": 0.00029435483870967743,
      "loss": 6.8418,
      "step": 2755
    },
    {
      "epoch": 0.8233624617223093,
      "grad_norm": 0.4252900779247284,
      "learning_rate": 0.00029428016726403824,
      "loss": 7.2246,
      "step": 2756
    },
    {
      "epoch": 0.8236612144297558,
      "grad_norm": 0.41125965118408203,
      "learning_rate": 0.00029420549581839905,
      "loss": 7.4092,
      "step": 2757
    },
    {
      "epoch": 0.8239599671372022,
      "grad_norm": 0.53678297996521,
      "learning_rate": 0.00029413082437275987,
      "loss": 7.1699,
      "step": 2758
    },
    {
      "epoch": 0.8242587198446486,
      "grad_norm": 0.4547121822834015,
      "learning_rate": 0.0002940561529271207,
      "loss": 7.2441,
      "step": 2759
    },
    {
      "epoch": 0.824557472552095,
      "grad_norm": 0.5361278653144836,
      "learning_rate": 0.00029398148148148144,
      "loss": 6.4541,
      "step": 2760
    },
    {
      "epoch": 0.8248562252595414,
      "grad_norm": 0.48177865147590637,
      "learning_rate": 0.0002939068100358423,
      "loss": 7.5576,
      "step": 2761
    },
    {
      "epoch": 0.8251549779669878,
      "grad_norm": 0.4187924265861511,
      "learning_rate": 0.0002938321385902031,
      "loss": 7.3672,
      "step": 2762
    },
    {
      "epoch": 0.8254537306744343,
      "grad_norm": 0.40285375714302063,
      "learning_rate": 0.00029375746714456393,
      "loss": 7.4199,
      "step": 2763
    },
    {
      "epoch": 0.8257524833818807,
      "grad_norm": 0.44679519534111023,
      "learning_rate": 0.00029368279569892474,
      "loss": 7.3623,
      "step": 2764
    },
    {
      "epoch": 0.8260512360893271,
      "grad_norm": 0.41308847069740295,
      "learning_rate": 0.00029360812425328556,
      "loss": 7.4297,
      "step": 2765
    },
    {
      "epoch": 0.8263499887967735,
      "grad_norm": 0.48299095034599304,
      "learning_rate": 0.00029353345280764637,
      "loss": 7.6406,
      "step": 2766
    },
    {
      "epoch": 0.8266487415042199,
      "grad_norm": 0.44991424679756165,
      "learning_rate": 0.00029345878136200713,
      "loss": 7.0586,
      "step": 2767
    },
    {
      "epoch": 0.8269474942116662,
      "grad_norm": 0.44403502345085144,
      "learning_rate": 0.000293384109916368,
      "loss": 7.042,
      "step": 2768
    },
    {
      "epoch": 0.8272462469191127,
      "grad_norm": 0.4693276584148407,
      "learning_rate": 0.00029330943847072875,
      "loss": 7.3975,
      "step": 2769
    },
    {
      "epoch": 0.8275449996265591,
      "grad_norm": 0.45607784390449524,
      "learning_rate": 0.0002932347670250896,
      "loss": 7.1592,
      "step": 2770
    },
    {
      "epoch": 0.8278437523340055,
      "grad_norm": 0.5658563375473022,
      "learning_rate": 0.00029316009557945043,
      "loss": 6.4258,
      "step": 2771
    },
    {
      "epoch": 0.8281425050414519,
      "grad_norm": 0.5063434839248657,
      "learning_rate": 0.00029308542413381125,
      "loss": 7.1543,
      "step": 2772
    },
    {
      "epoch": 0.8284412577488983,
      "grad_norm": 0.42354103922843933,
      "learning_rate": 0.00029301075268817206,
      "loss": 7.7734,
      "step": 2773
    },
    {
      "epoch": 0.8287400104563447,
      "grad_norm": 0.3754386305809021,
      "learning_rate": 0.00029293608124253287,
      "loss": 7.6035,
      "step": 2774
    },
    {
      "epoch": 0.8290387631637912,
      "grad_norm": 0.4234398901462555,
      "learning_rate": 0.0002928614097968937,
      "loss": 7.6602,
      "step": 2775
    },
    {
      "epoch": 0.8293375158712376,
      "grad_norm": 0.5376129150390625,
      "learning_rate": 0.00029278673835125444,
      "loss": 7.1572,
      "step": 2776
    },
    {
      "epoch": 0.829636268578684,
      "grad_norm": 0.5681226253509521,
      "learning_rate": 0.0002927120669056153,
      "loss": 7.2871,
      "step": 2777
    },
    {
      "epoch": 0.8299350212861304,
      "grad_norm": 0.45281997323036194,
      "learning_rate": 0.00029263739545997607,
      "loss": 7.2393,
      "step": 2778
    },
    {
      "epoch": 0.8302337739935768,
      "grad_norm": 0.4685898423194885,
      "learning_rate": 0.00029256272401433694,
      "loss": 7.2998,
      "step": 2779
    },
    {
      "epoch": 0.8305325267010232,
      "grad_norm": 0.47511300444602966,
      "learning_rate": 0.00029248805256869775,
      "loss": 6.8652,
      "step": 2780
    },
    {
      "epoch": 0.8308312794084697,
      "grad_norm": 0.39833545684814453,
      "learning_rate": 0.00029241338112305856,
      "loss": 7.6709,
      "step": 2781
    },
    {
      "epoch": 0.8311300321159161,
      "grad_norm": 0.4906240999698639,
      "learning_rate": 0.0002923387096774194,
      "loss": 7.1699,
      "step": 2782
    },
    {
      "epoch": 0.8314287848233625,
      "grad_norm": 0.4301178455352783,
      "learning_rate": 0.00029226403823178013,
      "loss": 7.3242,
      "step": 2783
    },
    {
      "epoch": 0.8317275375308089,
      "grad_norm": 0.5479039549827576,
      "learning_rate": 0.000292189366786141,
      "loss": 7.167,
      "step": 2784
    },
    {
      "epoch": 0.8320262902382553,
      "grad_norm": 0.4972570836544037,
      "learning_rate": 0.00029211469534050176,
      "loss": 6.8447,
      "step": 2785
    },
    {
      "epoch": 0.8323250429457016,
      "grad_norm": 0.4153672158718109,
      "learning_rate": 0.0002920400238948626,
      "loss": 7.2568,
      "step": 2786
    },
    {
      "epoch": 0.8326237956531481,
      "grad_norm": 0.5086768865585327,
      "learning_rate": 0.0002919653524492234,
      "loss": 7.2725,
      "step": 2787
    },
    {
      "epoch": 0.8329225483605945,
      "grad_norm": 0.4380502998828888,
      "learning_rate": 0.00029189068100358425,
      "loss": 7.2061,
      "step": 2788
    },
    {
      "epoch": 0.8332213010680409,
      "grad_norm": 0.4115675389766693,
      "learning_rate": 0.00029181600955794507,
      "loss": 7.2686,
      "step": 2789
    },
    {
      "epoch": 0.8335200537754873,
      "grad_norm": 0.39954736828804016,
      "learning_rate": 0.0002917413381123059,
      "loss": 6.7812,
      "step": 2790
    },
    {
      "epoch": 0.8338188064829337,
      "grad_norm": 0.5337361097335815,
      "learning_rate": 0.0002916666666666667,
      "loss": 6.9814,
      "step": 2791
    },
    {
      "epoch": 0.8341175591903801,
      "grad_norm": 0.4807845950126648,
      "learning_rate": 0.00029159199522102745,
      "loss": 6.9453,
      "step": 2792
    },
    {
      "epoch": 0.8344163118978266,
      "grad_norm": 0.453733891248703,
      "learning_rate": 0.0002915173237753883,
      "loss": 7.1494,
      "step": 2793
    },
    {
      "epoch": 0.834715064605273,
      "grad_norm": 0.4082781672477722,
      "learning_rate": 0.0002914426523297491,
      "loss": 7.1973,
      "step": 2794
    },
    {
      "epoch": 0.8350138173127194,
      "grad_norm": 0.3839901387691498,
      "learning_rate": 0.00029136798088410994,
      "loss": 7.708,
      "step": 2795
    },
    {
      "epoch": 0.8353125700201658,
      "grad_norm": 0.463464617729187,
      "learning_rate": 0.0002912933094384707,
      "loss": 7.0264,
      "step": 2796
    },
    {
      "epoch": 0.8356113227276122,
      "grad_norm": 0.5028683543205261,
      "learning_rate": 0.00029121863799283157,
      "loss": 7.1279,
      "step": 2797
    },
    {
      "epoch": 0.8359100754350586,
      "grad_norm": 0.3782767355442047,
      "learning_rate": 0.0002911439665471924,
      "loss": 7.6543,
      "step": 2798
    },
    {
      "epoch": 0.8362088281425051,
      "grad_norm": 0.3990234434604645,
      "learning_rate": 0.00029106929510155314,
      "loss": 7.582,
      "step": 2799
    },
    {
      "epoch": 0.8365075808499515,
      "grad_norm": 0.5186562538146973,
      "learning_rate": 0.000290994623655914,
      "loss": 6.7256,
      "step": 2800
    },
    {
      "epoch": 0.8365075808499515,
      "eval_bleu": 0.12690381220498925,
      "eval_loss": 7.03515625,
      "eval_runtime": 504.7559,
      "eval_samples_per_second": 2.791,
      "eval_steps_per_second": 0.176,
      "step": 2800
    },
    {
      "epoch": 0.8368063335573979,
      "grad_norm": 0.46354278922080994,
      "learning_rate": 0.00029091995221027477,
      "loss": 7.083,
      "step": 2801
    },
    {
      "epoch": 0.8371050862648443,
      "grad_norm": 0.403009295463562,
      "learning_rate": 0.00029084528076463563,
      "loss": 7.1758,
      "step": 2802
    },
    {
      "epoch": 0.8374038389722906,
      "grad_norm": 0.39849457144737244,
      "learning_rate": 0.0002907706093189964,
      "loss": 7.2354,
      "step": 2803
    },
    {
      "epoch": 0.8377025916797372,
      "grad_norm": 0.430390864610672,
      "learning_rate": 0.00029069593787335726,
      "loss": 7.1777,
      "step": 2804
    },
    {
      "epoch": 0.8380013443871835,
      "grad_norm": 0.4660358428955078,
      "learning_rate": 0.000290621266427718,
      "loss": 7.1318,
      "step": 2805
    },
    {
      "epoch": 0.8383000970946299,
      "grad_norm": 0.4522877335548401,
      "learning_rate": 0.0002905465949820789,
      "loss": 7.0684,
      "step": 2806
    },
    {
      "epoch": 0.8385988498020763,
      "grad_norm": 0.4851502776145935,
      "learning_rate": 0.0002904719235364397,
      "loss": 7.209,
      "step": 2807
    },
    {
      "epoch": 0.8388976025095227,
      "grad_norm": 0.49791714549064636,
      "learning_rate": 0.00029039725209080046,
      "loss": 6.1035,
      "step": 2808
    },
    {
      "epoch": 0.8391963552169691,
      "grad_norm": 0.5513445734977722,
      "learning_rate": 0.0002903225806451613,
      "loss": 6.7373,
      "step": 2809
    },
    {
      "epoch": 0.8394951079244156,
      "grad_norm": 0.4603677988052368,
      "learning_rate": 0.0002902479091995221,
      "loss": 6.7119,
      "step": 2810
    },
    {
      "epoch": 0.839793860631862,
      "grad_norm": 0.3717879056930542,
      "learning_rate": 0.00029017323775388295,
      "loss": 7.4316,
      "step": 2811
    },
    {
      "epoch": 0.8400926133393084,
      "grad_norm": 0.39114275574684143,
      "learning_rate": 0.0002900985663082437,
      "loss": 7.3389,
      "step": 2812
    },
    {
      "epoch": 0.8403913660467548,
      "grad_norm": 0.4411059319972992,
      "learning_rate": 0.0002900238948626046,
      "loss": 7.1787,
      "step": 2813
    },
    {
      "epoch": 0.8406901187542012,
      "grad_norm": 0.4096446633338928,
      "learning_rate": 0.00028994922341696533,
      "loss": 7.4043,
      "step": 2814
    },
    {
      "epoch": 0.8409888714616476,
      "grad_norm": 0.37470993399620056,
      "learning_rate": 0.00028987455197132615,
      "loss": 7.6396,
      "step": 2815
    },
    {
      "epoch": 0.8412876241690941,
      "grad_norm": 0.4676782190799713,
      "learning_rate": 0.000289799880525687,
      "loss": 7.3594,
      "step": 2816
    },
    {
      "epoch": 0.8415863768765405,
      "grad_norm": 0.45320746302604675,
      "learning_rate": 0.00028972520908004777,
      "loss": 6.9434,
      "step": 2817
    },
    {
      "epoch": 0.8418851295839869,
      "grad_norm": 0.48977920413017273,
      "learning_rate": 0.00028965053763440864,
      "loss": 7.1143,
      "step": 2818
    },
    {
      "epoch": 0.8421838822914333,
      "grad_norm": 0.45457905530929565,
      "learning_rate": 0.0002895758661887694,
      "loss": 7.6445,
      "step": 2819
    },
    {
      "epoch": 0.8424826349988797,
      "grad_norm": 0.4586370289325714,
      "learning_rate": 0.00028950119474313026,
      "loss": 7.6709,
      "step": 2820
    },
    {
      "epoch": 0.842781387706326,
      "grad_norm": 0.45420777797698975,
      "learning_rate": 0.000289426523297491,
      "loss": 7.0195,
      "step": 2821
    },
    {
      "epoch": 0.8430801404137725,
      "grad_norm": 0.4512117803096771,
      "learning_rate": 0.0002893518518518519,
      "loss": 7.0166,
      "step": 2822
    },
    {
      "epoch": 0.8433788931212189,
      "grad_norm": 0.4356169104576111,
      "learning_rate": 0.00028927718040621265,
      "loss": 7.3945,
      "step": 2823
    },
    {
      "epoch": 0.8436776458286653,
      "grad_norm": 0.37790659070014954,
      "learning_rate": 0.00028920250896057346,
      "loss": 7.4385,
      "step": 2824
    },
    {
      "epoch": 0.8439763985361117,
      "grad_norm": 0.4289514422416687,
      "learning_rate": 0.00028912783751493433,
      "loss": 7.0654,
      "step": 2825
    },
    {
      "epoch": 0.8442751512435581,
      "grad_norm": 0.43188753724098206,
      "learning_rate": 0.0002890531660692951,
      "loss": 7.3154,
      "step": 2826
    },
    {
      "epoch": 0.8445739039510045,
      "grad_norm": 0.5246962308883667,
      "learning_rate": 0.00028897849462365595,
      "loss": 7.1445,
      "step": 2827
    },
    {
      "epoch": 0.844872656658451,
      "grad_norm": 0.4918539226055145,
      "learning_rate": 0.0002889038231780167,
      "loss": 6.9873,
      "step": 2828
    },
    {
      "epoch": 0.8451714093658974,
      "grad_norm": 0.4385266602039337,
      "learning_rate": 0.0002888291517323776,
      "loss": 7.1426,
      "step": 2829
    },
    {
      "epoch": 0.8454701620733438,
      "grad_norm": 0.44617605209350586,
      "learning_rate": 0.00028875448028673834,
      "loss": 7.2627,
      "step": 2830
    },
    {
      "epoch": 0.8457689147807902,
      "grad_norm": 0.5083487629890442,
      "learning_rate": 0.00028867980884109915,
      "loss": 6.3945,
      "step": 2831
    },
    {
      "epoch": 0.8460676674882366,
      "grad_norm": 0.5343105792999268,
      "learning_rate": 0.00028860513739545996,
      "loss": 6.332,
      "step": 2832
    },
    {
      "epoch": 0.846366420195683,
      "grad_norm": 0.4042366147041321,
      "learning_rate": 0.0002885304659498208,
      "loss": 7.3389,
      "step": 2833
    },
    {
      "epoch": 0.8466651729031295,
      "grad_norm": 0.5385666489601135,
      "learning_rate": 0.00028845579450418164,
      "loss": 7.4248,
      "step": 2834
    },
    {
      "epoch": 0.8469639256105759,
      "grad_norm": 0.39818626642227173,
      "learning_rate": 0.0002883811230585424,
      "loss": 7.209,
      "step": 2835
    },
    {
      "epoch": 0.8472626783180223,
      "grad_norm": 0.4604324400424957,
      "learning_rate": 0.00028830645161290327,
      "loss": 7.4082,
      "step": 2836
    },
    {
      "epoch": 0.8475614310254687,
      "grad_norm": 0.3963361978530884,
      "learning_rate": 0.00028823178016726403,
      "loss": 7.251,
      "step": 2837
    },
    {
      "epoch": 0.847860183732915,
      "grad_norm": 0.4189436137676239,
      "learning_rate": 0.0002881571087216249,
      "loss": 7.6562,
      "step": 2838
    },
    {
      "epoch": 0.8481589364403614,
      "grad_norm": 0.43354400992393494,
      "learning_rate": 0.00028808243727598565,
      "loss": 7.8213,
      "step": 2839
    },
    {
      "epoch": 0.848457689147808,
      "grad_norm": 0.41382092237472534,
      "learning_rate": 0.00028800776583034647,
      "loss": 7.4922,
      "step": 2840
    },
    {
      "epoch": 0.8487564418552543,
      "grad_norm": 0.4236491024494171,
      "learning_rate": 0.0002879330943847073,
      "loss": 7.2637,
      "step": 2841
    },
    {
      "epoch": 0.8490551945627007,
      "grad_norm": 0.43338528275489807,
      "learning_rate": 0.0002878584229390681,
      "loss": 7.5107,
      "step": 2842
    },
    {
      "epoch": 0.8493539472701471,
      "grad_norm": 0.43825942277908325,
      "learning_rate": 0.0002877837514934289,
      "loss": 6.9746,
      "step": 2843
    },
    {
      "epoch": 0.8496526999775935,
      "grad_norm": 0.3931494355201721,
      "learning_rate": 0.0002877090800477897,
      "loss": 7.6904,
      "step": 2844
    },
    {
      "epoch": 0.8499514526850399,
      "grad_norm": 0.38716018199920654,
      "learning_rate": 0.0002876344086021506,
      "loss": 7.335,
      "step": 2845
    },
    {
      "epoch": 0.8502502053924864,
      "grad_norm": 0.5072653889656067,
      "learning_rate": 0.00028755973715651134,
      "loss": 6.7949,
      "step": 2846
    },
    {
      "epoch": 0.8505489580999328,
      "grad_norm": 0.4134554862976074,
      "learning_rate": 0.00028748506571087216,
      "loss": 7.2334,
      "step": 2847
    },
    {
      "epoch": 0.8508477108073792,
      "grad_norm": 0.4421137869358063,
      "learning_rate": 0.00028741039426523297,
      "loss": 7.4424,
      "step": 2848
    },
    {
      "epoch": 0.8511464635148256,
      "grad_norm": 0.4350341260433197,
      "learning_rate": 0.0002873357228195938,
      "loss": 7.1875,
      "step": 2849
    },
    {
      "epoch": 0.851445216222272,
      "grad_norm": 0.5220778584480286,
      "learning_rate": 0.0002872610513739546,
      "loss": 7.1641,
      "step": 2850
    },
    {
      "epoch": 0.8517439689297184,
      "grad_norm": 0.42403513193130493,
      "learning_rate": 0.0002871863799283154,
      "loss": 7.5869,
      "step": 2851
    },
    {
      "epoch": 0.8520427216371649,
      "grad_norm": 0.40837427973747253,
      "learning_rate": 0.0002871117084826762,
      "loss": 7.5938,
      "step": 2852
    },
    {
      "epoch": 0.8523414743446113,
      "grad_norm": 0.5987548828125,
      "learning_rate": 0.00028703703703703703,
      "loss": 7.1836,
      "step": 2853
    },
    {
      "epoch": 0.8526402270520577,
      "grad_norm": 0.45303720235824585,
      "learning_rate": 0.0002869623655913979,
      "loss": 7.1484,
      "step": 2854
    },
    {
      "epoch": 0.8529389797595041,
      "grad_norm": 0.4788224995136261,
      "learning_rate": 0.00028688769414575866,
      "loss": 7.2891,
      "step": 2855
    },
    {
      "epoch": 0.8532377324669504,
      "grad_norm": 0.45350348949432373,
      "learning_rate": 0.0002868130227001195,
      "loss": 7.1758,
      "step": 2856
    },
    {
      "epoch": 0.8535364851743968,
      "grad_norm": 0.44084250926971436,
      "learning_rate": 0.0002867383512544803,
      "loss": 7.5225,
      "step": 2857
    },
    {
      "epoch": 0.8538352378818433,
      "grad_norm": 0.33775052428245544,
      "learning_rate": 0.0002866636798088411,
      "loss": 7.5225,
      "step": 2858
    },
    {
      "epoch": 0.8541339905892897,
      "grad_norm": 0.403914213180542,
      "learning_rate": 0.0002865890083632019,
      "loss": 7.5059,
      "step": 2859
    },
    {
      "epoch": 0.8544327432967361,
      "grad_norm": 0.5942113399505615,
      "learning_rate": 0.0002865143369175627,
      "loss": 6.7207,
      "step": 2860
    },
    {
      "epoch": 0.8547314960041825,
      "grad_norm": 0.500728189945221,
      "learning_rate": 0.00028643966547192354,
      "loss": 7.2178,
      "step": 2861
    },
    {
      "epoch": 0.8550302487116289,
      "grad_norm": 0.4406989812850952,
      "learning_rate": 0.00028636499402628435,
      "loss": 7.249,
      "step": 2862
    },
    {
      "epoch": 0.8553290014190753,
      "grad_norm": 0.4074143171310425,
      "learning_rate": 0.00028629032258064516,
      "loss": 7.1982,
      "step": 2863
    },
    {
      "epoch": 0.8556277541265218,
      "grad_norm": 0.43442296981811523,
      "learning_rate": 0.000286215651135006,
      "loss": 7.2939,
      "step": 2864
    },
    {
      "epoch": 0.8559265068339682,
      "grad_norm": 0.4509405791759491,
      "learning_rate": 0.0002861409796893668,
      "loss": 7.4443,
      "step": 2865
    },
    {
      "epoch": 0.8562252595414146,
      "grad_norm": 0.4355354607105255,
      "learning_rate": 0.0002860663082437276,
      "loss": 7.2236,
      "step": 2866
    },
    {
      "epoch": 0.856524012248861,
      "grad_norm": 0.4965980350971222,
      "learning_rate": 0.0002859916367980884,
      "loss": 6.9814,
      "step": 2867
    },
    {
      "epoch": 0.8568227649563074,
      "grad_norm": 0.43170782923698425,
      "learning_rate": 0.00028591696535244923,
      "loss": 7.4512,
      "step": 2868
    },
    {
      "epoch": 0.8571215176637538,
      "grad_norm": 0.4363245964050293,
      "learning_rate": 0.00028584229390681004,
      "loss": 7.2676,
      "step": 2869
    },
    {
      "epoch": 0.8574202703712003,
      "grad_norm": 0.3461873531341553,
      "learning_rate": 0.00028576762246117085,
      "loss": 7.7109,
      "step": 2870
    },
    {
      "epoch": 0.8577190230786467,
      "grad_norm": 0.44332432746887207,
      "learning_rate": 0.00028569295101553167,
      "loss": 6.9199,
      "step": 2871
    },
    {
      "epoch": 0.8580177757860931,
      "grad_norm": 0.4478139579296112,
      "learning_rate": 0.0002856182795698925,
      "loss": 7.3174,
      "step": 2872
    },
    {
      "epoch": 0.8583165284935395,
      "grad_norm": 0.4601520299911499,
      "learning_rate": 0.0002855436081242533,
      "loss": 7.5898,
      "step": 2873
    },
    {
      "epoch": 0.8586152812009858,
      "grad_norm": 0.4141494333744049,
      "learning_rate": 0.0002854689366786141,
      "loss": 7.2002,
      "step": 2874
    },
    {
      "epoch": 0.8589140339084324,
      "grad_norm": 0.46396541595458984,
      "learning_rate": 0.0002853942652329749,
      "loss": 7.1328,
      "step": 2875
    },
    {
      "epoch": 0.8592127866158787,
      "grad_norm": 0.36574432253837585,
      "learning_rate": 0.00028531959378733573,
      "loss": 7.6494,
      "step": 2876
    },
    {
      "epoch": 0.8595115393233251,
      "grad_norm": 0.43644979596138,
      "learning_rate": 0.00028524492234169654,
      "loss": 7.2725,
      "step": 2877
    },
    {
      "epoch": 0.8598102920307715,
      "grad_norm": 0.5871119499206543,
      "learning_rate": 0.00028517025089605736,
      "loss": 6.498,
      "step": 2878
    },
    {
      "epoch": 0.8601090447382179,
      "grad_norm": 0.447020024061203,
      "learning_rate": 0.0002850955794504181,
      "loss": 7.5723,
      "step": 2879
    },
    {
      "epoch": 0.8604077974456643,
      "grad_norm": 0.41751644015312195,
      "learning_rate": 0.000285020908004779,
      "loss": 7.4053,
      "step": 2880
    },
    {
      "epoch": 0.8607065501531108,
      "grad_norm": 0.4906317889690399,
      "learning_rate": 0.0002849462365591398,
      "loss": 6.708,
      "step": 2881
    },
    {
      "epoch": 0.8610053028605572,
      "grad_norm": 0.45614397525787354,
      "learning_rate": 0.0002848715651135006,
      "loss": 6.9424,
      "step": 2882
    },
    {
      "epoch": 0.8613040555680036,
      "grad_norm": 0.46700379252433777,
      "learning_rate": 0.0002847968936678614,
      "loss": 6.5537,
      "step": 2883
    },
    {
      "epoch": 0.86160280827545,
      "grad_norm": 0.6027685403823853,
      "learning_rate": 0.00028472222222222223,
      "loss": 6.4658,
      "step": 2884
    },
    {
      "epoch": 0.8619015609828964,
      "grad_norm": 0.4368104338645935,
      "learning_rate": 0.00028464755077658305,
      "loss": 7.2871,
      "step": 2885
    },
    {
      "epoch": 0.8622003136903428,
      "grad_norm": 0.369529128074646,
      "learning_rate": 0.00028457287933094386,
      "loss": 7.0273,
      "step": 2886
    },
    {
      "epoch": 0.8624990663977893,
      "grad_norm": 0.4896170198917389,
      "learning_rate": 0.00028449820788530467,
      "loss": 7.3174,
      "step": 2887
    },
    {
      "epoch": 0.8627978191052357,
      "grad_norm": 0.41821160912513733,
      "learning_rate": 0.00028442353643966543,
      "loss": 7.5664,
      "step": 2888
    },
    {
      "epoch": 0.8630965718126821,
      "grad_norm": 0.40573519468307495,
      "learning_rate": 0.0002843488649940263,
      "loss": 7.6055,
      "step": 2889
    },
    {
      "epoch": 0.8633953245201285,
      "grad_norm": 0.4379202127456665,
      "learning_rate": 0.0002842741935483871,
      "loss": 7.2969,
      "step": 2890
    },
    {
      "epoch": 0.8636940772275749,
      "grad_norm": 0.3934026062488556,
      "learning_rate": 0.0002841995221027479,
      "loss": 7.7314,
      "step": 2891
    },
    {
      "epoch": 0.8639928299350212,
      "grad_norm": 0.47262299060821533,
      "learning_rate": 0.00028412485065710874,
      "loss": 7.5703,
      "step": 2892
    },
    {
      "epoch": 0.8642915826424677,
      "grad_norm": 0.4594586193561554,
      "learning_rate": 0.00028405017921146955,
      "loss": 7.084,
      "step": 2893
    },
    {
      "epoch": 0.8645903353499141,
      "grad_norm": 0.44612208008766174,
      "learning_rate": 0.00028397550776583036,
      "loss": 6.7178,
      "step": 2894
    },
    {
      "epoch": 0.8648890880573605,
      "grad_norm": 0.45559781789779663,
      "learning_rate": 0.0002839008363201911,
      "loss": 7.1514,
      "step": 2895
    },
    {
      "epoch": 0.8651878407648069,
      "grad_norm": 0.48235535621643066,
      "learning_rate": 0.000283826164874552,
      "loss": 6.7168,
      "step": 2896
    },
    {
      "epoch": 0.8654865934722533,
      "grad_norm": 0.46010348200798035,
      "learning_rate": 0.00028375149342891275,
      "loss": 7.2461,
      "step": 2897
    },
    {
      "epoch": 0.8657853461796997,
      "grad_norm": 0.472019225358963,
      "learning_rate": 0.0002836768219832736,
      "loss": 6.7246,
      "step": 2898
    },
    {
      "epoch": 0.8660840988871462,
      "grad_norm": 0.49887433648109436,
      "learning_rate": 0.00028360215053763443,
      "loss": 6.6865,
      "step": 2899
    },
    {
      "epoch": 0.8663828515945926,
      "grad_norm": 0.4275120496749878,
      "learning_rate": 0.00028352747909199524,
      "loss": 7.4277,
      "step": 2900
    },
    {
      "epoch": 0.866681604302039,
      "grad_norm": 0.5750777721405029,
      "learning_rate": 0.00028345280764635605,
      "loss": 6.8467,
      "step": 2901
    },
    {
      "epoch": 0.8669803570094854,
      "grad_norm": 0.4918806254863739,
      "learning_rate": 0.00028337813620071687,
      "loss": 6.9053,
      "step": 2902
    },
    {
      "epoch": 0.8672791097169318,
      "grad_norm": 0.522235095500946,
      "learning_rate": 0.0002833034647550777,
      "loss": 6.7227,
      "step": 2903
    },
    {
      "epoch": 0.8675778624243782,
      "grad_norm": 0.38303613662719727,
      "learning_rate": 0.00028322879330943844,
      "loss": 7.8271,
      "step": 2904
    },
    {
      "epoch": 0.8678766151318247,
      "grad_norm": 0.3914686143398285,
      "learning_rate": 0.0002831541218637993,
      "loss": 7.4619,
      "step": 2905
    },
    {
      "epoch": 0.8681753678392711,
      "grad_norm": 0.48048970103263855,
      "learning_rate": 0.00028307945041816006,
      "loss": 7.127,
      "step": 2906
    },
    {
      "epoch": 0.8684741205467175,
      "grad_norm": 0.42292115092277527,
      "learning_rate": 0.00028300477897252093,
      "loss": 7.3105,
      "step": 2907
    },
    {
      "epoch": 0.8687728732541639,
      "grad_norm": 0.45205000042915344,
      "learning_rate": 0.00028293010752688174,
      "loss": 7.0283,
      "step": 2908
    },
    {
      "epoch": 0.8690716259616103,
      "grad_norm": 0.4818322956562042,
      "learning_rate": 0.00028285543608124256,
      "loss": 6.625,
      "step": 2909
    },
    {
      "epoch": 0.8693703786690566,
      "grad_norm": 0.4162973463535309,
      "learning_rate": 0.00028278076463560337,
      "loss": 7.1445,
      "step": 2910
    },
    {
      "epoch": 0.8696691313765031,
      "grad_norm": 0.4014207422733307,
      "learning_rate": 0.00028270609318996413,
      "loss": 7.4854,
      "step": 2911
    },
    {
      "epoch": 0.8699678840839495,
      "grad_norm": 0.4809033274650574,
      "learning_rate": 0.000282631421744325,
      "loss": 7.085,
      "step": 2912
    },
    {
      "epoch": 0.8702666367913959,
      "grad_norm": 0.431305468082428,
      "learning_rate": 0.00028255675029868575,
      "loss": 7.333,
      "step": 2913
    },
    {
      "epoch": 0.8705653894988423,
      "grad_norm": 0.4814679026603699,
      "learning_rate": 0.0002824820788530466,
      "loss": 7.4561,
      "step": 2914
    },
    {
      "epoch": 0.8708641422062887,
      "grad_norm": 0.5079514384269714,
      "learning_rate": 0.0002824074074074074,
      "loss": 6.8496,
      "step": 2915
    },
    {
      "epoch": 0.8711628949137351,
      "grad_norm": 0.49089550971984863,
      "learning_rate": 0.00028233273596176825,
      "loss": 7.0762,
      "step": 2916
    },
    {
      "epoch": 0.8714616476211816,
      "grad_norm": 0.4823707342147827,
      "learning_rate": 0.00028225806451612906,
      "loss": 7.0889,
      "step": 2917
    },
    {
      "epoch": 0.871760400328628,
      "grad_norm": 0.49863624572753906,
      "learning_rate": 0.00028218339307048987,
      "loss": 7.0996,
      "step": 2918
    },
    {
      "epoch": 0.8720591530360744,
      "grad_norm": 0.48298630118370056,
      "learning_rate": 0.0002821087216248507,
      "loss": 6.8984,
      "step": 2919
    },
    {
      "epoch": 0.8723579057435208,
      "grad_norm": 0.6056305170059204,
      "learning_rate": 0.00028203405017921144,
      "loss": 6.3584,
      "step": 2920
    },
    {
      "epoch": 0.8726566584509672,
      "grad_norm": 0.4694022536277771,
      "learning_rate": 0.0002819593787335723,
      "loss": 7.4443,
      "step": 2921
    },
    {
      "epoch": 0.8729554111584136,
      "grad_norm": 0.46023669838905334,
      "learning_rate": 0.00028188470728793307,
      "loss": 7.4453,
      "step": 2922
    },
    {
      "epoch": 0.8732541638658601,
      "grad_norm": 0.4166361093521118,
      "learning_rate": 0.00028181003584229394,
      "loss": 7.6084,
      "step": 2923
    },
    {
      "epoch": 0.8735529165733065,
      "grad_norm": 0.4136611223220825,
      "learning_rate": 0.0002817353643966547,
      "loss": 7.4053,
      "step": 2924
    },
    {
      "epoch": 0.8738516692807529,
      "grad_norm": 0.49223601818084717,
      "learning_rate": 0.00028166069295101556,
      "loss": 7.0303,
      "step": 2925
    },
    {
      "epoch": 0.8741504219881993,
      "grad_norm": 0.4737670123577118,
      "learning_rate": 0.0002815860215053764,
      "loss": 7.2627,
      "step": 2926
    },
    {
      "epoch": 0.8744491746956456,
      "grad_norm": 0.5971027612686157,
      "learning_rate": 0.00028151135005973713,
      "loss": 5.9346,
      "step": 2927
    },
    {
      "epoch": 0.874747927403092,
      "grad_norm": 0.4674547016620636,
      "learning_rate": 0.000281436678614098,
      "loss": 7.6426,
      "step": 2928
    },
    {
      "epoch": 0.8750466801105385,
      "grad_norm": 0.359771192073822,
      "learning_rate": 0.00028136200716845876,
      "loss": 7.5225,
      "step": 2929
    },
    {
      "epoch": 0.8753454328179849,
      "grad_norm": 0.4418942332267761,
      "learning_rate": 0.0002812873357228196,
      "loss": 7.3564,
      "step": 2930
    },
    {
      "epoch": 0.8756441855254313,
      "grad_norm": 0.42898648977279663,
      "learning_rate": 0.0002812126642771804,
      "loss": 7.3105,
      "step": 2931
    },
    {
      "epoch": 0.8759429382328777,
      "grad_norm": 0.3849831819534302,
      "learning_rate": 0.00028113799283154125,
      "loss": 7.7139,
      "step": 2932
    },
    {
      "epoch": 0.8762416909403241,
      "grad_norm": 0.5589433312416077,
      "learning_rate": 0.000281063321385902,
      "loss": 6.7607,
      "step": 2933
    },
    {
      "epoch": 0.8765404436477705,
      "grad_norm": 0.5176551938056946,
      "learning_rate": 0.0002809886499402629,
      "loss": 6.8428,
      "step": 2934
    },
    {
      "epoch": 0.876839196355217,
      "grad_norm": 0.47520703077316284,
      "learning_rate": 0.00028091397849462364,
      "loss": 7.0527,
      "step": 2935
    },
    {
      "epoch": 0.8771379490626634,
      "grad_norm": 0.4149945378303528,
      "learning_rate": 0.00028083930704898445,
      "loss": 7.5146,
      "step": 2936
    },
    {
      "epoch": 0.8774367017701098,
      "grad_norm": 0.3576679229736328,
      "learning_rate": 0.0002807646356033453,
      "loss": 7.6924,
      "step": 2937
    },
    {
      "epoch": 0.8777354544775562,
      "grad_norm": 0.42452865839004517,
      "learning_rate": 0.0002806899641577061,
      "loss": 6.6191,
      "step": 2938
    },
    {
      "epoch": 0.8780342071850026,
      "grad_norm": 0.4945513606071472,
      "learning_rate": 0.00028061529271206694,
      "loss": 7.2578,
      "step": 2939
    },
    {
      "epoch": 0.878332959892449,
      "grad_norm": 0.4253973066806793,
      "learning_rate": 0.0002805406212664277,
      "loss": 7.4521,
      "step": 2940
    },
    {
      "epoch": 0.8786317125998955,
      "grad_norm": 0.5239453315734863,
      "learning_rate": 0.00028046594982078857,
      "loss": 7.1846,
      "step": 2941
    },
    {
      "epoch": 0.8789304653073419,
      "grad_norm": 0.43799275159835815,
      "learning_rate": 0.0002803912783751493,
      "loss": 7.4473,
      "step": 2942
    },
    {
      "epoch": 0.8792292180147883,
      "grad_norm": 0.45847415924072266,
      "learning_rate": 0.00028031660692951014,
      "loss": 7.0674,
      "step": 2943
    },
    {
      "epoch": 0.8795279707222347,
      "grad_norm": 0.4496445655822754,
      "learning_rate": 0.00028024193548387095,
      "loss": 7.3633,
      "step": 2944
    },
    {
      "epoch": 0.879826723429681,
      "grad_norm": 0.542636513710022,
      "learning_rate": 0.00028016726403823177,
      "loss": 6.9229,
      "step": 2945
    },
    {
      "epoch": 0.8801254761371274,
      "grad_norm": 0.45753195881843567,
      "learning_rate": 0.00028009259259259263,
      "loss": 6.9717,
      "step": 2946
    },
    {
      "epoch": 0.8804242288445739,
      "grad_norm": 0.4681088626384735,
      "learning_rate": 0.0002800179211469534,
      "loss": 7.585,
      "step": 2947
    },
    {
      "epoch": 0.8807229815520203,
      "grad_norm": 0.3429173529148102,
      "learning_rate": 0.00027994324970131426,
      "loss": 7.6758,
      "step": 2948
    },
    {
      "epoch": 0.8810217342594667,
      "grad_norm": 0.43861910700798035,
      "learning_rate": 0.000279868578255675,
      "loss": 6.9229,
      "step": 2949
    },
    {
      "epoch": 0.8813204869669131,
      "grad_norm": 0.501713216304779,
      "learning_rate": 0.0002797939068100359,
      "loss": 7.4102,
      "step": 2950
    },
    {
      "epoch": 0.8816192396743595,
      "grad_norm": 0.42551591992378235,
      "learning_rate": 0.00027971923536439664,
      "loss": 7.5352,
      "step": 2951
    },
    {
      "epoch": 0.881917992381806,
      "grad_norm": 0.36159616708755493,
      "learning_rate": 0.00027964456391875746,
      "loss": 7.8438,
      "step": 2952
    },
    {
      "epoch": 0.8822167450892524,
      "grad_norm": 0.4834609627723694,
      "learning_rate": 0.00027956989247311827,
      "loss": 7.1387,
      "step": 2953
    },
    {
      "epoch": 0.8825154977966988,
      "grad_norm": 0.47438275814056396,
      "learning_rate": 0.0002794952210274791,
      "loss": 7.5146,
      "step": 2954
    },
    {
      "epoch": 0.8828142505041452,
      "grad_norm": 0.4898781180381775,
      "learning_rate": 0.00027942054958183995,
      "loss": 6.8525,
      "step": 2955
    },
    {
      "epoch": 0.8831130032115916,
      "grad_norm": 0.42437613010406494,
      "learning_rate": 0.0002793458781362007,
      "loss": 7.2969,
      "step": 2956
    },
    {
      "epoch": 0.883411755919038,
      "grad_norm": 0.46281665563583374,
      "learning_rate": 0.0002792712066905616,
      "loss": 6.8467,
      "step": 2957
    },
    {
      "epoch": 0.8837105086264845,
      "grad_norm": 0.4877702593803406,
      "learning_rate": 0.00027919653524492233,
      "loss": 7.1299,
      "step": 2958
    },
    {
      "epoch": 0.8840092613339309,
      "grad_norm": 0.45409566164016724,
      "learning_rate": 0.00027912186379928315,
      "loss": 7.3613,
      "step": 2959
    },
    {
      "epoch": 0.8843080140413773,
      "grad_norm": 0.3859798312187195,
      "learning_rate": 0.00027904719235364396,
      "loss": 7.5107,
      "step": 2960
    },
    {
      "epoch": 0.8846067667488237,
      "grad_norm": 0.6096187233924866,
      "learning_rate": 0.00027897252090800477,
      "loss": 6.6304,
      "step": 2961
    },
    {
      "epoch": 0.88490551945627,
      "grad_norm": 0.47650858759880066,
      "learning_rate": 0.0002788978494623656,
      "loss": 6.9512,
      "step": 2962
    },
    {
      "epoch": 0.8852042721637164,
      "grad_norm": 0.530070424079895,
      "learning_rate": 0.0002788231780167264,
      "loss": 6.8311,
      "step": 2963
    },
    {
      "epoch": 0.885503024871163,
      "grad_norm": 0.5538867712020874,
      "learning_rate": 0.00027874850657108726,
      "loss": 6.9492,
      "step": 2964
    },
    {
      "epoch": 0.8858017775786093,
      "grad_norm": 0.4163953959941864,
      "learning_rate": 0.000278673835125448,
      "loss": 7.6562,
      "step": 2965
    },
    {
      "epoch": 0.8861005302860557,
      "grad_norm": 0.4527166783809662,
      "learning_rate": 0.0002785991636798089,
      "loss": 7.5068,
      "step": 2966
    },
    {
      "epoch": 0.8863992829935021,
      "grad_norm": 0.4839951694011688,
      "learning_rate": 0.00027852449223416965,
      "loss": 7.4033,
      "step": 2967
    },
    {
      "epoch": 0.8866980357009485,
      "grad_norm": 0.36135029792785645,
      "learning_rate": 0.00027844982078853046,
      "loss": 7.7822,
      "step": 2968
    },
    {
      "epoch": 0.8869967884083949,
      "grad_norm": 0.42976269125938416,
      "learning_rate": 0.0002783751493428913,
      "loss": 7.3809,
      "step": 2969
    },
    {
      "epoch": 0.8872955411158414,
      "grad_norm": 0.5212759971618652,
      "learning_rate": 0.0002783004778972521,
      "loss": 6.6553,
      "step": 2970
    },
    {
      "epoch": 0.8875942938232878,
      "grad_norm": 0.4845298230648041,
      "learning_rate": 0.0002782258064516129,
      "loss": 7.2617,
      "step": 2971
    },
    {
      "epoch": 0.8878930465307342,
      "grad_norm": 0.4383121430873871,
      "learning_rate": 0.0002781511350059737,
      "loss": 7.0781,
      "step": 2972
    },
    {
      "epoch": 0.8881917992381806,
      "grad_norm": 0.5322721600532532,
      "learning_rate": 0.0002780764635603346,
      "loss": 7.0322,
      "step": 2973
    },
    {
      "epoch": 0.888490551945627,
      "grad_norm": 0.45592066645622253,
      "learning_rate": 0.00027800179211469534,
      "loss": 7.5508,
      "step": 2974
    },
    {
      "epoch": 0.8887893046530734,
      "grad_norm": 0.46145978569984436,
      "learning_rate": 0.00027792712066905615,
      "loss": 7.1416,
      "step": 2975
    },
    {
      "epoch": 0.8890880573605199,
      "grad_norm": 0.4467754065990448,
      "learning_rate": 0.00027785244922341696,
      "loss": 7.5312,
      "step": 2976
    },
    {
      "epoch": 0.8893868100679663,
      "grad_norm": 0.5047934651374817,
      "learning_rate": 0.0002777777777777778,
      "loss": 6.8848,
      "step": 2977
    },
    {
      "epoch": 0.8896855627754127,
      "grad_norm": 0.44864267110824585,
      "learning_rate": 0.0002777031063321386,
      "loss": 7.9082,
      "step": 2978
    },
    {
      "epoch": 0.8899843154828591,
      "grad_norm": 0.4704163074493408,
      "learning_rate": 0.0002776284348864994,
      "loss": 7.2949,
      "step": 2979
    },
    {
      "epoch": 0.8902830681903054,
      "grad_norm": 0.5345843434333801,
      "learning_rate": 0.0002775537634408602,
      "loss": 6.7188,
      "step": 2980
    },
    {
      "epoch": 0.8905818208977518,
      "grad_norm": 0.63283371925354,
      "learning_rate": 0.00027747909199522103,
      "loss": 6.7305,
      "step": 2981
    },
    {
      "epoch": 0.8908805736051983,
      "grad_norm": 0.5000057816505432,
      "learning_rate": 0.0002774044205495819,
      "loss": 6.5303,
      "step": 2982
    },
    {
      "epoch": 0.8911793263126447,
      "grad_norm": 0.5612693428993225,
      "learning_rate": 0.00027732974910394265,
      "loss": 6.7852,
      "step": 2983
    },
    {
      "epoch": 0.8914780790200911,
      "grad_norm": 0.3810686469078064,
      "learning_rate": 0.00027725507765830347,
      "loss": 7.5146,
      "step": 2984
    },
    {
      "epoch": 0.8917768317275375,
      "grad_norm": 0.4516923427581787,
      "learning_rate": 0.0002771804062126643,
      "loss": 7.5684,
      "step": 2985
    },
    {
      "epoch": 0.8920755844349839,
      "grad_norm": 0.4170994758605957,
      "learning_rate": 0.0002771057347670251,
      "loss": 7.7676,
      "step": 2986
    },
    {
      "epoch": 0.8923743371424303,
      "grad_norm": 0.39765268564224243,
      "learning_rate": 0.0002770310633213859,
      "loss": 7.8691,
      "step": 2987
    },
    {
      "epoch": 0.8926730898498768,
      "grad_norm": 0.546833336353302,
      "learning_rate": 0.0002769563918757467,
      "loss": 7.0469,
      "step": 2988
    },
    {
      "epoch": 0.8929718425573232,
      "grad_norm": 0.42630535364151,
      "learning_rate": 0.00027688172043010753,
      "loss": 7.5605,
      "step": 2989
    },
    {
      "epoch": 0.8932705952647696,
      "grad_norm": 0.4842611849308014,
      "learning_rate": 0.00027680704898446834,
      "loss": 7.1387,
      "step": 2990
    },
    {
      "epoch": 0.893569347972216,
      "grad_norm": 0.4373195171356201,
      "learning_rate": 0.00027673237753882916,
      "loss": 7.125,
      "step": 2991
    },
    {
      "epoch": 0.8938681006796624,
      "grad_norm": 0.49916383624076843,
      "learning_rate": 0.00027665770609318997,
      "loss": 6.9219,
      "step": 2992
    },
    {
      "epoch": 0.8941668533871088,
      "grad_norm": 0.5597240328788757,
      "learning_rate": 0.0002765830346475508,
      "loss": 6.9229,
      "step": 2993
    },
    {
      "epoch": 0.8944656060945553,
      "grad_norm": 0.5144464373588562,
      "learning_rate": 0.0002765083632019116,
      "loss": 7.0508,
      "step": 2994
    },
    {
      "epoch": 0.8947643588020017,
      "grad_norm": 0.40604355931282043,
      "learning_rate": 0.0002764336917562724,
      "loss": 6.8281,
      "step": 2995
    },
    {
      "epoch": 0.8950631115094481,
      "grad_norm": 0.45971426367759705,
      "learning_rate": 0.0002763590203106332,
      "loss": 7.1475,
      "step": 2996
    },
    {
      "epoch": 0.8953618642168945,
      "grad_norm": 0.5433006882667542,
      "learning_rate": 0.00027628434886499404,
      "loss": 6.7734,
      "step": 2997
    },
    {
      "epoch": 0.8956606169243408,
      "grad_norm": 0.4917258024215698,
      "learning_rate": 0.00027620967741935485,
      "loss": 6.8662,
      "step": 2998
    },
    {
      "epoch": 0.8959593696317872,
      "grad_norm": 0.44443991780281067,
      "learning_rate": 0.00027613500597371566,
      "loss": 7.5156,
      "step": 2999
    },
    {
      "epoch": 0.8962581223392337,
      "grad_norm": 0.5594112277030945,
      "learning_rate": 0.0002760603345280765,
      "loss": 7.3086,
      "step": 3000
    },
    {
      "epoch": 0.8962581223392337,
      "eval_bleu": 0.1240588058166003,
      "eval_loss": 7.02734375,
      "eval_runtime": 517.9684,
      "eval_samples_per_second": 2.72,
      "eval_steps_per_second": 0.172,
      "step": 3000
    },
    {
      "epoch": 0.8965568750466801,
      "grad_norm": 0.4010852873325348,
      "learning_rate": 0.0002759856630824373,
      "loss": 7.1162,
      "step": 3001
    },
    {
      "epoch": 0.8968556277541265,
      "grad_norm": 0.4727211594581604,
      "learning_rate": 0.0002759109916367981,
      "loss": 7.3164,
      "step": 3002
    },
    {
      "epoch": 0.8971543804615729,
      "grad_norm": 0.38875824213027954,
      "learning_rate": 0.0002758363201911589,
      "loss": 7.1299,
      "step": 3003
    },
    {
      "epoch": 0.8974531331690193,
      "grad_norm": 0.37102311849594116,
      "learning_rate": 0.0002757616487455197,
      "loss": 7.8271,
      "step": 3004
    },
    {
      "epoch": 0.8977518858764657,
      "grad_norm": 0.43250054121017456,
      "learning_rate": 0.00027568697729988054,
      "loss": 7.1826,
      "step": 3005
    },
    {
      "epoch": 0.8980506385839122,
      "grad_norm": 0.4239758551120758,
      "learning_rate": 0.00027561230585424135,
      "loss": 7.3135,
      "step": 3006
    },
    {
      "epoch": 0.8983493912913586,
      "grad_norm": 0.4035213887691498,
      "learning_rate": 0.0002755376344086021,
      "loss": 7.1133,
      "step": 3007
    },
    {
      "epoch": 0.898648143998805,
      "grad_norm": 0.428027480840683,
      "learning_rate": 0.000275462962962963,
      "loss": 7.8369,
      "step": 3008
    },
    {
      "epoch": 0.8989468967062514,
      "grad_norm": 0.41272491216659546,
      "learning_rate": 0.0002753882915173238,
      "loss": 7.3408,
      "step": 3009
    },
    {
      "epoch": 0.8992456494136978,
      "grad_norm": 0.41398024559020996,
      "learning_rate": 0.0002753136200716846,
      "loss": 7.4033,
      "step": 3010
    },
    {
      "epoch": 0.8995444021211442,
      "grad_norm": 0.3751905560493469,
      "learning_rate": 0.0002752389486260454,
      "loss": 7.7471,
      "step": 3011
    },
    {
      "epoch": 0.8998431548285907,
      "grad_norm": 0.4172559082508087,
      "learning_rate": 0.00027516427718040623,
      "loss": 7.2012,
      "step": 3012
    },
    {
      "epoch": 0.9001419075360371,
      "grad_norm": 0.38760825991630554,
      "learning_rate": 0.00027508960573476704,
      "loss": 7.582,
      "step": 3013
    },
    {
      "epoch": 0.9004406602434835,
      "grad_norm": 0.5005232095718384,
      "learning_rate": 0.00027501493428912785,
      "loss": 6.7051,
      "step": 3014
    },
    {
      "epoch": 0.9007394129509299,
      "grad_norm": 0.45442402362823486,
      "learning_rate": 0.00027494026284348867,
      "loss": 7.1084,
      "step": 3015
    },
    {
      "epoch": 0.9010381656583762,
      "grad_norm": 0.43522971868515015,
      "learning_rate": 0.0002748655913978494,
      "loss": 7.457,
      "step": 3016
    },
    {
      "epoch": 0.9013369183658226,
      "grad_norm": 0.4454365372657776,
      "learning_rate": 0.0002747909199522103,
      "loss": 6.8174,
      "step": 3017
    },
    {
      "epoch": 0.9016356710732691,
      "grad_norm": 0.4935460686683655,
      "learning_rate": 0.0002747162485065711,
      "loss": 6.8955,
      "step": 3018
    },
    {
      "epoch": 0.9019344237807155,
      "grad_norm": 0.5585817098617554,
      "learning_rate": 0.0002746415770609319,
      "loss": 6.6807,
      "step": 3019
    },
    {
      "epoch": 0.9022331764881619,
      "grad_norm": 0.49968817830085754,
      "learning_rate": 0.00027456690561529273,
      "loss": 7.1592,
      "step": 3020
    },
    {
      "epoch": 0.9025319291956083,
      "grad_norm": 0.4082670509815216,
      "learning_rate": 0.00027449223416965354,
      "loss": 7.8389,
      "step": 3021
    },
    {
      "epoch": 0.9028306819030547,
      "grad_norm": 0.4725155234336853,
      "learning_rate": 0.00027441756272401436,
      "loss": 7.1191,
      "step": 3022
    },
    {
      "epoch": 0.9031294346105011,
      "grad_norm": 0.4770001769065857,
      "learning_rate": 0.0002743428912783751,
      "loss": 7.1025,
      "step": 3023
    },
    {
      "epoch": 0.9034281873179476,
      "grad_norm": 0.42868557572364807,
      "learning_rate": 0.000274268219832736,
      "loss": 7.3623,
      "step": 3024
    },
    {
      "epoch": 0.903726940025394,
      "grad_norm": 0.444071888923645,
      "learning_rate": 0.00027419354838709674,
      "loss": 7.3242,
      "step": 3025
    },
    {
      "epoch": 0.9040256927328404,
      "grad_norm": 0.38254013657569885,
      "learning_rate": 0.0002741188769414576,
      "loss": 7.5752,
      "step": 3026
    },
    {
      "epoch": 0.9043244454402868,
      "grad_norm": 0.4606790244579315,
      "learning_rate": 0.00027404420549581837,
      "loss": 6.7129,
      "step": 3027
    },
    {
      "epoch": 0.9046231981477332,
      "grad_norm": 0.500129222869873,
      "learning_rate": 0.00027396953405017923,
      "loss": 6.707,
      "step": 3028
    },
    {
      "epoch": 0.9049219508551797,
      "grad_norm": 0.43154123425483704,
      "learning_rate": 0.00027389486260454005,
      "loss": 7.5459,
      "step": 3029
    },
    {
      "epoch": 0.9052207035626261,
      "grad_norm": 0.4694916307926178,
      "learning_rate": 0.00027382019115890086,
      "loss": 6.8711,
      "step": 3030
    },
    {
      "epoch": 0.9055194562700725,
      "grad_norm": 0.45488616824150085,
      "learning_rate": 0.0002737455197132617,
      "loss": 7.5908,
      "step": 3031
    },
    {
      "epoch": 0.9058182089775189,
      "grad_norm": 0.42252734303474426,
      "learning_rate": 0.00027367084826762243,
      "loss": 7.3271,
      "step": 3032
    },
    {
      "epoch": 0.9061169616849652,
      "grad_norm": 0.37149545550346375,
      "learning_rate": 0.0002735961768219833,
      "loss": 8.0381,
      "step": 3033
    },
    {
      "epoch": 0.9064157143924116,
      "grad_norm": 0.4151034355163574,
      "learning_rate": 0.00027352150537634406,
      "loss": 7.2832,
      "step": 3034
    },
    {
      "epoch": 0.9067144670998581,
      "grad_norm": 0.48256534337997437,
      "learning_rate": 0.0002734468339307049,
      "loss": 6.6465,
      "step": 3035
    },
    {
      "epoch": 0.9070132198073045,
      "grad_norm": 0.4744178056716919,
      "learning_rate": 0.0002733721624850657,
      "loss": 7.5908,
      "step": 3036
    },
    {
      "epoch": 0.9073119725147509,
      "grad_norm": 0.42264819145202637,
      "learning_rate": 0.00027329749103942655,
      "loss": 7.4961,
      "step": 3037
    },
    {
      "epoch": 0.9076107252221973,
      "grad_norm": 0.5671931505203247,
      "learning_rate": 0.00027322281959378736,
      "loss": 6.2773,
      "step": 3038
    },
    {
      "epoch": 0.9079094779296437,
      "grad_norm": 0.45222780108451843,
      "learning_rate": 0.0002731481481481481,
      "loss": 7.2979,
      "step": 3039
    },
    {
      "epoch": 0.9082082306370901,
      "grad_norm": 0.4400732219219208,
      "learning_rate": 0.000273073476702509,
      "loss": 7.7402,
      "step": 3040
    },
    {
      "epoch": 0.9085069833445366,
      "grad_norm": 0.47608810663223267,
      "learning_rate": 0.00027299880525686975,
      "loss": 7.0898,
      "step": 3041
    },
    {
      "epoch": 0.908805736051983,
      "grad_norm": 0.39826682209968567,
      "learning_rate": 0.0002729241338112306,
      "loss": 7.7549,
      "step": 3042
    },
    {
      "epoch": 0.9091044887594294,
      "grad_norm": 0.3779836595058441,
      "learning_rate": 0.0002728494623655914,
      "loss": 7.6992,
      "step": 3043
    },
    {
      "epoch": 0.9094032414668758,
      "grad_norm": 0.48976635932922363,
      "learning_rate": 0.00027277479091995224,
      "loss": 7.0762,
      "step": 3044
    },
    {
      "epoch": 0.9097019941743222,
      "grad_norm": 0.4418179392814636,
      "learning_rate": 0.000272700119474313,
      "loss": 6.8428,
      "step": 3045
    },
    {
      "epoch": 0.9100007468817686,
      "grad_norm": 0.3451249599456787,
      "learning_rate": 0.00027262544802867387,
      "loss": 7.5732,
      "step": 3046
    },
    {
      "epoch": 0.9102994995892151,
      "grad_norm": 0.43849828839302063,
      "learning_rate": 0.0002725507765830347,
      "loss": 7.5635,
      "step": 3047
    },
    {
      "epoch": 0.9105982522966615,
      "grad_norm": 0.5387307405471802,
      "learning_rate": 0.00027247610513739544,
      "loss": 6.8242,
      "step": 3048
    },
    {
      "epoch": 0.9108970050041079,
      "grad_norm": 0.45980170369148254,
      "learning_rate": 0.0002724014336917563,
      "loss": 7.0059,
      "step": 3049
    },
    {
      "epoch": 0.9111957577115543,
      "grad_norm": 0.45224636793136597,
      "learning_rate": 0.00027232676224611706,
      "loss": 7.6299,
      "step": 3050
    },
    {
      "epoch": 0.9114945104190006,
      "grad_norm": 0.5801042914390564,
      "learning_rate": 0.00027225209080047793,
      "loss": 6.5566,
      "step": 3051
    },
    {
      "epoch": 0.911793263126447,
      "grad_norm": 0.5549864768981934,
      "learning_rate": 0.0002721774193548387,
      "loss": 6.5244,
      "step": 3052
    },
    {
      "epoch": 0.9120920158338935,
      "grad_norm": 0.33138638734817505,
      "learning_rate": 0.00027210274790919956,
      "loss": 7.7354,
      "step": 3053
    },
    {
      "epoch": 0.9123907685413399,
      "grad_norm": 0.5261480212211609,
      "learning_rate": 0.0002720280764635603,
      "loss": 7.2451,
      "step": 3054
    },
    {
      "epoch": 0.9126895212487863,
      "grad_norm": 0.4701143801212311,
      "learning_rate": 0.00027195340501792113,
      "loss": 7.3887,
      "step": 3055
    },
    {
      "epoch": 0.9129882739562327,
      "grad_norm": 0.4659379720687866,
      "learning_rate": 0.000271878733572282,
      "loss": 7.165,
      "step": 3056
    },
    {
      "epoch": 0.9132870266636791,
      "grad_norm": 0.3969342112541199,
      "learning_rate": 0.00027180406212664275,
      "loss": 7.3652,
      "step": 3057
    },
    {
      "epoch": 0.9135857793711255,
      "grad_norm": 0.4873097836971283,
      "learning_rate": 0.0002717293906810036,
      "loss": 7.127,
      "step": 3058
    },
    {
      "epoch": 0.913884532078572,
      "grad_norm": 0.4841636121273041,
      "learning_rate": 0.0002716547192353644,
      "loss": 6.6357,
      "step": 3059
    },
    {
      "epoch": 0.9141832847860184,
      "grad_norm": 0.37647005915641785,
      "learning_rate": 0.00027158004778972525,
      "loss": 7.1025,
      "step": 3060
    },
    {
      "epoch": 0.9144820374934648,
      "grad_norm": 0.43410229682922363,
      "learning_rate": 0.000271505376344086,
      "loss": 7.1025,
      "step": 3061
    },
    {
      "epoch": 0.9147807902009112,
      "grad_norm": 0.4228159189224243,
      "learning_rate": 0.00027143070489844687,
      "loss": 7.2891,
      "step": 3062
    },
    {
      "epoch": 0.9150795429083576,
      "grad_norm": 0.4559284448623657,
      "learning_rate": 0.00027135603345280763,
      "loss": 6.6494,
      "step": 3063
    },
    {
      "epoch": 0.915378295615804,
      "grad_norm": 0.3752296566963196,
      "learning_rate": 0.00027128136200716844,
      "loss": 7.4082,
      "step": 3064
    },
    {
      "epoch": 0.9156770483232505,
      "grad_norm": 0.4347222149372101,
      "learning_rate": 0.0002712066905615293,
      "loss": 7.209,
      "step": 3065
    },
    {
      "epoch": 0.9159758010306969,
      "grad_norm": 0.39270371198654175,
      "learning_rate": 0.00027113201911589007,
      "loss": 7.3184,
      "step": 3066
    },
    {
      "epoch": 0.9162745537381433,
      "grad_norm": 0.5462194085121155,
      "learning_rate": 0.00027105734767025094,
      "loss": 6.373,
      "step": 3067
    },
    {
      "epoch": 0.9165733064455897,
      "grad_norm": 0.5606086850166321,
      "learning_rate": 0.0002709826762246117,
      "loss": 6.5508,
      "step": 3068
    },
    {
      "epoch": 0.916872059153036,
      "grad_norm": 0.43751218914985657,
      "learning_rate": 0.00027090800477897256,
      "loss": 7.4902,
      "step": 3069
    },
    {
      "epoch": 0.9171708118604824,
      "grad_norm": 0.48233020305633545,
      "learning_rate": 0.0002708333333333333,
      "loss": 7.0449,
      "step": 3070
    },
    {
      "epoch": 0.9174695645679289,
      "grad_norm": 0.4672624170780182,
      "learning_rate": 0.00027075866188769413,
      "loss": 7.3125,
      "step": 3071
    },
    {
      "epoch": 0.9177683172753753,
      "grad_norm": 0.42294421792030334,
      "learning_rate": 0.00027068399044205495,
      "loss": 7.1904,
      "step": 3072
    },
    {
      "epoch": 0.9180670699828217,
      "grad_norm": 0.5288398861885071,
      "learning_rate": 0.00027060931899641576,
      "loss": 6.709,
      "step": 3073
    },
    {
      "epoch": 0.9183658226902681,
      "grad_norm": 0.48642969131469727,
      "learning_rate": 0.0002705346475507766,
      "loss": 6.9287,
      "step": 3074
    },
    {
      "epoch": 0.9186645753977145,
      "grad_norm": 0.5867666602134705,
      "learning_rate": 0.0002704599761051374,
      "loss": 6.7686,
      "step": 3075
    },
    {
      "epoch": 0.9189633281051609,
      "grad_norm": 0.5323323011398315,
      "learning_rate": 0.00027038530465949825,
      "loss": 7.2412,
      "step": 3076
    },
    {
      "epoch": 0.9192620808126074,
      "grad_norm": 0.5064767599105835,
      "learning_rate": 0.000270310633213859,
      "loss": 6.7061,
      "step": 3077
    },
    {
      "epoch": 0.9195608335200538,
      "grad_norm": 0.5239399671554565,
      "learning_rate": 0.0002702359617682199,
      "loss": 6.9355,
      "step": 3078
    },
    {
      "epoch": 0.9198595862275002,
      "grad_norm": 0.4920830726623535,
      "learning_rate": 0.00027016129032258064,
      "loss": 7.0127,
      "step": 3079
    },
    {
      "epoch": 0.9201583389349466,
      "grad_norm": 0.4752103388309479,
      "learning_rate": 0.00027008661887694145,
      "loss": 7.3721,
      "step": 3080
    },
    {
      "epoch": 0.920457091642393,
      "grad_norm": 0.44249069690704346,
      "learning_rate": 0.00027001194743130226,
      "loss": 7.3613,
      "step": 3081
    },
    {
      "epoch": 0.9207558443498394,
      "grad_norm": 0.44626566767692566,
      "learning_rate": 0.0002699372759856631,
      "loss": 7.1797,
      "step": 3082
    },
    {
      "epoch": 0.9210545970572859,
      "grad_norm": 0.4112602174282074,
      "learning_rate": 0.00026986260454002394,
      "loss": 7.3145,
      "step": 3083
    },
    {
      "epoch": 0.9213533497647323,
      "grad_norm": 0.4949991703033447,
      "learning_rate": 0.0002697879330943847,
      "loss": 7.0752,
      "step": 3084
    },
    {
      "epoch": 0.9216521024721787,
      "grad_norm": 0.39958274364471436,
      "learning_rate": 0.00026971326164874557,
      "loss": 7.3613,
      "step": 3085
    },
    {
      "epoch": 0.921950855179625,
      "grad_norm": 0.47228702902793884,
      "learning_rate": 0.0002696385902031063,
      "loss": 6.9316,
      "step": 3086
    },
    {
      "epoch": 0.9222496078870714,
      "grad_norm": 0.5713127851486206,
      "learning_rate": 0.00026956391875746714,
      "loss": 6.4795,
      "step": 3087
    },
    {
      "epoch": 0.9225483605945178,
      "grad_norm": 0.47885388135910034,
      "learning_rate": 0.00026948924731182795,
      "loss": 7.1328,
      "step": 3088
    },
    {
      "epoch": 0.9228471133019643,
      "grad_norm": 0.3749237358570099,
      "learning_rate": 0.00026941457586618877,
      "loss": 7.4053,
      "step": 3089
    },
    {
      "epoch": 0.9231458660094107,
      "grad_norm": 0.4462392330169678,
      "learning_rate": 0.0002693399044205496,
      "loss": 7.4473,
      "step": 3090
    },
    {
      "epoch": 0.9234446187168571,
      "grad_norm": 0.4385829567909241,
      "learning_rate": 0.0002692652329749104,
      "loss": 7.2061,
      "step": 3091
    },
    {
      "epoch": 0.9237433714243035,
      "grad_norm": 0.514124870300293,
      "learning_rate": 0.00026919056152927126,
      "loss": 7.1572,
      "step": 3092
    },
    {
      "epoch": 0.9240421241317499,
      "grad_norm": 0.650585949420929,
      "learning_rate": 0.000269115890083632,
      "loss": 7.5693,
      "step": 3093
    },
    {
      "epoch": 0.9243408768391963,
      "grad_norm": 0.5681659579277039,
      "learning_rate": 0.0002690412186379929,
      "loss": 6.5264,
      "step": 3094
    },
    {
      "epoch": 0.9246396295466428,
      "grad_norm": 0.5702127814292908,
      "learning_rate": 0.00026896654719235364,
      "loss": 6.7129,
      "step": 3095
    },
    {
      "epoch": 0.9249383822540892,
      "grad_norm": 0.41363802552223206,
      "learning_rate": 0.00026889187574671446,
      "loss": 7.3965,
      "step": 3096
    },
    {
      "epoch": 0.9252371349615356,
      "grad_norm": 0.491700679063797,
      "learning_rate": 0.00026881720430107527,
      "loss": 7.1855,
      "step": 3097
    },
    {
      "epoch": 0.925535887668982,
      "grad_norm": 0.4122714102268219,
      "learning_rate": 0.0002687425328554361,
      "loss": 7.3564,
      "step": 3098
    },
    {
      "epoch": 0.9258346403764284,
      "grad_norm": 0.4541487991809845,
      "learning_rate": 0.0002686678614097969,
      "loss": 6.8809,
      "step": 3099
    },
    {
      "epoch": 0.9261333930838748,
      "grad_norm": 0.5342715382575989,
      "learning_rate": 0.0002685931899641577,
      "loss": 6.4453,
      "step": 3100
    },
    {
      "epoch": 0.9264321457913213,
      "grad_norm": 0.4248993396759033,
      "learning_rate": 0.0002685185185185186,
      "loss": 6.9658,
      "step": 3101
    },
    {
      "epoch": 0.9267308984987677,
      "grad_norm": 0.40554213523864746,
      "learning_rate": 0.00026844384707287933,
      "loss": 7.0732,
      "step": 3102
    },
    {
      "epoch": 0.9270296512062141,
      "grad_norm": 0.4168265461921692,
      "learning_rate": 0.00026836917562724015,
      "loss": 7.5703,
      "step": 3103
    },
    {
      "epoch": 0.9273284039136604,
      "grad_norm": 0.3743738532066345,
      "learning_rate": 0.00026829450418160096,
      "loss": 7.7041,
      "step": 3104
    },
    {
      "epoch": 0.9276271566211068,
      "grad_norm": 0.37099891901016235,
      "learning_rate": 0.00026821983273596177,
      "loss": 7.7168,
      "step": 3105
    },
    {
      "epoch": 0.9279259093285533,
      "grad_norm": 0.41195687651634216,
      "learning_rate": 0.0002681451612903226,
      "loss": 7.1191,
      "step": 3106
    },
    {
      "epoch": 0.9282246620359997,
      "grad_norm": 0.44293513894081116,
      "learning_rate": 0.0002680704898446834,
      "loss": 7.0244,
      "step": 3107
    },
    {
      "epoch": 0.9285234147434461,
      "grad_norm": 0.45549818873405457,
      "learning_rate": 0.0002679958183990442,
      "loss": 7.25,
      "step": 3108
    },
    {
      "epoch": 0.9288221674508925,
      "grad_norm": 0.43812569975852966,
      "learning_rate": 0.000267921146953405,
      "loss": 7.585,
      "step": 3109
    },
    {
      "epoch": 0.9291209201583389,
      "grad_norm": 0.47788071632385254,
      "learning_rate": 0.0002678464755077659,
      "loss": 7.2646,
      "step": 3110
    },
    {
      "epoch": 0.9294196728657853,
      "grad_norm": 0.38981035351753235,
      "learning_rate": 0.00026777180406212665,
      "loss": 7.7979,
      "step": 3111
    },
    {
      "epoch": 0.9297184255732318,
      "grad_norm": 0.4338908791542053,
      "learning_rate": 0.00026769713261648746,
      "loss": 7.4141,
      "step": 3112
    },
    {
      "epoch": 0.9300171782806782,
      "grad_norm": 0.5845314860343933,
      "learning_rate": 0.0002676224611708483,
      "loss": 6.6611,
      "step": 3113
    },
    {
      "epoch": 0.9303159309881246,
      "grad_norm": 0.4699689745903015,
      "learning_rate": 0.0002675477897252091,
      "loss": 7.0166,
      "step": 3114
    },
    {
      "epoch": 0.930614683695571,
      "grad_norm": 0.47663015127182007,
      "learning_rate": 0.0002674731182795699,
      "loss": 6.9023,
      "step": 3115
    },
    {
      "epoch": 0.9309134364030174,
      "grad_norm": 0.40390413999557495,
      "learning_rate": 0.0002673984468339307,
      "loss": 7.6621,
      "step": 3116
    },
    {
      "epoch": 0.9312121891104638,
      "grad_norm": 0.5645497441291809,
      "learning_rate": 0.0002673237753882915,
      "loss": 6.6338,
      "step": 3117
    },
    {
      "epoch": 0.9315109418179103,
      "grad_norm": 0.4679022431373596,
      "learning_rate": 0.00026724910394265234,
      "loss": 7.125,
      "step": 3118
    },
    {
      "epoch": 0.9318096945253567,
      "grad_norm": 0.5192657113075256,
      "learning_rate": 0.0002671744324970131,
      "loss": 6.9111,
      "step": 3119
    },
    {
      "epoch": 0.9321084472328031,
      "grad_norm": 0.4830639064311981,
      "learning_rate": 0.00026709976105137396,
      "loss": 6.9355,
      "step": 3120
    },
    {
      "epoch": 0.9324071999402495,
      "grad_norm": 0.49609342217445374,
      "learning_rate": 0.0002670250896057348,
      "loss": 7.0635,
      "step": 3121
    },
    {
      "epoch": 0.9327059526476958,
      "grad_norm": 0.4340016543865204,
      "learning_rate": 0.0002669504181600956,
      "loss": 7.3291,
      "step": 3122
    },
    {
      "epoch": 0.9330047053551422,
      "grad_norm": 0.4291880130767822,
      "learning_rate": 0.0002668757467144564,
      "loss": 7.2559,
      "step": 3123
    },
    {
      "epoch": 0.9333034580625887,
      "grad_norm": 0.38845208287239075,
      "learning_rate": 0.0002668010752688172,
      "loss": 7.5215,
      "step": 3124
    },
    {
      "epoch": 0.9336022107700351,
      "grad_norm": 0.49210113286972046,
      "learning_rate": 0.00026672640382317803,
      "loss": 7.1406,
      "step": 3125
    },
    {
      "epoch": 0.9339009634774815,
      "grad_norm": 0.4948279857635498,
      "learning_rate": 0.0002666517323775388,
      "loss": 7.7705,
      "step": 3126
    },
    {
      "epoch": 0.9341997161849279,
      "grad_norm": 0.5011193156242371,
      "learning_rate": 0.00026657706093189965,
      "loss": 7.5225,
      "step": 3127
    },
    {
      "epoch": 0.9344984688923743,
      "grad_norm": 0.4253412187099457,
      "learning_rate": 0.0002665023894862604,
      "loss": 7.6475,
      "step": 3128
    },
    {
      "epoch": 0.9347972215998207,
      "grad_norm": 0.49691256880760193,
      "learning_rate": 0.0002664277180406213,
      "loss": 7.0205,
      "step": 3129
    },
    {
      "epoch": 0.9350959743072672,
      "grad_norm": 0.4203842282295227,
      "learning_rate": 0.0002663530465949821,
      "loss": 7.1719,
      "step": 3130
    },
    {
      "epoch": 0.9353947270147136,
      "grad_norm": 0.49747127294540405,
      "learning_rate": 0.0002662783751493429,
      "loss": 7.209,
      "step": 3131
    },
    {
      "epoch": 0.93569347972216,
      "grad_norm": 0.4216632544994354,
      "learning_rate": 0.0002662037037037037,
      "loss": 7.5098,
      "step": 3132
    },
    {
      "epoch": 0.9359922324296064,
      "grad_norm": 0.4920337498188019,
      "learning_rate": 0.00026612903225806453,
      "loss": 7.8457,
      "step": 3133
    },
    {
      "epoch": 0.9362909851370528,
      "grad_norm": 0.39791348576545715,
      "learning_rate": 0.00026605436081242535,
      "loss": 7.4062,
      "step": 3134
    },
    {
      "epoch": 0.9365897378444992,
      "grad_norm": 0.45692312717437744,
      "learning_rate": 0.0002659796893667861,
      "loss": 7.3242,
      "step": 3135
    },
    {
      "epoch": 0.9368884905519457,
      "grad_norm": 0.38700535893440247,
      "learning_rate": 0.00026590501792114697,
      "loss": 7.4941,
      "step": 3136
    },
    {
      "epoch": 0.9371872432593921,
      "grad_norm": 0.4885309338569641,
      "learning_rate": 0.00026583034647550773,
      "loss": 7.3721,
      "step": 3137
    },
    {
      "epoch": 0.9374859959668385,
      "grad_norm": 0.42192238569259644,
      "learning_rate": 0.0002657556750298686,
      "loss": 7.1924,
      "step": 3138
    },
    {
      "epoch": 0.9377847486742849,
      "grad_norm": 0.46015799045562744,
      "learning_rate": 0.0002656810035842294,
      "loss": 7.3506,
      "step": 3139
    },
    {
      "epoch": 0.9380835013817312,
      "grad_norm": 0.43964776396751404,
      "learning_rate": 0.0002656063321385902,
      "loss": 7.6426,
      "step": 3140
    },
    {
      "epoch": 0.9383822540891776,
      "grad_norm": 0.37914666533470154,
      "learning_rate": 0.00026553166069295104,
      "loss": 7.8086,
      "step": 3141
    },
    {
      "epoch": 0.9386810067966241,
      "grad_norm": 0.4537111520767212,
      "learning_rate": 0.0002654569892473118,
      "loss": 7.1309,
      "step": 3142
    },
    {
      "epoch": 0.9389797595040705,
      "grad_norm": 0.5254588723182678,
      "learning_rate": 0.00026538231780167266,
      "loss": 7.3047,
      "step": 3143
    },
    {
      "epoch": 0.9392785122115169,
      "grad_norm": 0.53351891040802,
      "learning_rate": 0.0002653076463560334,
      "loss": 6.6875,
      "step": 3144
    },
    {
      "epoch": 0.9395772649189633,
      "grad_norm": 0.431358277797699,
      "learning_rate": 0.0002652329749103943,
      "loss": 7.2295,
      "step": 3145
    },
    {
      "epoch": 0.9398760176264097,
      "grad_norm": 0.4846522808074951,
      "learning_rate": 0.00026515830346475505,
      "loss": 7.248,
      "step": 3146
    },
    {
      "epoch": 0.9401747703338561,
      "grad_norm": 0.4495736062526703,
      "learning_rate": 0.0002650836320191159,
      "loss": 7.4414,
      "step": 3147
    },
    {
      "epoch": 0.9404735230413026,
      "grad_norm": 0.5450928211212158,
      "learning_rate": 0.0002650089605734767,
      "loss": 6.625,
      "step": 3148
    },
    {
      "epoch": 0.940772275748749,
      "grad_norm": 0.5544266700744629,
      "learning_rate": 0.00026493428912783754,
      "loss": 6.6982,
      "step": 3149
    },
    {
      "epoch": 0.9410710284561954,
      "grad_norm": 0.43990039825439453,
      "learning_rate": 0.00026485961768219835,
      "loss": 7.3848,
      "step": 3150
    },
    {
      "epoch": 0.9413697811636418,
      "grad_norm": 0.5431423783302307,
      "learning_rate": 0.0002647849462365591,
      "loss": 6.7754,
      "step": 3151
    },
    {
      "epoch": 0.9416685338710882,
      "grad_norm": 0.5232115983963013,
      "learning_rate": 0.00026471027479092,
      "loss": 6.8945,
      "step": 3152
    },
    {
      "epoch": 0.9419672865785346,
      "grad_norm": 0.5032997727394104,
      "learning_rate": 0.00026463560334528074,
      "loss": 6.7051,
      "step": 3153
    },
    {
      "epoch": 0.9422660392859811,
      "grad_norm": 0.49183034896850586,
      "learning_rate": 0.0002645609318996416,
      "loss": 7.5664,
      "step": 3154
    },
    {
      "epoch": 0.9425647919934275,
      "grad_norm": 0.5114138722419739,
      "learning_rate": 0.00026448626045400236,
      "loss": 6.7988,
      "step": 3155
    },
    {
      "epoch": 0.9428635447008739,
      "grad_norm": 0.471657395362854,
      "learning_rate": 0.00026441158900836323,
      "loss": 7.2373,
      "step": 3156
    },
    {
      "epoch": 0.9431622974083202,
      "grad_norm": 0.44614657759666443,
      "learning_rate": 0.00026433691756272404,
      "loss": 7.3105,
      "step": 3157
    },
    {
      "epoch": 0.9434610501157666,
      "grad_norm": 0.4050704538822174,
      "learning_rate": 0.0002642622461170848,
      "loss": 7.0957,
      "step": 3158
    },
    {
      "epoch": 0.943759802823213,
      "grad_norm": 0.5063801407814026,
      "learning_rate": 0.00026418757467144567,
      "loss": 7.4131,
      "step": 3159
    },
    {
      "epoch": 0.9440585555306595,
      "grad_norm": 0.4020128846168518,
      "learning_rate": 0.0002641129032258064,
      "loss": 7.9854,
      "step": 3160
    },
    {
      "epoch": 0.9443573082381059,
      "grad_norm": 0.4654957950115204,
      "learning_rate": 0.0002640382317801673,
      "loss": 7.4512,
      "step": 3161
    },
    {
      "epoch": 0.9446560609455523,
      "grad_norm": 0.46196091175079346,
      "learning_rate": 0.00026396356033452805,
      "loss": 7.6172,
      "step": 3162
    },
    {
      "epoch": 0.9449548136529987,
      "grad_norm": 0.4496631622314453,
      "learning_rate": 0.0002638888888888889,
      "loss": 7.2119,
      "step": 3163
    },
    {
      "epoch": 0.9452535663604451,
      "grad_norm": 0.505041241645813,
      "learning_rate": 0.0002638142174432497,
      "loss": 6.832,
      "step": 3164
    },
    {
      "epoch": 0.9455523190678915,
      "grad_norm": 0.4374735653400421,
      "learning_rate": 0.00026373954599761054,
      "loss": 7.4912,
      "step": 3165
    },
    {
      "epoch": 0.945851071775338,
      "grad_norm": 0.4225420653820038,
      "learning_rate": 0.00026366487455197136,
      "loss": 7.0996,
      "step": 3166
    },
    {
      "epoch": 0.9461498244827844,
      "grad_norm": 0.4464210569858551,
      "learning_rate": 0.0002635902031063321,
      "loss": 7.2568,
      "step": 3167
    },
    {
      "epoch": 0.9464485771902308,
      "grad_norm": 0.44360193610191345,
      "learning_rate": 0.000263515531660693,
      "loss": 7.2119,
      "step": 3168
    },
    {
      "epoch": 0.9467473298976772,
      "grad_norm": 0.49047011137008667,
      "learning_rate": 0.00026344086021505374,
      "loss": 6.9814,
      "step": 3169
    },
    {
      "epoch": 0.9470460826051236,
      "grad_norm": 0.4489403963088989,
      "learning_rate": 0.0002633661887694146,
      "loss": 7.6094,
      "step": 3170
    },
    {
      "epoch": 0.94734483531257,
      "grad_norm": 0.45300590991973877,
      "learning_rate": 0.00026329151732377537,
      "loss": 7.2637,
      "step": 3171
    },
    {
      "epoch": 0.9476435880200165,
      "grad_norm": 0.3944733142852783,
      "learning_rate": 0.00026321684587813623,
      "loss": 7.1006,
      "step": 3172
    },
    {
      "epoch": 0.9479423407274629,
      "grad_norm": 0.6089692711830139,
      "learning_rate": 0.000263142174432497,
      "loss": 7.876,
      "step": 3173
    },
    {
      "epoch": 0.9482410934349093,
      "grad_norm": 0.44112369418144226,
      "learning_rate": 0.0002630675029868578,
      "loss": 7.2451,
      "step": 3174
    },
    {
      "epoch": 0.9485398461423556,
      "grad_norm": 0.4388148784637451,
      "learning_rate": 0.0002629928315412187,
      "loss": 6.9551,
      "step": 3175
    },
    {
      "epoch": 0.948838598849802,
      "grad_norm": 0.4018407166004181,
      "learning_rate": 0.00026291816009557943,
      "loss": 7.5361,
      "step": 3176
    },
    {
      "epoch": 0.9491373515572484,
      "grad_norm": 0.4890943169593811,
      "learning_rate": 0.0002628434886499403,
      "loss": 7.0879,
      "step": 3177
    },
    {
      "epoch": 0.9494361042646949,
      "grad_norm": 0.3948664665222168,
      "learning_rate": 0.00026276881720430106,
      "loss": 7.6191,
      "step": 3178
    },
    {
      "epoch": 0.9497348569721413,
      "grad_norm": 0.47771942615509033,
      "learning_rate": 0.0002626941457586619,
      "loss": 7.4248,
      "step": 3179
    },
    {
      "epoch": 0.9500336096795877,
      "grad_norm": 0.459521621465683,
      "learning_rate": 0.0002626194743130227,
      "loss": 7.3223,
      "step": 3180
    },
    {
      "epoch": 0.9503323623870341,
      "grad_norm": 0.4100942611694336,
      "learning_rate": 0.00026254480286738355,
      "loss": 7.4404,
      "step": 3181
    },
    {
      "epoch": 0.9506311150944805,
      "grad_norm": 0.43356478214263916,
      "learning_rate": 0.0002624701314217443,
      "loss": 7.1738,
      "step": 3182
    },
    {
      "epoch": 0.950929867801927,
      "grad_norm": 0.4403037428855896,
      "learning_rate": 0.0002623954599761051,
      "loss": 7.4756,
      "step": 3183
    },
    {
      "epoch": 0.9512286205093734,
      "grad_norm": 0.422056645154953,
      "learning_rate": 0.000262320788530466,
      "loss": 7.4316,
      "step": 3184
    },
    {
      "epoch": 0.9515273732168198,
      "grad_norm": 0.498365581035614,
      "learning_rate": 0.00026224611708482675,
      "loss": 7.1797,
      "step": 3185
    },
    {
      "epoch": 0.9518261259242662,
      "grad_norm": 0.400730699300766,
      "learning_rate": 0.0002621714456391876,
      "loss": 7.793,
      "step": 3186
    },
    {
      "epoch": 0.9521248786317126,
      "grad_norm": 0.3953801095485687,
      "learning_rate": 0.0002620967741935484,
      "loss": 7.4473,
      "step": 3187
    },
    {
      "epoch": 0.952423631339159,
      "grad_norm": 0.4944494366645813,
      "learning_rate": 0.00026202210274790924,
      "loss": 6.8936,
      "step": 3188
    },
    {
      "epoch": 0.9527223840466055,
      "grad_norm": 0.6443288922309875,
      "learning_rate": 0.00026194743130227,
      "loss": 6.0088,
      "step": 3189
    },
    {
      "epoch": 0.9530211367540519,
      "grad_norm": 0.47080251574516296,
      "learning_rate": 0.0002618727598566308,
      "loss": 7.2783,
      "step": 3190
    },
    {
      "epoch": 0.9533198894614983,
      "grad_norm": 0.4093814194202423,
      "learning_rate": 0.0002617980884109916,
      "loss": 7.541,
      "step": 3191
    },
    {
      "epoch": 0.9536186421689447,
      "grad_norm": 0.43158450722694397,
      "learning_rate": 0.00026172341696535244,
      "loss": 7.6826,
      "step": 3192
    },
    {
      "epoch": 0.953917394876391,
      "grad_norm": 0.46406492590904236,
      "learning_rate": 0.0002616487455197133,
      "loss": 7.2646,
      "step": 3193
    },
    {
      "epoch": 0.9542161475838374,
      "grad_norm": 0.4427144527435303,
      "learning_rate": 0.00026157407407407406,
      "loss": 7.4199,
      "step": 3194
    },
    {
      "epoch": 0.9545149002912839,
      "grad_norm": 0.4760298430919647,
      "learning_rate": 0.00026149940262843493,
      "loss": 6.7266,
      "step": 3195
    },
    {
      "epoch": 0.9548136529987303,
      "grad_norm": 0.48983147740364075,
      "learning_rate": 0.0002614247311827957,
      "loss": 7.1543,
      "step": 3196
    },
    {
      "epoch": 0.9551124057061767,
      "grad_norm": 0.4251229763031006,
      "learning_rate": 0.00026135005973715656,
      "loss": 7.1455,
      "step": 3197
    },
    {
      "epoch": 0.9554111584136231,
      "grad_norm": 0.5656460523605347,
      "learning_rate": 0.0002612753882915173,
      "loss": 6.8779,
      "step": 3198
    },
    {
      "epoch": 0.9557099111210695,
      "grad_norm": 0.4198139011859894,
      "learning_rate": 0.00026120071684587813,
      "loss": 7.6846,
      "step": 3199
    },
    {
      "epoch": 0.9560086638285159,
      "grad_norm": 0.4449145495891571,
      "learning_rate": 0.00026112604540023894,
      "loss": 7.5996,
      "step": 3200
    },
    {
      "epoch": 0.9560086638285159,
      "eval_bleu": 0.12605029341730664,
      "eval_loss": 7.03515625,
      "eval_runtime": 520.103,
      "eval_samples_per_second": 2.709,
      "eval_steps_per_second": 0.171,
      "step": 3200
    },
    {
      "epoch": 0.9563074165359624,
      "grad_norm": 0.39740490913391113,
      "learning_rate": 0.00026105137395459975,
      "loss": 7.543,
      "step": 3201
    },
    {
      "epoch": 0.9566061692434088,
      "grad_norm": 0.5481922030448914,
      "learning_rate": 0.0002609767025089606,
      "loss": 6.9277,
      "step": 3202
    },
    {
      "epoch": 0.9569049219508552,
      "grad_norm": 0.5358302593231201,
      "learning_rate": 0.0002609020310633214,
      "loss": 7.4766,
      "step": 3203
    },
    {
      "epoch": 0.9572036746583016,
      "grad_norm": 0.5446147918701172,
      "learning_rate": 0.00026082735961768225,
      "loss": 6.6855,
      "step": 3204
    },
    {
      "epoch": 0.957502427365748,
      "grad_norm": 0.5018018484115601,
      "learning_rate": 0.000260752688172043,
      "loss": 6.8965,
      "step": 3205
    },
    {
      "epoch": 0.9578011800731944,
      "grad_norm": 0.5157007575035095,
      "learning_rate": 0.0002606780167264038,
      "loss": 6.2373,
      "step": 3206
    },
    {
      "epoch": 0.9580999327806409,
      "grad_norm": 0.4989023506641388,
      "learning_rate": 0.00026060334528076463,
      "loss": 6.8965,
      "step": 3207
    },
    {
      "epoch": 0.9583986854880873,
      "grad_norm": 0.44141528010368347,
      "learning_rate": 0.00026052867383512544,
      "loss": 7.3916,
      "step": 3208
    },
    {
      "epoch": 0.9586974381955337,
      "grad_norm": 0.5090194344520569,
      "learning_rate": 0.00026045400238948626,
      "loss": 6.6182,
      "step": 3209
    },
    {
      "epoch": 0.95899619090298,
      "grad_norm": 0.4751645624637604,
      "learning_rate": 0.00026037933094384707,
      "loss": 7.2383,
      "step": 3210
    },
    {
      "epoch": 0.9592949436104264,
      "grad_norm": 0.42666929960250854,
      "learning_rate": 0.0002603046594982079,
      "loss": 7.3242,
      "step": 3211
    },
    {
      "epoch": 0.9595936963178728,
      "grad_norm": 0.3692876994609833,
      "learning_rate": 0.0002602299880525687,
      "loss": 7.7188,
      "step": 3212
    },
    {
      "epoch": 0.9598924490253193,
      "grad_norm": 0.5024865865707397,
      "learning_rate": 0.00026015531660692956,
      "loss": 6.5967,
      "step": 3213
    },
    {
      "epoch": 0.9601912017327657,
      "grad_norm": 0.5681595206260681,
      "learning_rate": 0.0002600806451612903,
      "loss": 6.9736,
      "step": 3214
    },
    {
      "epoch": 0.9604899544402121,
      "grad_norm": 0.3272618353366852,
      "learning_rate": 0.00026000597371565113,
      "loss": 7.7422,
      "step": 3215
    },
    {
      "epoch": 0.9607887071476585,
      "grad_norm": 0.4934331476688385,
      "learning_rate": 0.00025993130227001195,
      "loss": 7.626,
      "step": 3216
    },
    {
      "epoch": 0.9610874598551049,
      "grad_norm": 0.9623628258705139,
      "learning_rate": 0.00025985663082437276,
      "loss": 7.4424,
      "step": 3217
    },
    {
      "epoch": 0.9613862125625513,
      "grad_norm": 0.43457311391830444,
      "learning_rate": 0.00025978195937873357,
      "loss": 7.457,
      "step": 3218
    },
    {
      "epoch": 0.9616849652699978,
      "grad_norm": 0.45221051573753357,
      "learning_rate": 0.0002597072879330944,
      "loss": 7.2734,
      "step": 3219
    },
    {
      "epoch": 0.9619837179774442,
      "grad_norm": 0.45094266533851624,
      "learning_rate": 0.0002596326164874552,
      "loss": 7.0479,
      "step": 3220
    },
    {
      "epoch": 0.9622824706848906,
      "grad_norm": 0.44267329573631287,
      "learning_rate": 0.000259557945041816,
      "loss": 7.2969,
      "step": 3221
    },
    {
      "epoch": 0.962581223392337,
      "grad_norm": 0.38326287269592285,
      "learning_rate": 0.0002594832735961768,
      "loss": 7.6416,
      "step": 3222
    },
    {
      "epoch": 0.9628799760997834,
      "grad_norm": 0.5810279250144958,
      "learning_rate": 0.00025940860215053764,
      "loss": 6.8408,
      "step": 3223
    },
    {
      "epoch": 0.9631787288072298,
      "grad_norm": 0.46516507863998413,
      "learning_rate": 0.00025933393070489845,
      "loss": 7.1045,
      "step": 3224
    },
    {
      "epoch": 0.9634774815146763,
      "grad_norm": 0.3861982226371765,
      "learning_rate": 0.00025925925925925926,
      "loss": 7.3145,
      "step": 3225
    },
    {
      "epoch": 0.9637762342221227,
      "grad_norm": 0.42502155900001526,
      "learning_rate": 0.0002591845878136201,
      "loss": 7.3408,
      "step": 3226
    },
    {
      "epoch": 0.964074986929569,
      "grad_norm": 0.4289456605911255,
      "learning_rate": 0.0002591099163679809,
      "loss": 7.1982,
      "step": 3227
    },
    {
      "epoch": 0.9643737396370154,
      "grad_norm": 0.457900732755661,
      "learning_rate": 0.0002590352449223417,
      "loss": 7.1143,
      "step": 3228
    },
    {
      "epoch": 0.9646724923444618,
      "grad_norm": 0.5021187663078308,
      "learning_rate": 0.0002589605734767025,
      "loss": 6.8916,
      "step": 3229
    },
    {
      "epoch": 0.9649712450519082,
      "grad_norm": 0.3617522716522217,
      "learning_rate": 0.0002588859020310633,
      "loss": 7.5996,
      "step": 3230
    },
    {
      "epoch": 0.9652699977593547,
      "grad_norm": 0.3796043395996094,
      "learning_rate": 0.00025881123058542414,
      "loss": 7.3516,
      "step": 3231
    },
    {
      "epoch": 0.9655687504668011,
      "grad_norm": 0.3731037676334381,
      "learning_rate": 0.00025873655913978495,
      "loss": 7.1426,
      "step": 3232
    },
    {
      "epoch": 0.9658675031742475,
      "grad_norm": 0.4378076195716858,
      "learning_rate": 0.00025866188769414577,
      "loss": 7.7988,
      "step": 3233
    },
    {
      "epoch": 0.9661662558816939,
      "grad_norm": 0.41967692971229553,
      "learning_rate": 0.0002585872162485066,
      "loss": 7.4561,
      "step": 3234
    },
    {
      "epoch": 0.9664650085891403,
      "grad_norm": 0.41400185227394104,
      "learning_rate": 0.0002585125448028674,
      "loss": 7.5449,
      "step": 3235
    },
    {
      "epoch": 0.9667637612965867,
      "grad_norm": 0.5509756803512573,
      "learning_rate": 0.0002584378733572282,
      "loss": 6.6035,
      "step": 3236
    },
    {
      "epoch": 0.9670625140040332,
      "grad_norm": 0.46910786628723145,
      "learning_rate": 0.000258363201911589,
      "loss": 6.8252,
      "step": 3237
    },
    {
      "epoch": 0.9673612667114796,
      "grad_norm": 0.40833181142807007,
      "learning_rate": 0.0002582885304659498,
      "loss": 7.1689,
      "step": 3238
    },
    {
      "epoch": 0.967660019418926,
      "grad_norm": 0.47059381008148193,
      "learning_rate": 0.00025821385902031064,
      "loss": 7.2588,
      "step": 3239
    },
    {
      "epoch": 0.9679587721263724,
      "grad_norm": 0.5471946597099304,
      "learning_rate": 0.00025813918757467146,
      "loss": 6.877,
      "step": 3240
    },
    {
      "epoch": 0.9682575248338188,
      "grad_norm": 0.39748987555503845,
      "learning_rate": 0.00025806451612903227,
      "loss": 7.5918,
      "step": 3241
    },
    {
      "epoch": 0.9685562775412652,
      "grad_norm": 0.3926355242729187,
      "learning_rate": 0.0002579898446833931,
      "loss": 7.1523,
      "step": 3242
    },
    {
      "epoch": 0.9688550302487117,
      "grad_norm": 0.4905416965484619,
      "learning_rate": 0.0002579151732377539,
      "loss": 7.1816,
      "step": 3243
    },
    {
      "epoch": 0.9691537829561581,
      "grad_norm": 0.4166035056114197,
      "learning_rate": 0.0002578405017921147,
      "loss": 7.2969,
      "step": 3244
    },
    {
      "epoch": 0.9694525356636045,
      "grad_norm": 0.3890377879142761,
      "learning_rate": 0.0002577658303464755,
      "loss": 7.8633,
      "step": 3245
    },
    {
      "epoch": 0.9697512883710508,
      "grad_norm": 0.4363602101802826,
      "learning_rate": 0.00025769115890083633,
      "loss": 7.6074,
      "step": 3246
    },
    {
      "epoch": 0.9700500410784972,
      "grad_norm": 0.38249480724334717,
      "learning_rate": 0.0002576164874551971,
      "loss": 7.3525,
      "step": 3247
    },
    {
      "epoch": 0.9703487937859436,
      "grad_norm": 0.4696820080280304,
      "learning_rate": 0.00025754181600955796,
      "loss": 7.2256,
      "step": 3248
    },
    {
      "epoch": 0.9706475464933901,
      "grad_norm": 0.49005258083343506,
      "learning_rate": 0.00025746714456391877,
      "loss": 7.3916,
      "step": 3249
    },
    {
      "epoch": 0.9709462992008365,
      "grad_norm": 0.46818259358406067,
      "learning_rate": 0.0002573924731182796,
      "loss": 7.1816,
      "step": 3250
    },
    {
      "epoch": 0.9712450519082829,
      "grad_norm": 0.4514172673225403,
      "learning_rate": 0.0002573178016726404,
      "loss": 7.4932,
      "step": 3251
    },
    {
      "epoch": 0.9715438046157293,
      "grad_norm": 0.410940021276474,
      "learning_rate": 0.0002572431302270012,
      "loss": 7.9043,
      "step": 3252
    },
    {
      "epoch": 0.9718425573231757,
      "grad_norm": 0.3742438554763794,
      "learning_rate": 0.000257168458781362,
      "loss": 7.916,
      "step": 3253
    },
    {
      "epoch": 0.9721413100306222,
      "grad_norm": 0.5205013751983643,
      "learning_rate": 0.0002570937873357228,
      "loss": 7.1855,
      "step": 3254
    },
    {
      "epoch": 0.9724400627380686,
      "grad_norm": 0.5065129399299622,
      "learning_rate": 0.00025701911589008365,
      "loss": 7.0537,
      "step": 3255
    },
    {
      "epoch": 0.972738815445515,
      "grad_norm": 0.5422341823577881,
      "learning_rate": 0.0002569444444444444,
      "loss": 6.2852,
      "step": 3256
    },
    {
      "epoch": 0.9730375681529614,
      "grad_norm": 0.4359312057495117,
      "learning_rate": 0.0002568697729988053,
      "loss": 7.2383,
      "step": 3257
    },
    {
      "epoch": 0.9733363208604078,
      "grad_norm": 0.4207788109779358,
      "learning_rate": 0.0002567951015531661,
      "loss": 7.2158,
      "step": 3258
    },
    {
      "epoch": 0.9736350735678542,
      "grad_norm": 0.4455117881298065,
      "learning_rate": 0.0002567204301075269,
      "loss": 6.9131,
      "step": 3259
    },
    {
      "epoch": 0.9739338262753007,
      "grad_norm": 0.4073818325996399,
      "learning_rate": 0.0002566457586618877,
      "loss": 7.7334,
      "step": 3260
    },
    {
      "epoch": 0.9742325789827471,
      "grad_norm": 0.46895289421081543,
      "learning_rate": 0.0002565710872162485,
      "loss": 7.6836,
      "step": 3261
    },
    {
      "epoch": 0.9745313316901935,
      "grad_norm": 0.43250563740730286,
      "learning_rate": 0.00025649641577060934,
      "loss": 7.7529,
      "step": 3262
    },
    {
      "epoch": 0.9748300843976399,
      "grad_norm": 0.3868364989757538,
      "learning_rate": 0.0002564217443249701,
      "loss": 7.3789,
      "step": 3263
    },
    {
      "epoch": 0.9751288371050862,
      "grad_norm": 0.4121173918247223,
      "learning_rate": 0.00025634707287933096,
      "loss": 7.5166,
      "step": 3264
    },
    {
      "epoch": 0.9754275898125326,
      "grad_norm": 0.45743855834007263,
      "learning_rate": 0.0002562724014336917,
      "loss": 7.6543,
      "step": 3265
    },
    {
      "epoch": 0.9757263425199791,
      "grad_norm": 0.4787059426307678,
      "learning_rate": 0.0002561977299880526,
      "loss": 7.1133,
      "step": 3266
    },
    {
      "epoch": 0.9760250952274255,
      "grad_norm": 0.45386967062950134,
      "learning_rate": 0.0002561230585424134,
      "loss": 7.2773,
      "step": 3267
    },
    {
      "epoch": 0.9763238479348719,
      "grad_norm": 0.4794892370700836,
      "learning_rate": 0.0002560483870967742,
      "loss": 6.8125,
      "step": 3268
    },
    {
      "epoch": 0.9766226006423183,
      "grad_norm": 0.30828237533569336,
      "learning_rate": 0.00025597371565113503,
      "loss": 8.0742,
      "step": 3269
    },
    {
      "epoch": 0.9769213533497647,
      "grad_norm": 0.47166967391967773,
      "learning_rate": 0.0002558990442054958,
      "loss": 7.3496,
      "step": 3270
    },
    {
      "epoch": 0.9772201060572111,
      "grad_norm": 0.42933791875839233,
      "learning_rate": 0.00025582437275985665,
      "loss": 7.2637,
      "step": 3271
    },
    {
      "epoch": 0.9775188587646576,
      "grad_norm": 0.5532766580581665,
      "learning_rate": 0.0002557497013142174,
      "loss": 7.2627,
      "step": 3272
    },
    {
      "epoch": 0.977817611472104,
      "grad_norm": 0.4276476800441742,
      "learning_rate": 0.0002556750298685783,
      "loss": 7.2812,
      "step": 3273
    },
    {
      "epoch": 0.9781163641795504,
      "grad_norm": 0.4623894989490509,
      "learning_rate": 0.00025560035842293904,
      "loss": 7.6816,
      "step": 3274
    },
    {
      "epoch": 0.9784151168869968,
      "grad_norm": 0.4286614954471588,
      "learning_rate": 0.0002555256869772999,
      "loss": 7.1758,
      "step": 3275
    },
    {
      "epoch": 0.9787138695944432,
      "grad_norm": 0.4801713526248932,
      "learning_rate": 0.0002554510155316607,
      "loss": 7.0938,
      "step": 3276
    },
    {
      "epoch": 0.9790126223018896,
      "grad_norm": 0.4683179557323456,
      "learning_rate": 0.00025537634408602153,
      "loss": 7.2764,
      "step": 3277
    },
    {
      "epoch": 0.9793113750093361,
      "grad_norm": 0.46042710542678833,
      "learning_rate": 0.00025530167264038235,
      "loss": 7.3711,
      "step": 3278
    },
    {
      "epoch": 0.9796101277167825,
      "grad_norm": 0.5028820037841797,
      "learning_rate": 0.0002552270011947431,
      "loss": 6.3135,
      "step": 3279
    },
    {
      "epoch": 0.9799088804242289,
      "grad_norm": 0.4957653284072876,
      "learning_rate": 0.00025515232974910397,
      "loss": 6.8584,
      "step": 3280
    },
    {
      "epoch": 0.9802076331316752,
      "grad_norm": 0.4449442923069,
      "learning_rate": 0.00025507765830346473,
      "loss": 6.9092,
      "step": 3281
    },
    {
      "epoch": 0.9805063858391216,
      "grad_norm": 0.46062588691711426,
      "learning_rate": 0.0002550029868578256,
      "loss": 7.3389,
      "step": 3282
    },
    {
      "epoch": 0.980805138546568,
      "grad_norm": 0.7109373807907104,
      "learning_rate": 0.00025492831541218636,
      "loss": 7.4658,
      "step": 3283
    },
    {
      "epoch": 0.9811038912540145,
      "grad_norm": 0.5107170939445496,
      "learning_rate": 0.0002548536439665472,
      "loss": 7.2305,
      "step": 3284
    },
    {
      "epoch": 0.9814026439614609,
      "grad_norm": 0.4229851961135864,
      "learning_rate": 0.00025477897252090804,
      "loss": 7.1611,
      "step": 3285
    },
    {
      "epoch": 0.9817013966689073,
      "grad_norm": 0.5489566326141357,
      "learning_rate": 0.0002547043010752688,
      "loss": 6.2676,
      "step": 3286
    },
    {
      "epoch": 0.9820001493763537,
      "grad_norm": 0.46663087606430054,
      "learning_rate": 0.00025462962962962966,
      "loss": 7.1641,
      "step": 3287
    },
    {
      "epoch": 0.9822989020838001,
      "grad_norm": 0.46735018491744995,
      "learning_rate": 0.0002545549581839904,
      "loss": 7.2129,
      "step": 3288
    },
    {
      "epoch": 0.9825976547912465,
      "grad_norm": 0.3942517340183258,
      "learning_rate": 0.0002544802867383513,
      "loss": 7.4434,
      "step": 3289
    },
    {
      "epoch": 0.982896407498693,
      "grad_norm": 0.4836493730545044,
      "learning_rate": 0.00025440561529271205,
      "loss": 6.9961,
      "step": 3290
    },
    {
      "epoch": 0.9831951602061394,
      "grad_norm": 0.4656186103820801,
      "learning_rate": 0.0002543309438470729,
      "loss": 7.4434,
      "step": 3291
    },
    {
      "epoch": 0.9834939129135858,
      "grad_norm": 0.33160078525543213,
      "learning_rate": 0.00025425627240143367,
      "loss": 7.6514,
      "step": 3292
    },
    {
      "epoch": 0.9837926656210322,
      "grad_norm": 0.5135310888290405,
      "learning_rate": 0.00025418160095579454,
      "loss": 7.1855,
      "step": 3293
    },
    {
      "epoch": 0.9840914183284786,
      "grad_norm": 0.47765663266181946,
      "learning_rate": 0.00025410692951015535,
      "loss": 7.1582,
      "step": 3294
    },
    {
      "epoch": 0.984390171035925,
      "grad_norm": 0.361209511756897,
      "learning_rate": 0.0002540322580645161,
      "loss": 7.5459,
      "step": 3295
    },
    {
      "epoch": 0.9846889237433715,
      "grad_norm": 0.42141833901405334,
      "learning_rate": 0.000253957586618877,
      "loss": 7.4209,
      "step": 3296
    },
    {
      "epoch": 0.9849876764508179,
      "grad_norm": 0.3967345356941223,
      "learning_rate": 0.00025388291517323774,
      "loss": 7.2178,
      "step": 3297
    },
    {
      "epoch": 0.9852864291582643,
      "grad_norm": 0.45723235607147217,
      "learning_rate": 0.0002538082437275986,
      "loss": 7.1328,
      "step": 3298
    },
    {
      "epoch": 0.9855851818657106,
      "grad_norm": 0.45411214232444763,
      "learning_rate": 0.00025373357228195936,
      "loss": 6.8896,
      "step": 3299
    },
    {
      "epoch": 0.985883934573157,
      "grad_norm": 0.5843572616577148,
      "learning_rate": 0.00025365890083632023,
      "loss": 6.8115,
      "step": 3300
    },
    {
      "epoch": 0.9861826872806034,
      "grad_norm": 0.36440131068229675,
      "learning_rate": 0.000253584229390681,
      "loss": 7.7998,
      "step": 3301
    },
    {
      "epoch": 0.9864814399880499,
      "grad_norm": 0.4260774850845337,
      "learning_rate": 0.0002535095579450418,
      "loss": 7.6689,
      "step": 3302
    },
    {
      "epoch": 0.9867801926954963,
      "grad_norm": 0.4864409267902374,
      "learning_rate": 0.0002534348864994026,
      "loss": 7.4766,
      "step": 3303
    },
    {
      "epoch": 0.9870789454029427,
      "grad_norm": 0.42123425006866455,
      "learning_rate": 0.0002533602150537634,
      "loss": 7.291,
      "step": 3304
    },
    {
      "epoch": 0.9873776981103891,
      "grad_norm": 0.4133484363555908,
      "learning_rate": 0.0002532855436081243,
      "loss": 7.3916,
      "step": 3305
    },
    {
      "epoch": 0.9876764508178355,
      "grad_norm": 0.4726097583770752,
      "learning_rate": 0.00025321087216248505,
      "loss": 7.2949,
      "step": 3306
    },
    {
      "epoch": 0.9879752035252819,
      "grad_norm": 0.5458619594573975,
      "learning_rate": 0.0002531362007168459,
      "loss": 6.5732,
      "step": 3307
    },
    {
      "epoch": 0.9882739562327284,
      "grad_norm": 0.44113442301750183,
      "learning_rate": 0.0002530615292712067,
      "loss": 7.9443,
      "step": 3308
    },
    {
      "epoch": 0.9885727089401748,
      "grad_norm": 0.4105203151702881,
      "learning_rate": 0.00025298685782556754,
      "loss": 7.6787,
      "step": 3309
    },
    {
      "epoch": 0.9888714616476212,
      "grad_norm": 0.5371512770652771,
      "learning_rate": 0.0002529121863799283,
      "loss": 6.3682,
      "step": 3310
    },
    {
      "epoch": 0.9891702143550676,
      "grad_norm": 0.4169568121433258,
      "learning_rate": 0.0002528375149342891,
      "loss": 7.4697,
      "step": 3311
    },
    {
      "epoch": 0.989468967062514,
      "grad_norm": 0.4418712556362152,
      "learning_rate": 0.00025276284348864993,
      "loss": 7.5078,
      "step": 3312
    },
    {
      "epoch": 0.9897677197699604,
      "grad_norm": 0.4707721471786499,
      "learning_rate": 0.00025268817204301074,
      "loss": 7.2949,
      "step": 3313
    },
    {
      "epoch": 0.9900664724774069,
      "grad_norm": 0.43117034435272217,
      "learning_rate": 0.0002526135005973716,
      "loss": 7.6533,
      "step": 3314
    },
    {
      "epoch": 0.9903652251848533,
      "grad_norm": 0.4508243799209595,
      "learning_rate": 0.00025253882915173237,
      "loss": 7.084,
      "step": 3315
    },
    {
      "epoch": 0.9906639778922997,
      "grad_norm": 0.3503705561161041,
      "learning_rate": 0.00025246415770609323,
      "loss": 8.0371,
      "step": 3316
    },
    {
      "epoch": 0.990962730599746,
      "grad_norm": 0.4084177613258362,
      "learning_rate": 0.000252389486260454,
      "loss": 7.8135,
      "step": 3317
    },
    {
      "epoch": 0.9912614833071924,
      "grad_norm": 0.4101797938346863,
      "learning_rate": 0.0002523148148148148,
      "loss": 7.5635,
      "step": 3318
    },
    {
      "epoch": 0.9915602360146388,
      "grad_norm": 0.5389958620071411,
      "learning_rate": 0.0002522401433691756,
      "loss": 6.4326,
      "step": 3319
    },
    {
      "epoch": 0.9918589887220853,
      "grad_norm": 0.462379515171051,
      "learning_rate": 0.00025216547192353643,
      "loss": 7.21,
      "step": 3320
    },
    {
      "epoch": 0.9921577414295317,
      "grad_norm": 0.5474674105644226,
      "learning_rate": 0.00025209080047789724,
      "loss": 7.0107,
      "step": 3321
    },
    {
      "epoch": 0.9924564941369781,
      "grad_norm": 0.4895038604736328,
      "learning_rate": 0.00025201612903225806,
      "loss": 7.4541,
      "step": 3322
    },
    {
      "epoch": 0.9927552468444245,
      "grad_norm": 0.49642738699913025,
      "learning_rate": 0.0002519414575866189,
      "loss": 7.4102,
      "step": 3323
    },
    {
      "epoch": 0.9930539995518709,
      "grad_norm": 0.43349891901016235,
      "learning_rate": 0.0002518667861409797,
      "loss": 7.252,
      "step": 3324
    },
    {
      "epoch": 0.9933527522593173,
      "grad_norm": 0.5478997826576233,
      "learning_rate": 0.00025179211469534055,
      "loss": 7.4209,
      "step": 3325
    },
    {
      "epoch": 0.9936515049667638,
      "grad_norm": 0.4022563099861145,
      "learning_rate": 0.0002517174432497013,
      "loss": 7.5391,
      "step": 3326
    },
    {
      "epoch": 0.9939502576742102,
      "grad_norm": 0.42894503474235535,
      "learning_rate": 0.0002516427718040621,
      "loss": 7.1455,
      "step": 3327
    },
    {
      "epoch": 0.9942490103816566,
      "grad_norm": 0.4369811713695526,
      "learning_rate": 0.00025156810035842293,
      "loss": 7.1621,
      "step": 3328
    },
    {
      "epoch": 0.994547763089103,
      "grad_norm": 0.40901312232017517,
      "learning_rate": 0.00025149342891278375,
      "loss": 7.5205,
      "step": 3329
    },
    {
      "epoch": 0.9948465157965494,
      "grad_norm": 0.4775823950767517,
      "learning_rate": 0.00025141875746714456,
      "loss": 7.3525,
      "step": 3330
    },
    {
      "epoch": 0.9951452685039959,
      "grad_norm": 0.4561660885810852,
      "learning_rate": 0.0002513440860215054,
      "loss": 7.4004,
      "step": 3331
    },
    {
      "epoch": 0.9954440212114423,
      "grad_norm": 0.4096395671367645,
      "learning_rate": 0.00025126941457586624,
      "loss": 7.7021,
      "step": 3332
    },
    {
      "epoch": 0.9957427739188887,
      "grad_norm": 0.4100055694580078,
      "learning_rate": 0.000251194743130227,
      "loss": 7.0332,
      "step": 3333
    },
    {
      "epoch": 0.996041526626335,
      "grad_norm": 0.4665876030921936,
      "learning_rate": 0.0002511200716845878,
      "loss": 7.6318,
      "step": 3334
    },
    {
      "epoch": 0.9963402793337814,
      "grad_norm": 0.40613484382629395,
      "learning_rate": 0.0002510454002389486,
      "loss": 7.5557,
      "step": 3335
    },
    {
      "epoch": 0.9966390320412278,
      "grad_norm": 0.40970301628112793,
      "learning_rate": 0.00025097072879330944,
      "loss": 7.5322,
      "step": 3336
    },
    {
      "epoch": 0.9969377847486743,
      "grad_norm": 0.5266701579093933,
      "learning_rate": 0.00025089605734767025,
      "loss": 6.6768,
      "step": 3337
    },
    {
      "epoch": 0.9972365374561207,
      "grad_norm": 0.4376611113548279,
      "learning_rate": 0.00025082138590203106,
      "loss": 7.7295,
      "step": 3338
    },
    {
      "epoch": 0.9975352901635671,
      "grad_norm": 0.42293277382850647,
      "learning_rate": 0.0002507467144563919,
      "loss": 7.4258,
      "step": 3339
    },
    {
      "epoch": 0.9978340428710135,
      "grad_norm": 0.41515350341796875,
      "learning_rate": 0.0002506720430107527,
      "loss": 7.8291,
      "step": 3340
    },
    {
      "epoch": 0.9981327955784599,
      "grad_norm": 0.52701336145401,
      "learning_rate": 0.00025059737156511356,
      "loss": 6.8076,
      "step": 3341
    },
    {
      "epoch": 0.9984315482859063,
      "grad_norm": 0.42631110548973083,
      "learning_rate": 0.0002505227001194743,
      "loss": 6.9824,
      "step": 3342
    },
    {
      "epoch": 0.9987303009933528,
      "grad_norm": 0.5668794512748718,
      "learning_rate": 0.00025044802867383513,
      "loss": 7.5596,
      "step": 3343
    },
    {
      "epoch": 0.9990290537007992,
      "grad_norm": 0.5331038236618042,
      "learning_rate": 0.00025037335722819594,
      "loss": 7.1475,
      "step": 3344
    },
    {
      "epoch": 0.9993278064082456,
      "grad_norm": 0.4172385632991791,
      "learning_rate": 0.00025029868578255675,
      "loss": 7.6191,
      "step": 3345
    },
    {
      "epoch": 0.999626559115692,
      "grad_norm": 0.4026169776916504,
      "learning_rate": 0.00025022401433691757,
      "loss": 7.6436,
      "step": 3346
    },
    {
      "epoch": 0.9999253118231384,
      "grad_norm": 0.41687852144241333,
      "learning_rate": 0.0002501493428912784,
      "loss": 7.2178,
      "step": 3347
    },
    {
      "epoch": 1.0,
      "grad_norm": 2.8406572341918945,
      "learning_rate": 0.0002500746714456392,
      "loss": 4.9609,
      "step": 3348
    },
    {
      "epoch": 1.0002987527074465,
      "grad_norm": 0.4756811261177063,
      "learning_rate": 0.00025,
      "loss": 7.3555,
      "step": 3349
    },
    {
      "epoch": 1.0005975054148928,
      "grad_norm": 0.44442370533943176,
      "learning_rate": 0.0002499253285543608,
      "loss": 7.082,
      "step": 3350
    },
    {
      "epoch": 1.0008962581223393,
      "grad_norm": 0.4878856837749481,
      "learning_rate": 0.00024985065710872163,
      "loss": 7.0908,
      "step": 3351
    },
    {
      "epoch": 1.0011950108297856,
      "grad_norm": 0.693169891834259,
      "learning_rate": 0.00024977598566308244,
      "loss": 7.1582,
      "step": 3352
    },
    {
      "epoch": 1.001493763537232,
      "grad_norm": 0.46306559443473816,
      "learning_rate": 0.00024970131421744326,
      "loss": 7.0107,
      "step": 3353
    },
    {
      "epoch": 1.0017925162446786,
      "grad_norm": 0.49560001492500305,
      "learning_rate": 0.00024962664277180407,
      "loss": 6.873,
      "step": 3354
    },
    {
      "epoch": 1.0020912689521249,
      "grad_norm": 0.4384162425994873,
      "learning_rate": 0.0002495519713261649,
      "loss": 7.4873,
      "step": 3355
    },
    {
      "epoch": 1.0023900216595714,
      "grad_norm": 0.467030793428421,
      "learning_rate": 0.0002494772998805257,
      "loss": 7.0195,
      "step": 3356
    },
    {
      "epoch": 1.0026887743670176,
      "grad_norm": 0.4242129623889923,
      "learning_rate": 0.0002494026284348865,
      "loss": 7.4893,
      "step": 3357
    },
    {
      "epoch": 1.0029875270744641,
      "grad_norm": 0.4637131094932556,
      "learning_rate": 0.0002493279569892473,
      "loss": 7.1973,
      "step": 3358
    },
    {
      "epoch": 1.0032862797819104,
      "grad_norm": 0.55659019947052,
      "learning_rate": 0.00024925328554360813,
      "loss": 6.5527,
      "step": 3359
    },
    {
      "epoch": 1.003585032489357,
      "grad_norm": 0.3839854300022125,
      "learning_rate": 0.00024917861409796895,
      "loss": 7.623,
      "step": 3360
    },
    {
      "epoch": 1.0038837851968034,
      "grad_norm": 0.48539310693740845,
      "learning_rate": 0.00024910394265232976,
      "loss": 7.0498,
      "step": 3361
    },
    {
      "epoch": 1.0041825379042497,
      "grad_norm": 0.5432499647140503,
      "learning_rate": 0.00024902927120669057,
      "loss": 6.5713,
      "step": 3362
    },
    {
      "epoch": 1.0044812906116962,
      "grad_norm": 0.6348333954811096,
      "learning_rate": 0.0002489545997610514,
      "loss": 6.0938,
      "step": 3363
    },
    {
      "epoch": 1.0047800433191425,
      "grad_norm": 0.537625253200531,
      "learning_rate": 0.0002488799283154122,
      "loss": 6.9014,
      "step": 3364
    },
    {
      "epoch": 1.005078796026589,
      "grad_norm": 0.41297563910484314,
      "learning_rate": 0.000248805256869773,
      "loss": 7.1348,
      "step": 3365
    },
    {
      "epoch": 1.0053775487340355,
      "grad_norm": 0.5181509256362915,
      "learning_rate": 0.0002487305854241338,
      "loss": 6.916,
      "step": 3366
    },
    {
      "epoch": 1.0056763014414818,
      "grad_norm": 0.4368402659893036,
      "learning_rate": 0.00024865591397849464,
      "loss": 7.1133,
      "step": 3367
    },
    {
      "epoch": 1.0059750541489283,
      "grad_norm": 0.5454858541488647,
      "learning_rate": 0.00024858124253285545,
      "loss": 6.7275,
      "step": 3368
    },
    {
      "epoch": 1.0062738068563746,
      "grad_norm": 0.4288136959075928,
      "learning_rate": 0.00024850657108721626,
      "loss": 7.2969,
      "step": 3369
    },
    {
      "epoch": 1.006572559563821,
      "grad_norm": 0.46060851216316223,
      "learning_rate": 0.0002484318996415771,
      "loss": 6.9219,
      "step": 3370
    },
    {
      "epoch": 1.0068713122712674,
      "grad_norm": 0.44007259607315063,
      "learning_rate": 0.0002483572281959379,
      "loss": 7.1045,
      "step": 3371
    },
    {
      "epoch": 1.0071700649787139,
      "grad_norm": 0.3768695592880249,
      "learning_rate": 0.0002482825567502987,
      "loss": 7.1777,
      "step": 3372
    },
    {
      "epoch": 1.0074688176861604,
      "grad_norm": 0.37011227011680603,
      "learning_rate": 0.0002482078853046595,
      "loss": 7.8223,
      "step": 3373
    },
    {
      "epoch": 1.0077675703936066,
      "grad_norm": 0.4656447768211365,
      "learning_rate": 0.00024813321385902033,
      "loss": 7.2812,
      "step": 3374
    },
    {
      "epoch": 1.0080663231010532,
      "grad_norm": 0.44355571269989014,
      "learning_rate": 0.00024805854241338114,
      "loss": 7.373,
      "step": 3375
    },
    {
      "epoch": 1.0083650758084994,
      "grad_norm": 0.6134417057037354,
      "learning_rate": 0.00024798387096774195,
      "loss": 6.6152,
      "step": 3376
    },
    {
      "epoch": 1.008663828515946,
      "grad_norm": 0.43925192952156067,
      "learning_rate": 0.00024790919952210277,
      "loss": 7.1562,
      "step": 3377
    },
    {
      "epoch": 1.0089625812233924,
      "grad_norm": 0.4181433916091919,
      "learning_rate": 0.0002478345280764636,
      "loss": 7.3369,
      "step": 3378
    },
    {
      "epoch": 1.0092613339308387,
      "grad_norm": 0.37754687666893005,
      "learning_rate": 0.0002477598566308244,
      "loss": 7.6982,
      "step": 3379
    },
    {
      "epoch": 1.0095600866382852,
      "grad_norm": 0.5059182643890381,
      "learning_rate": 0.0002476851851851852,
      "loss": 6.7139,
      "step": 3380
    },
    {
      "epoch": 1.0098588393457315,
      "grad_norm": 0.4369875192642212,
      "learning_rate": 0.00024761051373954596,
      "loss": 7.1045,
      "step": 3381
    },
    {
      "epoch": 1.010157592053178,
      "grad_norm": 0.4724297821521759,
      "learning_rate": 0.00024753584229390683,
      "loss": 7.5547,
      "step": 3382
    },
    {
      "epoch": 1.0104563447606243,
      "grad_norm": 0.4458632469177246,
      "learning_rate": 0.00024746117084826764,
      "loss": 6.8701,
      "step": 3383
    },
    {
      "epoch": 1.0107550974680708,
      "grad_norm": 0.4398276209831238,
      "learning_rate": 0.00024738649940262846,
      "loss": 7.5703,
      "step": 3384
    },
    {
      "epoch": 1.0110538501755173,
      "grad_norm": 0.38416817784309387,
      "learning_rate": 0.00024731182795698927,
      "loss": 7.5459,
      "step": 3385
    },
    {
      "epoch": 1.0113526028829636,
      "grad_norm": 0.5081422924995422,
      "learning_rate": 0.0002472371565113501,
      "loss": 7.3154,
      "step": 3386
    },
    {
      "epoch": 1.01165135559041,
      "grad_norm": 0.4949505925178528,
      "learning_rate": 0.0002471624850657109,
      "loss": 7.0693,
      "step": 3387
    },
    {
      "epoch": 1.0119501082978564,
      "grad_norm": 0.4836929142475128,
      "learning_rate": 0.0002470878136200717,
      "loss": 6.8984,
      "step": 3388
    },
    {
      "epoch": 1.0122488610053029,
      "grad_norm": 0.3839610815048218,
      "learning_rate": 0.00024701314217443247,
      "loss": 7.8076,
      "step": 3389
    },
    {
      "epoch": 1.0125476137127494,
      "grad_norm": 0.4500613510608673,
      "learning_rate": 0.0002469384707287933,
      "loss": 7.3594,
      "step": 3390
    },
    {
      "epoch": 1.0128463664201957,
      "grad_norm": 0.4961785674095154,
      "learning_rate": 0.00024686379928315415,
      "loss": 6.3535,
      "step": 3391
    },
    {
      "epoch": 1.0131451191276422,
      "grad_norm": 0.5343946814537048,
      "learning_rate": 0.00024678912783751496,
      "loss": 6.9541,
      "step": 3392
    },
    {
      "epoch": 1.0134438718350884,
      "grad_norm": 0.5155302882194519,
      "learning_rate": 0.00024671445639187577,
      "loss": 6.8906,
      "step": 3393
    },
    {
      "epoch": 1.013742624542535,
      "grad_norm": 0.44505444169044495,
      "learning_rate": 0.0002466397849462366,
      "loss": 7.2871,
      "step": 3394
    },
    {
      "epoch": 1.0140413772499812,
      "grad_norm": 0.4344482123851776,
      "learning_rate": 0.0002465651135005974,
      "loss": 7.2227,
      "step": 3395
    },
    {
      "epoch": 1.0143401299574277,
      "grad_norm": 0.4498469829559326,
      "learning_rate": 0.0002464904420549582,
      "loss": 7.375,
      "step": 3396
    },
    {
      "epoch": 1.0146388826648742,
      "grad_norm": 0.4542418122291565,
      "learning_rate": 0.00024641577060931897,
      "loss": 7.3174,
      "step": 3397
    },
    {
      "epoch": 1.0149376353723205,
      "grad_norm": 0.4445802867412567,
      "learning_rate": 0.0002463410991636798,
      "loss": 7.5332,
      "step": 3398
    },
    {
      "epoch": 1.015236388079767,
      "grad_norm": 0.4179249405860901,
      "learning_rate": 0.0002462664277180406,
      "loss": 7.2295,
      "step": 3399
    },
    {
      "epoch": 1.0155351407872133,
      "grad_norm": 0.4325160086154938,
      "learning_rate": 0.00024619175627240146,
      "loss": 7.1045,
      "step": 3400
    },
    {
      "epoch": 1.0155351407872133,
      "eval_bleu": 0.1293474590700521,
      "eval_loss": 7.01953125,
      "eval_runtime": 480.8176,
      "eval_samples_per_second": 2.93,
      "eval_steps_per_second": 0.185,
      "step": 3400
    },
    {
      "epoch": 1.0158338934946598,
      "grad_norm": 0.4621394872665405,
      "learning_rate": 0.0002461170848267623,
      "loss": 7.3281,
      "step": 3401
    },
    {
      "epoch": 1.0161326462021063,
      "grad_norm": 0.37318742275238037,
      "learning_rate": 0.0002460424133811231,
      "loss": 8.0518,
      "step": 3402
    },
    {
      "epoch": 1.0164313989095526,
      "grad_norm": 0.49878618121147156,
      "learning_rate": 0.0002459677419354839,
      "loss": 6.8926,
      "step": 3403
    },
    {
      "epoch": 1.016730151616999,
      "grad_norm": 0.42202866077423096,
      "learning_rate": 0.0002458930704898447,
      "loss": 7.0244,
      "step": 3404
    },
    {
      "epoch": 1.0170289043244454,
      "grad_norm": 0.39239010214805603,
      "learning_rate": 0.00024581839904420547,
      "loss": 7.4971,
      "step": 3405
    },
    {
      "epoch": 1.0173276570318919,
      "grad_norm": 0.39782077074050903,
      "learning_rate": 0.0002457437275985663,
      "loss": 7.8252,
      "step": 3406
    },
    {
      "epoch": 1.0176264097393382,
      "grad_norm": 0.6122108697891235,
      "learning_rate": 0.0002456690561529271,
      "loss": 6.417,
      "step": 3407
    },
    {
      "epoch": 1.0179251624467847,
      "grad_norm": 0.5704073905944824,
      "learning_rate": 0.0002455943847072879,
      "loss": 6.8701,
      "step": 3408
    },
    {
      "epoch": 1.0182239151542312,
      "grad_norm": 0.431069016456604,
      "learning_rate": 0.0002455197132616488,
      "loss": 7.0078,
      "step": 3409
    },
    {
      "epoch": 1.0185226678616774,
      "grad_norm": 0.48325440287590027,
      "learning_rate": 0.0002454450418160096,
      "loss": 6.9795,
      "step": 3410
    },
    {
      "epoch": 1.018821420569124,
      "grad_norm": 0.4467199146747589,
      "learning_rate": 0.0002453703703703704,
      "loss": 6.9307,
      "step": 3411
    },
    {
      "epoch": 1.0191201732765702,
      "grad_norm": 0.45861294865608215,
      "learning_rate": 0.0002452956989247312,
      "loss": 6.9727,
      "step": 3412
    },
    {
      "epoch": 1.0194189259840167,
      "grad_norm": 0.5012481808662415,
      "learning_rate": 0.000245221027479092,
      "loss": 6.8174,
      "step": 3413
    },
    {
      "epoch": 1.0197176786914632,
      "grad_norm": 0.4536726474761963,
      "learning_rate": 0.0002451463560334528,
      "loss": 6.9014,
      "step": 3414
    },
    {
      "epoch": 1.0200164313989095,
      "grad_norm": 0.4303296208381653,
      "learning_rate": 0.0002450716845878136,
      "loss": 7.2119,
      "step": 3415
    },
    {
      "epoch": 1.020315184106356,
      "grad_norm": 0.42971116304397583,
      "learning_rate": 0.0002449970131421744,
      "loss": 7.5352,
      "step": 3416
    },
    {
      "epoch": 1.0206139368138023,
      "grad_norm": 0.39496684074401855,
      "learning_rate": 0.0002449223416965352,
      "loss": 7.2656,
      "step": 3417
    },
    {
      "epoch": 1.0209126895212488,
      "grad_norm": 0.4539097547531128,
      "learning_rate": 0.0002448476702508961,
      "loss": 7.1328,
      "step": 3418
    },
    {
      "epoch": 1.021211442228695,
      "grad_norm": 0.4645238518714905,
      "learning_rate": 0.0002447729988052569,
      "loss": 7.6104,
      "step": 3419
    },
    {
      "epoch": 1.0215101949361416,
      "grad_norm": 0.4207484722137451,
      "learning_rate": 0.0002446983273596177,
      "loss": 7.6641,
      "step": 3420
    },
    {
      "epoch": 1.021808947643588,
      "grad_norm": 0.4331395626068115,
      "learning_rate": 0.0002446236559139785,
      "loss": 7.584,
      "step": 3421
    },
    {
      "epoch": 1.0221077003510344,
      "grad_norm": 0.4062589108943939,
      "learning_rate": 0.0002445489844683393,
      "loss": 7.4404,
      "step": 3422
    },
    {
      "epoch": 1.0224064530584809,
      "grad_norm": 0.4680413603782654,
      "learning_rate": 0.0002444743130227001,
      "loss": 7.418,
      "step": 3423
    },
    {
      "epoch": 1.0227052057659272,
      "grad_norm": 0.48817208409309387,
      "learning_rate": 0.0002443996415770609,
      "loss": 6.708,
      "step": 3424
    },
    {
      "epoch": 1.0230039584733737,
      "grad_norm": 0.4803292453289032,
      "learning_rate": 0.00024432497013142173,
      "loss": 7.0732,
      "step": 3425
    },
    {
      "epoch": 1.0233027111808202,
      "grad_norm": 0.44479361176490784,
      "learning_rate": 0.00024425029868578254,
      "loss": 7.3818,
      "step": 3426
    },
    {
      "epoch": 1.0236014638882664,
      "grad_norm": 0.5160688161849976,
      "learning_rate": 0.00024417562724014336,
      "loss": 7.0645,
      "step": 3427
    },
    {
      "epoch": 1.023900216595713,
      "grad_norm": 0.4775119721889496,
      "learning_rate": 0.0002441009557945042,
      "loss": 7.2998,
      "step": 3428
    },
    {
      "epoch": 1.0241989693031592,
      "grad_norm": 0.5204400420188904,
      "learning_rate": 0.000244026284348865,
      "loss": 6.6055,
      "step": 3429
    },
    {
      "epoch": 1.0244977220106057,
      "grad_norm": 0.3924555480480194,
      "learning_rate": 0.00024395161290322582,
      "loss": 7.6914,
      "step": 3430
    },
    {
      "epoch": 1.0247964747180522,
      "grad_norm": 0.43248969316482544,
      "learning_rate": 0.00024387694145758663,
      "loss": 6.9219,
      "step": 3431
    },
    {
      "epoch": 1.0250952274254985,
      "grad_norm": 0.4512058198451996,
      "learning_rate": 0.00024380227001194745,
      "loss": 7.1445,
      "step": 3432
    },
    {
      "epoch": 1.025393980132945,
      "grad_norm": 0.45780646800994873,
      "learning_rate": 0.00024372759856630823,
      "loss": 7.1973,
      "step": 3433
    },
    {
      "epoch": 1.0256927328403913,
      "grad_norm": 0.4515080153942108,
      "learning_rate": 0.00024365292712066905,
      "loss": 7.0508,
      "step": 3434
    },
    {
      "epoch": 1.0259914855478378,
      "grad_norm": 0.48854774236679077,
      "learning_rate": 0.00024357825567502986,
      "loss": 7.4453,
      "step": 3435
    },
    {
      "epoch": 1.026290238255284,
      "grad_norm": 0.42303335666656494,
      "learning_rate": 0.00024350358422939067,
      "loss": 7.2373,
      "step": 3436
    },
    {
      "epoch": 1.0265889909627306,
      "grad_norm": 0.38880860805511475,
      "learning_rate": 0.0002434289127837515,
      "loss": 7.623,
      "step": 3437
    },
    {
      "epoch": 1.026887743670177,
      "grad_norm": 0.4121303856372833,
      "learning_rate": 0.00024335424133811232,
      "loss": 7.7568,
      "step": 3438
    },
    {
      "epoch": 1.0271864963776234,
      "grad_norm": 0.4722398519515991,
      "learning_rate": 0.00024327956989247314,
      "loss": 7.1221,
      "step": 3439
    },
    {
      "epoch": 1.0274852490850699,
      "grad_norm": 0.4417482614517212,
      "learning_rate": 0.00024320489844683395,
      "loss": 7.1436,
      "step": 3440
    },
    {
      "epoch": 1.0277840017925162,
      "grad_norm": 0.5008942484855652,
      "learning_rate": 0.00024313022700119474,
      "loss": 7.2559,
      "step": 3441
    },
    {
      "epoch": 1.0280827544999627,
      "grad_norm": 0.40294086933135986,
      "learning_rate": 0.00024305555555555555,
      "loss": 7.499,
      "step": 3442
    },
    {
      "epoch": 1.0283815072074092,
      "grad_norm": 0.470899373292923,
      "learning_rate": 0.00024298088410991636,
      "loss": 6.833,
      "step": 3443
    },
    {
      "epoch": 1.0286802599148555,
      "grad_norm": 0.5884604454040527,
      "learning_rate": 0.00024290621266427717,
      "loss": 7.2676,
      "step": 3444
    },
    {
      "epoch": 1.028979012622302,
      "grad_norm": 0.46264079213142395,
      "learning_rate": 0.000242831541218638,
      "loss": 6.9951,
      "step": 3445
    },
    {
      "epoch": 1.0292777653297482,
      "grad_norm": 0.4886637330055237,
      "learning_rate": 0.00024275686977299883,
      "loss": 7.2686,
      "step": 3446
    },
    {
      "epoch": 1.0295765180371947,
      "grad_norm": 0.4492754638195038,
      "learning_rate": 0.00024268219832735964,
      "loss": 7.4727,
      "step": 3447
    },
    {
      "epoch": 1.029875270744641,
      "grad_norm": 0.5559833645820618,
      "learning_rate": 0.00024260752688172045,
      "loss": 6.9766,
      "step": 3448
    },
    {
      "epoch": 1.0301740234520875,
      "grad_norm": 0.48459887504577637,
      "learning_rate": 0.00024253285543608124,
      "loss": 6.8164,
      "step": 3449
    },
    {
      "epoch": 1.030472776159534,
      "grad_norm": 0.47055646777153015,
      "learning_rate": 0.00024245818399044205,
      "loss": 7.1338,
      "step": 3450
    },
    {
      "epoch": 1.0307715288669803,
      "grad_norm": 0.43241098523139954,
      "learning_rate": 0.00024238351254480286,
      "loss": 6.791,
      "step": 3451
    },
    {
      "epoch": 1.0310702815744268,
      "grad_norm": 0.30281955003738403,
      "learning_rate": 0.00024230884109916368,
      "loss": 7.79,
      "step": 3452
    },
    {
      "epoch": 1.031369034281873,
      "grad_norm": 0.4227406084537506,
      "learning_rate": 0.0002422341696535245,
      "loss": 7.3125,
      "step": 3453
    },
    {
      "epoch": 1.0316677869893196,
      "grad_norm": 0.41361144185066223,
      "learning_rate": 0.0002421594982078853,
      "loss": 7.5117,
      "step": 3454
    },
    {
      "epoch": 1.031966539696766,
      "grad_norm": 0.5379975438117981,
      "learning_rate": 0.00024208482676224614,
      "loss": 6.9199,
      "step": 3455
    },
    {
      "epoch": 1.0322652924042124,
      "grad_norm": 0.5006034970283508,
      "learning_rate": 0.00024201015531660696,
      "loss": 6.5361,
      "step": 3456
    },
    {
      "epoch": 1.032564045111659,
      "grad_norm": 0.3983411192893982,
      "learning_rate": 0.00024193548387096774,
      "loss": 7.626,
      "step": 3457
    },
    {
      "epoch": 1.0328627978191052,
      "grad_norm": 0.47976863384246826,
      "learning_rate": 0.00024186081242532855,
      "loss": 6.6807,
      "step": 3458
    },
    {
      "epoch": 1.0331615505265517,
      "grad_norm": 0.5174379944801331,
      "learning_rate": 0.00024178614097968937,
      "loss": 6.9756,
      "step": 3459
    },
    {
      "epoch": 1.033460303233998,
      "grad_norm": 0.5763309597969055,
      "learning_rate": 0.00024171146953405018,
      "loss": 7.085,
      "step": 3460
    },
    {
      "epoch": 1.0337590559414445,
      "grad_norm": 0.48075342178344727,
      "learning_rate": 0.000241636798088411,
      "loss": 7.04,
      "step": 3461
    },
    {
      "epoch": 1.034057808648891,
      "grad_norm": 0.5332094430923462,
      "learning_rate": 0.0002415621266427718,
      "loss": 6.7529,
      "step": 3462
    },
    {
      "epoch": 1.0343565613563372,
      "grad_norm": 0.4466455578804016,
      "learning_rate": 0.00024148745519713262,
      "loss": 7.5176,
      "step": 3463
    },
    {
      "epoch": 1.0346553140637837,
      "grad_norm": 0.533166766166687,
      "learning_rate": 0.00024141278375149346,
      "loss": 6.8936,
      "step": 3464
    },
    {
      "epoch": 1.03495406677123,
      "grad_norm": 0.45532506704330444,
      "learning_rate": 0.00024133811230585424,
      "loss": 7.0029,
      "step": 3465
    },
    {
      "epoch": 1.0352528194786765,
      "grad_norm": 0.47452253103256226,
      "learning_rate": 0.00024126344086021506,
      "loss": 7.0547,
      "step": 3466
    },
    {
      "epoch": 1.035551572186123,
      "grad_norm": 0.5352041721343994,
      "learning_rate": 0.00024118876941457587,
      "loss": 6.5264,
      "step": 3467
    },
    {
      "epoch": 1.0358503248935693,
      "grad_norm": 0.44987383484840393,
      "learning_rate": 0.00024111409796893668,
      "loss": 7.5215,
      "step": 3468
    },
    {
      "epoch": 1.0361490776010158,
      "grad_norm": 0.4563024640083313,
      "learning_rate": 0.0002410394265232975,
      "loss": 7.1836,
      "step": 3469
    },
    {
      "epoch": 1.036447830308462,
      "grad_norm": 0.4992338716983795,
      "learning_rate": 0.0002409647550776583,
      "loss": 6.4678,
      "step": 3470
    },
    {
      "epoch": 1.0367465830159086,
      "grad_norm": 0.46520212292671204,
      "learning_rate": 0.00024089008363201912,
      "loss": 7.1299,
      "step": 3471
    },
    {
      "epoch": 1.0370453357233549,
      "grad_norm": 0.506927490234375,
      "learning_rate": 0.00024081541218637993,
      "loss": 6.9531,
      "step": 3472
    },
    {
      "epoch": 1.0373440884308014,
      "grad_norm": 0.4318285286426544,
      "learning_rate": 0.00024074074074074072,
      "loss": 7.332,
      "step": 3473
    },
    {
      "epoch": 1.037642841138248,
      "grad_norm": 0.5004772543907166,
      "learning_rate": 0.00024066606929510156,
      "loss": 7.0127,
      "step": 3474
    },
    {
      "epoch": 1.0379415938456942,
      "grad_norm": 0.42948922514915466,
      "learning_rate": 0.00024059139784946237,
      "loss": 7.0078,
      "step": 3475
    },
    {
      "epoch": 1.0382403465531407,
      "grad_norm": 0.44666358828544617,
      "learning_rate": 0.00024051672640382319,
      "loss": 7.5977,
      "step": 3476
    },
    {
      "epoch": 1.038539099260587,
      "grad_norm": 0.46333858370780945,
      "learning_rate": 0.000240442054958184,
      "loss": 7.248,
      "step": 3477
    },
    {
      "epoch": 1.0388378519680335,
      "grad_norm": 0.4608994424343109,
      "learning_rate": 0.0002403673835125448,
      "loss": 7.3125,
      "step": 3478
    },
    {
      "epoch": 1.03913660467548,
      "grad_norm": 0.49825507402420044,
      "learning_rate": 0.00024029271206690563,
      "loss": 6.6992,
      "step": 3479
    },
    {
      "epoch": 1.0394353573829263,
      "grad_norm": 0.4569357931613922,
      "learning_rate": 0.00024021804062126644,
      "loss": 6.9307,
      "step": 3480
    },
    {
      "epoch": 1.0397341100903728,
      "grad_norm": 0.5182752013206482,
      "learning_rate": 0.00024014336917562722,
      "loss": 7.3662,
      "step": 3481
    },
    {
      "epoch": 1.040032862797819,
      "grad_norm": 0.4279766082763672,
      "learning_rate": 0.00024006869772998804,
      "loss": 6.8945,
      "step": 3482
    },
    {
      "epoch": 1.0403316155052655,
      "grad_norm": 0.35756754875183105,
      "learning_rate": 0.00023999402628434888,
      "loss": 7.8174,
      "step": 3483
    },
    {
      "epoch": 1.0406303682127118,
      "grad_norm": 0.4601576626300812,
      "learning_rate": 0.0002399193548387097,
      "loss": 7.2188,
      "step": 3484
    },
    {
      "epoch": 1.0409291209201583,
      "grad_norm": 0.4305742084980011,
      "learning_rate": 0.0002398446833930705,
      "loss": 7.4131,
      "step": 3485
    },
    {
      "epoch": 1.0412278736276048,
      "grad_norm": 0.5233439207077026,
      "learning_rate": 0.00023977001194743132,
      "loss": 6.5928,
      "step": 3486
    },
    {
      "epoch": 1.041526626335051,
      "grad_norm": 0.37751495838165283,
      "learning_rate": 0.00023969534050179213,
      "loss": 7.5361,
      "step": 3487
    },
    {
      "epoch": 1.0418253790424976,
      "grad_norm": 0.42439886927604675,
      "learning_rate": 0.00023962066905615294,
      "loss": 7.2266,
      "step": 3488
    },
    {
      "epoch": 1.042124131749944,
      "grad_norm": 0.5055875778198242,
      "learning_rate": 0.00023954599761051373,
      "loss": 6.8945,
      "step": 3489
    },
    {
      "epoch": 1.0424228844573904,
      "grad_norm": 0.5219650864601135,
      "learning_rate": 0.00023947132616487454,
      "loss": 6.9814,
      "step": 3490
    },
    {
      "epoch": 1.042721637164837,
      "grad_norm": 0.5119924545288086,
      "learning_rate": 0.00023939665471923535,
      "loss": 6.6484,
      "step": 3491
    },
    {
      "epoch": 1.0430203898722832,
      "grad_norm": 0.4407166540622711,
      "learning_rate": 0.0002393219832735962,
      "loss": 7.4023,
      "step": 3492
    },
    {
      "epoch": 1.0433191425797297,
      "grad_norm": 0.4831078052520752,
      "learning_rate": 0.000239247311827957,
      "loss": 7.3203,
      "step": 3493
    },
    {
      "epoch": 1.043617895287176,
      "grad_norm": 0.48399579524993896,
      "learning_rate": 0.00023917264038231782,
      "loss": 7.209,
      "step": 3494
    },
    {
      "epoch": 1.0439166479946225,
      "grad_norm": 0.548481285572052,
      "learning_rate": 0.00023909796893667863,
      "loss": 6.8721,
      "step": 3495
    },
    {
      "epoch": 1.0442154007020688,
      "grad_norm": 0.5176121592521667,
      "learning_rate": 0.00023902329749103944,
      "loss": 6.6621,
      "step": 3496
    },
    {
      "epoch": 1.0445141534095153,
      "grad_norm": 0.5055406093597412,
      "learning_rate": 0.00023894862604540023,
      "loss": 6.8711,
      "step": 3497
    },
    {
      "epoch": 1.0448129061169618,
      "grad_norm": 0.49186834692955017,
      "learning_rate": 0.00023887395459976104,
      "loss": 6.3682,
      "step": 3498
    },
    {
      "epoch": 1.045111658824408,
      "grad_norm": 0.5214719772338867,
      "learning_rate": 0.00023879928315412186,
      "loss": 6.9404,
      "step": 3499
    },
    {
      "epoch": 1.0454104115318545,
      "grad_norm": 0.4995153248310089,
      "learning_rate": 0.00023872461170848267,
      "loss": 7.6436,
      "step": 3500
    },
    {
      "epoch": 1.0457091642393008,
      "grad_norm": 0.42922329902648926,
      "learning_rate": 0.0002386499402628435,
      "loss": 7.6221,
      "step": 3501
    },
    {
      "epoch": 1.0460079169467473,
      "grad_norm": 0.441771537065506,
      "learning_rate": 0.00023857526881720432,
      "loss": 7.5508,
      "step": 3502
    },
    {
      "epoch": 1.0463066696541938,
      "grad_norm": 0.47541213035583496,
      "learning_rate": 0.00023850059737156513,
      "loss": 7.3379,
      "step": 3503
    },
    {
      "epoch": 1.0466054223616401,
      "grad_norm": 0.43282827734947205,
      "learning_rate": 0.00023842592592592595,
      "loss": 7.5352,
      "step": 3504
    },
    {
      "epoch": 1.0469041750690866,
      "grad_norm": 0.4677108824253082,
      "learning_rate": 0.00023835125448028673,
      "loss": 7.1963,
      "step": 3505
    },
    {
      "epoch": 1.047202927776533,
      "grad_norm": 0.4242980182170868,
      "learning_rate": 0.00023827658303464755,
      "loss": 7.4062,
      "step": 3506
    },
    {
      "epoch": 1.0475016804839794,
      "grad_norm": 0.4138141870498657,
      "learning_rate": 0.00023820191158900836,
      "loss": 7.4141,
      "step": 3507
    },
    {
      "epoch": 1.047800433191426,
      "grad_norm": 0.4569915235042572,
      "learning_rate": 0.00023812724014336917,
      "loss": 7.1484,
      "step": 3508
    },
    {
      "epoch": 1.0480991858988722,
      "grad_norm": 0.41105329990386963,
      "learning_rate": 0.00023805256869772998,
      "loss": 7.5234,
      "step": 3509
    },
    {
      "epoch": 1.0483979386063187,
      "grad_norm": 0.4530278742313385,
      "learning_rate": 0.00023797789725209082,
      "loss": 7.126,
      "step": 3510
    },
    {
      "epoch": 1.048696691313765,
      "grad_norm": 0.4539632797241211,
      "learning_rate": 0.00023790322580645164,
      "loss": 7.1865,
      "step": 3511
    },
    {
      "epoch": 1.0489954440212115,
      "grad_norm": 0.5278471112251282,
      "learning_rate": 0.00023782855436081245,
      "loss": 6.5781,
      "step": 3512
    },
    {
      "epoch": 1.0492941967286578,
      "grad_norm": 0.4723598062992096,
      "learning_rate": 0.00023775388291517324,
      "loss": 7.2676,
      "step": 3513
    },
    {
      "epoch": 1.0495929494361043,
      "grad_norm": 0.4119863212108612,
      "learning_rate": 0.00023767921146953405,
      "loss": 7.3857,
      "step": 3514
    },
    {
      "epoch": 1.0498917021435508,
      "grad_norm": 0.3994031250476837,
      "learning_rate": 0.00023760454002389486,
      "loss": 7.4717,
      "step": 3515
    },
    {
      "epoch": 1.050190454850997,
      "grad_norm": 0.44921326637268066,
      "learning_rate": 0.00023752986857825567,
      "loss": 7.4688,
      "step": 3516
    },
    {
      "epoch": 1.0504892075584435,
      "grad_norm": 0.3806065022945404,
      "learning_rate": 0.0002374551971326165,
      "loss": 7.3906,
      "step": 3517
    },
    {
      "epoch": 1.0507879602658898,
      "grad_norm": 0.43277642130851746,
      "learning_rate": 0.0002373805256869773,
      "loss": 7.0537,
      "step": 3518
    },
    {
      "epoch": 1.0510867129733363,
      "grad_norm": 0.4955572783946991,
      "learning_rate": 0.0002373058542413381,
      "loss": 7.2676,
      "step": 3519
    },
    {
      "epoch": 1.0513854656807828,
      "grad_norm": 0.4657037556171417,
      "learning_rate": 0.00023723118279569895,
      "loss": 7.4219,
      "step": 3520
    },
    {
      "epoch": 1.0516842183882291,
      "grad_norm": 0.46725812554359436,
      "learning_rate": 0.00023715651135005974,
      "loss": 6.9238,
      "step": 3521
    },
    {
      "epoch": 1.0519829710956756,
      "grad_norm": 0.3864727318286896,
      "learning_rate": 0.00023708183990442055,
      "loss": 7.1924,
      "step": 3522
    },
    {
      "epoch": 1.052281723803122,
      "grad_norm": 0.5550384521484375,
      "learning_rate": 0.00023700716845878136,
      "loss": 7.2822,
      "step": 3523
    },
    {
      "epoch": 1.0525804765105684,
      "grad_norm": 0.4352722465991974,
      "learning_rate": 0.00023693249701314218,
      "loss": 7.0439,
      "step": 3524
    },
    {
      "epoch": 1.0528792292180147,
      "grad_norm": 0.4632464647293091,
      "learning_rate": 0.000236857825567503,
      "loss": 7.4111,
      "step": 3525
    },
    {
      "epoch": 1.0531779819254612,
      "grad_norm": 0.6342698335647583,
      "learning_rate": 0.0002367831541218638,
      "loss": 6.2627,
      "step": 3526
    },
    {
      "epoch": 1.0534767346329077,
      "grad_norm": 0.42959651350975037,
      "learning_rate": 0.00023670848267622462,
      "loss": 7.2373,
      "step": 3527
    },
    {
      "epoch": 1.053775487340354,
      "grad_norm": 0.41304928064346313,
      "learning_rate": 0.00023663381123058543,
      "loss": 7.3867,
      "step": 3528
    },
    {
      "epoch": 1.0540742400478005,
      "grad_norm": 0.5614072680473328,
      "learning_rate": 0.00023655913978494624,
      "loss": 6.7646,
      "step": 3529
    },
    {
      "epoch": 1.0543729927552468,
      "grad_norm": 0.5139108896255493,
      "learning_rate": 0.00023648446833930705,
      "loss": 6.8008,
      "step": 3530
    },
    {
      "epoch": 1.0546717454626933,
      "grad_norm": 0.511176347732544,
      "learning_rate": 0.00023640979689366787,
      "loss": 7.2939,
      "step": 3531
    },
    {
      "epoch": 1.0549704981701398,
      "grad_norm": 0.436585396528244,
      "learning_rate": 0.00023633512544802868,
      "loss": 7.3955,
      "step": 3532
    },
    {
      "epoch": 1.055269250877586,
      "grad_norm": 0.37389472126960754,
      "learning_rate": 0.0002362604540023895,
      "loss": 7.3135,
      "step": 3533
    },
    {
      "epoch": 1.0555680035850326,
      "grad_norm": 0.449990838766098,
      "learning_rate": 0.0002361857825567503,
      "loss": 7.5039,
      "step": 3534
    },
    {
      "epoch": 1.0558667562924788,
      "grad_norm": 0.45979785919189453,
      "learning_rate": 0.00023611111111111112,
      "loss": 7.3828,
      "step": 3535
    },
    {
      "epoch": 1.0561655089999253,
      "grad_norm": 0.45056813955307007,
      "learning_rate": 0.00023603643966547193,
      "loss": 7.4648,
      "step": 3536
    },
    {
      "epoch": 1.0564642617073716,
      "grad_norm": 0.4468417763710022,
      "learning_rate": 0.00023596176821983272,
      "loss": 7.1729,
      "step": 3537
    },
    {
      "epoch": 1.0567630144148181,
      "grad_norm": 0.4586280882358551,
      "learning_rate": 0.00023588709677419356,
      "loss": 7.2959,
      "step": 3538
    },
    {
      "epoch": 1.0570617671222646,
      "grad_norm": 0.4126024544239044,
      "learning_rate": 0.00023581242532855437,
      "loss": 7.1406,
      "step": 3539
    },
    {
      "epoch": 1.057360519829711,
      "grad_norm": 0.5698433518409729,
      "learning_rate": 0.00023573775388291518,
      "loss": 7.1113,
      "step": 3540
    },
    {
      "epoch": 1.0576592725371574,
      "grad_norm": 0.43380776047706604,
      "learning_rate": 0.000235663082437276,
      "loss": 7.4805,
      "step": 3541
    },
    {
      "epoch": 1.0579580252446037,
      "grad_norm": 0.3770984411239624,
      "learning_rate": 0.0002355884109916368,
      "loss": 7.3779,
      "step": 3542
    },
    {
      "epoch": 1.0582567779520502,
      "grad_norm": 0.4426564872264862,
      "learning_rate": 0.00023551373954599762,
      "loss": 7.5684,
      "step": 3543
    },
    {
      "epoch": 1.0585555306594967,
      "grad_norm": 0.501446008682251,
      "learning_rate": 0.00023543906810035843,
      "loss": 7.1611,
      "step": 3544
    },
    {
      "epoch": 1.058854283366943,
      "grad_norm": 0.4976034462451935,
      "learning_rate": 0.00023536439665471922,
      "loss": 6.9375,
      "step": 3545
    },
    {
      "epoch": 1.0591530360743895,
      "grad_norm": 0.41551291942596436,
      "learning_rate": 0.00023528972520908003,
      "loss": 7.2852,
      "step": 3546
    },
    {
      "epoch": 1.0594517887818358,
      "grad_norm": 0.532024621963501,
      "learning_rate": 0.00023521505376344087,
      "loss": 6.8428,
      "step": 3547
    },
    {
      "epoch": 1.0597505414892823,
      "grad_norm": 0.40817198157310486,
      "learning_rate": 0.00023514038231780169,
      "loss": 7.2764,
      "step": 3548
    },
    {
      "epoch": 1.0600492941967286,
      "grad_norm": 0.4813908636569977,
      "learning_rate": 0.0002350657108721625,
      "loss": 7.0273,
      "step": 3549
    },
    {
      "epoch": 1.060348046904175,
      "grad_norm": 0.4876035451889038,
      "learning_rate": 0.0002349910394265233,
      "loss": 7.5811,
      "step": 3550
    },
    {
      "epoch": 1.0606467996116216,
      "grad_norm": 0.5126113295555115,
      "learning_rate": 0.00023491636798088413,
      "loss": 6.6631,
      "step": 3551
    },
    {
      "epoch": 1.0609455523190678,
      "grad_norm": 0.5055624842643738,
      "learning_rate": 0.00023484169653524494,
      "loss": 6.8701,
      "step": 3552
    },
    {
      "epoch": 1.0612443050265143,
      "grad_norm": 0.4357185661792755,
      "learning_rate": 0.00023476702508960572,
      "loss": 7.3691,
      "step": 3553
    },
    {
      "epoch": 1.0615430577339606,
      "grad_norm": 0.5298268795013428,
      "learning_rate": 0.00023469235364396654,
      "loss": 6.0664,
      "step": 3554
    },
    {
      "epoch": 1.0618418104414071,
      "grad_norm": 0.28192147612571716,
      "learning_rate": 0.00023461768219832735,
      "loss": 7.8428,
      "step": 3555
    },
    {
      "epoch": 1.0621405631488536,
      "grad_norm": 0.37158578634262085,
      "learning_rate": 0.0002345430107526882,
      "loss": 7.6973,
      "step": 3556
    },
    {
      "epoch": 1.0624393158563,
      "grad_norm": 0.42419564723968506,
      "learning_rate": 0.000234468339307049,
      "loss": 7.1367,
      "step": 3557
    },
    {
      "epoch": 1.0627380685637464,
      "grad_norm": 0.48835110664367676,
      "learning_rate": 0.00023439366786140982,
      "loss": 7.0576,
      "step": 3558
    },
    {
      "epoch": 1.0630368212711927,
      "grad_norm": 0.4197358787059784,
      "learning_rate": 0.00023431899641577063,
      "loss": 7.5664,
      "step": 3559
    },
    {
      "epoch": 1.0633355739786392,
      "grad_norm": 0.45524272322654724,
      "learning_rate": 0.00023424432497013144,
      "loss": 7.0596,
      "step": 3560
    },
    {
      "epoch": 1.0636343266860857,
      "grad_norm": 0.4065592586994171,
      "learning_rate": 0.00023416965352449223,
      "loss": 7.4932,
      "step": 3561
    },
    {
      "epoch": 1.063933079393532,
      "grad_norm": 0.4454300105571747,
      "learning_rate": 0.00023409498207885304,
      "loss": 7.4395,
      "step": 3562
    },
    {
      "epoch": 1.0642318321009785,
      "grad_norm": 0.3705102503299713,
      "learning_rate": 0.00023402031063321385,
      "loss": 7.7588,
      "step": 3563
    },
    {
      "epoch": 1.0645305848084248,
      "grad_norm": 0.4747895300388336,
      "learning_rate": 0.00023394563918757467,
      "loss": 6.4707,
      "step": 3564
    },
    {
      "epoch": 1.0648293375158713,
      "grad_norm": 0.4126259684562683,
      "learning_rate": 0.00023387096774193548,
      "loss": 7.4971,
      "step": 3565
    },
    {
      "epoch": 1.0651280902233176,
      "grad_norm": 0.385772168636322,
      "learning_rate": 0.00023379629629629632,
      "loss": 7.6152,
      "step": 3566
    },
    {
      "epoch": 1.065426842930764,
      "grad_norm": 0.4924488067626953,
      "learning_rate": 0.00023372162485065713,
      "loss": 7.5391,
      "step": 3567
    },
    {
      "epoch": 1.0657255956382106,
      "grad_norm": 0.5660316348075867,
      "learning_rate": 0.00023364695340501794,
      "loss": 6.9834,
      "step": 3568
    },
    {
      "epoch": 1.0660243483456568,
      "grad_norm": 0.5175976157188416,
      "learning_rate": 0.00023357228195937873,
      "loss": 7.2197,
      "step": 3569
    },
    {
      "epoch": 1.0663231010531034,
      "grad_norm": 0.4313836097717285,
      "learning_rate": 0.00023349761051373954,
      "loss": 7.1768,
      "step": 3570
    },
    {
      "epoch": 1.0666218537605496,
      "grad_norm": 0.4824002683162689,
      "learning_rate": 0.00023342293906810036,
      "loss": 7.4424,
      "step": 3571
    },
    {
      "epoch": 1.0669206064679961,
      "grad_norm": 0.5320261716842651,
      "learning_rate": 0.00023334826762246117,
      "loss": 6.624,
      "step": 3572
    },
    {
      "epoch": 1.0672193591754424,
      "grad_norm": 0.48247095942497253,
      "learning_rate": 0.00023327359617682198,
      "loss": 7.3789,
      "step": 3573
    },
    {
      "epoch": 1.067518111882889,
      "grad_norm": 0.45813021063804626,
      "learning_rate": 0.0002331989247311828,
      "loss": 7.3271,
      "step": 3574
    },
    {
      "epoch": 1.0678168645903354,
      "grad_norm": 0.48781096935272217,
      "learning_rate": 0.00023312425328554363,
      "loss": 7.5625,
      "step": 3575
    },
    {
      "epoch": 1.0681156172977817,
      "grad_norm": 0.4279775023460388,
      "learning_rate": 0.00023304958183990445,
      "loss": 7.3115,
      "step": 3576
    },
    {
      "epoch": 1.0684143700052282,
      "grad_norm": 0.47480636835098267,
      "learning_rate": 0.00023297491039426523,
      "loss": 6.9375,
      "step": 3577
    },
    {
      "epoch": 1.0687131227126745,
      "grad_norm": 0.44886255264282227,
      "learning_rate": 0.00023290023894862605,
      "loss": 7.3545,
      "step": 3578
    },
    {
      "epoch": 1.069011875420121,
      "grad_norm": 0.47554802894592285,
      "learning_rate": 0.00023282556750298686,
      "loss": 7.3164,
      "step": 3579
    },
    {
      "epoch": 1.0693106281275675,
      "grad_norm": 0.49178600311279297,
      "learning_rate": 0.00023275089605734767,
      "loss": 7.4814,
      "step": 3580
    },
    {
      "epoch": 1.0696093808350138,
      "grad_norm": 0.4431050717830658,
      "learning_rate": 0.00023267622461170848,
      "loss": 6.8828,
      "step": 3581
    },
    {
      "epoch": 1.0699081335424603,
      "grad_norm": 0.5045993328094482,
      "learning_rate": 0.0002326015531660693,
      "loss": 7.4502,
      "step": 3582
    },
    {
      "epoch": 1.0702068862499066,
      "grad_norm": 0.490064412355423,
      "learning_rate": 0.0002325268817204301,
      "loss": 7.0459,
      "step": 3583
    },
    {
      "epoch": 1.070505638957353,
      "grad_norm": 0.41037675738334656,
      "learning_rate": 0.00023245221027479095,
      "loss": 7.0332,
      "step": 3584
    },
    {
      "epoch": 1.0708043916647996,
      "grad_norm": 0.36451292037963867,
      "learning_rate": 0.00023237753882915174,
      "loss": 7.6396,
      "step": 3585
    },
    {
      "epoch": 1.0711031443722459,
      "grad_norm": 0.49065929651260376,
      "learning_rate": 0.00023230286738351255,
      "loss": 7.1611,
      "step": 3586
    },
    {
      "epoch": 1.0714018970796924,
      "grad_norm": 0.5464848875999451,
      "learning_rate": 0.00023222819593787336,
      "loss": 7.5498,
      "step": 3587
    },
    {
      "epoch": 1.0717006497871386,
      "grad_norm": 0.6212460994720459,
      "learning_rate": 0.00023215352449223417,
      "loss": 6.2744,
      "step": 3588
    },
    {
      "epoch": 1.0719994024945851,
      "grad_norm": 0.42151927947998047,
      "learning_rate": 0.000232078853046595,
      "loss": 7.792,
      "step": 3589
    },
    {
      "epoch": 1.0722981552020314,
      "grad_norm": 0.4956859052181244,
      "learning_rate": 0.0002320041816009558,
      "loss": 7.2227,
      "step": 3590
    },
    {
      "epoch": 1.072596907909478,
      "grad_norm": 0.46807554364204407,
      "learning_rate": 0.0002319295101553166,
      "loss": 6.9785,
      "step": 3591
    },
    {
      "epoch": 1.0728956606169244,
      "grad_norm": 0.4742286801338196,
      "learning_rate": 0.00023185483870967743,
      "loss": 7.0215,
      "step": 3592
    },
    {
      "epoch": 1.0731944133243707,
      "grad_norm": 0.6089000701904297,
      "learning_rate": 0.00023178016726403824,
      "loss": 6.6162,
      "step": 3593
    },
    {
      "epoch": 1.0734931660318172,
      "grad_norm": 0.4700191915035248,
      "learning_rate": 0.00023170549581839905,
      "loss": 7.5928,
      "step": 3594
    },
    {
      "epoch": 1.0737919187392635,
      "grad_norm": 0.40066248178482056,
      "learning_rate": 0.00023163082437275986,
      "loss": 7.4229,
      "step": 3595
    },
    {
      "epoch": 1.07409067144671,
      "grad_norm": 0.4659693241119385,
      "learning_rate": 0.00023155615292712068,
      "loss": 7.3662,
      "step": 3596
    },
    {
      "epoch": 1.0743894241541563,
      "grad_norm": 0.5348121523857117,
      "learning_rate": 0.0002314814814814815,
      "loss": 6.5713,
      "step": 3597
    },
    {
      "epoch": 1.0746881768616028,
      "grad_norm": 0.40935805439949036,
      "learning_rate": 0.0002314068100358423,
      "loss": 7.5391,
      "step": 3598
    },
    {
      "epoch": 1.0749869295690493,
      "grad_norm": 0.5250968933105469,
      "learning_rate": 0.00023133213859020312,
      "loss": 7.0645,
      "step": 3599
    },
    {
      "epoch": 1.0752856822764956,
      "grad_norm": 0.41725078225135803,
      "learning_rate": 0.00023125746714456393,
      "loss": 7.2959,
      "step": 3600
    },
    {
      "epoch": 1.0752856822764956,
      "eval_bleu": 0.13331821230049218,
      "eval_loss": 7.02734375,
      "eval_runtime": 544.5237,
      "eval_samples_per_second": 2.588,
      "eval_steps_per_second": 0.163,
      "step": 3600
    },
    {
      "epoch": 1.075584434983942,
      "grad_norm": 0.4431425929069519,
      "learning_rate": 0.00023118279569892471,
      "loss": 7.374,
      "step": 3601
    },
    {
      "epoch": 1.0758831876913884,
      "grad_norm": 0.4315720498561859,
      "learning_rate": 0.00023110812425328555,
      "loss": 7.248,
      "step": 3602
    },
    {
      "epoch": 1.0761819403988349,
      "grad_norm": 0.5024608373641968,
      "learning_rate": 0.00023103345280764637,
      "loss": 7.1943,
      "step": 3603
    },
    {
      "epoch": 1.0764806931062814,
      "grad_norm": 0.4210495352745056,
      "learning_rate": 0.00023095878136200718,
      "loss": 7.6162,
      "step": 3604
    },
    {
      "epoch": 1.0767794458137276,
      "grad_norm": 0.41957131028175354,
      "learning_rate": 0.000230884109916368,
      "loss": 7.6963,
      "step": 3605
    },
    {
      "epoch": 1.0770781985211741,
      "grad_norm": 0.633371889591217,
      "learning_rate": 0.0002308094384707288,
      "loss": 6.5068,
      "step": 3606
    },
    {
      "epoch": 1.0773769512286204,
      "grad_norm": 0.4887670576572418,
      "learning_rate": 0.00023073476702508962,
      "loss": 6.8809,
      "step": 3607
    },
    {
      "epoch": 1.077675703936067,
      "grad_norm": 0.413811594247818,
      "learning_rate": 0.00023066009557945043,
      "loss": 7.4971,
      "step": 3608
    },
    {
      "epoch": 1.0779744566435134,
      "grad_norm": 0.4432021677494049,
      "learning_rate": 0.00023058542413381122,
      "loss": 7.2207,
      "step": 3609
    },
    {
      "epoch": 1.0782732093509597,
      "grad_norm": 0.41436460614204407,
      "learning_rate": 0.00023051075268817203,
      "loss": 7.2793,
      "step": 3610
    },
    {
      "epoch": 1.0785719620584062,
      "grad_norm": 0.4566155970096588,
      "learning_rate": 0.00023043608124253284,
      "loss": 7.1084,
      "step": 3611
    },
    {
      "epoch": 1.0788707147658525,
      "grad_norm": 0.4969249367713928,
      "learning_rate": 0.00023036140979689368,
      "loss": 7.3809,
      "step": 3612
    },
    {
      "epoch": 1.079169467473299,
      "grad_norm": 0.48869797587394714,
      "learning_rate": 0.0002302867383512545,
      "loss": 6.96,
      "step": 3613
    },
    {
      "epoch": 1.0794682201807453,
      "grad_norm": 0.4469222128391266,
      "learning_rate": 0.0002302120669056153,
      "loss": 6.9434,
      "step": 3614
    },
    {
      "epoch": 1.0797669728881918,
      "grad_norm": 0.5009050965309143,
      "learning_rate": 0.00023013739545997612,
      "loss": 6.792,
      "step": 3615
    },
    {
      "epoch": 1.0800657255956383,
      "grad_norm": 0.42757469415664673,
      "learning_rate": 0.00023006272401433693,
      "loss": 7.084,
      "step": 3616
    },
    {
      "epoch": 1.0803644783030846,
      "grad_norm": 0.4744983911514282,
      "learning_rate": 0.00022998805256869772,
      "loss": 7.4141,
      "step": 3617
    },
    {
      "epoch": 1.080663231010531,
      "grad_norm": 0.41049787402153015,
      "learning_rate": 0.00022991338112305853,
      "loss": 7.4912,
      "step": 3618
    },
    {
      "epoch": 1.0809619837179774,
      "grad_norm": 0.4809112846851349,
      "learning_rate": 0.00022983870967741935,
      "loss": 7.0605,
      "step": 3619
    },
    {
      "epoch": 1.0812607364254239,
      "grad_norm": 0.5223240256309509,
      "learning_rate": 0.00022976403823178016,
      "loss": 6.6748,
      "step": 3620
    },
    {
      "epoch": 1.0815594891328704,
      "grad_norm": 0.5157420039176941,
      "learning_rate": 0.000229689366786141,
      "loss": 6.8818,
      "step": 3621
    },
    {
      "epoch": 1.0818582418403166,
      "grad_norm": 0.4686109125614166,
      "learning_rate": 0.0002296146953405018,
      "loss": 7.458,
      "step": 3622
    },
    {
      "epoch": 1.0821569945477632,
      "grad_norm": 0.33010005950927734,
      "learning_rate": 0.00022954002389486263,
      "loss": 8.0596,
      "step": 3623
    },
    {
      "epoch": 1.0824557472552094,
      "grad_norm": 0.3202129006385803,
      "learning_rate": 0.00022946535244922344,
      "loss": 7.9795,
      "step": 3624
    },
    {
      "epoch": 1.082754499962656,
      "grad_norm": 0.41053906083106995,
      "learning_rate": 0.00022939068100358422,
      "loss": 7.6592,
      "step": 3625
    },
    {
      "epoch": 1.0830532526701022,
      "grad_norm": 0.5165395736694336,
      "learning_rate": 0.00022931600955794504,
      "loss": 7.2256,
      "step": 3626
    },
    {
      "epoch": 1.0833520053775487,
      "grad_norm": 0.4033238887786865,
      "learning_rate": 0.00022924133811230585,
      "loss": 7.5732,
      "step": 3627
    },
    {
      "epoch": 1.0836507580849952,
      "grad_norm": 0.4217696189880371,
      "learning_rate": 0.00022916666666666666,
      "loss": 7.4482,
      "step": 3628
    },
    {
      "epoch": 1.0839495107924415,
      "grad_norm": 0.45531514286994934,
      "learning_rate": 0.00022909199522102748,
      "loss": 7.5439,
      "step": 3629
    },
    {
      "epoch": 1.084248263499888,
      "grad_norm": 0.4496816396713257,
      "learning_rate": 0.00022901732377538832,
      "loss": 7.6113,
      "step": 3630
    },
    {
      "epoch": 1.0845470162073343,
      "grad_norm": 0.4297560453414917,
      "learning_rate": 0.00022894265232974913,
      "loss": 7.3955,
      "step": 3631
    },
    {
      "epoch": 1.0848457689147808,
      "grad_norm": 0.38464900851249695,
      "learning_rate": 0.00022886798088410994,
      "loss": 7.6592,
      "step": 3632
    },
    {
      "epoch": 1.0851445216222273,
      "grad_norm": 0.3810707926750183,
      "learning_rate": 0.00022879330943847073,
      "loss": 7.291,
      "step": 3633
    },
    {
      "epoch": 1.0854432743296736,
      "grad_norm": 0.4724106788635254,
      "learning_rate": 0.00022871863799283154,
      "loss": 6.9121,
      "step": 3634
    },
    {
      "epoch": 1.08574202703712,
      "grad_norm": 0.5038411617279053,
      "learning_rate": 0.00022864396654719235,
      "loss": 7.168,
      "step": 3635
    },
    {
      "epoch": 1.0860407797445664,
      "grad_norm": 0.4978819787502289,
      "learning_rate": 0.00022856929510155317,
      "loss": 6.6816,
      "step": 3636
    },
    {
      "epoch": 1.0863395324520129,
      "grad_norm": 0.4870164394378662,
      "learning_rate": 0.00022849462365591398,
      "loss": 6.9502,
      "step": 3637
    },
    {
      "epoch": 1.0866382851594594,
      "grad_norm": 0.5583674907684326,
      "learning_rate": 0.0002284199522102748,
      "loss": 6.9424,
      "step": 3638
    },
    {
      "epoch": 1.0869370378669057,
      "grad_norm": 0.4769722521305084,
      "learning_rate": 0.00022834528076463563,
      "loss": 7.0938,
      "step": 3639
    },
    {
      "epoch": 1.0872357905743522,
      "grad_norm": 0.4232918620109558,
      "learning_rate": 0.00022827060931899644,
      "loss": 7.2637,
      "step": 3640
    },
    {
      "epoch": 1.0875345432817984,
      "grad_norm": 0.46359649300575256,
      "learning_rate": 0.00022819593787335723,
      "loss": 7.5908,
      "step": 3641
    },
    {
      "epoch": 1.087833295989245,
      "grad_norm": 0.45621782541275024,
      "learning_rate": 0.00022812126642771804,
      "loss": 6.9111,
      "step": 3642
    },
    {
      "epoch": 1.0881320486966912,
      "grad_norm": 0.462837278842926,
      "learning_rate": 0.00022804659498207886,
      "loss": 7.0332,
      "step": 3643
    },
    {
      "epoch": 1.0884308014041377,
      "grad_norm": 0.40335196256637573,
      "learning_rate": 0.00022797192353643967,
      "loss": 7.8076,
      "step": 3644
    },
    {
      "epoch": 1.0887295541115842,
      "grad_norm": 0.45220163464546204,
      "learning_rate": 0.00022789725209080048,
      "loss": 7.6719,
      "step": 3645
    },
    {
      "epoch": 1.0890283068190305,
      "grad_norm": 0.4944794476032257,
      "learning_rate": 0.0002278225806451613,
      "loss": 6.7812,
      "step": 3646
    },
    {
      "epoch": 1.089327059526477,
      "grad_norm": 0.43171384930610657,
      "learning_rate": 0.0002277479091995221,
      "loss": 7.6357,
      "step": 3647
    },
    {
      "epoch": 1.0896258122339233,
      "grad_norm": 0.5145819187164307,
      "learning_rate": 0.00022767323775388295,
      "loss": 7.0059,
      "step": 3648
    },
    {
      "epoch": 1.0899245649413698,
      "grad_norm": 0.3682888150215149,
      "learning_rate": 0.00022759856630824373,
      "loss": 7.9092,
      "step": 3649
    },
    {
      "epoch": 1.090223317648816,
      "grad_norm": 0.4234005808830261,
      "learning_rate": 0.00022752389486260455,
      "loss": 7.2842,
      "step": 3650
    },
    {
      "epoch": 1.0905220703562626,
      "grad_norm": 0.48093074560165405,
      "learning_rate": 0.00022744922341696536,
      "loss": 6.833,
      "step": 3651
    },
    {
      "epoch": 1.090820823063709,
      "grad_norm": 0.45319032669067383,
      "learning_rate": 0.00022737455197132617,
      "loss": 7.4795,
      "step": 3652
    },
    {
      "epoch": 1.0911195757711554,
      "grad_norm": 0.5264228582382202,
      "learning_rate": 0.00022729988052568698,
      "loss": 7.3223,
      "step": 3653
    },
    {
      "epoch": 1.0914183284786019,
      "grad_norm": 0.5728379487991333,
      "learning_rate": 0.0002272252090800478,
      "loss": 6.7041,
      "step": 3654
    },
    {
      "epoch": 1.0917170811860482,
      "grad_norm": 0.603995680809021,
      "learning_rate": 0.0002271505376344086,
      "loss": 6.2344,
      "step": 3655
    },
    {
      "epoch": 1.0920158338934947,
      "grad_norm": 0.4285690486431122,
      "learning_rate": 0.0002270758661887694,
      "loss": 7.293,
      "step": 3656
    },
    {
      "epoch": 1.0923145866009412,
      "grad_norm": 0.3832232654094696,
      "learning_rate": 0.0002270011947431302,
      "loss": 7.8418,
      "step": 3657
    },
    {
      "epoch": 1.0926133393083874,
      "grad_norm": 0.5517590045928955,
      "learning_rate": 0.00022692652329749105,
      "loss": 5.6416,
      "step": 3658
    },
    {
      "epoch": 1.092912092015834,
      "grad_norm": 0.3680458962917328,
      "learning_rate": 0.00022685185185185186,
      "loss": 7.499,
      "step": 3659
    },
    {
      "epoch": 1.0932108447232802,
      "grad_norm": 0.5124699473381042,
      "learning_rate": 0.00022677718040621267,
      "loss": 6.6924,
      "step": 3660
    },
    {
      "epoch": 1.0935095974307267,
      "grad_norm": 0.5643318295478821,
      "learning_rate": 0.0002267025089605735,
      "loss": 6.7393,
      "step": 3661
    },
    {
      "epoch": 1.0938083501381732,
      "grad_norm": 0.44834256172180176,
      "learning_rate": 0.0002266278375149343,
      "loss": 7.5459,
      "step": 3662
    },
    {
      "epoch": 1.0941071028456195,
      "grad_norm": 0.5248439908027649,
      "learning_rate": 0.0002265531660692951,
      "loss": 7.6318,
      "step": 3663
    },
    {
      "epoch": 1.094405855553066,
      "grad_norm": 0.5393211245536804,
      "learning_rate": 0.0002264784946236559,
      "loss": 7.1562,
      "step": 3664
    },
    {
      "epoch": 1.0947046082605123,
      "grad_norm": 0.43075811862945557,
      "learning_rate": 0.0002264038231780167,
      "loss": 7.3799,
      "step": 3665
    },
    {
      "epoch": 1.0950033609679588,
      "grad_norm": 0.4262203872203827,
      "learning_rate": 0.00022632915173237752,
      "loss": 7.7373,
      "step": 3666
    },
    {
      "epoch": 1.095302113675405,
      "grad_norm": 0.5226148366928101,
      "learning_rate": 0.00022625448028673836,
      "loss": 7.3604,
      "step": 3667
    },
    {
      "epoch": 1.0956008663828516,
      "grad_norm": 0.38375383615493774,
      "learning_rate": 0.00022617980884109918,
      "loss": 7.4912,
      "step": 3668
    },
    {
      "epoch": 1.095899619090298,
      "grad_norm": 0.43966707587242126,
      "learning_rate": 0.00022610513739546,
      "loss": 7.5508,
      "step": 3669
    },
    {
      "epoch": 1.0961983717977444,
      "grad_norm": 0.4803163409233093,
      "learning_rate": 0.0002260304659498208,
      "loss": 7.0615,
      "step": 3670
    },
    {
      "epoch": 1.0964971245051909,
      "grad_norm": 0.5080010890960693,
      "learning_rate": 0.00022595579450418162,
      "loss": 7.7061,
      "step": 3671
    },
    {
      "epoch": 1.0967958772126372,
      "grad_norm": 0.4158889651298523,
      "learning_rate": 0.0002258811230585424,
      "loss": 7.5527,
      "step": 3672
    },
    {
      "epoch": 1.0970946299200837,
      "grad_norm": 0.3766680061817169,
      "learning_rate": 0.00022580645161290321,
      "loss": 7.6572,
      "step": 3673
    },
    {
      "epoch": 1.09739338262753,
      "grad_norm": 0.3838436007499695,
      "learning_rate": 0.00022573178016726403,
      "loss": 7.8291,
      "step": 3674
    },
    {
      "epoch": 1.0976921353349764,
      "grad_norm": 0.481825053691864,
      "learning_rate": 0.00022565710872162484,
      "loss": 6.918,
      "step": 3675
    },
    {
      "epoch": 1.097990888042423,
      "grad_norm": 0.47038620710372925,
      "learning_rate": 0.00022558243727598568,
      "loss": 7.2607,
      "step": 3676
    },
    {
      "epoch": 1.0982896407498692,
      "grad_norm": 0.460730642080307,
      "learning_rate": 0.0002255077658303465,
      "loss": 7.6104,
      "step": 3677
    },
    {
      "epoch": 1.0985883934573157,
      "grad_norm": 0.3348842263221741,
      "learning_rate": 0.0002254330943847073,
      "loss": 8.0615,
      "step": 3678
    },
    {
      "epoch": 1.098887146164762,
      "grad_norm": 0.558883547782898,
      "learning_rate": 0.00022535842293906812,
      "loss": 7.002,
      "step": 3679
    },
    {
      "epoch": 1.0991858988722085,
      "grad_norm": 0.467217355966568,
      "learning_rate": 0.0002252837514934289,
      "loss": 6.8555,
      "step": 3680
    },
    {
      "epoch": 1.099484651579655,
      "grad_norm": 0.5150229930877686,
      "learning_rate": 0.00022520908004778972,
      "loss": 6.9785,
      "step": 3681
    },
    {
      "epoch": 1.0997834042871013,
      "grad_norm": 0.37269553542137146,
      "learning_rate": 0.00022513440860215053,
      "loss": 7.3682,
      "step": 3682
    },
    {
      "epoch": 1.1000821569945478,
      "grad_norm": 0.3928232789039612,
      "learning_rate": 0.00022505973715651134,
      "loss": 7.3848,
      "step": 3683
    },
    {
      "epoch": 1.100380909701994,
      "grad_norm": 0.3720259368419647,
      "learning_rate": 0.00022498506571087216,
      "loss": 7.625,
      "step": 3684
    },
    {
      "epoch": 1.1006796624094406,
      "grad_norm": 0.4219159781932831,
      "learning_rate": 0.000224910394265233,
      "loss": 7.3506,
      "step": 3685
    },
    {
      "epoch": 1.100978415116887,
      "grad_norm": 0.5047927498817444,
      "learning_rate": 0.0002248357228195938,
      "loss": 7.1836,
      "step": 3686
    },
    {
      "epoch": 1.1012771678243334,
      "grad_norm": 0.4423632025718689,
      "learning_rate": 0.00022476105137395462,
      "loss": 7.5762,
      "step": 3687
    },
    {
      "epoch": 1.1015759205317799,
      "grad_norm": 0.336566299200058,
      "learning_rate": 0.0002246863799283154,
      "loss": 7.6973,
      "step": 3688
    },
    {
      "epoch": 1.1018746732392262,
      "grad_norm": 0.7191488742828369,
      "learning_rate": 0.00022461170848267622,
      "loss": 6.5254,
      "step": 3689
    },
    {
      "epoch": 1.1021734259466727,
      "grad_norm": 0.44292840361595154,
      "learning_rate": 0.00022453703703703703,
      "loss": 7.4912,
      "step": 3690
    },
    {
      "epoch": 1.102472178654119,
      "grad_norm": 0.5078006386756897,
      "learning_rate": 0.00022446236559139785,
      "loss": 6.6006,
      "step": 3691
    },
    {
      "epoch": 1.1027709313615655,
      "grad_norm": 0.4573490023612976,
      "learning_rate": 0.00022438769414575866,
      "loss": 7.1211,
      "step": 3692
    },
    {
      "epoch": 1.103069684069012,
      "grad_norm": 0.4976747930049896,
      "learning_rate": 0.00022431302270011947,
      "loss": 6.4385,
      "step": 3693
    },
    {
      "epoch": 1.1033684367764582,
      "grad_norm": 0.4464266002178192,
      "learning_rate": 0.0002242383512544803,
      "loss": 7.2607,
      "step": 3694
    },
    {
      "epoch": 1.1036671894839047,
      "grad_norm": 0.5975297093391418,
      "learning_rate": 0.00022416367980884113,
      "loss": 6.4043,
      "step": 3695
    },
    {
      "epoch": 1.103965942191351,
      "grad_norm": 0.5725892186164856,
      "learning_rate": 0.0002240890083632019,
      "loss": 6.8379,
      "step": 3696
    },
    {
      "epoch": 1.1042646948987975,
      "grad_norm": 0.3910488486289978,
      "learning_rate": 0.00022401433691756272,
      "loss": 7.7197,
      "step": 3697
    },
    {
      "epoch": 1.104563447606244,
      "grad_norm": 0.4237326681613922,
      "learning_rate": 0.00022393966547192354,
      "loss": 7.3213,
      "step": 3698
    },
    {
      "epoch": 1.1048622003136903,
      "grad_norm": 0.5856989026069641,
      "learning_rate": 0.00022386499402628435,
      "loss": 6.208,
      "step": 3699
    },
    {
      "epoch": 1.1051609530211368,
      "grad_norm": 0.569258987903595,
      "learning_rate": 0.00022379032258064516,
      "loss": 7.125,
      "step": 3700
    },
    {
      "epoch": 1.105459705728583,
      "grad_norm": 0.4820390045642853,
      "learning_rate": 0.00022371565113500598,
      "loss": 7.1738,
      "step": 3701
    },
    {
      "epoch": 1.1057584584360296,
      "grad_norm": 0.5075317025184631,
      "learning_rate": 0.0002236409796893668,
      "loss": 7.0527,
      "step": 3702
    },
    {
      "epoch": 1.1060572111434759,
      "grad_norm": 0.5596185922622681,
      "learning_rate": 0.0002235663082437276,
      "loss": 6.7773,
      "step": 3703
    },
    {
      "epoch": 1.1063559638509224,
      "grad_norm": 0.4668913781642914,
      "learning_rate": 0.00022349163679808841,
      "loss": 7.2139,
      "step": 3704
    },
    {
      "epoch": 1.106654716558369,
      "grad_norm": 0.4731612503528595,
      "learning_rate": 0.00022341696535244923,
      "loss": 7.2803,
      "step": 3705
    },
    {
      "epoch": 1.1069534692658152,
      "grad_norm": 0.4278222322463989,
      "learning_rate": 0.00022334229390681004,
      "loss": 7.5693,
      "step": 3706
    },
    {
      "epoch": 1.1072522219732617,
      "grad_norm": 0.5118570923805237,
      "learning_rate": 0.00022326762246117085,
      "loss": 6.9482,
      "step": 3707
    },
    {
      "epoch": 1.107550974680708,
      "grad_norm": 0.4479198157787323,
      "learning_rate": 0.00022319295101553167,
      "loss": 7.3281,
      "step": 3708
    },
    {
      "epoch": 1.1078497273881545,
      "grad_norm": 0.5565574169158936,
      "learning_rate": 0.00022311827956989248,
      "loss": 6.7832,
      "step": 3709
    },
    {
      "epoch": 1.108148480095601,
      "grad_norm": 0.42696332931518555,
      "learning_rate": 0.0002230436081242533,
      "loss": 7.4189,
      "step": 3710
    },
    {
      "epoch": 1.1084472328030472,
      "grad_norm": 0.4496011435985565,
      "learning_rate": 0.0002229689366786141,
      "loss": 7.1719,
      "step": 3711
    },
    {
      "epoch": 1.1087459855104937,
      "grad_norm": 0.4817701280117035,
      "learning_rate": 0.0002228942652329749,
      "loss": 7.2148,
      "step": 3712
    },
    {
      "epoch": 1.10904473821794,
      "grad_norm": 0.45263469219207764,
      "learning_rate": 0.00022281959378733573,
      "loss": 7.6826,
      "step": 3713
    },
    {
      "epoch": 1.1093434909253865,
      "grad_norm": 0.46634289622306824,
      "learning_rate": 0.00022274492234169654,
      "loss": 6.9111,
      "step": 3714
    },
    {
      "epoch": 1.109642243632833,
      "grad_norm": 0.45453667640686035,
      "learning_rate": 0.00022267025089605736,
      "loss": 7.1104,
      "step": 3715
    },
    {
      "epoch": 1.1099409963402793,
      "grad_norm": 0.39525067806243896,
      "learning_rate": 0.00022259557945041817,
      "loss": 7.21,
      "step": 3716
    },
    {
      "epoch": 1.1102397490477258,
      "grad_norm": 0.5643311738967896,
      "learning_rate": 0.00022252090800477898,
      "loss": 6.8193,
      "step": 3717
    },
    {
      "epoch": 1.110538501755172,
      "grad_norm": 0.46697357296943665,
      "learning_rate": 0.0002224462365591398,
      "loss": 7.2686,
      "step": 3718
    },
    {
      "epoch": 1.1108372544626186,
      "grad_norm": 0.45903605222702026,
      "learning_rate": 0.0002223715651135006,
      "loss": 7.2295,
      "step": 3719
    },
    {
      "epoch": 1.1111360071700649,
      "grad_norm": 0.4206157624721527,
      "learning_rate": 0.0002222968936678614,
      "loss": 7.3477,
      "step": 3720
    },
    {
      "epoch": 1.1114347598775114,
      "grad_norm": 0.43716198205947876,
      "learning_rate": 0.0002222222222222222,
      "loss": 7.1885,
      "step": 3721
    },
    {
      "epoch": 1.111733512584958,
      "grad_norm": 0.5289592146873474,
      "learning_rate": 0.00022214755077658305,
      "loss": 7.7764,
      "step": 3722
    },
    {
      "epoch": 1.1120322652924042,
      "grad_norm": 0.49974173307418823,
      "learning_rate": 0.00022207287933094386,
      "loss": 6.5439,
      "step": 3723
    },
    {
      "epoch": 1.1123310179998507,
      "grad_norm": 0.4650977849960327,
      "learning_rate": 0.00022199820788530467,
      "loss": 6.4863,
      "step": 3724
    },
    {
      "epoch": 1.112629770707297,
      "grad_norm": 0.4813796877861023,
      "learning_rate": 0.00022192353643966548,
      "loss": 7.0596,
      "step": 3725
    },
    {
      "epoch": 1.1129285234147435,
      "grad_norm": 0.4826425313949585,
      "learning_rate": 0.0002218488649940263,
      "loss": 7.0889,
      "step": 3726
    },
    {
      "epoch": 1.1132272761221897,
      "grad_norm": 0.4018382430076599,
      "learning_rate": 0.0002217741935483871,
      "loss": 7.3252,
      "step": 3727
    },
    {
      "epoch": 1.1135260288296362,
      "grad_norm": 0.6104487180709839,
      "learning_rate": 0.0002216995221027479,
      "loss": 6.2734,
      "step": 3728
    },
    {
      "epoch": 1.1138247815370828,
      "grad_norm": 0.5202031135559082,
      "learning_rate": 0.0002216248506571087,
      "loss": 6.9951,
      "step": 3729
    },
    {
      "epoch": 1.114123534244529,
      "grad_norm": 0.39571887254714966,
      "learning_rate": 0.00022155017921146952,
      "loss": 7.3887,
      "step": 3730
    },
    {
      "epoch": 1.1144222869519755,
      "grad_norm": 0.49455341696739197,
      "learning_rate": 0.00022147550776583036,
      "loss": 6.7646,
      "step": 3731
    },
    {
      "epoch": 1.1147210396594218,
      "grad_norm": 0.5208725333213806,
      "learning_rate": 0.00022140083632019117,
      "loss": 7.249,
      "step": 3732
    },
    {
      "epoch": 1.1150197923668683,
      "grad_norm": 0.47040948271751404,
      "learning_rate": 0.000221326164874552,
      "loss": 7.0498,
      "step": 3733
    },
    {
      "epoch": 1.1153185450743148,
      "grad_norm": 0.49994224309921265,
      "learning_rate": 0.0002212514934289128,
      "loss": 6.8252,
      "step": 3734
    },
    {
      "epoch": 1.115617297781761,
      "grad_norm": 0.44855907559394836,
      "learning_rate": 0.0002211768219832736,
      "loss": 6.9756,
      "step": 3735
    },
    {
      "epoch": 1.1159160504892076,
      "grad_norm": 0.4818030595779419,
      "learning_rate": 0.0002211021505376344,
      "loss": 7.3193,
      "step": 3736
    },
    {
      "epoch": 1.116214803196654,
      "grad_norm": 0.4457997977733612,
      "learning_rate": 0.0002210274790919952,
      "loss": 6.9883,
      "step": 3737
    },
    {
      "epoch": 1.1165135559041004,
      "grad_norm": 0.4232712388038635,
      "learning_rate": 0.00022095280764635602,
      "loss": 7.707,
      "step": 3738
    },
    {
      "epoch": 1.116812308611547,
      "grad_norm": 0.45303410291671753,
      "learning_rate": 0.00022087813620071684,
      "loss": 7.5,
      "step": 3739
    },
    {
      "epoch": 1.1171110613189932,
      "grad_norm": 0.39295944571495056,
      "learning_rate": 0.00022080346475507768,
      "loss": 7.6631,
      "step": 3740
    },
    {
      "epoch": 1.1174098140264397,
      "grad_norm": 0.43249619007110596,
      "learning_rate": 0.0002207287933094385,
      "loss": 7.6113,
      "step": 3741
    },
    {
      "epoch": 1.117708566733886,
      "grad_norm": 0.45323747396469116,
      "learning_rate": 0.0002206541218637993,
      "loss": 7.0732,
      "step": 3742
    },
    {
      "epoch": 1.1180073194413325,
      "grad_norm": 0.49883607029914856,
      "learning_rate": 0.00022057945041816012,
      "loss": 6.8008,
      "step": 3743
    },
    {
      "epoch": 1.1183060721487788,
      "grad_norm": 0.5029357671737671,
      "learning_rate": 0.0002205047789725209,
      "loss": 7.1162,
      "step": 3744
    },
    {
      "epoch": 1.1186048248562253,
      "grad_norm": 0.4335944652557373,
      "learning_rate": 0.00022043010752688171,
      "loss": 7.2861,
      "step": 3745
    },
    {
      "epoch": 1.1189035775636718,
      "grad_norm": 0.5125839710235596,
      "learning_rate": 0.00022035543608124253,
      "loss": 7.5586,
      "step": 3746
    },
    {
      "epoch": 1.119202330271118,
      "grad_norm": 0.46060511469841003,
      "learning_rate": 0.00022028076463560334,
      "loss": 7.459,
      "step": 3747
    },
    {
      "epoch": 1.1195010829785645,
      "grad_norm": 0.4027964174747467,
      "learning_rate": 0.00022020609318996415,
      "loss": 7.3857,
      "step": 3748
    },
    {
      "epoch": 1.1197998356860108,
      "grad_norm": 0.5604196786880493,
      "learning_rate": 0.00022013142174432497,
      "loss": 6.6338,
      "step": 3749
    },
    {
      "epoch": 1.1200985883934573,
      "grad_norm": 0.38111162185668945,
      "learning_rate": 0.0002200567502986858,
      "loss": 7.5527,
      "step": 3750
    },
    {
      "epoch": 1.1203973411009036,
      "grad_norm": 0.37159842252731323,
      "learning_rate": 0.00021998207885304662,
      "loss": 7.4902,
      "step": 3751
    },
    {
      "epoch": 1.1206960938083501,
      "grad_norm": 0.4078318774700165,
      "learning_rate": 0.0002199074074074074,
      "loss": 7.4443,
      "step": 3752
    },
    {
      "epoch": 1.1209948465157966,
      "grad_norm": 0.41761571168899536,
      "learning_rate": 0.00021983273596176822,
      "loss": 7.499,
      "step": 3753
    },
    {
      "epoch": 1.121293599223243,
      "grad_norm": 0.4266662895679474,
      "learning_rate": 0.00021975806451612903,
      "loss": 7.3252,
      "step": 3754
    },
    {
      "epoch": 1.1215923519306894,
      "grad_norm": 0.49849164485931396,
      "learning_rate": 0.00021968339307048984,
      "loss": 6.8457,
      "step": 3755
    },
    {
      "epoch": 1.1218911046381357,
      "grad_norm": 0.5034048557281494,
      "learning_rate": 0.00021960872162485066,
      "loss": 6.5479,
      "step": 3756
    },
    {
      "epoch": 1.1221898573455822,
      "grad_norm": 0.4473223388195038,
      "learning_rate": 0.00021953405017921147,
      "loss": 7.4854,
      "step": 3757
    },
    {
      "epoch": 1.1224886100530287,
      "grad_norm": 0.4731491208076477,
      "learning_rate": 0.00021945937873357228,
      "loss": 7.2637,
      "step": 3758
    },
    {
      "epoch": 1.122787362760475,
      "grad_norm": 0.4855106472969055,
      "learning_rate": 0.00021938470728793312,
      "loss": 7.2861,
      "step": 3759
    },
    {
      "epoch": 1.1230861154679215,
      "grad_norm": 0.4329335689544678,
      "learning_rate": 0.0002193100358422939,
      "loss": 7.6475,
      "step": 3760
    },
    {
      "epoch": 1.1233848681753678,
      "grad_norm": 0.4856734871864319,
      "learning_rate": 0.00021923536439665472,
      "loss": 6.9814,
      "step": 3761
    },
    {
      "epoch": 1.1236836208828143,
      "grad_norm": 0.43459904193878174,
      "learning_rate": 0.00021916069295101553,
      "loss": 7.3799,
      "step": 3762
    },
    {
      "epoch": 1.1239823735902608,
      "grad_norm": 0.4595241844654083,
      "learning_rate": 0.00021908602150537635,
      "loss": 7.0254,
      "step": 3763
    },
    {
      "epoch": 1.124281126297707,
      "grad_norm": 0.4872322976589203,
      "learning_rate": 0.00021901135005973716,
      "loss": 6.9209,
      "step": 3764
    },
    {
      "epoch": 1.1245798790051535,
      "grad_norm": 0.4879283010959625,
      "learning_rate": 0.00021893667861409797,
      "loss": 6.9785,
      "step": 3765
    },
    {
      "epoch": 1.1248786317125998,
      "grad_norm": 0.5169468522071838,
      "learning_rate": 0.00021886200716845879,
      "loss": 7.0488,
      "step": 3766
    },
    {
      "epoch": 1.1251773844200463,
      "grad_norm": 0.41647449135780334,
      "learning_rate": 0.0002187873357228196,
      "loss": 7.1436,
      "step": 3767
    },
    {
      "epoch": 1.1254761371274928,
      "grad_norm": 0.38806745409965515,
      "learning_rate": 0.0002187126642771804,
      "loss": 7.6729,
      "step": 3768
    },
    {
      "epoch": 1.1257748898349391,
      "grad_norm": 0.379250168800354,
      "learning_rate": 0.00021863799283154122,
      "loss": 7.5361,
      "step": 3769
    },
    {
      "epoch": 1.1260736425423856,
      "grad_norm": 0.4734434187412262,
      "learning_rate": 0.00021856332138590204,
      "loss": 7.5918,
      "step": 3770
    },
    {
      "epoch": 1.126372395249832,
      "grad_norm": 0.4362119734287262,
      "learning_rate": 0.00021848864994026285,
      "loss": 7.0752,
      "step": 3771
    },
    {
      "epoch": 1.1266711479572784,
      "grad_norm": 0.4703393876552582,
      "learning_rate": 0.00021841397849462366,
      "loss": 7.0654,
      "step": 3772
    },
    {
      "epoch": 1.1269699006647247,
      "grad_norm": 0.40223097801208496,
      "learning_rate": 0.00021833930704898448,
      "loss": 7.6709,
      "step": 3773
    },
    {
      "epoch": 1.1272686533721712,
      "grad_norm": 0.493314653635025,
      "learning_rate": 0.0002182646356033453,
      "loss": 6.7549,
      "step": 3774
    },
    {
      "epoch": 1.1275674060796175,
      "grad_norm": 0.48207199573516846,
      "learning_rate": 0.0002181899641577061,
      "loss": 7.4424,
      "step": 3775
    },
    {
      "epoch": 1.127866158787064,
      "grad_norm": 0.4377724826335907,
      "learning_rate": 0.0002181152927120669,
      "loss": 7.168,
      "step": 3776
    },
    {
      "epoch": 1.1281649114945105,
      "grad_norm": 0.40646207332611084,
      "learning_rate": 0.00021804062126642773,
      "loss": 8.0557,
      "step": 3777
    },
    {
      "epoch": 1.1284636642019568,
      "grad_norm": 0.46660804748535156,
      "learning_rate": 0.00021796594982078854,
      "loss": 7.2559,
      "step": 3778
    },
    {
      "epoch": 1.1287624169094033,
      "grad_norm": 0.47412705421447754,
      "learning_rate": 0.00021789127837514935,
      "loss": 6.9014,
      "step": 3779
    },
    {
      "epoch": 1.1290611696168495,
      "grad_norm": 0.4213813543319702,
      "learning_rate": 0.00021781660692951017,
      "loss": 7.1885,
      "step": 3780
    },
    {
      "epoch": 1.129359922324296,
      "grad_norm": 0.41999560594558716,
      "learning_rate": 0.00021774193548387098,
      "loss": 7.8076,
      "step": 3781
    },
    {
      "epoch": 1.1296586750317426,
      "grad_norm": 0.35680049657821655,
      "learning_rate": 0.0002176672640382318,
      "loss": 7.6104,
      "step": 3782
    },
    {
      "epoch": 1.1299574277391888,
      "grad_norm": 0.41665202379226685,
      "learning_rate": 0.0002175925925925926,
      "loss": 7.502,
      "step": 3783
    },
    {
      "epoch": 1.1302561804466353,
      "grad_norm": 0.4220694601535797,
      "learning_rate": 0.0002175179211469534,
      "loss": 7.5283,
      "step": 3784
    },
    {
      "epoch": 1.1305549331540816,
      "grad_norm": 0.4120127558708191,
      "learning_rate": 0.0002174432497013142,
      "loss": 7.6162,
      "step": 3785
    },
    {
      "epoch": 1.1308536858615281,
      "grad_norm": 0.5564220547676086,
      "learning_rate": 0.00021736857825567504,
      "loss": 6.9531,
      "step": 3786
    },
    {
      "epoch": 1.1311524385689746,
      "grad_norm": 0.3867650330066681,
      "learning_rate": 0.00021729390681003586,
      "loss": 7.6396,
      "step": 3787
    },
    {
      "epoch": 1.131451191276421,
      "grad_norm": 0.5144965648651123,
      "learning_rate": 0.00021721923536439667,
      "loss": 6.4131,
      "step": 3788
    },
    {
      "epoch": 1.1317499439838674,
      "grad_norm": 0.4645341634750366,
      "learning_rate": 0.00021714456391875748,
      "loss": 7.4336,
      "step": 3789
    },
    {
      "epoch": 1.1320486966913137,
      "grad_norm": 0.49677276611328125,
      "learning_rate": 0.0002170698924731183,
      "loss": 7.2295,
      "step": 3790
    },
    {
      "epoch": 1.1323474493987602,
      "grad_norm": 0.4107356071472168,
      "learning_rate": 0.0002169952210274791,
      "loss": 7.3369,
      "step": 3791
    },
    {
      "epoch": 1.1326462021062067,
      "grad_norm": 0.5105007290840149,
      "learning_rate": 0.0002169205495818399,
      "loss": 7.1689,
      "step": 3792
    },
    {
      "epoch": 1.132944954813653,
      "grad_norm": 0.43992555141448975,
      "learning_rate": 0.0002168458781362007,
      "loss": 7.2432,
      "step": 3793
    },
    {
      "epoch": 1.1332437075210995,
      "grad_norm": 0.4357776343822479,
      "learning_rate": 0.00021677120669056152,
      "loss": 7.6387,
      "step": 3794
    },
    {
      "epoch": 1.1335424602285458,
      "grad_norm": 0.4862111210823059,
      "learning_rate": 0.00021669653524492233,
      "loss": 7.4062,
      "step": 3795
    },
    {
      "epoch": 1.1338412129359923,
      "grad_norm": 0.5221295356750488,
      "learning_rate": 0.00021662186379928317,
      "loss": 7.0039,
      "step": 3796
    },
    {
      "epoch": 1.1341399656434386,
      "grad_norm": 0.4733259081840515,
      "learning_rate": 0.00021654719235364398,
      "loss": 7.0762,
      "step": 3797
    },
    {
      "epoch": 1.134438718350885,
      "grad_norm": 0.5168536901473999,
      "learning_rate": 0.0002164725209080048,
      "loss": 7.2354,
      "step": 3798
    },
    {
      "epoch": 1.1347374710583316,
      "grad_norm": 0.46356460452079773,
      "learning_rate": 0.0002163978494623656,
      "loss": 7.251,
      "step": 3799
    },
    {
      "epoch": 1.1350362237657778,
      "grad_norm": 0.5892835259437561,
      "learning_rate": 0.0002163231780167264,
      "loss": 6.582,
      "step": 3800
    },
    {
      "epoch": 1.1350362237657778,
      "eval_bleu": 0.13416533616135193,
      "eval_loss": 7.02734375,
      "eval_runtime": 535.9642,
      "eval_samples_per_second": 2.629,
      "eval_steps_per_second": 0.166,
      "step": 3800
    },
    {
      "epoch": 1.1353349764732243,
      "grad_norm": 0.6572684049606323,
      "learning_rate": 0.0002162485065710872,
      "loss": 5.957,
      "step": 3801
    },
    {
      "epoch": 1.1356337291806706,
      "grad_norm": 0.459113210439682,
      "learning_rate": 0.00021617383512544802,
      "loss": 7.0342,
      "step": 3802
    },
    {
      "epoch": 1.1359324818881171,
      "grad_norm": 0.3980069160461426,
      "learning_rate": 0.00021609916367980883,
      "loss": 7.3896,
      "step": 3803
    },
    {
      "epoch": 1.1362312345955634,
      "grad_norm": 0.48388010263442993,
      "learning_rate": 0.00021602449223416965,
      "loss": 6.9111,
      "step": 3804
    },
    {
      "epoch": 1.13652998730301,
      "grad_norm": 0.44851773977279663,
      "learning_rate": 0.0002159498207885305,
      "loss": 7.4033,
      "step": 3805
    },
    {
      "epoch": 1.1368287400104564,
      "grad_norm": 0.42445093393325806,
      "learning_rate": 0.0002158751493428913,
      "loss": 7.8984,
      "step": 3806
    },
    {
      "epoch": 1.1371274927179027,
      "grad_norm": 0.4869628846645355,
      "learning_rate": 0.0002158004778972521,
      "loss": 7.2764,
      "step": 3807
    },
    {
      "epoch": 1.1374262454253492,
      "grad_norm": 0.37282320857048035,
      "learning_rate": 0.0002157258064516129,
      "loss": 7.6074,
      "step": 3808
    },
    {
      "epoch": 1.1377249981327955,
      "grad_norm": 0.4982071816921234,
      "learning_rate": 0.0002156511350059737,
      "loss": 6.6611,
      "step": 3809
    },
    {
      "epoch": 1.138023750840242,
      "grad_norm": 0.4920947253704071,
      "learning_rate": 0.00021557646356033452,
      "loss": 7.3027,
      "step": 3810
    },
    {
      "epoch": 1.1383225035476885,
      "grad_norm": 0.4256998598575592,
      "learning_rate": 0.00021550179211469534,
      "loss": 7.3057,
      "step": 3811
    },
    {
      "epoch": 1.1386212562551348,
      "grad_norm": 0.46859702467918396,
      "learning_rate": 0.00021542712066905615,
      "loss": 7.0986,
      "step": 3812
    },
    {
      "epoch": 1.1389200089625813,
      "grad_norm": 0.5315213203430176,
      "learning_rate": 0.00021535244922341696,
      "loss": 6.7588,
      "step": 3813
    },
    {
      "epoch": 1.1392187616700276,
      "grad_norm": 0.5308994650840759,
      "learning_rate": 0.0002152777777777778,
      "loss": 6.877,
      "step": 3814
    },
    {
      "epoch": 1.139517514377474,
      "grad_norm": 0.40235471725463867,
      "learning_rate": 0.00021520310633213862,
      "loss": 7.1777,
      "step": 3815
    },
    {
      "epoch": 1.1398162670849206,
      "grad_norm": 0.47722432017326355,
      "learning_rate": 0.0002151284348864994,
      "loss": 6.8213,
      "step": 3816
    },
    {
      "epoch": 1.1401150197923668,
      "grad_norm": 0.48701536655426025,
      "learning_rate": 0.00021505376344086021,
      "loss": 6.8818,
      "step": 3817
    },
    {
      "epoch": 1.1404137724998133,
      "grad_norm": 0.39339855313301086,
      "learning_rate": 0.00021497909199522103,
      "loss": 7.749,
      "step": 3818
    },
    {
      "epoch": 1.1407125252072596,
      "grad_norm": 0.46285194158554077,
      "learning_rate": 0.00021490442054958184,
      "loss": 6.752,
      "step": 3819
    },
    {
      "epoch": 1.1410112779147061,
      "grad_norm": 0.4626557230949402,
      "learning_rate": 0.00021482974910394265,
      "loss": 6.5244,
      "step": 3820
    },
    {
      "epoch": 1.1413100306221526,
      "grad_norm": 0.4139101505279541,
      "learning_rate": 0.00021475507765830347,
      "loss": 7.084,
      "step": 3821
    },
    {
      "epoch": 1.141608783329599,
      "grad_norm": 0.44808393716812134,
      "learning_rate": 0.00021468040621266428,
      "loss": 7.6074,
      "step": 3822
    },
    {
      "epoch": 1.1419075360370454,
      "grad_norm": 0.4640653133392334,
      "learning_rate": 0.00021460573476702512,
      "loss": 7.0615,
      "step": 3823
    },
    {
      "epoch": 1.1422062887444917,
      "grad_norm": 0.4584619700908661,
      "learning_rate": 0.0002145310633213859,
      "loss": 7.1299,
      "step": 3824
    },
    {
      "epoch": 1.1425050414519382,
      "grad_norm": 0.5512561798095703,
      "learning_rate": 0.00021445639187574672,
      "loss": 6.1113,
      "step": 3825
    },
    {
      "epoch": 1.1428037941593845,
      "grad_norm": 0.4177223742008209,
      "learning_rate": 0.00021438172043010753,
      "loss": 7.6074,
      "step": 3826
    },
    {
      "epoch": 1.143102546866831,
      "grad_norm": 0.45018118619918823,
      "learning_rate": 0.00021430704898446834,
      "loss": 7.1777,
      "step": 3827
    },
    {
      "epoch": 1.1434012995742773,
      "grad_norm": 0.39913180470466614,
      "learning_rate": 0.00021423237753882916,
      "loss": 7.4268,
      "step": 3828
    },
    {
      "epoch": 1.1437000522817238,
      "grad_norm": 0.47896623611450195,
      "learning_rate": 0.00021415770609318997,
      "loss": 6.96,
      "step": 3829
    },
    {
      "epoch": 1.1439988049891703,
      "grad_norm": 0.4396567940711975,
      "learning_rate": 0.00021408303464755078,
      "loss": 7.999,
      "step": 3830
    },
    {
      "epoch": 1.1442975576966166,
      "grad_norm": 0.4919717609882355,
      "learning_rate": 0.0002140083632019116,
      "loss": 6.8662,
      "step": 3831
    },
    {
      "epoch": 1.144596310404063,
      "grad_norm": 0.5378244519233704,
      "learning_rate": 0.0002139336917562724,
      "loss": 6.4824,
      "step": 3832
    },
    {
      "epoch": 1.1448950631115093,
      "grad_norm": 0.4982544183731079,
      "learning_rate": 0.00021385902031063322,
      "loss": 7.1172,
      "step": 3833
    },
    {
      "epoch": 1.1451938158189559,
      "grad_norm": 0.44981706142425537,
      "learning_rate": 0.00021378434886499403,
      "loss": 7.1865,
      "step": 3834
    },
    {
      "epoch": 1.1454925685264024,
      "grad_norm": 0.5263251066207886,
      "learning_rate": 0.00021370967741935485,
      "loss": 6.7803,
      "step": 3835
    },
    {
      "epoch": 1.1457913212338486,
      "grad_norm": 0.4895780384540558,
      "learning_rate": 0.00021363500597371566,
      "loss": 6.9541,
      "step": 3836
    },
    {
      "epoch": 1.1460900739412951,
      "grad_norm": 0.5241305828094482,
      "learning_rate": 0.00021356033452807647,
      "loss": 6.8105,
      "step": 3837
    },
    {
      "epoch": 1.1463888266487414,
      "grad_norm": 0.5517986416816711,
      "learning_rate": 0.00021348566308243729,
      "loss": 6.2979,
      "step": 3838
    },
    {
      "epoch": 1.146687579356188,
      "grad_norm": 0.6349418759346008,
      "learning_rate": 0.0002134109916367981,
      "loss": 6.4531,
      "step": 3839
    },
    {
      "epoch": 1.1469863320636344,
      "grad_norm": 0.4214991331100464,
      "learning_rate": 0.00021333632019115888,
      "loss": 6.8965,
      "step": 3840
    },
    {
      "epoch": 1.1472850847710807,
      "grad_norm": 0.4849632978439331,
      "learning_rate": 0.0002132616487455197,
      "loss": 7.4951,
      "step": 3841
    },
    {
      "epoch": 1.1475838374785272,
      "grad_norm": 0.5219829082489014,
      "learning_rate": 0.00021318697729988054,
      "loss": 7.2783,
      "step": 3842
    },
    {
      "epoch": 1.1478825901859735,
      "grad_norm": 0.5206825137138367,
      "learning_rate": 0.00021311230585424135,
      "loss": 6.8164,
      "step": 3843
    },
    {
      "epoch": 1.14818134289342,
      "grad_norm": 0.5164372324943542,
      "learning_rate": 0.00021303763440860216,
      "loss": 7.2842,
      "step": 3844
    },
    {
      "epoch": 1.1484800956008665,
      "grad_norm": 0.5476871728897095,
      "learning_rate": 0.00021296296296296298,
      "loss": 6.9863,
      "step": 3845
    },
    {
      "epoch": 1.1487788483083128,
      "grad_norm": 0.537002444267273,
      "learning_rate": 0.0002128882915173238,
      "loss": 6.5908,
      "step": 3846
    },
    {
      "epoch": 1.1490776010157593,
      "grad_norm": 0.4895840585231781,
      "learning_rate": 0.0002128136200716846,
      "loss": 7.3809,
      "step": 3847
    },
    {
      "epoch": 1.1493763537232056,
      "grad_norm": 0.4258129894733429,
      "learning_rate": 0.0002127389486260454,
      "loss": 7.6943,
      "step": 3848
    },
    {
      "epoch": 1.149675106430652,
      "grad_norm": 0.4681243300437927,
      "learning_rate": 0.0002126642771804062,
      "loss": 7.0107,
      "step": 3849
    },
    {
      "epoch": 1.1499738591380984,
      "grad_norm": 0.4260539412498474,
      "learning_rate": 0.000212589605734767,
      "loss": 7.252,
      "step": 3850
    },
    {
      "epoch": 1.1502726118455449,
      "grad_norm": 0.47493776679039,
      "learning_rate": 0.00021251493428912785,
      "loss": 7.2783,
      "step": 3851
    },
    {
      "epoch": 1.1505713645529911,
      "grad_norm": 0.5160245895385742,
      "learning_rate": 0.00021244026284348867,
      "loss": 6.7051,
      "step": 3852
    },
    {
      "epoch": 1.1508701172604376,
      "grad_norm": 0.4461134374141693,
      "learning_rate": 0.00021236559139784948,
      "loss": 7.3018,
      "step": 3853
    },
    {
      "epoch": 1.1511688699678841,
      "grad_norm": 0.4643280506134033,
      "learning_rate": 0.0002122909199522103,
      "loss": 7.0293,
      "step": 3854
    },
    {
      "epoch": 1.1514676226753304,
      "grad_norm": 0.4346127212047577,
      "learning_rate": 0.0002122162485065711,
      "loss": 7.4521,
      "step": 3855
    },
    {
      "epoch": 1.151766375382777,
      "grad_norm": 0.4179359972476959,
      "learning_rate": 0.0002121415770609319,
      "loss": 7.5811,
      "step": 3856
    },
    {
      "epoch": 1.1520651280902232,
      "grad_norm": 0.38710692524909973,
      "learning_rate": 0.0002120669056152927,
      "loss": 7.417,
      "step": 3857
    },
    {
      "epoch": 1.1523638807976697,
      "grad_norm": 0.43030503392219543,
      "learning_rate": 0.00021199223416965352,
      "loss": 7.6631,
      "step": 3858
    },
    {
      "epoch": 1.1526626335051162,
      "grad_norm": 0.41322973370552063,
      "learning_rate": 0.00021191756272401433,
      "loss": 7.6816,
      "step": 3859
    },
    {
      "epoch": 1.1529613862125625,
      "grad_norm": 0.44038164615631104,
      "learning_rate": 0.00021184289127837517,
      "loss": 7.2803,
      "step": 3860
    },
    {
      "epoch": 1.153260138920009,
      "grad_norm": 0.5019447207450867,
      "learning_rate": 0.00021176821983273598,
      "loss": 6.917,
      "step": 3861
    },
    {
      "epoch": 1.1535588916274553,
      "grad_norm": 0.4523415267467499,
      "learning_rate": 0.0002116935483870968,
      "loss": 7.8145,
      "step": 3862
    },
    {
      "epoch": 1.1538576443349018,
      "grad_norm": 0.5138977766036987,
      "learning_rate": 0.0002116188769414576,
      "loss": 7.3789,
      "step": 3863
    },
    {
      "epoch": 1.1541563970423483,
      "grad_norm": 0.4145982563495636,
      "learning_rate": 0.0002115442054958184,
      "loss": 7.4941,
      "step": 3864
    },
    {
      "epoch": 1.1544551497497946,
      "grad_norm": 0.414593905210495,
      "learning_rate": 0.0002114695340501792,
      "loss": 7.4297,
      "step": 3865
    },
    {
      "epoch": 1.154753902457241,
      "grad_norm": 0.5029203295707703,
      "learning_rate": 0.00021139486260454002,
      "loss": 7.4639,
      "step": 3866
    },
    {
      "epoch": 1.1550526551646874,
      "grad_norm": 0.46211162209510803,
      "learning_rate": 0.00021132019115890083,
      "loss": 7.5195,
      "step": 3867
    },
    {
      "epoch": 1.1553514078721339,
      "grad_norm": 0.4419783353805542,
      "learning_rate": 0.00021124551971326164,
      "loss": 7.752,
      "step": 3868
    },
    {
      "epoch": 1.1556501605795804,
      "grad_norm": 0.6003404259681702,
      "learning_rate": 0.00021117084826762248,
      "loss": 6.3789,
      "step": 3869
    },
    {
      "epoch": 1.1559489132870266,
      "grad_norm": 0.48813971877098083,
      "learning_rate": 0.0002110961768219833,
      "loss": 7.0703,
      "step": 3870
    },
    {
      "epoch": 1.1562476659944732,
      "grad_norm": 0.47235700488090515,
      "learning_rate": 0.0002110215053763441,
      "loss": 6.7734,
      "step": 3871
    },
    {
      "epoch": 1.1565464187019194,
      "grad_norm": 0.4609719514846802,
      "learning_rate": 0.0002109468339307049,
      "loss": 7.251,
      "step": 3872
    },
    {
      "epoch": 1.156845171409366,
      "grad_norm": 0.5197207927703857,
      "learning_rate": 0.0002108721624850657,
      "loss": 6.9102,
      "step": 3873
    },
    {
      "epoch": 1.1571439241168122,
      "grad_norm": 0.4178347885608673,
      "learning_rate": 0.00021079749103942652,
      "loss": 7.2266,
      "step": 3874
    },
    {
      "epoch": 1.1574426768242587,
      "grad_norm": 0.5263208746910095,
      "learning_rate": 0.00021072281959378733,
      "loss": 7.0059,
      "step": 3875
    },
    {
      "epoch": 1.1577414295317052,
      "grad_norm": 0.4281965494155884,
      "learning_rate": 0.00021064814814814815,
      "loss": 7.4297,
      "step": 3876
    },
    {
      "epoch": 1.1580401822391515,
      "grad_norm": 0.3497859835624695,
      "learning_rate": 0.00021057347670250896,
      "loss": 7.7207,
      "step": 3877
    },
    {
      "epoch": 1.158338934946598,
      "grad_norm": 0.5354208946228027,
      "learning_rate": 0.0002104988052568698,
      "loss": 7.0752,
      "step": 3878
    },
    {
      "epoch": 1.1586376876540443,
      "grad_norm": 0.5001868009567261,
      "learning_rate": 0.0002104241338112306,
      "loss": 7.2871,
      "step": 3879
    },
    {
      "epoch": 1.1589364403614908,
      "grad_norm": 0.4328208267688751,
      "learning_rate": 0.0002103494623655914,
      "loss": 7.6064,
      "step": 3880
    },
    {
      "epoch": 1.159235193068937,
      "grad_norm": 0.38619670271873474,
      "learning_rate": 0.0002102747909199522,
      "loss": 7.3633,
      "step": 3881
    },
    {
      "epoch": 1.1595339457763836,
      "grad_norm": 0.47943416237831116,
      "learning_rate": 0.00021020011947431302,
      "loss": 6.7441,
      "step": 3882
    },
    {
      "epoch": 1.15983269848383,
      "grad_norm": 0.4590722620487213,
      "learning_rate": 0.00021012544802867384,
      "loss": 6.958,
      "step": 3883
    },
    {
      "epoch": 1.1601314511912764,
      "grad_norm": 0.47627905011177063,
      "learning_rate": 0.00021005077658303465,
      "loss": 7.0137,
      "step": 3884
    },
    {
      "epoch": 1.1604302038987229,
      "grad_norm": 0.49474048614501953,
      "learning_rate": 0.00020997610513739546,
      "loss": 6.9785,
      "step": 3885
    },
    {
      "epoch": 1.1607289566061691,
      "grad_norm": 0.5605520606040955,
      "learning_rate": 0.00020990143369175628,
      "loss": 7.0879,
      "step": 3886
    },
    {
      "epoch": 1.1610277093136157,
      "grad_norm": 0.5075913071632385,
      "learning_rate": 0.00020982676224611706,
      "loss": 6.6611,
      "step": 3887
    },
    {
      "epoch": 1.1613264620210622,
      "grad_norm": 0.46532052755355835,
      "learning_rate": 0.0002097520908004779,
      "loss": 7.1562,
      "step": 3888
    },
    {
      "epoch": 1.1616252147285084,
      "grad_norm": 0.5723769068717957,
      "learning_rate": 0.00020967741935483871,
      "loss": 6.9229,
      "step": 3889
    },
    {
      "epoch": 1.161923967435955,
      "grad_norm": 0.4196627736091614,
      "learning_rate": 0.00020960274790919953,
      "loss": 7.2256,
      "step": 3890
    },
    {
      "epoch": 1.1622227201434012,
      "grad_norm": 0.44987183809280396,
      "learning_rate": 0.00020952807646356034,
      "loss": 7.6592,
      "step": 3891
    },
    {
      "epoch": 1.1625214728508477,
      "grad_norm": 0.5024405121803284,
      "learning_rate": 0.00020945340501792115,
      "loss": 7.3428,
      "step": 3892
    },
    {
      "epoch": 1.1628202255582942,
      "grad_norm": 0.627509355545044,
      "learning_rate": 0.00020937873357228197,
      "loss": 6.6758,
      "step": 3893
    },
    {
      "epoch": 1.1631189782657405,
      "grad_norm": 0.5260562896728516,
      "learning_rate": 0.00020930406212664278,
      "loss": 7.4229,
      "step": 3894
    },
    {
      "epoch": 1.163417730973187,
      "grad_norm": 0.46394485235214233,
      "learning_rate": 0.00020922939068100357,
      "loss": 7.0674,
      "step": 3895
    },
    {
      "epoch": 1.1637164836806333,
      "grad_norm": 0.4030151665210724,
      "learning_rate": 0.00020915471923536438,
      "loss": 7.5684,
      "step": 3896
    },
    {
      "epoch": 1.1640152363880798,
      "grad_norm": 0.5773317813873291,
      "learning_rate": 0.00020908004778972522,
      "loss": 6.4453,
      "step": 3897
    },
    {
      "epoch": 1.1643139890955263,
      "grad_norm": 0.4262455105781555,
      "learning_rate": 0.00020900537634408603,
      "loss": 6.9658,
      "step": 3898
    },
    {
      "epoch": 1.1646127418029726,
      "grad_norm": 0.49675828218460083,
      "learning_rate": 0.00020893070489844684,
      "loss": 7.1992,
      "step": 3899
    },
    {
      "epoch": 1.164911494510419,
      "grad_norm": 0.6434192657470703,
      "learning_rate": 0.00020885603345280766,
      "loss": 6.417,
      "step": 3900
    },
    {
      "epoch": 1.1652102472178654,
      "grad_norm": 0.5290645360946655,
      "learning_rate": 0.00020878136200716847,
      "loss": 6.7676,
      "step": 3901
    },
    {
      "epoch": 1.1655089999253119,
      "grad_norm": 0.465471476316452,
      "learning_rate": 0.00020870669056152928,
      "loss": 7.0107,
      "step": 3902
    },
    {
      "epoch": 1.1658077526327582,
      "grad_norm": 0.4668038487434387,
      "learning_rate": 0.00020863201911589007,
      "loss": 7.1221,
      "step": 3903
    },
    {
      "epoch": 1.1661065053402047,
      "grad_norm": 0.509053647518158,
      "learning_rate": 0.00020855734767025088,
      "loss": 7.2383,
      "step": 3904
    },
    {
      "epoch": 1.166405258047651,
      "grad_norm": 0.4468303620815277,
      "learning_rate": 0.0002084826762246117,
      "loss": 7.4932,
      "step": 3905
    },
    {
      "epoch": 1.1667040107550974,
      "grad_norm": 0.422696977853775,
      "learning_rate": 0.00020840800477897253,
      "loss": 7.7373,
      "step": 3906
    },
    {
      "epoch": 1.167002763462544,
      "grad_norm": 0.48736298084259033,
      "learning_rate": 0.00020833333333333335,
      "loss": 6.9131,
      "step": 3907
    },
    {
      "epoch": 1.1673015161699902,
      "grad_norm": 0.4434458911418915,
      "learning_rate": 0.00020825866188769416,
      "loss": 7.126,
      "step": 3908
    },
    {
      "epoch": 1.1676002688774367,
      "grad_norm": 0.42584115266799927,
      "learning_rate": 0.00020818399044205497,
      "loss": 7.3525,
      "step": 3909
    },
    {
      "epoch": 1.167899021584883,
      "grad_norm": 0.3936540186405182,
      "learning_rate": 0.00020810931899641579,
      "loss": 7.8057,
      "step": 3910
    },
    {
      "epoch": 1.1681977742923295,
      "grad_norm": 0.4122653603553772,
      "learning_rate": 0.00020803464755077657,
      "loss": 7.5049,
      "step": 3911
    },
    {
      "epoch": 1.168496526999776,
      "grad_norm": 0.46994921565055847,
      "learning_rate": 0.00020795997610513738,
      "loss": 7.3408,
      "step": 3912
    },
    {
      "epoch": 1.1687952797072223,
      "grad_norm": 0.5000635981559753,
      "learning_rate": 0.0002078853046594982,
      "loss": 6.749,
      "step": 3913
    },
    {
      "epoch": 1.1690940324146688,
      "grad_norm": 0.5060635209083557,
      "learning_rate": 0.000207810633213859,
      "loss": 7.0361,
      "step": 3914
    },
    {
      "epoch": 1.169392785122115,
      "grad_norm": 0.6030532717704773,
      "learning_rate": 0.00020773596176821985,
      "loss": 6.7568,
      "step": 3915
    },
    {
      "epoch": 1.1696915378295616,
      "grad_norm": 0.4428997337818146,
      "learning_rate": 0.00020766129032258066,
      "loss": 7.0273,
      "step": 3916
    },
    {
      "epoch": 1.169990290537008,
      "grad_norm": 0.5437923669815063,
      "learning_rate": 0.00020758661887694148,
      "loss": 6.3281,
      "step": 3917
    },
    {
      "epoch": 1.1702890432444544,
      "grad_norm": 0.5219132304191589,
      "learning_rate": 0.0002075119474313023,
      "loss": 6.5469,
      "step": 3918
    },
    {
      "epoch": 1.1705877959519009,
      "grad_norm": 0.4338000416755676,
      "learning_rate": 0.00020743727598566307,
      "loss": 7.251,
      "step": 3919
    },
    {
      "epoch": 1.1708865486593472,
      "grad_norm": 0.4574502408504486,
      "learning_rate": 0.0002073626045400239,
      "loss": 7.207,
      "step": 3920
    },
    {
      "epoch": 1.1711853013667937,
      "grad_norm": 0.4607994854450226,
      "learning_rate": 0.0002072879330943847,
      "loss": 6.6992,
      "step": 3921
    },
    {
      "epoch": 1.1714840540742402,
      "grad_norm": 0.5326517820358276,
      "learning_rate": 0.0002072132616487455,
      "loss": 7.3594,
      "step": 3922
    },
    {
      "epoch": 1.1717828067816864,
      "grad_norm": 0.5702673196792603,
      "learning_rate": 0.00020713859020310633,
      "loss": 6.6152,
      "step": 3923
    },
    {
      "epoch": 1.172081559489133,
      "grad_norm": 0.5263898968696594,
      "learning_rate": 0.00020706391875746717,
      "loss": 7.1963,
      "step": 3924
    },
    {
      "epoch": 1.1723803121965792,
      "grad_norm": 0.4910295009613037,
      "learning_rate": 0.00020698924731182798,
      "loss": 7.0615,
      "step": 3925
    },
    {
      "epoch": 1.1726790649040257,
      "grad_norm": 0.41051024198532104,
      "learning_rate": 0.0002069145758661888,
      "loss": 7.293,
      "step": 3926
    },
    {
      "epoch": 1.172977817611472,
      "grad_norm": 0.4114055931568146,
      "learning_rate": 0.00020683990442054958,
      "loss": 7.3301,
      "step": 3927
    },
    {
      "epoch": 1.1732765703189185,
      "grad_norm": 0.4468679428100586,
      "learning_rate": 0.0002067652329749104,
      "loss": 7.3486,
      "step": 3928
    },
    {
      "epoch": 1.1735753230263648,
      "grad_norm": 0.4722606837749481,
      "learning_rate": 0.0002066905615292712,
      "loss": 6.916,
      "step": 3929
    },
    {
      "epoch": 1.1738740757338113,
      "grad_norm": 0.47916412353515625,
      "learning_rate": 0.00020661589008363202,
      "loss": 7.4287,
      "step": 3930
    },
    {
      "epoch": 1.1741728284412578,
      "grad_norm": 0.4832097589969635,
      "learning_rate": 0.00020654121863799283,
      "loss": 6.8174,
      "step": 3931
    },
    {
      "epoch": 1.174471581148704,
      "grad_norm": 0.44557997584342957,
      "learning_rate": 0.00020646654719235364,
      "loss": 7.1338,
      "step": 3932
    },
    {
      "epoch": 1.1747703338561506,
      "grad_norm": 0.4800894260406494,
      "learning_rate": 0.00020639187574671445,
      "loss": 6.9521,
      "step": 3933
    },
    {
      "epoch": 1.1750690865635969,
      "grad_norm": 0.42209383845329285,
      "learning_rate": 0.0002063172043010753,
      "loss": 7.5449,
      "step": 3934
    },
    {
      "epoch": 1.1753678392710434,
      "grad_norm": 0.5344094038009644,
      "learning_rate": 0.00020624253285543608,
      "loss": 7.0791,
      "step": 3935
    },
    {
      "epoch": 1.1756665919784899,
      "grad_norm": 0.48158615827560425,
      "learning_rate": 0.0002061678614097969,
      "loss": 6.8896,
      "step": 3936
    },
    {
      "epoch": 1.1759653446859362,
      "grad_norm": 0.39911001920700073,
      "learning_rate": 0.0002060931899641577,
      "loss": 7.4043,
      "step": 3937
    },
    {
      "epoch": 1.1762640973933827,
      "grad_norm": 0.5053055286407471,
      "learning_rate": 0.00020601851851851852,
      "loss": 7.0674,
      "step": 3938
    },
    {
      "epoch": 1.176562850100829,
      "grad_norm": 0.45491325855255127,
      "learning_rate": 0.00020594384707287933,
      "loss": 7.3809,
      "step": 3939
    },
    {
      "epoch": 1.1768616028082755,
      "grad_norm": 0.3996627628803253,
      "learning_rate": 0.00020586917562724014,
      "loss": 7.665,
      "step": 3940
    },
    {
      "epoch": 1.177160355515722,
      "grad_norm": 0.43917620182037354,
      "learning_rate": 0.00020579450418160096,
      "loss": 7.3604,
      "step": 3941
    },
    {
      "epoch": 1.1774591082231682,
      "grad_norm": 0.4590000510215759,
      "learning_rate": 0.00020571983273596177,
      "loss": 7.1426,
      "step": 3942
    },
    {
      "epoch": 1.1777578609306147,
      "grad_norm": 0.4624743163585663,
      "learning_rate": 0.00020564516129032258,
      "loss": 7.2627,
      "step": 3943
    },
    {
      "epoch": 1.178056613638061,
      "grad_norm": 0.479784220457077,
      "learning_rate": 0.0002055704898446834,
      "loss": 7.5342,
      "step": 3944
    },
    {
      "epoch": 1.1783553663455075,
      "grad_norm": 0.43356645107269287,
      "learning_rate": 0.0002054958183990442,
      "loss": 7.5391,
      "step": 3945
    },
    {
      "epoch": 1.178654119052954,
      "grad_norm": 0.4166199862957001,
      "learning_rate": 0.00020542114695340502,
      "loss": 7.2451,
      "step": 3946
    },
    {
      "epoch": 1.1789528717604003,
      "grad_norm": 0.4601888656616211,
      "learning_rate": 0.00020534647550776583,
      "loss": 7.3418,
      "step": 3947
    },
    {
      "epoch": 1.1792516244678468,
      "grad_norm": 0.3942689001560211,
      "learning_rate": 0.00020527180406212665,
      "loss": 7.4492,
      "step": 3948
    },
    {
      "epoch": 1.179550377175293,
      "grad_norm": 0.3904118835926056,
      "learning_rate": 0.00020519713261648746,
      "loss": 7.5195,
      "step": 3949
    },
    {
      "epoch": 1.1798491298827396,
      "grad_norm": 0.4202747941017151,
      "learning_rate": 0.00020512246117084827,
      "loss": 7.7178,
      "step": 3950
    },
    {
      "epoch": 1.1801478825901859,
      "grad_norm": 0.5284658670425415,
      "learning_rate": 0.00020504778972520906,
      "loss": 6.6797,
      "step": 3951
    },
    {
      "epoch": 1.1804466352976324,
      "grad_norm": 0.4661327004432678,
      "learning_rate": 0.0002049731182795699,
      "loss": 6.8818,
      "step": 3952
    },
    {
      "epoch": 1.1807453880050789,
      "grad_norm": 0.3978860080242157,
      "learning_rate": 0.0002048984468339307,
      "loss": 7.5615,
      "step": 3953
    },
    {
      "epoch": 1.1810441407125252,
      "grad_norm": 0.4598178565502167,
      "learning_rate": 0.00020482377538829152,
      "loss": 7.1152,
      "step": 3954
    },
    {
      "epoch": 1.1813428934199717,
      "grad_norm": 0.4784679114818573,
      "learning_rate": 0.00020474910394265234,
      "loss": 7.3359,
      "step": 3955
    },
    {
      "epoch": 1.181641646127418,
      "grad_norm": 0.46622008085250854,
      "learning_rate": 0.00020467443249701315,
      "loss": 6.8848,
      "step": 3956
    },
    {
      "epoch": 1.1819403988348645,
      "grad_norm": 0.3977414071559906,
      "learning_rate": 0.00020459976105137396,
      "loss": 7.5605,
      "step": 3957
    },
    {
      "epoch": 1.1822391515423107,
      "grad_norm": 0.4190252125263214,
      "learning_rate": 0.00020452508960573478,
      "loss": 7.6758,
      "step": 3958
    },
    {
      "epoch": 1.1825379042497572,
      "grad_norm": 0.39781296253204346,
      "learning_rate": 0.00020445041816009556,
      "loss": 7.1797,
      "step": 3959
    },
    {
      "epoch": 1.1828366569572037,
      "grad_norm": 0.4679655432701111,
      "learning_rate": 0.00020437574671445637,
      "loss": 6.9492,
      "step": 3960
    },
    {
      "epoch": 1.18313540966465,
      "grad_norm": 0.4885140061378479,
      "learning_rate": 0.00020430107526881721,
      "loss": 7.2979,
      "step": 3961
    },
    {
      "epoch": 1.1834341623720965,
      "grad_norm": 0.4306926429271698,
      "learning_rate": 0.00020422640382317803,
      "loss": 7.5029,
      "step": 3962
    },
    {
      "epoch": 1.1837329150795428,
      "grad_norm": 0.4235607981681824,
      "learning_rate": 0.00020415173237753884,
      "loss": 7.8066,
      "step": 3963
    },
    {
      "epoch": 1.1840316677869893,
      "grad_norm": 0.37504842877388,
      "learning_rate": 0.00020407706093189965,
      "loss": 7.4082,
      "step": 3964
    },
    {
      "epoch": 1.1843304204944358,
      "grad_norm": 0.4947068691253662,
      "learning_rate": 0.00020400238948626047,
      "loss": 6.9639,
      "step": 3965
    },
    {
      "epoch": 1.184629173201882,
      "grad_norm": 0.4175197184085846,
      "learning_rate": 0.00020392771804062128,
      "loss": 7.25,
      "step": 3966
    },
    {
      "epoch": 1.1849279259093286,
      "grad_norm": 0.4750911593437195,
      "learning_rate": 0.00020385304659498207,
      "loss": 7.291,
      "step": 3967
    },
    {
      "epoch": 1.1852266786167749,
      "grad_norm": 0.46188223361968994,
      "learning_rate": 0.00020377837514934288,
      "loss": 7.5488,
      "step": 3968
    },
    {
      "epoch": 1.1855254313242214,
      "grad_norm": 0.4930635392665863,
      "learning_rate": 0.0002037037037037037,
      "loss": 6.668,
      "step": 3969
    },
    {
      "epoch": 1.185824184031668,
      "grad_norm": 0.4463103115558624,
      "learning_rate": 0.00020362903225806453,
      "loss": 7.4844,
      "step": 3970
    },
    {
      "epoch": 1.1861229367391142,
      "grad_norm": 0.4829006493091583,
      "learning_rate": 0.00020355436081242534,
      "loss": 6.8955,
      "step": 3971
    },
    {
      "epoch": 1.1864216894465607,
      "grad_norm": 0.451051265001297,
      "learning_rate": 0.00020347968936678616,
      "loss": 6.5762,
      "step": 3972
    },
    {
      "epoch": 1.186720442154007,
      "grad_norm": 0.5362138152122498,
      "learning_rate": 0.00020340501792114697,
      "loss": 6.6162,
      "step": 3973
    },
    {
      "epoch": 1.1870191948614535,
      "grad_norm": 0.4667745530605316,
      "learning_rate": 0.00020333034647550778,
      "loss": 6.9912,
      "step": 3974
    },
    {
      "epoch": 1.1873179475689,
      "grad_norm": 0.4700617492198944,
      "learning_rate": 0.00020325567502986857,
      "loss": 7.4062,
      "step": 3975
    },
    {
      "epoch": 1.1876167002763462,
      "grad_norm": 0.49597471952438354,
      "learning_rate": 0.00020318100358422938,
      "loss": 7.1133,
      "step": 3976
    },
    {
      "epoch": 1.1879154529837928,
      "grad_norm": 0.41027143597602844,
      "learning_rate": 0.0002031063321385902,
      "loss": 7.5205,
      "step": 3977
    },
    {
      "epoch": 1.188214205691239,
      "grad_norm": 0.5593256950378418,
      "learning_rate": 0.000203031660692951,
      "loss": 6.4746,
      "step": 3978
    },
    {
      "epoch": 1.1885129583986855,
      "grad_norm": 0.5497032403945923,
      "learning_rate": 0.00020295698924731182,
      "loss": 6.2539,
      "step": 3979
    },
    {
      "epoch": 1.1888117111061318,
      "grad_norm": 0.5000796318054199,
      "learning_rate": 0.00020288231780167266,
      "loss": 6.7627,
      "step": 3980
    },
    {
      "epoch": 1.1891104638135783,
      "grad_norm": 0.5022057890892029,
      "learning_rate": 0.00020280764635603347,
      "loss": 7.1797,
      "step": 3981
    },
    {
      "epoch": 1.1894092165210246,
      "grad_norm": 0.477823942899704,
      "learning_rate": 0.00020273297491039429,
      "loss": 6.9873,
      "step": 3982
    },
    {
      "epoch": 1.189707969228471,
      "grad_norm": 0.4350290596485138,
      "learning_rate": 0.00020265830346475507,
      "loss": 7.4541,
      "step": 3983
    },
    {
      "epoch": 1.1900067219359176,
      "grad_norm": 0.6334542036056519,
      "learning_rate": 0.00020258363201911588,
      "loss": 6.9824,
      "step": 3984
    },
    {
      "epoch": 1.190305474643364,
      "grad_norm": 0.4025689661502838,
      "learning_rate": 0.0002025089605734767,
      "loss": 7.4658,
      "step": 3985
    },
    {
      "epoch": 1.1906042273508104,
      "grad_norm": 0.42691293358802795,
      "learning_rate": 0.0002024342891278375,
      "loss": 7.3223,
      "step": 3986
    },
    {
      "epoch": 1.1909029800582567,
      "grad_norm": 0.5171722769737244,
      "learning_rate": 0.00020235961768219832,
      "loss": 7.3848,
      "step": 3987
    },
    {
      "epoch": 1.1912017327657032,
      "grad_norm": 0.42452722787857056,
      "learning_rate": 0.00020228494623655914,
      "loss": 7.8027,
      "step": 3988
    },
    {
      "epoch": 1.1915004854731497,
      "grad_norm": 0.5995463132858276,
      "learning_rate": 0.00020221027479091998,
      "loss": 7.207,
      "step": 3989
    },
    {
      "epoch": 1.191799238180596,
      "grad_norm": 0.41224727034568787,
      "learning_rate": 0.0002021356033452808,
      "loss": 7.502,
      "step": 3990
    },
    {
      "epoch": 1.1920979908880425,
      "grad_norm": 0.5026121139526367,
      "learning_rate": 0.00020206093189964157,
      "loss": 6.9834,
      "step": 3991
    },
    {
      "epoch": 1.1923967435954888,
      "grad_norm": 0.4354913830757141,
      "learning_rate": 0.0002019862604540024,
      "loss": 7.1299,
      "step": 3992
    },
    {
      "epoch": 1.1926954963029353,
      "grad_norm": 0.45462366938591003,
      "learning_rate": 0.0002019115890083632,
      "loss": 7.3047,
      "step": 3993
    },
    {
      "epoch": 1.1929942490103818,
      "grad_norm": 0.5587489008903503,
      "learning_rate": 0.000201836917562724,
      "loss": 7.4775,
      "step": 3994
    },
    {
      "epoch": 1.193293001717828,
      "grad_norm": 0.4506802558898926,
      "learning_rate": 0.00020176224611708483,
      "loss": 7.2129,
      "step": 3995
    },
    {
      "epoch": 1.1935917544252745,
      "grad_norm": 0.4385800361633301,
      "learning_rate": 0.00020168757467144564,
      "loss": 7.6816,
      "step": 3996
    },
    {
      "epoch": 1.1938905071327208,
      "grad_norm": 0.500421404838562,
      "learning_rate": 0.00020161290322580645,
      "loss": 7.3486,
      "step": 3997
    },
    {
      "epoch": 1.1941892598401673,
      "grad_norm": 0.434900164604187,
      "learning_rate": 0.0002015382317801673,
      "loss": 7.3486,
      "step": 3998
    },
    {
      "epoch": 1.1944880125476138,
      "grad_norm": 0.4302893280982971,
      "learning_rate": 0.00020146356033452808,
      "loss": 7.4414,
      "step": 3999
    },
    {
      "epoch": 1.1947867652550601,
      "grad_norm": 0.4882693290710449,
      "learning_rate": 0.0002013888888888889,
      "loss": 7.3506,
      "step": 4000
    },
    {
      "epoch": 1.1947867652550601,
      "eval_bleu": 0.13730463459019412,
      "eval_loss": 7.03515625,
      "eval_runtime": 450.034,
      "eval_samples_per_second": 3.131,
      "eval_steps_per_second": 0.198,
      "step": 4000
    }
  ],
  "logging_steps": 1,
  "max_steps": 6696,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 200,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 8695506879381504.0,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
