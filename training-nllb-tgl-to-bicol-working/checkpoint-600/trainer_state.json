{
  "best_global_step": 400,
  "best_metric": 7.07421875,
  "best_model_checkpoint": "training-nllb-tgl-to-bicol-working\\checkpoint-400",
  "epoch": 0.17925162446784673,
  "eval_steps": 200,
  "global_step": 600,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.00029875270744641124,
      "grad_norm": 0.10821348428726196,
      "learning_rate": 0.0005,
      "loss": 10.5107,
      "step": 1
    },
    {
      "epoch": 0.0005975054148928225,
      "grad_norm": 0.11005502939224243,
      "learning_rate": 0.0004999253285543608,
      "loss": 11.3281,
      "step": 2
    },
    {
      "epoch": 0.0008962581223392337,
      "grad_norm": 0.17715798318386078,
      "learning_rate": 0.0004998506571087216,
      "loss": 10.7266,
      "step": 3
    },
    {
      "epoch": 0.001195010829785645,
      "grad_norm": 0.18851852416992188,
      "learning_rate": 0.0004997759856630824,
      "loss": 10.7266,
      "step": 4
    },
    {
      "epoch": 0.001493763537232056,
      "grad_norm": 0.27457132935523987,
      "learning_rate": 0.0004997013142174433,
      "loss": 11.1172,
      "step": 5
    },
    {
      "epoch": 0.0017925162446784674,
      "grad_norm": 0.32356637716293335,
      "learning_rate": 0.0004996266427718041,
      "loss": 11.127,
      "step": 6
    },
    {
      "epoch": 0.002091268952124879,
      "grad_norm": 0.38698747754096985,
      "learning_rate": 0.0004995519713261649,
      "loss": 11.0762,
      "step": 7
    },
    {
      "epoch": 0.00239002165957129,
      "grad_norm": 0.4688027501106262,
      "learning_rate": 0.0004994772998805257,
      "loss": 11.0762,
      "step": 8
    },
    {
      "epoch": 0.002688774367017701,
      "grad_norm": 0.5016260147094727,
      "learning_rate": 0.0004994026284348865,
      "loss": 10.5703,
      "step": 9
    },
    {
      "epoch": 0.002987527074464112,
      "grad_norm": 0.6383640170097351,
      "learning_rate": 0.0004993279569892473,
      "loss": 10.9414,
      "step": 10
    },
    {
      "epoch": 0.0032862797819105238,
      "grad_norm": 0.6936139464378357,
      "learning_rate": 0.0004992532855436081,
      "loss": 11.3438,
      "step": 11
    },
    {
      "epoch": 0.003585032489356935,
      "grad_norm": 0.6707635521888733,
      "learning_rate": 0.000499178614097969,
      "loss": 10.9121,
      "step": 12
    },
    {
      "epoch": 0.003883785196803346,
      "grad_norm": 0.6898233890533447,
      "learning_rate": 0.0004991039426523298,
      "loss": 10.5566,
      "step": 13
    },
    {
      "epoch": 0.004182537904249758,
      "grad_norm": 0.6574143767356873,
      "learning_rate": 0.0004990292712066906,
      "loss": 11.2637,
      "step": 14
    },
    {
      "epoch": 0.004481290611696168,
      "grad_norm": 0.5881863832473755,
      "learning_rate": 0.0004989545997610514,
      "loss": 10.5469,
      "step": 15
    },
    {
      "epoch": 0.00478004331914258,
      "grad_norm": 0.5066705942153931,
      "learning_rate": 0.0004988799283154122,
      "loss": 10.5879,
      "step": 16
    },
    {
      "epoch": 0.0050787960265889906,
      "grad_norm": 0.5587491989135742,
      "learning_rate": 0.000498805256869773,
      "loss": 11.0312,
      "step": 17
    },
    {
      "epoch": 0.005377548734035402,
      "grad_norm": 0.4914754033088684,
      "learning_rate": 0.0004987305854241338,
      "loss": 10.9629,
      "step": 18
    },
    {
      "epoch": 0.005676301441481814,
      "grad_norm": 0.47820284962654114,
      "learning_rate": 0.0004986559139784946,
      "loss": 10.7637,
      "step": 19
    },
    {
      "epoch": 0.005975054148928224,
      "grad_norm": 0.6641180515289307,
      "learning_rate": 0.0004985812425328555,
      "loss": 11.5117,
      "step": 20
    },
    {
      "epoch": 0.006273806856374636,
      "grad_norm": 0.4952411949634552,
      "learning_rate": 0.0004985065710872163,
      "loss": 10.7676,
      "step": 21
    },
    {
      "epoch": 0.0065725595638210475,
      "grad_norm": 0.436328262090683,
      "learning_rate": 0.0004984318996415771,
      "loss": 9.9941,
      "step": 22
    },
    {
      "epoch": 0.006871312271267458,
      "grad_norm": 0.5016461610794067,
      "learning_rate": 0.0004983572281959379,
      "loss": 10.2441,
      "step": 23
    },
    {
      "epoch": 0.00717006497871387,
      "grad_norm": 0.48784565925598145,
      "learning_rate": 0.0004982825567502987,
      "loss": 10.7227,
      "step": 24
    },
    {
      "epoch": 0.0074688176861602805,
      "grad_norm": 0.46044033765792847,
      "learning_rate": 0.0004982078853046595,
      "loss": 9.875,
      "step": 25
    },
    {
      "epoch": 0.007767570393606692,
      "grad_norm": 0.4846183657646179,
      "learning_rate": 0.0004981332138590203,
      "loss": 10.5723,
      "step": 26
    },
    {
      "epoch": 0.008066323101053104,
      "grad_norm": 0.45062729716300964,
      "learning_rate": 0.0004980585424133811,
      "loss": 9.9922,
      "step": 27
    },
    {
      "epoch": 0.008365075808499515,
      "grad_norm": 0.5014210343360901,
      "learning_rate": 0.000497983870967742,
      "loss": 10.6836,
      "step": 28
    },
    {
      "epoch": 0.008663828515945925,
      "grad_norm": 0.5104053616523743,
      "learning_rate": 0.0004979091995221028,
      "loss": 10.3477,
      "step": 29
    },
    {
      "epoch": 0.008962581223392337,
      "grad_norm": 0.4748744070529938,
      "learning_rate": 0.0004978345280764636,
      "loss": 9.6602,
      "step": 30
    },
    {
      "epoch": 0.009261333930838748,
      "grad_norm": 0.49339559674263,
      "learning_rate": 0.0004977598566308244,
      "loss": 10.7129,
      "step": 31
    },
    {
      "epoch": 0.00956008663828516,
      "grad_norm": 0.39628931879997253,
      "learning_rate": 0.0004976851851851852,
      "loss": 9.627,
      "step": 32
    },
    {
      "epoch": 0.009858839345731571,
      "grad_norm": 0.42897528409957886,
      "learning_rate": 0.000497610513739546,
      "loss": 10.4668,
      "step": 33
    },
    {
      "epoch": 0.010157592053177981,
      "grad_norm": 0.4145178198814392,
      "learning_rate": 0.0004975358422939068,
      "loss": 10.3262,
      "step": 34
    },
    {
      "epoch": 0.010456344760624393,
      "grad_norm": 0.4588963985443115,
      "learning_rate": 0.0004974611708482676,
      "loss": 11.2031,
      "step": 35
    },
    {
      "epoch": 0.010755097468070804,
      "grad_norm": 0.4078393578529358,
      "learning_rate": 0.0004973864994026285,
      "loss": 9.9121,
      "step": 36
    },
    {
      "epoch": 0.011053850175517216,
      "grad_norm": 0.36339741945266724,
      "learning_rate": 0.0004973118279569893,
      "loss": 9.2246,
      "step": 37
    },
    {
      "epoch": 0.011352602882963627,
      "grad_norm": 0.40116608142852783,
      "learning_rate": 0.0004972371565113501,
      "loss": 9.7773,
      "step": 38
    },
    {
      "epoch": 0.011651355590410037,
      "grad_norm": 0.379043847322464,
      "learning_rate": 0.0004971624850657109,
      "loss": 9.5439,
      "step": 39
    },
    {
      "epoch": 0.011950108297856449,
      "grad_norm": 0.38495877385139465,
      "learning_rate": 0.0004970878136200717,
      "loss": 9.5215,
      "step": 40
    },
    {
      "epoch": 0.01224886100530286,
      "grad_norm": 0.41120296716690063,
      "learning_rate": 0.0004970131421744325,
      "loss": 10.4277,
      "step": 41
    },
    {
      "epoch": 0.012547613712749272,
      "grad_norm": 0.40164628624916077,
      "learning_rate": 0.0004969384707287933,
      "loss": 10.0625,
      "step": 42
    },
    {
      "epoch": 0.012846366420195683,
      "grad_norm": 0.4104005694389343,
      "learning_rate": 0.0004968637992831542,
      "loss": 9.9902,
      "step": 43
    },
    {
      "epoch": 0.013145119127642095,
      "grad_norm": 0.4140368103981018,
      "learning_rate": 0.000496789127837515,
      "loss": 9.8848,
      "step": 44
    },
    {
      "epoch": 0.013443871835088505,
      "grad_norm": 0.3986252248287201,
      "learning_rate": 0.0004967144563918758,
      "loss": 9.3008,
      "step": 45
    },
    {
      "epoch": 0.013742624542534916,
      "grad_norm": 0.40926143527030945,
      "learning_rate": 0.0004966397849462366,
      "loss": 9.6934,
      "step": 46
    },
    {
      "epoch": 0.014041377249981328,
      "grad_norm": 0.39645010232925415,
      "learning_rate": 0.0004965651135005974,
      "loss": 8.3613,
      "step": 47
    },
    {
      "epoch": 0.01434012995742774,
      "grad_norm": 0.4267260730266571,
      "learning_rate": 0.0004964904420549582,
      "loss": 9.5488,
      "step": 48
    },
    {
      "epoch": 0.014638882664874151,
      "grad_norm": 0.3833109736442566,
      "learning_rate": 0.000496415770609319,
      "loss": 8.9131,
      "step": 49
    },
    {
      "epoch": 0.014937635372320561,
      "grad_norm": 0.38322240114212036,
      "learning_rate": 0.0004963410991636798,
      "loss": 9.1816,
      "step": 50
    },
    {
      "epoch": 0.015236388079766973,
      "grad_norm": 0.41379764676094055,
      "learning_rate": 0.0004962664277180407,
      "loss": 9.498,
      "step": 51
    },
    {
      "epoch": 0.015535140787213384,
      "grad_norm": 0.36599066853523254,
      "learning_rate": 0.0004961917562724015,
      "loss": 8.71,
      "step": 52
    },
    {
      "epoch": 0.015833893494659794,
      "grad_norm": 0.3643648028373718,
      "learning_rate": 0.0004961170848267623,
      "loss": 8.5176,
      "step": 53
    },
    {
      "epoch": 0.016132646202106207,
      "grad_norm": 0.361419677734375,
      "learning_rate": 0.0004960424133811231,
      "loss": 8.8027,
      "step": 54
    },
    {
      "epoch": 0.016431398909552617,
      "grad_norm": 0.3777402341365814,
      "learning_rate": 0.0004959677419354839,
      "loss": 8.957,
      "step": 55
    },
    {
      "epoch": 0.01673015161699903,
      "grad_norm": 0.3822852373123169,
      "learning_rate": 0.0004958930704898447,
      "loss": 8.3691,
      "step": 56
    },
    {
      "epoch": 0.01702890432444544,
      "grad_norm": 0.3792678415775299,
      "learning_rate": 0.0004958183990442055,
      "loss": 9.1738,
      "step": 57
    },
    {
      "epoch": 0.01732765703189185,
      "grad_norm": 0.37306538224220276,
      "learning_rate": 0.0004957437275985663,
      "loss": 8.4873,
      "step": 58
    },
    {
      "epoch": 0.017626409739338263,
      "grad_norm": 0.38344499468803406,
      "learning_rate": 0.0004956690561529272,
      "loss": 9.1914,
      "step": 59
    },
    {
      "epoch": 0.017925162446784673,
      "grad_norm": 0.38386160135269165,
      "learning_rate": 0.000495594384707288,
      "loss": 8.8926,
      "step": 60
    },
    {
      "epoch": 0.018223915154231086,
      "grad_norm": 0.348595529794693,
      "learning_rate": 0.0004955197132616488,
      "loss": 7.8936,
      "step": 61
    },
    {
      "epoch": 0.018522667861677496,
      "grad_norm": 0.403987854719162,
      "learning_rate": 0.0004954450418160096,
      "loss": 8.7227,
      "step": 62
    },
    {
      "epoch": 0.018821420569123906,
      "grad_norm": 0.3449745774269104,
      "learning_rate": 0.0004953703703703704,
      "loss": 8.5898,
      "step": 63
    },
    {
      "epoch": 0.01912017327657032,
      "grad_norm": 0.35804587602615356,
      "learning_rate": 0.0004952956989247312,
      "loss": 8.5215,
      "step": 64
    },
    {
      "epoch": 0.01941892598401673,
      "grad_norm": 0.34505829215049744,
      "learning_rate": 0.0004952210274790919,
      "loss": 8.1533,
      "step": 65
    },
    {
      "epoch": 0.019717678691463143,
      "grad_norm": 0.34219563007354736,
      "learning_rate": 0.0004951463560334528,
      "loss": 7.9258,
      "step": 66
    },
    {
      "epoch": 0.020016431398909552,
      "grad_norm": 0.33780527114868164,
      "learning_rate": 0.0004950716845878137,
      "loss": 8.4648,
      "step": 67
    },
    {
      "epoch": 0.020315184106355962,
      "grad_norm": 0.3311116397380829,
      "learning_rate": 0.0004949970131421745,
      "loss": 8.3604,
      "step": 68
    },
    {
      "epoch": 0.020613936813802376,
      "grad_norm": 0.3329460918903351,
      "learning_rate": 0.0004949223416965353,
      "loss": 8.0234,
      "step": 69
    },
    {
      "epoch": 0.020912689521248785,
      "grad_norm": 0.3190329968929291,
      "learning_rate": 0.0004948476702508961,
      "loss": 8.4531,
      "step": 70
    },
    {
      "epoch": 0.0212114422286952,
      "grad_norm": 0.33799904584884644,
      "learning_rate": 0.0004947729988052569,
      "loss": 8.2656,
      "step": 71
    },
    {
      "epoch": 0.02151019493614161,
      "grad_norm": 0.31499767303466797,
      "learning_rate": 0.0004946983273596177,
      "loss": 8.4141,
      "step": 72
    },
    {
      "epoch": 0.02180894764358802,
      "grad_norm": 0.31318432092666626,
      "learning_rate": 0.0004946236559139785,
      "loss": 8.0254,
      "step": 73
    },
    {
      "epoch": 0.02210770035103443,
      "grad_norm": 0.3035445213317871,
      "learning_rate": 0.0004945489844683392,
      "loss": 7.7998,
      "step": 74
    },
    {
      "epoch": 0.02240645305848084,
      "grad_norm": 0.3642057776451111,
      "learning_rate": 0.0004944743130227002,
      "loss": 7.5146,
      "step": 75
    },
    {
      "epoch": 0.022705205765927255,
      "grad_norm": 0.39204663038253784,
      "learning_rate": 0.000494399641577061,
      "loss": 7.9434,
      "step": 76
    },
    {
      "epoch": 0.023003958473373665,
      "grad_norm": 0.3150036334991455,
      "learning_rate": 0.0004943249701314218,
      "loss": 7.7666,
      "step": 77
    },
    {
      "epoch": 0.023302711180820074,
      "grad_norm": 0.3041803240776062,
      "learning_rate": 0.0004942502986857826,
      "loss": 8.1553,
      "step": 78
    },
    {
      "epoch": 0.023601463888266488,
      "grad_norm": 0.32640504837036133,
      "learning_rate": 0.0004941756272401434,
      "loss": 8.2227,
      "step": 79
    },
    {
      "epoch": 0.023900216595712898,
      "grad_norm": 0.3017626404762268,
      "learning_rate": 0.0004941009557945042,
      "loss": 8.0449,
      "step": 80
    },
    {
      "epoch": 0.02419896930315931,
      "grad_norm": 0.2896050810813904,
      "learning_rate": 0.0004940262843488649,
      "loss": 7.8027,
      "step": 81
    },
    {
      "epoch": 0.02449772201060572,
      "grad_norm": 0.2856549918651581,
      "learning_rate": 0.0004939516129032259,
      "loss": 7.5742,
      "step": 82
    },
    {
      "epoch": 0.024796474718052134,
      "grad_norm": 0.2921745479106903,
      "learning_rate": 0.0004938769414575866,
      "loss": 7.7959,
      "step": 83
    },
    {
      "epoch": 0.025095227425498544,
      "grad_norm": 0.2688756585121155,
      "learning_rate": 0.0004938022700119475,
      "loss": 8.083,
      "step": 84
    },
    {
      "epoch": 0.025393980132944954,
      "grad_norm": 0.275928258895874,
      "learning_rate": 0.0004937275985663083,
      "loss": 7.583,
      "step": 85
    },
    {
      "epoch": 0.025692732840391367,
      "grad_norm": 0.272701621055603,
      "learning_rate": 0.0004936529271206691,
      "loss": 7.8018,
      "step": 86
    },
    {
      "epoch": 0.025991485547837777,
      "grad_norm": 0.2676122784614563,
      "learning_rate": 0.0004935782556750299,
      "loss": 8.0166,
      "step": 87
    },
    {
      "epoch": 0.02629023825528419,
      "grad_norm": 0.3009990453720093,
      "learning_rate": 0.0004935035842293907,
      "loss": 7.5195,
      "step": 88
    },
    {
      "epoch": 0.0265889909627306,
      "grad_norm": 0.32099971175193787,
      "learning_rate": 0.0004934289127837515,
      "loss": 7.2861,
      "step": 89
    },
    {
      "epoch": 0.02688774367017701,
      "grad_norm": 0.2675405740737915,
      "learning_rate": 0.0004933542413381122,
      "loss": 7.667,
      "step": 90
    },
    {
      "epoch": 0.027186496377623423,
      "grad_norm": 0.3011398911476135,
      "learning_rate": 0.0004932795698924732,
      "loss": 7.2764,
      "step": 91
    },
    {
      "epoch": 0.027485249085069833,
      "grad_norm": 0.24202702939510345,
      "learning_rate": 0.0004932048984468339,
      "loss": 7.7969,
      "step": 92
    },
    {
      "epoch": 0.027784001792516246,
      "grad_norm": 0.2650386393070221,
      "learning_rate": 0.0004931302270011948,
      "loss": 7.8994,
      "step": 93
    },
    {
      "epoch": 0.028082754499962656,
      "grad_norm": 0.2493761032819748,
      "learning_rate": 0.0004930555555555556,
      "loss": 7.7256,
      "step": 94
    },
    {
      "epoch": 0.028381507207409066,
      "grad_norm": 0.24684636294841766,
      "learning_rate": 0.0004929808841099164,
      "loss": 7.6885,
      "step": 95
    },
    {
      "epoch": 0.02868025991485548,
      "grad_norm": 0.32382333278656006,
      "learning_rate": 0.0004929062126642772,
      "loss": 7.3018,
      "step": 96
    },
    {
      "epoch": 0.02897901262230189,
      "grad_norm": 0.2478044331073761,
      "learning_rate": 0.0004928315412186379,
      "loss": 8.1084,
      "step": 97
    },
    {
      "epoch": 0.029277765329748302,
      "grad_norm": 0.28959137201309204,
      "learning_rate": 0.0004927568697729989,
      "loss": 7.8066,
      "step": 98
    },
    {
      "epoch": 0.029576518037194712,
      "grad_norm": 0.26750457286834717,
      "learning_rate": 0.0004926821983273596,
      "loss": 7.3613,
      "step": 99
    },
    {
      "epoch": 0.029875270744641122,
      "grad_norm": 0.24644550681114197,
      "learning_rate": 0.0004926075268817205,
      "loss": 7.8154,
      "step": 100
    },
    {
      "epoch": 0.030174023452087535,
      "grad_norm": 0.21514536440372467,
      "learning_rate": 0.0004925328554360812,
      "loss": 8.0107,
      "step": 101
    },
    {
      "epoch": 0.030472776159533945,
      "grad_norm": 0.24907834827899933,
      "learning_rate": 0.0004924581839904421,
      "loss": 7.8574,
      "step": 102
    },
    {
      "epoch": 0.03077152886698036,
      "grad_norm": 0.2640770375728607,
      "learning_rate": 0.0004923835125448029,
      "loss": 7.7686,
      "step": 103
    },
    {
      "epoch": 0.031070281574426768,
      "grad_norm": 0.3616735637187958,
      "learning_rate": 0.0004923088410991637,
      "loss": 7.6465,
      "step": 104
    },
    {
      "epoch": 0.03136903428187318,
      "grad_norm": 0.47297847270965576,
      "learning_rate": 0.0004922341696535245,
      "loss": 7.2412,
      "step": 105
    },
    {
      "epoch": 0.03166778698931959,
      "grad_norm": 0.2724394202232361,
      "learning_rate": 0.0004921594982078853,
      "loss": 7.6572,
      "step": 106
    },
    {
      "epoch": 0.031966539696766,
      "grad_norm": 0.23566237092018127,
      "learning_rate": 0.0004920848267622462,
      "loss": 7.7812,
      "step": 107
    },
    {
      "epoch": 0.032265292404212415,
      "grad_norm": 0.24848216772079468,
      "learning_rate": 0.0004920101553166069,
      "loss": 7.4004,
      "step": 108
    },
    {
      "epoch": 0.03256404511165883,
      "grad_norm": 0.3075593113899231,
      "learning_rate": 0.0004919354838709678,
      "loss": 6.8564,
      "step": 109
    },
    {
      "epoch": 0.032862797819105234,
      "grad_norm": 0.3262488543987274,
      "learning_rate": 0.0004918608124253285,
      "loss": 7.0518,
      "step": 110
    },
    {
      "epoch": 0.03316155052655165,
      "grad_norm": 0.2236112356185913,
      "learning_rate": 0.0004917861409796894,
      "loss": 7.7959,
      "step": 111
    },
    {
      "epoch": 0.03346030323399806,
      "grad_norm": 0.2831442952156067,
      "learning_rate": 0.0004917114695340502,
      "loss": 7.1348,
      "step": 112
    },
    {
      "epoch": 0.03375905594144447,
      "grad_norm": 0.24397167563438416,
      "learning_rate": 0.0004916367980884109,
      "loss": 7.8721,
      "step": 113
    },
    {
      "epoch": 0.03405780864889088,
      "grad_norm": 0.2545168697834015,
      "learning_rate": 0.0004915621266427719,
      "loss": 7.4492,
      "step": 114
    },
    {
      "epoch": 0.034356561356337294,
      "grad_norm": 0.22379270195960999,
      "learning_rate": 0.0004914874551971326,
      "loss": 7.6738,
      "step": 115
    },
    {
      "epoch": 0.0346553140637837,
      "grad_norm": 0.36304497718811035,
      "learning_rate": 0.0004914127837514935,
      "loss": 6.917,
      "step": 116
    },
    {
      "epoch": 0.03495406677123011,
      "grad_norm": 0.24335503578186035,
      "learning_rate": 0.0004913381123058542,
      "loss": 7.46,
      "step": 117
    },
    {
      "epoch": 0.03525281947867653,
      "grad_norm": 0.19890791177749634,
      "learning_rate": 0.0004912634408602151,
      "loss": 7.8145,
      "step": 118
    },
    {
      "epoch": 0.03555157218612294,
      "grad_norm": 0.2813917398452759,
      "learning_rate": 0.0004911887694145758,
      "loss": 7.0918,
      "step": 119
    },
    {
      "epoch": 0.035850324893569346,
      "grad_norm": 0.2551155686378479,
      "learning_rate": 0.0004911140979689367,
      "loss": 7.415,
      "step": 120
    },
    {
      "epoch": 0.03614907760101576,
      "grad_norm": 0.21404586732387543,
      "learning_rate": 0.0004910394265232976,
      "loss": 7.7285,
      "step": 121
    },
    {
      "epoch": 0.03644783030846217,
      "grad_norm": 0.2606382369995117,
      "learning_rate": 0.0004909647550776583,
      "loss": 7.4502,
      "step": 122
    },
    {
      "epoch": 0.03674658301590858,
      "grad_norm": 0.27905747294425964,
      "learning_rate": 0.0004908900836320192,
      "loss": 7.3135,
      "step": 123
    },
    {
      "epoch": 0.03704533572335499,
      "grad_norm": 0.27660632133483887,
      "learning_rate": 0.0004908154121863799,
      "loss": 7.1143,
      "step": 124
    },
    {
      "epoch": 0.037344088430801406,
      "grad_norm": 0.26993387937545776,
      "learning_rate": 0.0004907407407407408,
      "loss": 7.5371,
      "step": 125
    },
    {
      "epoch": 0.03764284113824781,
      "grad_norm": 0.20496590435504913,
      "learning_rate": 0.0004906660692951015,
      "loss": 7.7764,
      "step": 126
    },
    {
      "epoch": 0.037941593845694226,
      "grad_norm": 0.26359736919403076,
      "learning_rate": 0.0004905913978494624,
      "loss": 7.5264,
      "step": 127
    },
    {
      "epoch": 0.03824034655314064,
      "grad_norm": 0.2587951421737671,
      "learning_rate": 0.0004905167264038231,
      "loss": 7.4697,
      "step": 128
    },
    {
      "epoch": 0.03853909926058705,
      "grad_norm": 0.27789872884750366,
      "learning_rate": 0.000490442054958184,
      "loss": 7.0439,
      "step": 129
    },
    {
      "epoch": 0.03883785196803346,
      "grad_norm": 0.24561715126037598,
      "learning_rate": 0.0004903673835125449,
      "loss": 7.5801,
      "step": 130
    },
    {
      "epoch": 0.03913660467547987,
      "grad_norm": 0.29134175181388855,
      "learning_rate": 0.0004902927120669056,
      "loss": 7.2666,
      "step": 131
    },
    {
      "epoch": 0.039435357382926285,
      "grad_norm": 0.29421618580818176,
      "learning_rate": 0.0004902180406212665,
      "loss": 7.6748,
      "step": 132
    },
    {
      "epoch": 0.03973411009037269,
      "grad_norm": 0.30324870347976685,
      "learning_rate": 0.0004901433691756272,
      "loss": 7.2764,
      "step": 133
    },
    {
      "epoch": 0.040032862797819105,
      "grad_norm": 0.24822299182415009,
      "learning_rate": 0.0004900686977299881,
      "loss": 7.5156,
      "step": 134
    },
    {
      "epoch": 0.04033161550526552,
      "grad_norm": 0.3090725243091583,
      "learning_rate": 0.0004899940262843488,
      "loss": 7.4834,
      "step": 135
    },
    {
      "epoch": 0.040630368212711925,
      "grad_norm": 0.2992507517337799,
      "learning_rate": 0.0004899193548387097,
      "loss": 7.3252,
      "step": 136
    },
    {
      "epoch": 0.04092912092015834,
      "grad_norm": 0.3073965907096863,
      "learning_rate": 0.0004898446833930705,
      "loss": 7.2656,
      "step": 137
    },
    {
      "epoch": 0.04122787362760475,
      "grad_norm": 0.32228711247444153,
      "learning_rate": 0.0004897700119474313,
      "loss": 6.9688,
      "step": 138
    },
    {
      "epoch": 0.041526626335051164,
      "grad_norm": 0.290147066116333,
      "learning_rate": 0.0004896953405017922,
      "loss": 7.46,
      "step": 139
    },
    {
      "epoch": 0.04182537904249757,
      "grad_norm": 0.3048872947692871,
      "learning_rate": 0.0004896206690561529,
      "loss": 7.1982,
      "step": 140
    },
    {
      "epoch": 0.042124131749943984,
      "grad_norm": 0.2502696216106415,
      "learning_rate": 0.0004895459976105138,
      "loss": 7.7822,
      "step": 141
    },
    {
      "epoch": 0.0424228844573904,
      "grad_norm": 0.277056485414505,
      "learning_rate": 0.0004894713261648745,
      "loss": 6.9238,
      "step": 142
    },
    {
      "epoch": 0.042721637164836804,
      "grad_norm": 0.3534789979457855,
      "learning_rate": 0.0004893966547192354,
      "loss": 7.1211,
      "step": 143
    },
    {
      "epoch": 0.04302038987228322,
      "grad_norm": 0.2861802875995636,
      "learning_rate": 0.0004893219832735961,
      "loss": 7.2949,
      "step": 144
    },
    {
      "epoch": 0.04331914257972963,
      "grad_norm": 0.2737436890602112,
      "learning_rate": 0.000489247311827957,
      "loss": 7.168,
      "step": 145
    },
    {
      "epoch": 0.04361789528717604,
      "grad_norm": 0.3162184953689575,
      "learning_rate": 0.0004891726403823178,
      "loss": 7.1924,
      "step": 146
    },
    {
      "epoch": 0.04391664799462245,
      "grad_norm": 0.2845972776412964,
      "learning_rate": 0.0004890979689366786,
      "loss": 7.0381,
      "step": 147
    },
    {
      "epoch": 0.04421540070206886,
      "grad_norm": 0.35005950927734375,
      "learning_rate": 0.0004890232974910394,
      "loss": 7.3398,
      "step": 148
    },
    {
      "epoch": 0.04451415340951528,
      "grad_norm": 0.25128173828125,
      "learning_rate": 0.0004889486260454002,
      "loss": 7.3232,
      "step": 149
    },
    {
      "epoch": 0.04481290611696168,
      "grad_norm": 0.2506413757801056,
      "learning_rate": 0.0004888739545997611,
      "loss": 7.5654,
      "step": 150
    },
    {
      "epoch": 0.045111658824408096,
      "grad_norm": 0.3365420699119568,
      "learning_rate": 0.0004887992831541218,
      "loss": 7.0918,
      "step": 151
    },
    {
      "epoch": 0.04541041153185451,
      "grad_norm": 0.30113887786865234,
      "learning_rate": 0.0004887246117084828,
      "loss": 7.0449,
      "step": 152
    },
    {
      "epoch": 0.045709164239300916,
      "grad_norm": 0.31116804480552673,
      "learning_rate": 0.0004886499402628435,
      "loss": 7.4502,
      "step": 153
    },
    {
      "epoch": 0.04600791694674733,
      "grad_norm": 0.30247560143470764,
      "learning_rate": 0.0004885752688172043,
      "loss": 7.1523,
      "step": 154
    },
    {
      "epoch": 0.04630666965419374,
      "grad_norm": 0.21981866657733917,
      "learning_rate": 0.0004885005973715651,
      "loss": 7.5488,
      "step": 155
    },
    {
      "epoch": 0.04660542236164015,
      "grad_norm": 0.3004038631916046,
      "learning_rate": 0.0004884259259259259,
      "loss": 7.5127,
      "step": 156
    },
    {
      "epoch": 0.04690417506908656,
      "grad_norm": 0.25513529777526855,
      "learning_rate": 0.0004883512544802867,
      "loss": 7.4385,
      "step": 157
    },
    {
      "epoch": 0.047202927776532976,
      "grad_norm": 0.33559417724609375,
      "learning_rate": 0.0004882765830346476,
      "loss": 7.4717,
      "step": 158
    },
    {
      "epoch": 0.04750168048397939,
      "grad_norm": 0.26063433289527893,
      "learning_rate": 0.0004882019115890084,
      "loss": 7.6895,
      "step": 159
    },
    {
      "epoch": 0.047800433191425795,
      "grad_norm": 0.39380547404289246,
      "learning_rate": 0.0004881272401433692,
      "loss": 7.2266,
      "step": 160
    },
    {
      "epoch": 0.04809918589887221,
      "grad_norm": 0.33631765842437744,
      "learning_rate": 0.00048805256869773,
      "loss": 6.8701,
      "step": 161
    },
    {
      "epoch": 0.04839793860631862,
      "grad_norm": 0.2894502878189087,
      "learning_rate": 0.0004879778972520908,
      "loss": 7.4658,
      "step": 162
    },
    {
      "epoch": 0.04869669131376503,
      "grad_norm": 0.3700374364852905,
      "learning_rate": 0.00048790322580645164,
      "loss": 6.8213,
      "step": 163
    },
    {
      "epoch": 0.04899544402121144,
      "grad_norm": 0.2870594263076782,
      "learning_rate": 0.0004878285543608124,
      "loss": 7.2793,
      "step": 164
    },
    {
      "epoch": 0.049294196728657855,
      "grad_norm": 0.2825789153575897,
      "learning_rate": 0.00048775388291517327,
      "loss": 7.5088,
      "step": 165
    },
    {
      "epoch": 0.04959294943610427,
      "grad_norm": 0.24907280504703522,
      "learning_rate": 0.000487679211469534,
      "loss": 7.5146,
      "step": 166
    },
    {
      "epoch": 0.049891702143550674,
      "grad_norm": 0.257586807012558,
      "learning_rate": 0.0004876045400238949,
      "loss": 7.5244,
      "step": 167
    },
    {
      "epoch": 0.05019045485099709,
      "grad_norm": 0.2747199535369873,
      "learning_rate": 0.0004875298685782557,
      "loss": 7.7803,
      "step": 168
    },
    {
      "epoch": 0.0504892075584435,
      "grad_norm": 0.2723439633846283,
      "learning_rate": 0.00048745519713261647,
      "loss": 7.4814,
      "step": 169
    },
    {
      "epoch": 0.05078796026588991,
      "grad_norm": 0.3231061100959778,
      "learning_rate": 0.00048738052568697733,
      "loss": 7.7725,
      "step": 170
    },
    {
      "epoch": 0.05108671297333632,
      "grad_norm": 0.25988444685935974,
      "learning_rate": 0.0004873058542413381,
      "loss": 7.7178,
      "step": 171
    },
    {
      "epoch": 0.051385465680782734,
      "grad_norm": 0.3010958433151245,
      "learning_rate": 0.00048723118279569896,
      "loss": 7.2129,
      "step": 172
    },
    {
      "epoch": 0.05168421838822914,
      "grad_norm": 0.292570561170578,
      "learning_rate": 0.0004871565113500597,
      "loss": 7.3984,
      "step": 173
    },
    {
      "epoch": 0.051982971095675554,
      "grad_norm": 0.2666943669319153,
      "learning_rate": 0.0004870818399044206,
      "loss": 7.2725,
      "step": 174
    },
    {
      "epoch": 0.05228172380312197,
      "grad_norm": 0.3564080595970154,
      "learning_rate": 0.00048700716845878134,
      "loss": 7.4648,
      "step": 175
    },
    {
      "epoch": 0.05258047651056838,
      "grad_norm": 0.3185379207134247,
      "learning_rate": 0.0004869324970131422,
      "loss": 7.1289,
      "step": 176
    },
    {
      "epoch": 0.05287922921801479,
      "grad_norm": 0.2607346773147583,
      "learning_rate": 0.000486857825567503,
      "loss": 7.3965,
      "step": 177
    },
    {
      "epoch": 0.0531779819254612,
      "grad_norm": 0.3033246695995331,
      "learning_rate": 0.0004867831541218638,
      "loss": 7.2402,
      "step": 178
    },
    {
      "epoch": 0.05347673463290761,
      "grad_norm": 0.29828473925590515,
      "learning_rate": 0.00048670848267622465,
      "loss": 7.3066,
      "step": 179
    },
    {
      "epoch": 0.05377548734035402,
      "grad_norm": 0.46538135409355164,
      "learning_rate": 0.0004866338112305854,
      "loss": 7.1484,
      "step": 180
    },
    {
      "epoch": 0.05407424004780043,
      "grad_norm": 0.34644702076911926,
      "learning_rate": 0.0004865591397849463,
      "loss": 7.0264,
      "step": 181
    },
    {
      "epoch": 0.054372992755246846,
      "grad_norm": 0.2750479578971863,
      "learning_rate": 0.00048648446833930703,
      "loss": 7.6006,
      "step": 182
    },
    {
      "epoch": 0.05467174546269325,
      "grad_norm": 0.39400234818458557,
      "learning_rate": 0.0004864097968936679,
      "loss": 6.7461,
      "step": 183
    },
    {
      "epoch": 0.054970498170139666,
      "grad_norm": 0.2518077492713928,
      "learning_rate": 0.00048633512544802866,
      "loss": 7.4766,
      "step": 184
    },
    {
      "epoch": 0.05526925087758608,
      "grad_norm": 0.31082457304000854,
      "learning_rate": 0.00048626045400238947,
      "loss": 7.292,
      "step": 185
    },
    {
      "epoch": 0.05556800358503249,
      "grad_norm": 0.24337168037891388,
      "learning_rate": 0.00048618578255675034,
      "loss": 7.6064,
      "step": 186
    },
    {
      "epoch": 0.0558667562924789,
      "grad_norm": 0.26724016666412354,
      "learning_rate": 0.0004861111111111111,
      "loss": 7.6484,
      "step": 187
    },
    {
      "epoch": 0.05616550899992531,
      "grad_norm": 0.33405354619026184,
      "learning_rate": 0.00048603643966547196,
      "loss": 7.5039,
      "step": 188
    },
    {
      "epoch": 0.056464261707371725,
      "grad_norm": 0.3647632300853729,
      "learning_rate": 0.0004859617682198327,
      "loss": 7.1309,
      "step": 189
    },
    {
      "epoch": 0.05676301441481813,
      "grad_norm": 0.3169994652271271,
      "learning_rate": 0.0004858870967741936,
      "loss": 7.7275,
      "step": 190
    },
    {
      "epoch": 0.057061767122264545,
      "grad_norm": 0.3262353837490082,
      "learning_rate": 0.00048581242532855435,
      "loss": 7.2393,
      "step": 191
    },
    {
      "epoch": 0.05736051982971096,
      "grad_norm": 0.28487446904182434,
      "learning_rate": 0.0004857377538829152,
      "loss": 7.4795,
      "step": 192
    },
    {
      "epoch": 0.057659272537157365,
      "grad_norm": 0.2790040373802185,
      "learning_rate": 0.000485663082437276,
      "loss": 7.5664,
      "step": 193
    },
    {
      "epoch": 0.05795802524460378,
      "grad_norm": 0.4375808537006378,
      "learning_rate": 0.0004855884109916368,
      "loss": 7.2256,
      "step": 194
    },
    {
      "epoch": 0.05825677795205019,
      "grad_norm": 0.2864578664302826,
      "learning_rate": 0.00048551373954599765,
      "loss": 7.1836,
      "step": 195
    },
    {
      "epoch": 0.058555530659496605,
      "grad_norm": 0.36548492312431335,
      "learning_rate": 0.0004854390681003584,
      "loss": 7.1211,
      "step": 196
    },
    {
      "epoch": 0.05885428336694301,
      "grad_norm": 0.4068611264228821,
      "learning_rate": 0.0004853643966547193,
      "loss": 7.3398,
      "step": 197
    },
    {
      "epoch": 0.059153036074389424,
      "grad_norm": 0.34425196051597595,
      "learning_rate": 0.00048528972520908004,
      "loss": 6.7324,
      "step": 198
    },
    {
      "epoch": 0.05945178878183584,
      "grad_norm": 0.3324277102947235,
      "learning_rate": 0.0004852150537634409,
      "loss": 7.2988,
      "step": 199
    },
    {
      "epoch": 0.059750541489282244,
      "grad_norm": 0.3078002631664276,
      "learning_rate": 0.00048514038231780166,
      "loss": 7.2334,
      "step": 200
    },
    {
      "epoch": 0.059750541489282244,
      "eval_bleu": 0.03907982849907875,
      "eval_loss": 7.1328125,
      "eval_runtime": 2607.2509,
      "eval_samples_per_second": 0.54,
      "eval_steps_per_second": 0.034,
      "step": 200
    },
    {
      "epoch": 0.06004929419672866,
      "grad_norm": 0.3123011589050293,
      "learning_rate": 0.0004850657108721625,
      "loss": 7.2529,
      "step": 201
    },
    {
      "epoch": 0.06034804690417507,
      "grad_norm": 0.2943539619445801,
      "learning_rate": 0.0004849910394265233,
      "loss": 7.2393,
      "step": 202
    },
    {
      "epoch": 0.06064679961162148,
      "grad_norm": 0.2779425084590912,
      "learning_rate": 0.0004849163679808841,
      "loss": 7.3496,
      "step": 203
    },
    {
      "epoch": 0.06094555231906789,
      "grad_norm": 0.29054632782936096,
      "learning_rate": 0.00048484169653524497,
      "loss": 7.4219,
      "step": 204
    },
    {
      "epoch": 0.061244305026514304,
      "grad_norm": 0.26427578926086426,
      "learning_rate": 0.00048476702508960573,
      "loss": 7.4111,
      "step": 205
    },
    {
      "epoch": 0.06154305773396072,
      "grad_norm": 0.3190177083015442,
      "learning_rate": 0.0004846923536439666,
      "loss": 7.2773,
      "step": 206
    },
    {
      "epoch": 0.06184181044140712,
      "grad_norm": 0.28507933020591736,
      "learning_rate": 0.00048461768219832735,
      "loss": 7.21,
      "step": 207
    },
    {
      "epoch": 0.062140563148853536,
      "grad_norm": 0.28055888414382935,
      "learning_rate": 0.0004845430107526882,
      "loss": 7.5439,
      "step": 208
    },
    {
      "epoch": 0.06243931585629995,
      "grad_norm": 0.305084228515625,
      "learning_rate": 0.000484468339307049,
      "loss": 7.2793,
      "step": 209
    },
    {
      "epoch": 0.06273806856374636,
      "grad_norm": 0.3277842700481415,
      "learning_rate": 0.0004843936678614098,
      "loss": 7.7324,
      "step": 210
    },
    {
      "epoch": 0.06303682127119277,
      "grad_norm": 0.2979891300201416,
      "learning_rate": 0.0004843189964157706,
      "loss": 7.0898,
      "step": 211
    },
    {
      "epoch": 0.06333557397863918,
      "grad_norm": 0.31598925590515137,
      "learning_rate": 0.0004842443249701314,
      "loss": 7.291,
      "step": 212
    },
    {
      "epoch": 0.0636343266860856,
      "grad_norm": 0.2918887734413147,
      "learning_rate": 0.0004841696535244923,
      "loss": 7.4854,
      "step": 213
    },
    {
      "epoch": 0.063933079393532,
      "grad_norm": 0.30401235818862915,
      "learning_rate": 0.00048409498207885304,
      "loss": 7.2002,
      "step": 214
    },
    {
      "epoch": 0.06423183210097841,
      "grad_norm": 0.3682520389556885,
      "learning_rate": 0.0004840203106332139,
      "loss": 7.2324,
      "step": 215
    },
    {
      "epoch": 0.06453058480842483,
      "grad_norm": 0.3752498924732208,
      "learning_rate": 0.00048394563918757467,
      "loss": 7.0205,
      "step": 216
    },
    {
      "epoch": 0.06482933751587124,
      "grad_norm": 0.3327663838863373,
      "learning_rate": 0.0004838709677419355,
      "loss": 7.4141,
      "step": 217
    },
    {
      "epoch": 0.06512809022331766,
      "grad_norm": 0.38402241468429565,
      "learning_rate": 0.0004837962962962963,
      "loss": 6.9395,
      "step": 218
    },
    {
      "epoch": 0.06542684293076406,
      "grad_norm": 0.271368145942688,
      "learning_rate": 0.0004837216248506571,
      "loss": 7.5869,
      "step": 219
    },
    {
      "epoch": 0.06572559563821047,
      "grad_norm": 0.25743579864501953,
      "learning_rate": 0.0004836469534050179,
      "loss": 7.5557,
      "step": 220
    },
    {
      "epoch": 0.06602434834565689,
      "grad_norm": 0.28932467103004456,
      "learning_rate": 0.00048357228195937873,
      "loss": 7.6787,
      "step": 221
    },
    {
      "epoch": 0.0663231010531033,
      "grad_norm": 0.33489927649497986,
      "learning_rate": 0.0004834976105137396,
      "loss": 7.4746,
      "step": 222
    },
    {
      "epoch": 0.0666218537605497,
      "grad_norm": 0.3144931197166443,
      "learning_rate": 0.00048342293906810036,
      "loss": 7.459,
      "step": 223
    },
    {
      "epoch": 0.06692060646799612,
      "grad_norm": 0.3177618980407715,
      "learning_rate": 0.00048334826762246123,
      "loss": 7.585,
      "step": 224
    },
    {
      "epoch": 0.06721935917544253,
      "grad_norm": 0.40274351835250854,
      "learning_rate": 0.000483273596176822,
      "loss": 7.2705,
      "step": 225
    },
    {
      "epoch": 0.06751811188288893,
      "grad_norm": 0.3625737130641937,
      "learning_rate": 0.0004831989247311828,
      "loss": 7.0977,
      "step": 226
    },
    {
      "epoch": 0.06781686459033535,
      "grad_norm": 0.3747721016407013,
      "learning_rate": 0.0004831242532855436,
      "loss": 7.3496,
      "step": 227
    },
    {
      "epoch": 0.06811561729778176,
      "grad_norm": 0.30746516585350037,
      "learning_rate": 0.0004830495818399044,
      "loss": 7.4443,
      "step": 228
    },
    {
      "epoch": 0.06841437000522817,
      "grad_norm": 0.35161441564559937,
      "learning_rate": 0.00048297491039426524,
      "loss": 6.709,
      "step": 229
    },
    {
      "epoch": 0.06871312271267459,
      "grad_norm": 0.3419186770915985,
      "learning_rate": 0.00048290023894862605,
      "loss": 7.2734,
      "step": 230
    },
    {
      "epoch": 0.069011875420121,
      "grad_norm": 0.32175663113594055,
      "learning_rate": 0.0004828255675029869,
      "loss": 7.5088,
      "step": 231
    },
    {
      "epoch": 0.0693106281275674,
      "grad_norm": 0.310066819190979,
      "learning_rate": 0.0004827508960573477,
      "loss": 7.2734,
      "step": 232
    },
    {
      "epoch": 0.06960938083501382,
      "grad_norm": 0.38308191299438477,
      "learning_rate": 0.0004826762246117085,
      "loss": 7.2852,
      "step": 233
    },
    {
      "epoch": 0.06990813354246023,
      "grad_norm": 0.33334752917289734,
      "learning_rate": 0.0004826015531660693,
      "loss": 7.2979,
      "step": 234
    },
    {
      "epoch": 0.07020688624990663,
      "grad_norm": 0.3677629828453064,
      "learning_rate": 0.0004825268817204301,
      "loss": 6.959,
      "step": 235
    },
    {
      "epoch": 0.07050563895735305,
      "grad_norm": 0.6028388142585754,
      "learning_rate": 0.00048245221027479093,
      "loss": 6.6934,
      "step": 236
    },
    {
      "epoch": 0.07080439166479946,
      "grad_norm": 0.30435067415237427,
      "learning_rate": 0.00048237753882915174,
      "loss": 7.4219,
      "step": 237
    },
    {
      "epoch": 0.07110314437224588,
      "grad_norm": 0.3424874246120453,
      "learning_rate": 0.00048230286738351255,
      "loss": 6.9922,
      "step": 238
    },
    {
      "epoch": 0.07140189707969229,
      "grad_norm": 0.38234105706214905,
      "learning_rate": 0.00048222819593787337,
      "loss": 7.5273,
      "step": 239
    },
    {
      "epoch": 0.07170064978713869,
      "grad_norm": 0.34200963377952576,
      "learning_rate": 0.0004821535244922341,
      "loss": 7.0391,
      "step": 240
    },
    {
      "epoch": 0.07199940249458511,
      "grad_norm": 0.38949450850486755,
      "learning_rate": 0.000482078853046595,
      "loss": 7.6494,
      "step": 241
    },
    {
      "epoch": 0.07229815520203152,
      "grad_norm": 0.3774808943271637,
      "learning_rate": 0.0004820041816009558,
      "loss": 6.9707,
      "step": 242
    },
    {
      "epoch": 0.07259690790947793,
      "grad_norm": 0.3394872844219208,
      "learning_rate": 0.0004819295101553166,
      "loss": 7.375,
      "step": 243
    },
    {
      "epoch": 0.07289566061692435,
      "grad_norm": 0.2433769851922989,
      "learning_rate": 0.00048185483870967743,
      "loss": 7.415,
      "step": 244
    },
    {
      "epoch": 0.07319441332437075,
      "grad_norm": 0.5029091238975525,
      "learning_rate": 0.00048178016726403824,
      "loss": 6.7168,
      "step": 245
    },
    {
      "epoch": 0.07349316603181716,
      "grad_norm": 0.2827988564968109,
      "learning_rate": 0.00048170549581839906,
      "loss": 7.499,
      "step": 246
    },
    {
      "epoch": 0.07379191873926358,
      "grad_norm": 0.3065880537033081,
      "learning_rate": 0.00048163082437275987,
      "loss": 7.5312,
      "step": 247
    },
    {
      "epoch": 0.07409067144670999,
      "grad_norm": 0.2757122218608856,
      "learning_rate": 0.0004815561529271207,
      "loss": 7.4023,
      "step": 248
    },
    {
      "epoch": 0.07438942415415639,
      "grad_norm": 0.35145482420921326,
      "learning_rate": 0.00048148148148148144,
      "loss": 7.1299,
      "step": 249
    },
    {
      "epoch": 0.07468817686160281,
      "grad_norm": 0.3465346097946167,
      "learning_rate": 0.0004814068100358423,
      "loss": 7.1553,
      "step": 250
    },
    {
      "epoch": 0.07498692956904922,
      "grad_norm": 0.32320737838745117,
      "learning_rate": 0.0004813321385902031,
      "loss": 7.1562,
      "step": 251
    },
    {
      "epoch": 0.07528568227649562,
      "grad_norm": 0.2975021004676819,
      "learning_rate": 0.00048125746714456393,
      "loss": 7.1943,
      "step": 252
    },
    {
      "epoch": 0.07558443498394204,
      "grad_norm": 0.30543196201324463,
      "learning_rate": 0.00048118279569892475,
      "loss": 7.6523,
      "step": 253
    },
    {
      "epoch": 0.07588318769138845,
      "grad_norm": 0.31832584738731384,
      "learning_rate": 0.00048110812425328556,
      "loss": 7.7295,
      "step": 254
    },
    {
      "epoch": 0.07618194039883486,
      "grad_norm": 0.45462581515312195,
      "learning_rate": 0.00048103345280764637,
      "loss": 6.6504,
      "step": 255
    },
    {
      "epoch": 0.07648069310628128,
      "grad_norm": 0.29367244243621826,
      "learning_rate": 0.00048095878136200713,
      "loss": 7.5576,
      "step": 256
    },
    {
      "epoch": 0.07677944581372768,
      "grad_norm": 0.3785807490348816,
      "learning_rate": 0.000480884109916368,
      "loss": 6.9893,
      "step": 257
    },
    {
      "epoch": 0.0770781985211741,
      "grad_norm": 0.42041096091270447,
      "learning_rate": 0.00048080943847072876,
      "loss": 7.0098,
      "step": 258
    },
    {
      "epoch": 0.07737695122862051,
      "grad_norm": 0.4015696346759796,
      "learning_rate": 0.0004807347670250896,
      "loss": 6.8574,
      "step": 259
    },
    {
      "epoch": 0.07767570393606692,
      "grad_norm": 0.31781068444252014,
      "learning_rate": 0.00048066009557945044,
      "loss": 7.1729,
      "step": 260
    },
    {
      "epoch": 0.07797445664351334,
      "grad_norm": 0.33713677525520325,
      "learning_rate": 0.00048058542413381125,
      "loss": 7.2793,
      "step": 261
    },
    {
      "epoch": 0.07827320935095974,
      "grad_norm": 0.32415229082107544,
      "learning_rate": 0.00048051075268817206,
      "loss": 7.2158,
      "step": 262
    },
    {
      "epoch": 0.07857196205840615,
      "grad_norm": 0.3313237428665161,
      "learning_rate": 0.0004804360812425329,
      "loss": 7.7979,
      "step": 263
    },
    {
      "epoch": 0.07887071476585257,
      "grad_norm": 0.28631895780563354,
      "learning_rate": 0.0004803614097968937,
      "loss": 7.6855,
      "step": 264
    },
    {
      "epoch": 0.07916946747329898,
      "grad_norm": 0.34740912914276123,
      "learning_rate": 0.00048028673835125445,
      "loss": 7.2217,
      "step": 265
    },
    {
      "epoch": 0.07946822018074538,
      "grad_norm": 0.2941768765449524,
      "learning_rate": 0.0004802120669056153,
      "loss": 7.4521,
      "step": 266
    },
    {
      "epoch": 0.0797669728881918,
      "grad_norm": 0.3617632985115051,
      "learning_rate": 0.0004801373954599761,
      "loss": 7.1104,
      "step": 267
    },
    {
      "epoch": 0.08006572559563821,
      "grad_norm": 0.31673726439476013,
      "learning_rate": 0.00048006272401433694,
      "loss": 7.0762,
      "step": 268
    },
    {
      "epoch": 0.08036447830308462,
      "grad_norm": 0.3049163818359375,
      "learning_rate": 0.00047998805256869775,
      "loss": 7.334,
      "step": 269
    },
    {
      "epoch": 0.08066323101053104,
      "grad_norm": 0.3509286940097809,
      "learning_rate": 0.00047991338112305857,
      "loss": 7.0996,
      "step": 270
    },
    {
      "epoch": 0.08096198371797744,
      "grad_norm": 0.36868366599082947,
      "learning_rate": 0.0004798387096774194,
      "loss": 6.9736,
      "step": 271
    },
    {
      "epoch": 0.08126073642542385,
      "grad_norm": 0.4257866442203522,
      "learning_rate": 0.00047976403823178014,
      "loss": 6.5889,
      "step": 272
    },
    {
      "epoch": 0.08155948913287027,
      "grad_norm": 0.33760908246040344,
      "learning_rate": 0.000479689366786141,
      "loss": 6.8174,
      "step": 273
    },
    {
      "epoch": 0.08185824184031668,
      "grad_norm": 0.3890152871608734,
      "learning_rate": 0.00047961469534050176,
      "loss": 7.4414,
      "step": 274
    },
    {
      "epoch": 0.0821569945477631,
      "grad_norm": 0.4200459420681,
      "learning_rate": 0.00047954002389486263,
      "loss": 6.8447,
      "step": 275
    },
    {
      "epoch": 0.0824557472552095,
      "grad_norm": 0.32538771629333496,
      "learning_rate": 0.0004794653524492234,
      "loss": 7.3564,
      "step": 276
    },
    {
      "epoch": 0.08275449996265591,
      "grad_norm": 0.3766273856163025,
      "learning_rate": 0.00047939068100358426,
      "loss": 6.8135,
      "step": 277
    },
    {
      "epoch": 0.08305325267010233,
      "grad_norm": 0.2985558807849884,
      "learning_rate": 0.00047931600955794507,
      "loss": 7.3838,
      "step": 278
    },
    {
      "epoch": 0.08335200537754874,
      "grad_norm": 0.3554684817790985,
      "learning_rate": 0.0004792413381123059,
      "loss": 7.5234,
      "step": 279
    },
    {
      "epoch": 0.08365075808499514,
      "grad_norm": 0.31132543087005615,
      "learning_rate": 0.0004791666666666667,
      "loss": 7.2695,
      "step": 280
    },
    {
      "epoch": 0.08394951079244156,
      "grad_norm": 0.36146849393844604,
      "learning_rate": 0.00047909199522102745,
      "loss": 7.4658,
      "step": 281
    },
    {
      "epoch": 0.08424826349988797,
      "grad_norm": 0.32686829566955566,
      "learning_rate": 0.0004790173237753883,
      "loss": 7.4521,
      "step": 282
    },
    {
      "epoch": 0.08454701620733437,
      "grad_norm": 0.28750407695770264,
      "learning_rate": 0.0004789426523297491,
      "loss": 7.3955,
      "step": 283
    },
    {
      "epoch": 0.0848457689147808,
      "grad_norm": 0.36340340971946716,
      "learning_rate": 0.00047886798088410995,
      "loss": 7.5283,
      "step": 284
    },
    {
      "epoch": 0.0851445216222272,
      "grad_norm": 0.29256385564804077,
      "learning_rate": 0.0004787933094384707,
      "loss": 7.7695,
      "step": 285
    },
    {
      "epoch": 0.08544327432967361,
      "grad_norm": 0.29111194610595703,
      "learning_rate": 0.00047871863799283157,
      "loss": 7.4336,
      "step": 286
    },
    {
      "epoch": 0.08574202703712003,
      "grad_norm": 0.38031360507011414,
      "learning_rate": 0.0004786439665471924,
      "loss": 7.3438,
      "step": 287
    },
    {
      "epoch": 0.08604077974456643,
      "grad_norm": 0.5033569931983948,
      "learning_rate": 0.00047856929510155314,
      "loss": 6.9629,
      "step": 288
    },
    {
      "epoch": 0.08633953245201284,
      "grad_norm": 0.32559508085250854,
      "learning_rate": 0.000478494623655914,
      "loss": 7.1074,
      "step": 289
    },
    {
      "epoch": 0.08663828515945926,
      "grad_norm": 0.35896870493888855,
      "learning_rate": 0.00047841995221027477,
      "loss": 7.1719,
      "step": 290
    },
    {
      "epoch": 0.08693703786690567,
      "grad_norm": 0.3534773588180542,
      "learning_rate": 0.00047834528076463564,
      "loss": 7.4482,
      "step": 291
    },
    {
      "epoch": 0.08723579057435207,
      "grad_norm": 0.3783564567565918,
      "learning_rate": 0.0004782706093189964,
      "loss": 7.1924,
      "step": 292
    },
    {
      "epoch": 0.0875345432817985,
      "grad_norm": 0.33855393528938293,
      "learning_rate": 0.00047819593787335726,
      "loss": 7.1904,
      "step": 293
    },
    {
      "epoch": 0.0878332959892449,
      "grad_norm": 0.2666965126991272,
      "learning_rate": 0.000478121266427718,
      "loss": 7.3926,
      "step": 294
    },
    {
      "epoch": 0.08813204869669132,
      "grad_norm": 0.328940749168396,
      "learning_rate": 0.0004780465949820789,
      "loss": 7.2334,
      "step": 295
    },
    {
      "epoch": 0.08843080140413773,
      "grad_norm": 0.35646528005599976,
      "learning_rate": 0.0004779719235364397,
      "loss": 7.416,
      "step": 296
    },
    {
      "epoch": 0.08872955411158413,
      "grad_norm": 0.41437315940856934,
      "learning_rate": 0.00047789725209080046,
      "loss": 7.0273,
      "step": 297
    },
    {
      "epoch": 0.08902830681903055,
      "grad_norm": 0.32371196150779724,
      "learning_rate": 0.0004778225806451613,
      "loss": 7.1855,
      "step": 298
    },
    {
      "epoch": 0.08932705952647696,
      "grad_norm": 0.286600798368454,
      "learning_rate": 0.0004777479091995221,
      "loss": 7.3711,
      "step": 299
    },
    {
      "epoch": 0.08962581223392337,
      "grad_norm": 0.36828580498695374,
      "learning_rate": 0.00047767323775388295,
      "loss": 6.6221,
      "step": 300
    },
    {
      "epoch": 0.08992456494136979,
      "grad_norm": 0.3530673384666443,
      "learning_rate": 0.0004775985663082437,
      "loss": 7.2061,
      "step": 301
    },
    {
      "epoch": 0.09022331764881619,
      "grad_norm": 0.30934393405914307,
      "learning_rate": 0.0004775238948626046,
      "loss": 7.2178,
      "step": 302
    },
    {
      "epoch": 0.0905220703562626,
      "grad_norm": 0.3234090805053711,
      "learning_rate": 0.00047744922341696534,
      "loss": 7.6318,
      "step": 303
    },
    {
      "epoch": 0.09082082306370902,
      "grad_norm": 0.41697925329208374,
      "learning_rate": 0.00047737455197132615,
      "loss": 6.9072,
      "step": 304
    },
    {
      "epoch": 0.09111957577115543,
      "grad_norm": 0.40609318017959595,
      "learning_rate": 0.000477299880525687,
      "loss": 7.4297,
      "step": 305
    },
    {
      "epoch": 0.09141832847860183,
      "grad_norm": 0.33139768242836,
      "learning_rate": 0.0004772252090800478,
      "loss": 7.3711,
      "step": 306
    },
    {
      "epoch": 0.09171708118604825,
      "grad_norm": 0.3366144597530365,
      "learning_rate": 0.00047715053763440864,
      "loss": 7.4678,
      "step": 307
    },
    {
      "epoch": 0.09201583389349466,
      "grad_norm": 0.373856782913208,
      "learning_rate": 0.0004770758661887694,
      "loss": 6.9414,
      "step": 308
    },
    {
      "epoch": 0.09231458660094106,
      "grad_norm": 0.3595760762691498,
      "learning_rate": 0.00047700119474313027,
      "loss": 7.2773,
      "step": 309
    },
    {
      "epoch": 0.09261333930838749,
      "grad_norm": 0.38620641827583313,
      "learning_rate": 0.000476926523297491,
      "loss": 7.0107,
      "step": 310
    },
    {
      "epoch": 0.09291209201583389,
      "grad_norm": 0.28860586881637573,
      "learning_rate": 0.0004768518518518519,
      "loss": 7.3516,
      "step": 311
    },
    {
      "epoch": 0.0932108447232803,
      "grad_norm": 0.3319552540779114,
      "learning_rate": 0.00047677718040621265,
      "loss": 7.6865,
      "step": 312
    },
    {
      "epoch": 0.09350959743072672,
      "grad_norm": 0.35501229763031006,
      "learning_rate": 0.00047670250896057347,
      "loss": 7.1748,
      "step": 313
    },
    {
      "epoch": 0.09380835013817312,
      "grad_norm": 0.3376871347427368,
      "learning_rate": 0.00047662783751493433,
      "loss": 7.5293,
      "step": 314
    },
    {
      "epoch": 0.09410710284561954,
      "grad_norm": 0.34964245557785034,
      "learning_rate": 0.0004765531660692951,
      "loss": 7.1279,
      "step": 315
    },
    {
      "epoch": 0.09440585555306595,
      "grad_norm": 0.2928626835346222,
      "learning_rate": 0.00047647849462365596,
      "loss": 7.4678,
      "step": 316
    },
    {
      "epoch": 0.09470460826051236,
      "grad_norm": 0.27204424142837524,
      "learning_rate": 0.0004764038231780167,
      "loss": 7.457,
      "step": 317
    },
    {
      "epoch": 0.09500336096795878,
      "grad_norm": 0.34205707907676697,
      "learning_rate": 0.0004763291517323776,
      "loss": 7.3604,
      "step": 318
    },
    {
      "epoch": 0.09530211367540518,
      "grad_norm": 0.43689972162246704,
      "learning_rate": 0.00047625448028673834,
      "loss": 6.9404,
      "step": 319
    },
    {
      "epoch": 0.09560086638285159,
      "grad_norm": 0.34572312235832214,
      "learning_rate": 0.00047617980884109916,
      "loss": 7.1553,
      "step": 320
    },
    {
      "epoch": 0.09589961909029801,
      "grad_norm": 0.5221048593521118,
      "learning_rate": 0.00047610513739545997,
      "loss": 6.8701,
      "step": 321
    },
    {
      "epoch": 0.09619837179774442,
      "grad_norm": 0.32879388332366943,
      "learning_rate": 0.0004760304659498208,
      "loss": 7.1484,
      "step": 322
    },
    {
      "epoch": 0.09649712450519082,
      "grad_norm": 0.335427850484848,
      "learning_rate": 0.00047595579450418165,
      "loss": 7.4492,
      "step": 323
    },
    {
      "epoch": 0.09679587721263724,
      "grad_norm": 0.32932671904563904,
      "learning_rate": 0.0004758811230585424,
      "loss": 7.2305,
      "step": 324
    },
    {
      "epoch": 0.09709462992008365,
      "grad_norm": 0.4528493881225586,
      "learning_rate": 0.0004758064516129033,
      "loss": 6.5498,
      "step": 325
    },
    {
      "epoch": 0.09739338262753006,
      "grad_norm": 0.43360480666160583,
      "learning_rate": 0.00047573178016726403,
      "loss": 6.8643,
      "step": 326
    },
    {
      "epoch": 0.09769213533497648,
      "grad_norm": 0.37920624017715454,
      "learning_rate": 0.0004756571087216249,
      "loss": 6.8877,
      "step": 327
    },
    {
      "epoch": 0.09799088804242288,
      "grad_norm": 0.2969019114971161,
      "learning_rate": 0.00047558243727598566,
      "loss": 7.5674,
      "step": 328
    },
    {
      "epoch": 0.09828964074986929,
      "grad_norm": 0.35542958974838257,
      "learning_rate": 0.00047550776583034647,
      "loss": 7.251,
      "step": 329
    },
    {
      "epoch": 0.09858839345731571,
      "grad_norm": 0.4044097065925598,
      "learning_rate": 0.0004754330943847073,
      "loss": 6.9648,
      "step": 330
    },
    {
      "epoch": 0.09888714616476212,
      "grad_norm": 0.3543989956378937,
      "learning_rate": 0.0004753584229390681,
      "loss": 7.2373,
      "step": 331
    },
    {
      "epoch": 0.09918589887220854,
      "grad_norm": 0.39775583148002625,
      "learning_rate": 0.0004752837514934289,
      "loss": 6.7412,
      "step": 332
    },
    {
      "epoch": 0.09948465157965494,
      "grad_norm": 0.38995787501335144,
      "learning_rate": 0.0004752090800477897,
      "loss": 7.3545,
      "step": 333
    },
    {
      "epoch": 0.09978340428710135,
      "grad_norm": 0.3022054433822632,
      "learning_rate": 0.0004751344086021506,
      "loss": 7.4277,
      "step": 334
    },
    {
      "epoch": 0.10008215699454777,
      "grad_norm": 0.2856612503528595,
      "learning_rate": 0.00047505973715651135,
      "loss": 7.6006,
      "step": 335
    },
    {
      "epoch": 0.10038090970199418,
      "grad_norm": 0.3730432689189911,
      "learning_rate": 0.00047498506571087216,
      "loss": 7.1973,
      "step": 336
    },
    {
      "epoch": 0.10067966240944058,
      "grad_norm": 0.3948811888694763,
      "learning_rate": 0.000474910394265233,
      "loss": 6.9893,
      "step": 337
    },
    {
      "epoch": 0.100978415116887,
      "grad_norm": 0.31810784339904785,
      "learning_rate": 0.0004748357228195938,
      "loss": 7.5664,
      "step": 338
    },
    {
      "epoch": 0.10127716782433341,
      "grad_norm": 0.33774954080581665,
      "learning_rate": 0.0004747610513739546,
      "loss": 7.0801,
      "step": 339
    },
    {
      "epoch": 0.10157592053177981,
      "grad_norm": 0.3480311632156372,
      "learning_rate": 0.0004746863799283154,
      "loss": 7.2236,
      "step": 340
    },
    {
      "epoch": 0.10187467323922623,
      "grad_norm": 0.4085802733898163,
      "learning_rate": 0.0004746117084826762,
      "loss": 7.1309,
      "step": 341
    },
    {
      "epoch": 0.10217342594667264,
      "grad_norm": 0.3025522232055664,
      "learning_rate": 0.00047453703703703704,
      "loss": 7.1445,
      "step": 342
    },
    {
      "epoch": 0.10247217865411905,
      "grad_norm": 0.4028434753417969,
      "learning_rate": 0.0004744623655913979,
      "loss": 7.0723,
      "step": 343
    },
    {
      "epoch": 0.10277093136156547,
      "grad_norm": 0.34659409523010254,
      "learning_rate": 0.00047438769414575866,
      "loss": 7.4443,
      "step": 344
    },
    {
      "epoch": 0.10306968406901187,
      "grad_norm": 0.33226320147514343,
      "learning_rate": 0.0004743130227001195,
      "loss": 7.542,
      "step": 345
    },
    {
      "epoch": 0.10336843677645828,
      "grad_norm": 0.4248802363872528,
      "learning_rate": 0.0004742383512544803,
      "loss": 7.4424,
      "step": 346
    },
    {
      "epoch": 0.1036671894839047,
      "grad_norm": 0.3419833481311798,
      "learning_rate": 0.0004741636798088411,
      "loss": 7.3096,
      "step": 347
    },
    {
      "epoch": 0.10396594219135111,
      "grad_norm": 0.42626187205314636,
      "learning_rate": 0.0004740890083632019,
      "loss": 6.5156,
      "step": 348
    },
    {
      "epoch": 0.10426469489879751,
      "grad_norm": 0.3002922534942627,
      "learning_rate": 0.00047401433691756273,
      "loss": 7.5244,
      "step": 349
    },
    {
      "epoch": 0.10456344760624393,
      "grad_norm": 0.30830204486846924,
      "learning_rate": 0.00047393966547192354,
      "loss": 7.0938,
      "step": 350
    },
    {
      "epoch": 0.10486220031369034,
      "grad_norm": 0.3372485339641571,
      "learning_rate": 0.00047386499402628435,
      "loss": 7.4814,
      "step": 351
    },
    {
      "epoch": 0.10516095302113676,
      "grad_norm": 0.39707690477371216,
      "learning_rate": 0.00047379032258064517,
      "loss": 6.9561,
      "step": 352
    },
    {
      "epoch": 0.10545970572858317,
      "grad_norm": 0.32670754194259644,
      "learning_rate": 0.000473715651135006,
      "loss": 7.4912,
      "step": 353
    },
    {
      "epoch": 0.10575845843602957,
      "grad_norm": 0.32411718368530273,
      "learning_rate": 0.0004736409796893668,
      "loss": 7.1201,
      "step": 354
    },
    {
      "epoch": 0.106057211143476,
      "grad_norm": 0.30876603722572327,
      "learning_rate": 0.0004735663082437276,
      "loss": 7.6797,
      "step": 355
    },
    {
      "epoch": 0.1063559638509224,
      "grad_norm": 0.3555131256580353,
      "learning_rate": 0.0004734916367980884,
      "loss": 7.4648,
      "step": 356
    },
    {
      "epoch": 0.1066547165583688,
      "grad_norm": 0.38018181920051575,
      "learning_rate": 0.00047341696535244923,
      "loss": 7.1465,
      "step": 357
    },
    {
      "epoch": 0.10695346926581523,
      "grad_norm": 0.47448158264160156,
      "learning_rate": 0.00047334229390681004,
      "loss": 6.9893,
      "step": 358
    },
    {
      "epoch": 0.10725222197326163,
      "grad_norm": 0.3358205258846283,
      "learning_rate": 0.00047326762246117086,
      "loss": 7.1406,
      "step": 359
    },
    {
      "epoch": 0.10755097468070804,
      "grad_norm": 0.33809614181518555,
      "learning_rate": 0.00047319295101553167,
      "loss": 7.1885,
      "step": 360
    },
    {
      "epoch": 0.10784972738815446,
      "grad_norm": 0.3168056905269623,
      "learning_rate": 0.0004731182795698925,
      "loss": 7.5479,
      "step": 361
    },
    {
      "epoch": 0.10814848009560087,
      "grad_norm": 0.46827954053878784,
      "learning_rate": 0.0004730436081242533,
      "loss": 6.7139,
      "step": 362
    },
    {
      "epoch": 0.10844723280304727,
      "grad_norm": 0.40343886613845825,
      "learning_rate": 0.0004729689366786141,
      "loss": 6.8506,
      "step": 363
    },
    {
      "epoch": 0.10874598551049369,
      "grad_norm": 0.3390718102455139,
      "learning_rate": 0.0004728942652329749,
      "loss": 7.123,
      "step": 364
    },
    {
      "epoch": 0.1090447382179401,
      "grad_norm": 0.43942832946777344,
      "learning_rate": 0.00047281959378733574,
      "loss": 7.5029,
      "step": 365
    },
    {
      "epoch": 0.1093434909253865,
      "grad_norm": 0.3825077414512634,
      "learning_rate": 0.00047274492234169655,
      "loss": 7.2178,
      "step": 366
    },
    {
      "epoch": 0.10964224363283293,
      "grad_norm": 0.36402952671051025,
      "learning_rate": 0.00047267025089605736,
      "loss": 7.2979,
      "step": 367
    },
    {
      "epoch": 0.10994099634027933,
      "grad_norm": 0.36446794867515564,
      "learning_rate": 0.0004725955794504181,
      "loss": 7.251,
      "step": 368
    },
    {
      "epoch": 0.11023974904772575,
      "grad_norm": 0.3983413875102997,
      "learning_rate": 0.000472520908004779,
      "loss": 6.7393,
      "step": 369
    },
    {
      "epoch": 0.11053850175517216,
      "grad_norm": 0.3563667833805084,
      "learning_rate": 0.0004724462365591398,
      "loss": 6.9756,
      "step": 370
    },
    {
      "epoch": 0.11083725446261856,
      "grad_norm": 0.34414103627204895,
      "learning_rate": 0.0004723715651135006,
      "loss": 7.2285,
      "step": 371
    },
    {
      "epoch": 0.11113600717006498,
      "grad_norm": 0.4532737731933594,
      "learning_rate": 0.0004722968936678614,
      "loss": 6.9023,
      "step": 372
    },
    {
      "epoch": 0.11143475987751139,
      "grad_norm": 0.296292245388031,
      "learning_rate": 0.00047222222222222224,
      "loss": 7.3359,
      "step": 373
    },
    {
      "epoch": 0.1117335125849578,
      "grad_norm": 0.2869156301021576,
      "learning_rate": 0.00047214755077658305,
      "loss": 7.3701,
      "step": 374
    },
    {
      "epoch": 0.11203226529240422,
      "grad_norm": 0.3389177620410919,
      "learning_rate": 0.00047207287933094386,
      "loss": 7.3691,
      "step": 375
    },
    {
      "epoch": 0.11233101799985062,
      "grad_norm": 0.34489476680755615,
      "learning_rate": 0.0004719982078853047,
      "loss": 7.4756,
      "step": 376
    },
    {
      "epoch": 0.11262977070729703,
      "grad_norm": 0.31314054131507874,
      "learning_rate": 0.00047192353643966544,
      "loss": 7.6172,
      "step": 377
    },
    {
      "epoch": 0.11292852341474345,
      "grad_norm": 0.34616023302078247,
      "learning_rate": 0.0004718488649940263,
      "loss": 7.6846,
      "step": 378
    },
    {
      "epoch": 0.11322727612218986,
      "grad_norm": 0.3795040249824524,
      "learning_rate": 0.0004717741935483871,
      "loss": 7.3447,
      "step": 379
    },
    {
      "epoch": 0.11352602882963626,
      "grad_norm": 0.34627050161361694,
      "learning_rate": 0.00047169952210274793,
      "loss": 7.3418,
      "step": 380
    },
    {
      "epoch": 0.11382478153708268,
      "grad_norm": 0.47329452633857727,
      "learning_rate": 0.00047162485065710874,
      "loss": 6.9062,
      "step": 381
    },
    {
      "epoch": 0.11412353424452909,
      "grad_norm": 0.390525758266449,
      "learning_rate": 0.00047155017921146955,
      "loss": 6.7793,
      "step": 382
    },
    {
      "epoch": 0.1144222869519755,
      "grad_norm": 0.33880671858787537,
      "learning_rate": 0.00047147550776583037,
      "loss": 7.083,
      "step": 383
    },
    {
      "epoch": 0.11472103965942192,
      "grad_norm": 0.436779260635376,
      "learning_rate": 0.0004714008363201911,
      "loss": 7.0723,
      "step": 384
    },
    {
      "epoch": 0.11501979236686832,
      "grad_norm": 0.31517294049263,
      "learning_rate": 0.000471326164874552,
      "loss": 7.5107,
      "step": 385
    },
    {
      "epoch": 0.11531854507431473,
      "grad_norm": 0.35628557205200195,
      "learning_rate": 0.00047125149342891275,
      "loss": 7.3232,
      "step": 386
    },
    {
      "epoch": 0.11561729778176115,
      "grad_norm": 0.3770134747028351,
      "learning_rate": 0.0004711768219832736,
      "loss": 7.6406,
      "step": 387
    },
    {
      "epoch": 0.11591605048920756,
      "grad_norm": 0.3829798698425293,
      "learning_rate": 0.00047110215053763443,
      "loss": 7.2451,
      "step": 388
    },
    {
      "epoch": 0.11621480319665398,
      "grad_norm": 0.41653376817703247,
      "learning_rate": 0.00047102747909199524,
      "loss": 7.0498,
      "step": 389
    },
    {
      "epoch": 0.11651355590410038,
      "grad_norm": 0.3544130325317383,
      "learning_rate": 0.00047095280764635606,
      "loss": 7.0684,
      "step": 390
    },
    {
      "epoch": 0.11681230861154679,
      "grad_norm": 0.3239371180534363,
      "learning_rate": 0.00047087813620071687,
      "loss": 7.0918,
      "step": 391
    },
    {
      "epoch": 0.11711106131899321,
      "grad_norm": 0.3773457109928131,
      "learning_rate": 0.0004708034647550777,
      "loss": 7.2773,
      "step": 392
    },
    {
      "epoch": 0.11740981402643962,
      "grad_norm": 0.3058832883834839,
      "learning_rate": 0.00047072879330943844,
      "loss": 7.5107,
      "step": 393
    },
    {
      "epoch": 0.11770856673388602,
      "grad_norm": 0.38404345512390137,
      "learning_rate": 0.0004706541218637993,
      "loss": 6.7852,
      "step": 394
    },
    {
      "epoch": 0.11800731944133244,
      "grad_norm": 0.4163118004798889,
      "learning_rate": 0.00047057945041816007,
      "loss": 6.7666,
      "step": 395
    },
    {
      "epoch": 0.11830607214877885,
      "grad_norm": 0.2919059693813324,
      "learning_rate": 0.00047050477897252093,
      "loss": 7.4551,
      "step": 396
    },
    {
      "epoch": 0.11860482485622525,
      "grad_norm": 0.41334113478660583,
      "learning_rate": 0.00047043010752688175,
      "loss": 6.9238,
      "step": 397
    },
    {
      "epoch": 0.11890357756367168,
      "grad_norm": 0.330368310213089,
      "learning_rate": 0.00047035543608124256,
      "loss": 7.8213,
      "step": 398
    },
    {
      "epoch": 0.11920233027111808,
      "grad_norm": 0.3507167100906372,
      "learning_rate": 0.00047028076463560337,
      "loss": 7.3389,
      "step": 399
    },
    {
      "epoch": 0.11950108297856449,
      "grad_norm": 0.33408859372138977,
      "learning_rate": 0.00047020609318996413,
      "loss": 7.5156,
      "step": 400
    },
    {
      "epoch": 0.11950108297856449,
      "eval_bleu": 0.06314290633984992,
      "eval_loss": 7.07421875,
      "eval_runtime": 615.7724,
      "eval_samples_per_second": 2.288,
      "eval_steps_per_second": 0.145,
      "step": 400
    },
    {
      "epoch": 0.11979983568601091,
      "grad_norm": 0.3407604992389679,
      "learning_rate": 0.000470131421744325,
      "loss": 7.0703,
      "step": 401
    },
    {
      "epoch": 0.12009858839345731,
      "grad_norm": 0.352294921875,
      "learning_rate": 0.00047005675029868576,
      "loss": 7.248,
      "step": 402
    },
    {
      "epoch": 0.12039734110090372,
      "grad_norm": 0.4790753126144409,
      "learning_rate": 0.0004699820788530466,
      "loss": 6.5674,
      "step": 403
    },
    {
      "epoch": 0.12069609380835014,
      "grad_norm": 0.34463468194007874,
      "learning_rate": 0.0004699074074074074,
      "loss": 7.1855,
      "step": 404
    },
    {
      "epoch": 0.12099484651579655,
      "grad_norm": 0.34944072365760803,
      "learning_rate": 0.00046983273596176825,
      "loss": 7.2295,
      "step": 405
    },
    {
      "epoch": 0.12129359922324295,
      "grad_norm": 0.3610077202320099,
      "learning_rate": 0.00046975806451612906,
      "loss": 7.0049,
      "step": 406
    },
    {
      "epoch": 0.12159235193068937,
      "grad_norm": 0.3886076807975769,
      "learning_rate": 0.0004696833930704899,
      "loss": 7.4717,
      "step": 407
    },
    {
      "epoch": 0.12189110463813578,
      "grad_norm": 0.4166484475135803,
      "learning_rate": 0.0004696087216248507,
      "loss": 7.3281,
      "step": 408
    },
    {
      "epoch": 0.1221898573455822,
      "grad_norm": 0.37506043910980225,
      "learning_rate": 0.00046953405017921145,
      "loss": 7.4717,
      "step": 409
    },
    {
      "epoch": 0.12248861005302861,
      "grad_norm": 0.39027926325798035,
      "learning_rate": 0.0004694593787335723,
      "loss": 7.1924,
      "step": 410
    },
    {
      "epoch": 0.12278736276047501,
      "grad_norm": 0.3593742251396179,
      "learning_rate": 0.0004693847072879331,
      "loss": 7.2305,
      "step": 411
    },
    {
      "epoch": 0.12308611546792143,
      "grad_norm": 0.4341390132904053,
      "learning_rate": 0.00046931003584229394,
      "loss": 7.0459,
      "step": 412
    },
    {
      "epoch": 0.12338486817536784,
      "grad_norm": 0.45067688822746277,
      "learning_rate": 0.0004692353643966547,
      "loss": 7.1045,
      "step": 413
    },
    {
      "epoch": 0.12368362088281425,
      "grad_norm": 0.5502626299858093,
      "learning_rate": 0.00046916069295101557,
      "loss": 7.3369,
      "step": 414
    },
    {
      "epoch": 0.12398237359026067,
      "grad_norm": 0.33474794030189514,
      "learning_rate": 0.0004690860215053764,
      "loss": 7.4951,
      "step": 415
    },
    {
      "epoch": 0.12428112629770707,
      "grad_norm": 0.39838525652885437,
      "learning_rate": 0.00046901135005973714,
      "loss": 7.2979,
      "step": 416
    },
    {
      "epoch": 0.12457987900515348,
      "grad_norm": 0.37522467970848083,
      "learning_rate": 0.000468936678614098,
      "loss": 7.3535,
      "step": 417
    },
    {
      "epoch": 0.1248786317125999,
      "grad_norm": 0.38580048084259033,
      "learning_rate": 0.00046886200716845876,
      "loss": 7.335,
      "step": 418
    },
    {
      "epoch": 0.1251773844200463,
      "grad_norm": 0.42477577924728394,
      "learning_rate": 0.00046878733572281963,
      "loss": 7.3984,
      "step": 419
    },
    {
      "epoch": 0.12547613712749273,
      "grad_norm": 0.369338721036911,
      "learning_rate": 0.0004687126642771804,
      "loss": 7.3545,
      "step": 420
    },
    {
      "epoch": 0.12577488983493912,
      "grad_norm": 0.28646552562713623,
      "learning_rate": 0.00046863799283154126,
      "loss": 7.3369,
      "step": 421
    },
    {
      "epoch": 0.12607364254238554,
      "grad_norm": 0.33757954835891724,
      "learning_rate": 0.000468563321385902,
      "loss": 7.4355,
      "step": 422
    },
    {
      "epoch": 0.12637239524983196,
      "grad_norm": 0.43328431248664856,
      "learning_rate": 0.0004684886499402629,
      "loss": 6.9902,
      "step": 423
    },
    {
      "epoch": 0.12667114795727835,
      "grad_norm": 0.2920494079589844,
      "learning_rate": 0.00046841397849462364,
      "loss": 7.4912,
      "step": 424
    },
    {
      "epoch": 0.12696990066472477,
      "grad_norm": 0.39742282032966614,
      "learning_rate": 0.00046833930704898445,
      "loss": 6.8477,
      "step": 425
    },
    {
      "epoch": 0.1272686533721712,
      "grad_norm": 0.3749666213989258,
      "learning_rate": 0.0004682646356033453,
      "loss": 6.9873,
      "step": 426
    },
    {
      "epoch": 0.12756740607961758,
      "grad_norm": 0.35190659761428833,
      "learning_rate": 0.0004681899641577061,
      "loss": 7.4014,
      "step": 427
    },
    {
      "epoch": 0.127866158787064,
      "grad_norm": 0.42471548914909363,
      "learning_rate": 0.00046811529271206695,
      "loss": 7.2129,
      "step": 428
    },
    {
      "epoch": 0.12816491149451043,
      "grad_norm": 0.3918190002441406,
      "learning_rate": 0.0004680406212664277,
      "loss": 7.166,
      "step": 429
    },
    {
      "epoch": 0.12846366420195682,
      "grad_norm": 0.36660444736480713,
      "learning_rate": 0.00046796594982078857,
      "loss": 7.2119,
      "step": 430
    },
    {
      "epoch": 0.12876241690940324,
      "grad_norm": 0.3479898273944855,
      "learning_rate": 0.00046789127837514933,
      "loss": 7.7314,
      "step": 431
    },
    {
      "epoch": 0.12906116961684966,
      "grad_norm": 0.3594509959220886,
      "learning_rate": 0.00046781660692951014,
      "loss": 7.3701,
      "step": 432
    },
    {
      "epoch": 0.12935992232429605,
      "grad_norm": 0.3519911468029022,
      "learning_rate": 0.00046774193548387096,
      "loss": 7.668,
      "step": 433
    },
    {
      "epoch": 0.12965867503174247,
      "grad_norm": 0.46653422713279724,
      "learning_rate": 0.00046766726403823177,
      "loss": 6.6914,
      "step": 434
    },
    {
      "epoch": 0.1299574277391889,
      "grad_norm": 0.4118472635746002,
      "learning_rate": 0.00046759259259259264,
      "loss": 7.2285,
      "step": 435
    },
    {
      "epoch": 0.1302561804466353,
      "grad_norm": 0.39476075768470764,
      "learning_rate": 0.0004675179211469534,
      "loss": 7.166,
      "step": 436
    },
    {
      "epoch": 0.1305549331540817,
      "grad_norm": 0.45234039425849915,
      "learning_rate": 0.00046744324970131426,
      "loss": 7.0479,
      "step": 437
    },
    {
      "epoch": 0.13085368586152812,
      "grad_norm": 0.3527635335922241,
      "learning_rate": 0.000467368578255675,
      "loss": 7.4775,
      "step": 438
    },
    {
      "epoch": 0.13115243856897454,
      "grad_norm": 0.4166572093963623,
      "learning_rate": 0.0004672939068100359,
      "loss": 7.4004,
      "step": 439
    },
    {
      "epoch": 0.13145119127642094,
      "grad_norm": 0.5076642632484436,
      "learning_rate": 0.00046721923536439665,
      "loss": 7.1572,
      "step": 440
    },
    {
      "epoch": 0.13174994398386736,
      "grad_norm": 0.4013204872608185,
      "learning_rate": 0.00046714456391875746,
      "loss": 7.0508,
      "step": 441
    },
    {
      "epoch": 0.13204869669131378,
      "grad_norm": 0.319581001996994,
      "learning_rate": 0.00046706989247311827,
      "loss": 7.5166,
      "step": 442
    },
    {
      "epoch": 0.13234744939876017,
      "grad_norm": 0.3800637125968933,
      "learning_rate": 0.0004669952210274791,
      "loss": 7.124,
      "step": 443
    },
    {
      "epoch": 0.1326462021062066,
      "grad_norm": 0.34482765197753906,
      "learning_rate": 0.00046692054958183995,
      "loss": 7.2949,
      "step": 444
    },
    {
      "epoch": 0.132944954813653,
      "grad_norm": 0.3178139925003052,
      "learning_rate": 0.0004668458781362007,
      "loss": 7.7031,
      "step": 445
    },
    {
      "epoch": 0.1332437075210994,
      "grad_norm": 0.6007941365242004,
      "learning_rate": 0.0004667712066905616,
      "loss": 6.5684,
      "step": 446
    },
    {
      "epoch": 0.13354246022854582,
      "grad_norm": 0.38168299198150635,
      "learning_rate": 0.00046669653524492234,
      "loss": 7.3184,
      "step": 447
    },
    {
      "epoch": 0.13384121293599224,
      "grad_norm": 0.3468656539916992,
      "learning_rate": 0.00046662186379928315,
      "loss": 7.0537,
      "step": 448
    },
    {
      "epoch": 0.13413996564343864,
      "grad_norm": 0.3573014438152313,
      "learning_rate": 0.00046654719235364396,
      "loss": 7.2559,
      "step": 449
    },
    {
      "epoch": 0.13443871835088506,
      "grad_norm": 0.37460553646087646,
      "learning_rate": 0.0004664725209080048,
      "loss": 7.3516,
      "step": 450
    },
    {
      "epoch": 0.13473747105833148,
      "grad_norm": 0.37861835956573486,
      "learning_rate": 0.0004663978494623656,
      "loss": 7.1768,
      "step": 451
    },
    {
      "epoch": 0.13503622376577787,
      "grad_norm": 0.3449447751045227,
      "learning_rate": 0.0004663231780167264,
      "loss": 7.4678,
      "step": 452
    },
    {
      "epoch": 0.1353349764732243,
      "grad_norm": 0.3241126835346222,
      "learning_rate": 0.00046624850657108727,
      "loss": 7.3457,
      "step": 453
    },
    {
      "epoch": 0.1356337291806707,
      "grad_norm": 0.3104764521121979,
      "learning_rate": 0.000466173835125448,
      "loss": 7.3584,
      "step": 454
    },
    {
      "epoch": 0.1359324818881171,
      "grad_norm": 0.4853258430957794,
      "learning_rate": 0.0004660991636798089,
      "loss": 6.4795,
      "step": 455
    },
    {
      "epoch": 0.13623123459556352,
      "grad_norm": 0.3746856451034546,
      "learning_rate": 0.00046602449223416965,
      "loss": 7.5654,
      "step": 456
    },
    {
      "epoch": 0.13652998730300994,
      "grad_norm": 0.3340097963809967,
      "learning_rate": 0.00046594982078853047,
      "loss": 7.1611,
      "step": 457
    },
    {
      "epoch": 0.13682874001045633,
      "grad_norm": 0.3395751714706421,
      "learning_rate": 0.0004658751493428913,
      "loss": 7.1582,
      "step": 458
    },
    {
      "epoch": 0.13712749271790275,
      "grad_norm": 0.3319547772407532,
      "learning_rate": 0.0004658004778972521,
      "loss": 7.3584,
      "step": 459
    },
    {
      "epoch": 0.13742624542534917,
      "grad_norm": 0.34483182430267334,
      "learning_rate": 0.0004657258064516129,
      "loss": 7.5312,
      "step": 460
    },
    {
      "epoch": 0.13772499813279557,
      "grad_norm": 0.37583398818969727,
      "learning_rate": 0.0004656511350059737,
      "loss": 7.127,
      "step": 461
    },
    {
      "epoch": 0.138023750840242,
      "grad_norm": 0.3963184952735901,
      "learning_rate": 0.0004655764635603346,
      "loss": 6.9551,
      "step": 462
    },
    {
      "epoch": 0.1383225035476884,
      "grad_norm": 0.31748080253601074,
      "learning_rate": 0.00046550179211469534,
      "loss": 7.4551,
      "step": 463
    },
    {
      "epoch": 0.1386212562551348,
      "grad_norm": 0.3935743272304535,
      "learning_rate": 0.00046542712066905616,
      "loss": 7.5693,
      "step": 464
    },
    {
      "epoch": 0.13892000896258122,
      "grad_norm": 0.3253386914730072,
      "learning_rate": 0.00046535244922341697,
      "loss": 7.0908,
      "step": 465
    },
    {
      "epoch": 0.13921876167002764,
      "grad_norm": 0.3798268735408783,
      "learning_rate": 0.0004652777777777778,
      "loss": 7.4902,
      "step": 466
    },
    {
      "epoch": 0.13951751437747403,
      "grad_norm": 0.31957101821899414,
      "learning_rate": 0.0004652031063321386,
      "loss": 7.542,
      "step": 467
    },
    {
      "epoch": 0.13981626708492045,
      "grad_norm": 0.4815508723258972,
      "learning_rate": 0.0004651284348864994,
      "loss": 6.9668,
      "step": 468
    },
    {
      "epoch": 0.14011501979236687,
      "grad_norm": 0.4467724561691284,
      "learning_rate": 0.0004650537634408602,
      "loss": 7.0322,
      "step": 469
    },
    {
      "epoch": 0.14041377249981327,
      "grad_norm": 0.489765465259552,
      "learning_rate": 0.00046497909199522103,
      "loss": 7.1465,
      "step": 470
    },
    {
      "epoch": 0.1407125252072597,
      "grad_norm": 0.3792406916618347,
      "learning_rate": 0.0004649044205495819,
      "loss": 7.0361,
      "step": 471
    },
    {
      "epoch": 0.1410112779147061,
      "grad_norm": 0.44332659244537354,
      "learning_rate": 0.00046482974910394266,
      "loss": 7.3105,
      "step": 472
    },
    {
      "epoch": 0.14131003062215253,
      "grad_norm": 0.39608466625213623,
      "learning_rate": 0.00046475507765830347,
      "loss": 7.3779,
      "step": 473
    },
    {
      "epoch": 0.14160878332959892,
      "grad_norm": 0.42770296335220337,
      "learning_rate": 0.0004646804062126643,
      "loss": 7.3154,
      "step": 474
    },
    {
      "epoch": 0.14190753603704534,
      "grad_norm": 0.36429211497306824,
      "learning_rate": 0.0004646057347670251,
      "loss": 7.459,
      "step": 475
    },
    {
      "epoch": 0.14220628874449176,
      "grad_norm": 0.37419646978378296,
      "learning_rate": 0.0004645310633213859,
      "loss": 7.5225,
      "step": 476
    },
    {
      "epoch": 0.14250504145193815,
      "grad_norm": 0.36639687418937683,
      "learning_rate": 0.0004644563918757467,
      "loss": 7.3213,
      "step": 477
    },
    {
      "epoch": 0.14280379415938457,
      "grad_norm": 0.3705472946166992,
      "learning_rate": 0.00046438172043010754,
      "loss": 7.4824,
      "step": 478
    },
    {
      "epoch": 0.143102546866831,
      "grad_norm": 0.4703329801559448,
      "learning_rate": 0.00046430704898446835,
      "loss": 7.0781,
      "step": 479
    },
    {
      "epoch": 0.14340129957427739,
      "grad_norm": 0.4923274517059326,
      "learning_rate": 0.00046423237753882916,
      "loss": 6.5352,
      "step": 480
    },
    {
      "epoch": 0.1437000522817238,
      "grad_norm": 0.4432821273803711,
      "learning_rate": 0.00046415770609319,
      "loss": 6.8555,
      "step": 481
    },
    {
      "epoch": 0.14399880498917023,
      "grad_norm": 0.4010286331176758,
      "learning_rate": 0.0004640830346475508,
      "loss": 7.2812,
      "step": 482
    },
    {
      "epoch": 0.14429755769661662,
      "grad_norm": 0.34480640292167664,
      "learning_rate": 0.0004640083632019116,
      "loss": 7.4512,
      "step": 483
    },
    {
      "epoch": 0.14459631040406304,
      "grad_norm": 0.3909822702407837,
      "learning_rate": 0.0004639336917562724,
      "loss": 6.8369,
      "step": 484
    },
    {
      "epoch": 0.14489506311150946,
      "grad_norm": 0.35184359550476074,
      "learning_rate": 0.0004638590203106332,
      "loss": 7.3018,
      "step": 485
    },
    {
      "epoch": 0.14519381581895585,
      "grad_norm": 0.47893083095550537,
      "learning_rate": 0.00046378434886499404,
      "loss": 7.0566,
      "step": 486
    },
    {
      "epoch": 0.14549256852640227,
      "grad_norm": 0.4085986614227295,
      "learning_rate": 0.00046370967741935485,
      "loss": 6.96,
      "step": 487
    },
    {
      "epoch": 0.1457913212338487,
      "grad_norm": 0.36285147070884705,
      "learning_rate": 0.00046363500597371566,
      "loss": 7.5215,
      "step": 488
    },
    {
      "epoch": 0.14609007394129508,
      "grad_norm": 0.39923295378685,
      "learning_rate": 0.0004635603345280765,
      "loss": 7.29,
      "step": 489
    },
    {
      "epoch": 0.1463888266487415,
      "grad_norm": 0.4298717677593231,
      "learning_rate": 0.0004634856630824373,
      "loss": 6.9131,
      "step": 490
    },
    {
      "epoch": 0.14668757935618792,
      "grad_norm": 0.3964043855667114,
      "learning_rate": 0.0004634109916367981,
      "loss": 7.3252,
      "step": 491
    },
    {
      "epoch": 0.14698633206363432,
      "grad_norm": 0.34743672609329224,
      "learning_rate": 0.0004633363201911589,
      "loss": 7.1904,
      "step": 492
    },
    {
      "epoch": 0.14728508477108074,
      "grad_norm": 0.3526354134082794,
      "learning_rate": 0.00046326164874551973,
      "loss": 7.5615,
      "step": 493
    },
    {
      "epoch": 0.14758383747852716,
      "grad_norm": 0.4242677688598633,
      "learning_rate": 0.00046318697729988054,
      "loss": 7.4014,
      "step": 494
    },
    {
      "epoch": 0.14788259018597355,
      "grad_norm": 0.3925628960132599,
      "learning_rate": 0.00046311230585424135,
      "loss": 7.2998,
      "step": 495
    },
    {
      "epoch": 0.14818134289341997,
      "grad_norm": 0.3802424669265747,
      "learning_rate": 0.0004630376344086021,
      "loss": 7.2988,
      "step": 496
    },
    {
      "epoch": 0.1484800956008664,
      "grad_norm": 0.3823290169239044,
      "learning_rate": 0.000462962962962963,
      "loss": 7.3926,
      "step": 497
    },
    {
      "epoch": 0.14877884830831278,
      "grad_norm": 0.3304677903652191,
      "learning_rate": 0.0004628882915173238,
      "loss": 7.6025,
      "step": 498
    },
    {
      "epoch": 0.1490776010157592,
      "grad_norm": 0.4154568314552307,
      "learning_rate": 0.0004628136200716846,
      "loss": 7.291,
      "step": 499
    },
    {
      "epoch": 0.14937635372320562,
      "grad_norm": 0.4527128338813782,
      "learning_rate": 0.0004627389486260454,
      "loss": 7.3662,
      "step": 500
    },
    {
      "epoch": 0.14967510643065202,
      "grad_norm": 0.4651779234409332,
      "learning_rate": 0.00046266427718040623,
      "loss": 7.3271,
      "step": 501
    },
    {
      "epoch": 0.14997385913809844,
      "grad_norm": 0.43253764510154724,
      "learning_rate": 0.00046258960573476705,
      "loss": 7.2441,
      "step": 502
    },
    {
      "epoch": 0.15027261184554486,
      "grad_norm": 0.41578447818756104,
      "learning_rate": 0.00046251493428912786,
      "loss": 7.3076,
      "step": 503
    },
    {
      "epoch": 0.15057136455299125,
      "grad_norm": 0.33692219853401184,
      "learning_rate": 0.00046244026284348867,
      "loss": 7.5645,
      "step": 504
    },
    {
      "epoch": 0.15087011726043767,
      "grad_norm": 0.40838131308555603,
      "learning_rate": 0.00046236559139784943,
      "loss": 7.1182,
      "step": 505
    },
    {
      "epoch": 0.1511688699678841,
      "grad_norm": 0.4092601537704468,
      "learning_rate": 0.0004622909199522103,
      "loss": 6.8379,
      "step": 506
    },
    {
      "epoch": 0.15146762267533048,
      "grad_norm": 0.4096202850341797,
      "learning_rate": 0.0004622162485065711,
      "loss": 7.0781,
      "step": 507
    },
    {
      "epoch": 0.1517663753827769,
      "grad_norm": 0.3555811941623688,
      "learning_rate": 0.0004621415770609319,
      "loss": 7.2588,
      "step": 508
    },
    {
      "epoch": 0.15206512809022332,
      "grad_norm": 0.32640114426612854,
      "learning_rate": 0.00046206690561529274,
      "loss": 7.5889,
      "step": 509
    },
    {
      "epoch": 0.15236388079766972,
      "grad_norm": 0.35323473811149597,
      "learning_rate": 0.00046199223416965355,
      "loss": 7.3721,
      "step": 510
    },
    {
      "epoch": 0.15266263350511614,
      "grad_norm": 0.3315449059009552,
      "learning_rate": 0.00046191756272401436,
      "loss": 7.4639,
      "step": 511
    },
    {
      "epoch": 0.15296138621256256,
      "grad_norm": 0.4315294921398163,
      "learning_rate": 0.0004618428912783751,
      "loss": 7.5361,
      "step": 512
    },
    {
      "epoch": 0.15326013892000898,
      "grad_norm": 0.43292179703712463,
      "learning_rate": 0.000461768219832736,
      "loss": 7.292,
      "step": 513
    },
    {
      "epoch": 0.15355889162745537,
      "grad_norm": 0.4063829779624939,
      "learning_rate": 0.00046169354838709675,
      "loss": 7.0205,
      "step": 514
    },
    {
      "epoch": 0.1538576443349018,
      "grad_norm": 0.3746349513530731,
      "learning_rate": 0.0004616188769414576,
      "loss": 7.1104,
      "step": 515
    },
    {
      "epoch": 0.1541563970423482,
      "grad_norm": 0.4410933554172516,
      "learning_rate": 0.00046154420549581837,
      "loss": 6.8438,
      "step": 516
    },
    {
      "epoch": 0.1544551497497946,
      "grad_norm": 0.3735065162181854,
      "learning_rate": 0.00046146953405017924,
      "loss": 6.873,
      "step": 517
    },
    {
      "epoch": 0.15475390245724102,
      "grad_norm": 0.38182929158210754,
      "learning_rate": 0.00046139486260454005,
      "loss": 7.4883,
      "step": 518
    },
    {
      "epoch": 0.15505265516468744,
      "grad_norm": 0.34415221214294434,
      "learning_rate": 0.00046132019115890086,
      "loss": 7.5508,
      "step": 519
    },
    {
      "epoch": 0.15535140787213383,
      "grad_norm": 0.3995930850505829,
      "learning_rate": 0.0004612455197132617,
      "loss": 7.2061,
      "step": 520
    },
    {
      "epoch": 0.15565016057958025,
      "grad_norm": 0.37925347685813904,
      "learning_rate": 0.00046117084826762244,
      "loss": 7.252,
      "step": 521
    },
    {
      "epoch": 0.15594891328702667,
      "grad_norm": 0.46988874673843384,
      "learning_rate": 0.0004610961768219833,
      "loss": 7.207,
      "step": 522
    },
    {
      "epoch": 0.15624766599447307,
      "grad_norm": 0.39252549409866333,
      "learning_rate": 0.00046102150537634406,
      "loss": 7.3164,
      "step": 523
    },
    {
      "epoch": 0.1565464187019195,
      "grad_norm": 0.4602091610431671,
      "learning_rate": 0.00046094683393070493,
      "loss": 7.0176,
      "step": 524
    },
    {
      "epoch": 0.1568451714093659,
      "grad_norm": 0.4840177595615387,
      "learning_rate": 0.0004608721624850657,
      "loss": 6.6836,
      "step": 525
    },
    {
      "epoch": 0.1571439241168123,
      "grad_norm": 0.41737475991249084,
      "learning_rate": 0.00046079749103942655,
      "loss": 7.4014,
      "step": 526
    },
    {
      "epoch": 0.15744267682425872,
      "grad_norm": 0.33526843786239624,
      "learning_rate": 0.00046072281959378737,
      "loss": 7.7256,
      "step": 527
    },
    {
      "epoch": 0.15774142953170514,
      "grad_norm": 0.5068597793579102,
      "learning_rate": 0.0004606481481481481,
      "loss": 6.2852,
      "step": 528
    },
    {
      "epoch": 0.15804018223915153,
      "grad_norm": 0.3731829524040222,
      "learning_rate": 0.000460573476702509,
      "loss": 7.2822,
      "step": 529
    },
    {
      "epoch": 0.15833893494659795,
      "grad_norm": 0.35565051436424255,
      "learning_rate": 0.00046049880525686975,
      "loss": 7.4336,
      "step": 530
    },
    {
      "epoch": 0.15863768765404437,
      "grad_norm": 0.3721979558467865,
      "learning_rate": 0.0004604241338112306,
      "loss": 7.0195,
      "step": 531
    },
    {
      "epoch": 0.15893644036149077,
      "grad_norm": 0.3453672528266907,
      "learning_rate": 0.0004603494623655914,
      "loss": 7.6523,
      "step": 532
    },
    {
      "epoch": 0.1592351930689372,
      "grad_norm": 0.3656001687049866,
      "learning_rate": 0.00046027479091995224,
      "loss": 7.6797,
      "step": 533
    },
    {
      "epoch": 0.1595339457763836,
      "grad_norm": 0.39576786756515503,
      "learning_rate": 0.000460200119474313,
      "loss": 7.418,
      "step": 534
    },
    {
      "epoch": 0.15983269848383,
      "grad_norm": 0.39797618985176086,
      "learning_rate": 0.00046012544802867387,
      "loss": 7.4248,
      "step": 535
    },
    {
      "epoch": 0.16013145119127642,
      "grad_norm": 0.4094032347202301,
      "learning_rate": 0.0004600507765830347,
      "loss": 7.4854,
      "step": 536
    },
    {
      "epoch": 0.16043020389872284,
      "grad_norm": 0.38115039467811584,
      "learning_rate": 0.00045997610513739544,
      "loss": 7.5127,
      "step": 537
    },
    {
      "epoch": 0.16072895660616923,
      "grad_norm": 0.4406241774559021,
      "learning_rate": 0.0004599014336917563,
      "loss": 7.0752,
      "step": 538
    },
    {
      "epoch": 0.16102770931361565,
      "grad_norm": 0.3622308075428009,
      "learning_rate": 0.00045982676224611707,
      "loss": 7.3613,
      "step": 539
    },
    {
      "epoch": 0.16132646202106207,
      "grad_norm": 0.49825045466423035,
      "learning_rate": 0.00045975209080047793,
      "loss": 6.6807,
      "step": 540
    },
    {
      "epoch": 0.16162521472850847,
      "grad_norm": 0.38409075140953064,
      "learning_rate": 0.0004596774193548387,
      "loss": 7.2686,
      "step": 541
    },
    {
      "epoch": 0.16192396743595489,
      "grad_norm": 0.3863038122653961,
      "learning_rate": 0.00045960274790919956,
      "loss": 7.5938,
      "step": 542
    },
    {
      "epoch": 0.1622227201434013,
      "grad_norm": 0.4190234839916229,
      "learning_rate": 0.0004595280764635603,
      "loss": 7.2627,
      "step": 543
    },
    {
      "epoch": 0.1625214728508477,
      "grad_norm": 0.3243887722492218,
      "learning_rate": 0.00045945340501792113,
      "loss": 7.5244,
      "step": 544
    },
    {
      "epoch": 0.16282022555829412,
      "grad_norm": 0.4027411937713623,
      "learning_rate": 0.000459378733572282,
      "loss": 7.3145,
      "step": 545
    },
    {
      "epoch": 0.16311897826574054,
      "grad_norm": 0.4423351585865021,
      "learning_rate": 0.00045930406212664276,
      "loss": 6.7139,
      "step": 546
    },
    {
      "epoch": 0.16341773097318693,
      "grad_norm": 0.45697882771492004,
      "learning_rate": 0.0004592293906810036,
      "loss": 7.0107,
      "step": 547
    },
    {
      "epoch": 0.16371648368063335,
      "grad_norm": 0.3535310626029968,
      "learning_rate": 0.0004591547192353644,
      "loss": 7.5635,
      "step": 548
    },
    {
      "epoch": 0.16401523638807977,
      "grad_norm": 0.40528586506843567,
      "learning_rate": 0.00045908004778972525,
      "loss": 7.2578,
      "step": 549
    },
    {
      "epoch": 0.1643139890955262,
      "grad_norm": 0.3310297429561615,
      "learning_rate": 0.000459005376344086,
      "loss": 7.6299,
      "step": 550
    },
    {
      "epoch": 0.16461274180297258,
      "grad_norm": 0.36236876249313354,
      "learning_rate": 0.0004589307048984469,
      "loss": 7.2324,
      "step": 551
    },
    {
      "epoch": 0.164911494510419,
      "grad_norm": 0.3836616277694702,
      "learning_rate": 0.00045885603345280763,
      "loss": 7.1973,
      "step": 552
    },
    {
      "epoch": 0.16521024721786542,
      "grad_norm": 0.39154064655303955,
      "learning_rate": 0.00045878136200716845,
      "loss": 7.499,
      "step": 553
    },
    {
      "epoch": 0.16550899992531182,
      "grad_norm": 0.3483290374279022,
      "learning_rate": 0.0004587066905615293,
      "loss": 7.5039,
      "step": 554
    },
    {
      "epoch": 0.16580775263275824,
      "grad_norm": 0.38793253898620605,
      "learning_rate": 0.0004586320191158901,
      "loss": 7.3457,
      "step": 555
    },
    {
      "epoch": 0.16610650534020466,
      "grad_norm": 0.3821764290332794,
      "learning_rate": 0.00045855734767025094,
      "loss": 7.3965,
      "step": 556
    },
    {
      "epoch": 0.16640525804765105,
      "grad_norm": 0.340007483959198,
      "learning_rate": 0.0004584826762246117,
      "loss": 7.5859,
      "step": 557
    },
    {
      "epoch": 0.16670401075509747,
      "grad_norm": 0.40256068110466003,
      "learning_rate": 0.00045840800477897257,
      "loss": 7.25,
      "step": 558
    },
    {
      "epoch": 0.1670027634625439,
      "grad_norm": 0.5629507899284363,
      "learning_rate": 0.0004583333333333333,
      "loss": 7.0137,
      "step": 559
    },
    {
      "epoch": 0.16730151616999028,
      "grad_norm": 0.534640908241272,
      "learning_rate": 0.00045825866188769414,
      "loss": 6.8975,
      "step": 560
    },
    {
      "epoch": 0.1676002688774367,
      "grad_norm": 0.4003329873085022,
      "learning_rate": 0.00045818399044205495,
      "loss": 7.2285,
      "step": 561
    },
    {
      "epoch": 0.16789902158488312,
      "grad_norm": 0.41849029064178467,
      "learning_rate": 0.00045810931899641576,
      "loss": 7.4932,
      "step": 562
    },
    {
      "epoch": 0.16819777429232952,
      "grad_norm": 0.43317610025405884,
      "learning_rate": 0.00045803464755077663,
      "loss": 7.1191,
      "step": 563
    },
    {
      "epoch": 0.16849652699977594,
      "grad_norm": 0.36945387721061707,
      "learning_rate": 0.0004579599761051374,
      "loss": 7.4355,
      "step": 564
    },
    {
      "epoch": 0.16879527970722236,
      "grad_norm": 0.428309828042984,
      "learning_rate": 0.00045788530465949826,
      "loss": 6.957,
      "step": 565
    },
    {
      "epoch": 0.16909403241466875,
      "grad_norm": 0.43416914343833923,
      "learning_rate": 0.000457810633213859,
      "loss": 7.3232,
      "step": 566
    },
    {
      "epoch": 0.16939278512211517,
      "grad_norm": 0.3761763274669647,
      "learning_rate": 0.0004577359617682199,
      "loss": 7.4131,
      "step": 567
    },
    {
      "epoch": 0.1696915378295616,
      "grad_norm": 0.38552385568618774,
      "learning_rate": 0.00045766129032258064,
      "loss": 6.957,
      "step": 568
    },
    {
      "epoch": 0.16999029053700798,
      "grad_norm": 0.37265893816947937,
      "learning_rate": 0.00045758661887694145,
      "loss": 7.5215,
      "step": 569
    },
    {
      "epoch": 0.1702890432444544,
      "grad_norm": 0.3897472321987152,
      "learning_rate": 0.00045751194743130227,
      "loss": 7.3398,
      "step": 570
    },
    {
      "epoch": 0.17058779595190082,
      "grad_norm": 0.3238326609134674,
      "learning_rate": 0.0004574372759856631,
      "loss": 7.7412,
      "step": 571
    },
    {
      "epoch": 0.17088654865934721,
      "grad_norm": 0.42091798782348633,
      "learning_rate": 0.00045736260454002395,
      "loss": 7.376,
      "step": 572
    },
    {
      "epoch": 0.17118530136679364,
      "grad_norm": 0.4016720950603485,
      "learning_rate": 0.0004572879330943847,
      "loss": 7.5918,
      "step": 573
    },
    {
      "epoch": 0.17148405407424006,
      "grad_norm": 0.38962221145629883,
      "learning_rate": 0.00045721326164874557,
      "loss": 7.2871,
      "step": 574
    },
    {
      "epoch": 0.17178280678168645,
      "grad_norm": 0.39090844988822937,
      "learning_rate": 0.00045713859020310633,
      "loss": 7.4746,
      "step": 575
    },
    {
      "epoch": 0.17208155948913287,
      "grad_norm": 0.40731289982795715,
      "learning_rate": 0.00045706391875746714,
      "loss": 7.0703,
      "step": 576
    },
    {
      "epoch": 0.1723803121965793,
      "grad_norm": 0.4257664680480957,
      "learning_rate": 0.00045698924731182796,
      "loss": 7.0127,
      "step": 577
    },
    {
      "epoch": 0.17267906490402568,
      "grad_norm": 0.4589594602584839,
      "learning_rate": 0.00045691457586618877,
      "loss": 6.8076,
      "step": 578
    },
    {
      "epoch": 0.1729778176114721,
      "grad_norm": 0.37811416387557983,
      "learning_rate": 0.0004568399044205496,
      "loss": 7.1123,
      "step": 579
    },
    {
      "epoch": 0.17327657031891852,
      "grad_norm": 0.401071161031723,
      "learning_rate": 0.0004567652329749104,
      "loss": 7.0098,
      "step": 580
    },
    {
      "epoch": 0.1735753230263649,
      "grad_norm": 0.4928409457206726,
      "learning_rate": 0.00045669056152927126,
      "loss": 7.0869,
      "step": 581
    },
    {
      "epoch": 0.17387407573381133,
      "grad_norm": 0.3705659806728363,
      "learning_rate": 0.000456615890083632,
      "loss": 7.2725,
      "step": 582
    },
    {
      "epoch": 0.17417282844125775,
      "grad_norm": 0.35716262459754944,
      "learning_rate": 0.0004565412186379929,
      "loss": 7.5283,
      "step": 583
    },
    {
      "epoch": 0.17447158114870415,
      "grad_norm": 0.44968387484550476,
      "learning_rate": 0.00045646654719235365,
      "loss": 6.8555,
      "step": 584
    },
    {
      "epoch": 0.17477033385615057,
      "grad_norm": 0.4309314489364624,
      "learning_rate": 0.00045639187574671446,
      "loss": 6.9453,
      "step": 585
    },
    {
      "epoch": 0.175069086563597,
      "grad_norm": 0.5305352807044983,
      "learning_rate": 0.00045631720430107527,
      "loss": 6.3955,
      "step": 586
    },
    {
      "epoch": 0.1753678392710434,
      "grad_norm": 0.36297738552093506,
      "learning_rate": 0.0004562425328554361,
      "loss": 7.1943,
      "step": 587
    },
    {
      "epoch": 0.1756665919784898,
      "grad_norm": 0.39075636863708496,
      "learning_rate": 0.0004561678614097969,
      "loss": 7.2637,
      "step": 588
    },
    {
      "epoch": 0.17596534468593622,
      "grad_norm": 0.4245891571044922,
      "learning_rate": 0.0004560931899641577,
      "loss": 6.9678,
      "step": 589
    },
    {
      "epoch": 0.17626409739338264,
      "grad_norm": 0.3536333441734314,
      "learning_rate": 0.0004560185185185186,
      "loss": 7.6035,
      "step": 590
    },
    {
      "epoch": 0.17656285010082903,
      "grad_norm": 0.37007611989974976,
      "learning_rate": 0.00045594384707287934,
      "loss": 7.542,
      "step": 591
    },
    {
      "epoch": 0.17686160280827545,
      "grad_norm": 0.37865617871284485,
      "learning_rate": 0.00045586917562724015,
      "loss": 7.1719,
      "step": 592
    },
    {
      "epoch": 0.17716035551572187,
      "grad_norm": 0.41175633668899536,
      "learning_rate": 0.00045579450418160096,
      "loss": 7.4404,
      "step": 593
    },
    {
      "epoch": 0.17745910822316827,
      "grad_norm": 0.3914108872413635,
      "learning_rate": 0.0004557198327359618,
      "loss": 7.2676,
      "step": 594
    },
    {
      "epoch": 0.1777578609306147,
      "grad_norm": 0.45254841446876526,
      "learning_rate": 0.0004556451612903226,
      "loss": 6.8359,
      "step": 595
    },
    {
      "epoch": 0.1780566136380611,
      "grad_norm": 0.36402076482772827,
      "learning_rate": 0.0004555704898446834,
      "loss": 7.5029,
      "step": 596
    },
    {
      "epoch": 0.1783553663455075,
      "grad_norm": 0.3413120210170746,
      "learning_rate": 0.0004554958183990442,
      "loss": 7.5625,
      "step": 597
    },
    {
      "epoch": 0.17865411905295392,
      "grad_norm": 0.3583661615848541,
      "learning_rate": 0.000455421146953405,
      "loss": 7.4912,
      "step": 598
    },
    {
      "epoch": 0.17895287176040034,
      "grad_norm": 0.3917255103588104,
      "learning_rate": 0.0004553464755077659,
      "loss": 7.9229,
      "step": 599
    },
    {
      "epoch": 0.17925162446784673,
      "grad_norm": 0.46217119693756104,
      "learning_rate": 0.00045527180406212665,
      "loss": 6.7598,
      "step": 600
    },
    {
      "epoch": 0.17925162446784673,
      "eval_bleu": 0.07854102719186938,
      "eval_loss": 7.11328125,
      "eval_runtime": 527.0984,
      "eval_samples_per_second": 2.673,
      "eval_steps_per_second": 0.169,
      "step": 600
    }
  ],
  "logging_steps": 1,
  "max_steps": 6696,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 200,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1304611415654400.0,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
