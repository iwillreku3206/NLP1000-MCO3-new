{
  "best_global_step": 400,
  "best_metric": 7.00390625,
  "best_model_checkpoint": "trained-nllb-en-to-bicol\\checkpoint-400",
  "epoch": 0.5124555160142349,
  "eval_steps": 200,
  "global_step": 1800,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.00028469750889679714,
      "grad_norm": 0.13050267100334167,
      "learning_rate": 0.0005,
      "loss": 11.8242,
      "step": 1
    },
    {
      "epoch": 0.0005693950177935943,
      "grad_norm": 0.16353872418403625,
      "learning_rate": 0.0004999288357529178,
      "loss": 12.7012,
      "step": 2
    },
    {
      "epoch": 0.0008540925266903915,
      "grad_norm": 0.1825779378414154,
      "learning_rate": 0.0004998576715058355,
      "loss": 11.4883,
      "step": 3
    },
    {
      "epoch": 0.0011387900355871886,
      "grad_norm": 0.2419777810573578,
      "learning_rate": 0.0004997865072587532,
      "loss": 11.9531,
      "step": 4
    },
    {
      "epoch": 0.0014234875444839859,
      "grad_norm": 0.31029146909713745,
      "learning_rate": 0.0004997153430116709,
      "loss": 11.7852,
      "step": 5
    },
    {
      "epoch": 0.001708185053380783,
      "grad_norm": 0.3981788754463196,
      "learning_rate": 0.0004996441787645887,
      "loss": 11.8242,
      "step": 6
    },
    {
      "epoch": 0.00199288256227758,
      "grad_norm": 0.44912195205688477,
      "learning_rate": 0.0004995730145175065,
      "loss": 11.1836,
      "step": 7
    },
    {
      "epoch": 0.002277580071174377,
      "grad_norm": 0.5424309372901917,
      "learning_rate": 0.0004995018502704241,
      "loss": 11.6504,
      "step": 8
    },
    {
      "epoch": 0.002562277580071174,
      "grad_norm": 0.6373918056488037,
      "learning_rate": 0.0004994306860233419,
      "loss": 11.3926,
      "step": 9
    },
    {
      "epoch": 0.0028469750889679717,
      "grad_norm": 0.7767484188079834,
      "learning_rate": 0.0004993595217762596,
      "loss": 11.5098,
      "step": 10
    },
    {
      "epoch": 0.003131672597864769,
      "grad_norm": 0.7795925736427307,
      "learning_rate": 0.0004992883575291773,
      "loss": 12.1816,
      "step": 11
    },
    {
      "epoch": 0.003416370106761566,
      "grad_norm": 0.5827488303184509,
      "learning_rate": 0.0004992171932820951,
      "loss": 11.4355,
      "step": 12
    },
    {
      "epoch": 0.003701067615658363,
      "grad_norm": 0.5459573864936829,
      "learning_rate": 0.0004991460290350128,
      "loss": 11.0156,
      "step": 13
    },
    {
      "epoch": 0.00398576512455516,
      "grad_norm": 0.513734757900238,
      "learning_rate": 0.0004990748647879306,
      "loss": 11.6016,
      "step": 14
    },
    {
      "epoch": 0.004270462633451958,
      "grad_norm": 0.5204881429672241,
      "learning_rate": 0.0004990037005408483,
      "loss": 11.1738,
      "step": 15
    },
    {
      "epoch": 0.004555160142348754,
      "grad_norm": 0.5095188021659851,
      "learning_rate": 0.000498932536293766,
      "loss": 11.6738,
      "step": 16
    },
    {
      "epoch": 0.004839857651245552,
      "grad_norm": 0.5002894997596741,
      "learning_rate": 0.0004988613720466838,
      "loss": 11.1172,
      "step": 17
    },
    {
      "epoch": 0.005124555160142348,
      "grad_norm": 0.5175882577896118,
      "learning_rate": 0.0004987902077996015,
      "loss": 11.709,
      "step": 18
    },
    {
      "epoch": 0.005409252669039146,
      "grad_norm": 0.481122225522995,
      "learning_rate": 0.0004987190435525192,
      "loss": 11.2383,
      "step": 19
    },
    {
      "epoch": 0.0056939501779359435,
      "grad_norm": 0.4916481077671051,
      "learning_rate": 0.000498647879305437,
      "loss": 11.1875,
      "step": 20
    },
    {
      "epoch": 0.00597864768683274,
      "grad_norm": 0.4639431834220886,
      "learning_rate": 0.0004985767150583547,
      "loss": 10.959,
      "step": 21
    },
    {
      "epoch": 0.006263345195729538,
      "grad_norm": 0.471029669046402,
      "learning_rate": 0.0004985055508112725,
      "loss": 10.5039,
      "step": 22
    },
    {
      "epoch": 0.006548042704626334,
      "grad_norm": 0.5494607090950012,
      "learning_rate": 0.0004984343865641901,
      "loss": 10.7871,
      "step": 23
    },
    {
      "epoch": 0.006832740213523132,
      "grad_norm": 0.542377769947052,
      "learning_rate": 0.0004983632223171079,
      "loss": 10.7402,
      "step": 24
    },
    {
      "epoch": 0.0071174377224199285,
      "grad_norm": 0.6290743947029114,
      "learning_rate": 0.0004982920580700257,
      "loss": 11.1191,
      "step": 25
    },
    {
      "epoch": 0.007402135231316726,
      "grad_norm": 0.5208401083946228,
      "learning_rate": 0.0004982208938229434,
      "loss": 10.3535,
      "step": 26
    },
    {
      "epoch": 0.0076868327402135235,
      "grad_norm": 0.5653201937675476,
      "learning_rate": 0.0004981497295758611,
      "loss": 10.2676,
      "step": 27
    },
    {
      "epoch": 0.00797153024911032,
      "grad_norm": 0.5279284119606018,
      "learning_rate": 0.0004980785653287788,
      "loss": 10.6504,
      "step": 28
    },
    {
      "epoch": 0.008256227758007117,
      "grad_norm": 0.5287833213806152,
      "learning_rate": 0.0004980074010816966,
      "loss": 9.9648,
      "step": 29
    },
    {
      "epoch": 0.008540925266903915,
      "grad_norm": 0.4779149293899536,
      "learning_rate": 0.0004979362368346143,
      "loss": 11.1738,
      "step": 30
    },
    {
      "epoch": 0.008825622775800712,
      "grad_norm": 0.4311399459838867,
      "learning_rate": 0.000497865072587532,
      "loss": 10.7617,
      "step": 31
    },
    {
      "epoch": 0.009110320284697508,
      "grad_norm": 0.4880855083465576,
      "learning_rate": 0.0004977939083404498,
      "loss": 10.6113,
      "step": 32
    },
    {
      "epoch": 0.009395017793594307,
      "grad_norm": 0.4771440029144287,
      "learning_rate": 0.0004977227440933675,
      "loss": 10.2031,
      "step": 33
    },
    {
      "epoch": 0.009679715302491104,
      "grad_norm": 0.4335767924785614,
      "learning_rate": 0.0004976515798462852,
      "loss": 9.9766,
      "step": 34
    },
    {
      "epoch": 0.0099644128113879,
      "grad_norm": 0.47250261902809143,
      "learning_rate": 0.000497580415599203,
      "loss": 10.582,
      "step": 35
    },
    {
      "epoch": 0.010249110320284697,
      "grad_norm": 0.5027048587799072,
      "learning_rate": 0.0004975092513521207,
      "loss": 9.252,
      "step": 36
    },
    {
      "epoch": 0.010533807829181495,
      "grad_norm": 0.4936491847038269,
      "learning_rate": 0.0004974380871050385,
      "loss": 9.9648,
      "step": 37
    },
    {
      "epoch": 0.010818505338078292,
      "grad_norm": 0.4429067373275757,
      "learning_rate": 0.0004973669228579561,
      "loss": 10.1699,
      "step": 38
    },
    {
      "epoch": 0.011103202846975089,
      "grad_norm": 0.43701812624931335,
      "learning_rate": 0.0004972957586108739,
      "loss": 10.1875,
      "step": 39
    },
    {
      "epoch": 0.011387900355871887,
      "grad_norm": 0.44829320907592773,
      "learning_rate": 0.0004972245943637917,
      "loss": 9.7109,
      "step": 40
    },
    {
      "epoch": 0.011672597864768684,
      "grad_norm": 0.44973376393318176,
      "learning_rate": 0.0004971534301167094,
      "loss": 9.9023,
      "step": 41
    },
    {
      "epoch": 0.01195729537366548,
      "grad_norm": 0.4918513894081116,
      "learning_rate": 0.0004970822658696271,
      "loss": 10.1934,
      "step": 42
    },
    {
      "epoch": 0.012241992882562277,
      "grad_norm": 0.4607585668563843,
      "learning_rate": 0.0004970111016225449,
      "loss": 9.9941,
      "step": 43
    },
    {
      "epoch": 0.012526690391459075,
      "grad_norm": 0.45157185196876526,
      "learning_rate": 0.0004969399373754626,
      "loss": 9.6797,
      "step": 44
    },
    {
      "epoch": 0.012811387900355872,
      "grad_norm": 0.45155540108680725,
      "learning_rate": 0.0004968687731283804,
      "loss": 9.8535,
      "step": 45
    },
    {
      "epoch": 0.013096085409252669,
      "grad_norm": 0.45471465587615967,
      "learning_rate": 0.000496797608881298,
      "loss": 9.6699,
      "step": 46
    },
    {
      "epoch": 0.013380782918149467,
      "grad_norm": 0.45609229803085327,
      "learning_rate": 0.0004967264446342158,
      "loss": 9.834,
      "step": 47
    },
    {
      "epoch": 0.013665480427046264,
      "grad_norm": 0.46065598726272583,
      "learning_rate": 0.0004966552803871336,
      "loss": 9.1973,
      "step": 48
    },
    {
      "epoch": 0.01395017793594306,
      "grad_norm": 0.4395595192909241,
      "learning_rate": 0.0004965841161400512,
      "loss": 9.3379,
      "step": 49
    },
    {
      "epoch": 0.014234875444839857,
      "grad_norm": 0.42804303765296936,
      "learning_rate": 0.0004965129518929689,
      "loss": 8.1885,
      "step": 50
    },
    {
      "epoch": 0.014519572953736655,
      "grad_norm": 0.4336685538291931,
      "learning_rate": 0.0004964417876458867,
      "loss": 9.4141,
      "step": 51
    },
    {
      "epoch": 0.014804270462633452,
      "grad_norm": 0.4083915054798126,
      "learning_rate": 0.0004963706233988045,
      "loss": 9.2363,
      "step": 52
    },
    {
      "epoch": 0.015088967971530249,
      "grad_norm": 0.39727815985679626,
      "learning_rate": 0.0004962994591517222,
      "loss": 8.5439,
      "step": 53
    },
    {
      "epoch": 0.015373665480427047,
      "grad_norm": 0.4038327634334564,
      "learning_rate": 0.0004962282949046399,
      "loss": 9.0723,
      "step": 54
    },
    {
      "epoch": 0.015658362989323844,
      "grad_norm": 0.4088612198829651,
      "learning_rate": 0.0004961571306575576,
      "loss": 8.5332,
      "step": 55
    },
    {
      "epoch": 0.01594306049822064,
      "grad_norm": 0.39859700202941895,
      "learning_rate": 0.0004960859664104754,
      "loss": 9.1191,
      "step": 56
    },
    {
      "epoch": 0.016227758007117437,
      "grad_norm": 0.39439693093299866,
      "learning_rate": 0.0004960148021633931,
      "loss": 8.0508,
      "step": 57
    },
    {
      "epoch": 0.016512455516014234,
      "grad_norm": 0.41306084394454956,
      "learning_rate": 0.0004959436379163109,
      "loss": 8.2998,
      "step": 58
    },
    {
      "epoch": 0.016797153024911034,
      "grad_norm": 0.4390506148338318,
      "learning_rate": 0.0004958724736692286,
      "loss": 8.1816,
      "step": 59
    },
    {
      "epoch": 0.01708185053380783,
      "grad_norm": 0.40014925599098206,
      "learning_rate": 0.0004958013094221464,
      "loss": 8.248,
      "step": 60
    },
    {
      "epoch": 0.017366548042704627,
      "grad_norm": 0.3710031807422638,
      "learning_rate": 0.000495730145175064,
      "loss": 7.7432,
      "step": 61
    },
    {
      "epoch": 0.017651245551601424,
      "grad_norm": 0.3743155002593994,
      "learning_rate": 0.0004956589809279818,
      "loss": 8.9805,
      "step": 62
    },
    {
      "epoch": 0.01793594306049822,
      "grad_norm": 0.4065592288970947,
      "learning_rate": 0.0004955878166808996,
      "loss": 8.9219,
      "step": 63
    },
    {
      "epoch": 0.018220640569395017,
      "grad_norm": 0.3826305866241455,
      "learning_rate": 0.0004955166524338172,
      "loss": 8.377,
      "step": 64
    },
    {
      "epoch": 0.018505338078291814,
      "grad_norm": 0.3792859613895416,
      "learning_rate": 0.000495445488186735,
      "loss": 7.8789,
      "step": 65
    },
    {
      "epoch": 0.018790035587188614,
      "grad_norm": 0.3933613896369934,
      "learning_rate": 0.0004953743239396527,
      "loss": 8.4092,
      "step": 66
    },
    {
      "epoch": 0.01907473309608541,
      "grad_norm": 0.39127129316329956,
      "learning_rate": 0.0004953031596925705,
      "loss": 7.9512,
      "step": 67
    },
    {
      "epoch": 0.019359430604982207,
      "grad_norm": 0.34493568539619446,
      "learning_rate": 0.0004952319954454881,
      "loss": 8.3105,
      "step": 68
    },
    {
      "epoch": 0.019644128113879004,
      "grad_norm": 0.3507670760154724,
      "learning_rate": 0.0004951608311984059,
      "loss": 7.9043,
      "step": 69
    },
    {
      "epoch": 0.0199288256227758,
      "grad_norm": 0.3754495084285736,
      "learning_rate": 0.0004950896669513237,
      "loss": 7.9131,
      "step": 70
    },
    {
      "epoch": 0.020213523131672597,
      "grad_norm": 0.3600054383277893,
      "learning_rate": 0.0004950185027042415,
      "loss": 8.1426,
      "step": 71
    },
    {
      "epoch": 0.020498220640569394,
      "grad_norm": 0.3194250166416168,
      "learning_rate": 0.0004949473384571591,
      "loss": 7.8213,
      "step": 72
    },
    {
      "epoch": 0.020782918149466194,
      "grad_norm": 0.3438234329223633,
      "learning_rate": 0.0004948761742100768,
      "loss": 8.1318,
      "step": 73
    },
    {
      "epoch": 0.02106761565836299,
      "grad_norm": 0.34680306911468506,
      "learning_rate": 0.0004948050099629946,
      "loss": 8.1611,
      "step": 74
    },
    {
      "epoch": 0.021352313167259787,
      "grad_norm": 0.3298954665660858,
      "learning_rate": 0.0004947338457159124,
      "loss": 7.6816,
      "step": 75
    },
    {
      "epoch": 0.021637010676156584,
      "grad_norm": 0.35546237230300903,
      "learning_rate": 0.0004946626814688301,
      "loss": 7.9482,
      "step": 76
    },
    {
      "epoch": 0.02192170818505338,
      "grad_norm": 0.31350335478782654,
      "learning_rate": 0.0004945915172217478,
      "loss": 7.916,
      "step": 77
    },
    {
      "epoch": 0.022206405693950177,
      "grad_norm": 0.2875521183013916,
      "learning_rate": 0.0004945203529746655,
      "loss": 8.1787,
      "step": 78
    },
    {
      "epoch": 0.022491103202846974,
      "grad_norm": 0.2984481453895569,
      "learning_rate": 0.0004944491887275833,
      "loss": 8.0068,
      "step": 79
    },
    {
      "epoch": 0.022775800711743774,
      "grad_norm": 0.32493481040000916,
      "learning_rate": 0.000494378024480501,
      "loss": 7.7959,
      "step": 80
    },
    {
      "epoch": 0.02306049822064057,
      "grad_norm": 0.3053205907344818,
      "learning_rate": 0.0004943068602334188,
      "loss": 7.7959,
      "step": 81
    },
    {
      "epoch": 0.023345195729537367,
      "grad_norm": 0.29084035754203796,
      "learning_rate": 0.0004942356959863365,
      "loss": 7.9629,
      "step": 82
    },
    {
      "epoch": 0.023629893238434164,
      "grad_norm": 0.3073493242263794,
      "learning_rate": 0.0004941645317392541,
      "loss": 7.7549,
      "step": 83
    },
    {
      "epoch": 0.02391459074733096,
      "grad_norm": 0.30308839678764343,
      "learning_rate": 0.0004940933674921719,
      "loss": 8.0791,
      "step": 84
    },
    {
      "epoch": 0.024199288256227757,
      "grad_norm": 0.38702166080474854,
      "learning_rate": 0.0004940222032450897,
      "loss": 7.2637,
      "step": 85
    },
    {
      "epoch": 0.024483985765124554,
      "grad_norm": 0.33084896206855774,
      "learning_rate": 0.0004939510389980074,
      "loss": 7.833,
      "step": 86
    },
    {
      "epoch": 0.024768683274021354,
      "grad_norm": 0.2839316129684448,
      "learning_rate": 0.0004938798747509251,
      "loss": 7.9053,
      "step": 87
    },
    {
      "epoch": 0.02505338078291815,
      "grad_norm": 0.35194167494773865,
      "learning_rate": 0.0004938087105038429,
      "loss": 7.4053,
      "step": 88
    },
    {
      "epoch": 0.025338078291814947,
      "grad_norm": 0.34102603793144226,
      "learning_rate": 0.0004937375462567606,
      "loss": 7.6406,
      "step": 89
    },
    {
      "epoch": 0.025622775800711744,
      "grad_norm": 0.34196290373802185,
      "learning_rate": 0.0004936663820096784,
      "loss": 7.7129,
      "step": 90
    },
    {
      "epoch": 0.02590747330960854,
      "grad_norm": 0.33342525362968445,
      "learning_rate": 0.000493595217762596,
      "loss": 7.1934,
      "step": 91
    },
    {
      "epoch": 0.026192170818505337,
      "grad_norm": 0.2916184365749359,
      "learning_rate": 0.0004935240535155138,
      "loss": 7.7559,
      "step": 92
    },
    {
      "epoch": 0.026476868327402134,
      "grad_norm": 0.3170425295829773,
      "learning_rate": 0.0004934528892684316,
      "loss": 7.7617,
      "step": 93
    },
    {
      "epoch": 0.026761565836298934,
      "grad_norm": 0.3077421188354492,
      "learning_rate": 0.0004933817250213493,
      "loss": 7.7852,
      "step": 94
    },
    {
      "epoch": 0.02704626334519573,
      "grad_norm": 0.34794363379478455,
      "learning_rate": 0.000493310560774267,
      "loss": 7.5391,
      "step": 95
    },
    {
      "epoch": 0.027330960854092527,
      "grad_norm": 0.27851757407188416,
      "learning_rate": 0.0004932393965271847,
      "loss": 8.0039,
      "step": 96
    },
    {
      "epoch": 0.027615658362989324,
      "grad_norm": 0.3033468723297119,
      "learning_rate": 0.0004931682322801025,
      "loss": 7.8076,
      "step": 97
    },
    {
      "epoch": 0.02790035587188612,
      "grad_norm": 0.29290416836738586,
      "learning_rate": 0.0004930970680330203,
      "loss": 7.8281,
      "step": 98
    },
    {
      "epoch": 0.028185053380782917,
      "grad_norm": 0.27407997846603394,
      "learning_rate": 0.000493025903785938,
      "loss": 7.8564,
      "step": 99
    },
    {
      "epoch": 0.028469750889679714,
      "grad_norm": 0.32197993993759155,
      "learning_rate": 0.0004929547395388557,
      "loss": 7.3594,
      "step": 100
    },
    {
      "epoch": 0.028754448398576514,
      "grad_norm": 0.27607426047325134,
      "learning_rate": 0.0004928835752917734,
      "loss": 7.9219,
      "step": 101
    },
    {
      "epoch": 0.02903914590747331,
      "grad_norm": 0.25733911991119385,
      "learning_rate": 0.0004928124110446911,
      "loss": 7.6963,
      "step": 102
    },
    {
      "epoch": 0.029323843416370107,
      "grad_norm": 0.33134591579437256,
      "learning_rate": 0.0004927412467976089,
      "loss": 7.5918,
      "step": 103
    },
    {
      "epoch": 0.029608540925266904,
      "grad_norm": 0.3447519838809967,
      "learning_rate": 0.0004926700825505266,
      "loss": 7.3262,
      "step": 104
    },
    {
      "epoch": 0.0298932384341637,
      "grad_norm": 0.30481886863708496,
      "learning_rate": 0.0004925989183034444,
      "loss": 7.5986,
      "step": 105
    },
    {
      "epoch": 0.030177935943060497,
      "grad_norm": 0.3810344338417053,
      "learning_rate": 0.000492527754056362,
      "loss": 7.2734,
      "step": 106
    },
    {
      "epoch": 0.030462633451957294,
      "grad_norm": 0.26661789417266846,
      "learning_rate": 0.0004924565898092798,
      "loss": 7.8818,
      "step": 107
    },
    {
      "epoch": 0.030747330960854094,
      "grad_norm": 0.3144116997718811,
      "learning_rate": 0.0004923854255621976,
      "loss": 7.4697,
      "step": 108
    },
    {
      "epoch": 0.03103202846975089,
      "grad_norm": 0.3234042525291443,
      "learning_rate": 0.0004923142613151153,
      "loss": 7.6611,
      "step": 109
    },
    {
      "epoch": 0.03131672597864769,
      "grad_norm": 0.2908277213573456,
      "learning_rate": 0.000492243097068033,
      "loss": 7.6924,
      "step": 110
    },
    {
      "epoch": 0.03160142348754449,
      "grad_norm": 0.282621830701828,
      "learning_rate": 0.0004921719328209507,
      "loss": 7.6748,
      "step": 111
    },
    {
      "epoch": 0.03188612099644128,
      "grad_norm": 0.25899139046669006,
      "learning_rate": 0.0004921007685738685,
      "loss": 7.7725,
      "step": 112
    },
    {
      "epoch": 0.03217081850533808,
      "grad_norm": 0.30150163173675537,
      "learning_rate": 0.0004920296043267863,
      "loss": 7.6016,
      "step": 113
    },
    {
      "epoch": 0.032455516014234874,
      "grad_norm": 0.3718966245651245,
      "learning_rate": 0.0004919584400797039,
      "loss": 7.0254,
      "step": 114
    },
    {
      "epoch": 0.032740213523131674,
      "grad_norm": 0.30984169244766235,
      "learning_rate": 0.0004918872758326217,
      "loss": 7.4707,
      "step": 115
    },
    {
      "epoch": 0.03302491103202847,
      "grad_norm": 0.25567907094955444,
      "learning_rate": 0.0004918161115855395,
      "loss": 7.668,
      "step": 116
    },
    {
      "epoch": 0.03330960854092527,
      "grad_norm": 0.38791143894195557,
      "learning_rate": 0.0004917449473384572,
      "loss": 7.3613,
      "step": 117
    },
    {
      "epoch": 0.03359430604982207,
      "grad_norm": 0.28487956523895264,
      "learning_rate": 0.0004916737830913749,
      "loss": 7.5039,
      "step": 118
    },
    {
      "epoch": 0.03387900355871886,
      "grad_norm": 0.4281052350997925,
      "learning_rate": 0.0004916026188442926,
      "loss": 7.0996,
      "step": 119
    },
    {
      "epoch": 0.03416370106761566,
      "grad_norm": 0.30284109711647034,
      "learning_rate": 0.0004915314545972104,
      "loss": 7.2402,
      "step": 120
    },
    {
      "epoch": 0.034448398576512454,
      "grad_norm": 0.3210189938545227,
      "learning_rate": 0.0004914602903501282,
      "loss": 7.3271,
      "step": 121
    },
    {
      "epoch": 0.034733096085409254,
      "grad_norm": 0.373404324054718,
      "learning_rate": 0.0004913891261030458,
      "loss": 6.876,
      "step": 122
    },
    {
      "epoch": 0.03501779359430605,
      "grad_norm": 0.34432628750801086,
      "learning_rate": 0.0004913179618559636,
      "loss": 7.5469,
      "step": 123
    },
    {
      "epoch": 0.03530249110320285,
      "grad_norm": 0.3198752999305725,
      "learning_rate": 0.0004912467976088813,
      "loss": 7.6924,
      "step": 124
    },
    {
      "epoch": 0.03558718861209965,
      "grad_norm": 0.38547608256340027,
      "learning_rate": 0.000491175633361799,
      "loss": 7.0049,
      "step": 125
    },
    {
      "epoch": 0.03587188612099644,
      "grad_norm": 0.34361645579338074,
      "learning_rate": 0.0004911044691147168,
      "loss": 7.3477,
      "step": 126
    },
    {
      "epoch": 0.03615658362989324,
      "grad_norm": 0.32878419756889343,
      "learning_rate": 0.0004910333048676345,
      "loss": 7.167,
      "step": 127
    },
    {
      "epoch": 0.036441281138790034,
      "grad_norm": 0.2640332579612732,
      "learning_rate": 0.0004909621406205523,
      "loss": 7.5791,
      "step": 128
    },
    {
      "epoch": 0.036725978647686834,
      "grad_norm": 0.34086892008781433,
      "learning_rate": 0.0004908909763734699,
      "loss": 7.5127,
      "step": 129
    },
    {
      "epoch": 0.03701067615658363,
      "grad_norm": 0.3057088851928711,
      "learning_rate": 0.0004908198121263877,
      "loss": 7.6523,
      "step": 130
    },
    {
      "epoch": 0.03729537366548043,
      "grad_norm": 0.325603187084198,
      "learning_rate": 0.0004907486478793055,
      "loss": 7.2979,
      "step": 131
    },
    {
      "epoch": 0.03758007117437723,
      "grad_norm": 0.3464660942554474,
      "learning_rate": 0.0004906774836322232,
      "loss": 7.4541,
      "step": 132
    },
    {
      "epoch": 0.03786476868327402,
      "grad_norm": 0.3628098964691162,
      "learning_rate": 0.0004906063193851409,
      "loss": 7.2969,
      "step": 133
    },
    {
      "epoch": 0.03814946619217082,
      "grad_norm": 0.33785781264305115,
      "learning_rate": 0.0004905351551380586,
      "loss": 7.4209,
      "step": 134
    },
    {
      "epoch": 0.038434163701067614,
      "grad_norm": 0.3329750895500183,
      "learning_rate": 0.0004904639908909764,
      "loss": 7.5674,
      "step": 135
    },
    {
      "epoch": 0.038718861209964414,
      "grad_norm": 0.3496299088001251,
      "learning_rate": 0.0004903928266438942,
      "loss": 7.4629,
      "step": 136
    },
    {
      "epoch": 0.03900355871886121,
      "grad_norm": 0.3395816385746002,
      "learning_rate": 0.0004903216623968118,
      "loss": 7.5713,
      "step": 137
    },
    {
      "epoch": 0.03928825622775801,
      "grad_norm": 0.3240024447441101,
      "learning_rate": 0.0004902504981497296,
      "loss": 7.6055,
      "step": 138
    },
    {
      "epoch": 0.03957295373665481,
      "grad_norm": 0.2986396253108978,
      "learning_rate": 0.0004901793339026473,
      "loss": 7.4951,
      "step": 139
    },
    {
      "epoch": 0.0398576512455516,
      "grad_norm": 0.30460289120674133,
      "learning_rate": 0.000490108169655565,
      "loss": 7.7354,
      "step": 140
    },
    {
      "epoch": 0.0401423487544484,
      "grad_norm": 0.2872254252433777,
      "learning_rate": 0.0004900370054084828,
      "loss": 7.7559,
      "step": 141
    },
    {
      "epoch": 0.040427046263345194,
      "grad_norm": 0.3015432059764862,
      "learning_rate": 0.0004899658411614005,
      "loss": 7.7129,
      "step": 142
    },
    {
      "epoch": 0.040711743772241994,
      "grad_norm": 0.33976587653160095,
      "learning_rate": 0.0004898946769143183,
      "loss": 7.4961,
      "step": 143
    },
    {
      "epoch": 0.04099644128113879,
      "grad_norm": 0.3869113028049469,
      "learning_rate": 0.000489823512667236,
      "loss": 7.2979,
      "step": 144
    },
    {
      "epoch": 0.04128113879003559,
      "grad_norm": 0.3606325685977936,
      "learning_rate": 0.0004897523484201537,
      "loss": 7.4873,
      "step": 145
    },
    {
      "epoch": 0.04156583629893239,
      "grad_norm": 0.3285134434700012,
      "learning_rate": 0.0004896811841730715,
      "loss": 7.8018,
      "step": 146
    },
    {
      "epoch": 0.04185053380782918,
      "grad_norm": 0.371952086687088,
      "learning_rate": 0.0004896100199259892,
      "loss": 7.2168,
      "step": 147
    },
    {
      "epoch": 0.04213523131672598,
      "grad_norm": 0.3381865620613098,
      "learning_rate": 0.0004895388556789069,
      "loss": 7.3682,
      "step": 148
    },
    {
      "epoch": 0.042419928825622774,
      "grad_norm": 0.30638259649276733,
      "learning_rate": 0.0004894676914318247,
      "loss": 7.459,
      "step": 149
    },
    {
      "epoch": 0.042704626334519574,
      "grad_norm": 0.3618326187133789,
      "learning_rate": 0.0004893965271847424,
      "loss": 7.293,
      "step": 150
    },
    {
      "epoch": 0.04298932384341637,
      "grad_norm": 0.34743648767471313,
      "learning_rate": 0.0004893253629376602,
      "loss": 7.2275,
      "step": 151
    },
    {
      "epoch": 0.04327402135231317,
      "grad_norm": 0.24103522300720215,
      "learning_rate": 0.0004892541986905778,
      "loss": 7.6436,
      "step": 152
    },
    {
      "epoch": 0.04355871886120997,
      "grad_norm": 0.47605815529823303,
      "learning_rate": 0.0004891830344434956,
      "loss": 6.8848,
      "step": 153
    },
    {
      "epoch": 0.04384341637010676,
      "grad_norm": 0.2737746834754944,
      "learning_rate": 0.0004891118701964134,
      "loss": 7.5234,
      "step": 154
    },
    {
      "epoch": 0.04412811387900356,
      "grad_norm": 0.34337320923805237,
      "learning_rate": 0.000489040705949331,
      "loss": 7.3691,
      "step": 155
    },
    {
      "epoch": 0.044412811387900354,
      "grad_norm": 0.29171985387802124,
      "learning_rate": 0.0004889695417022488,
      "loss": 7.7021,
      "step": 156
    },
    {
      "epoch": 0.044697508896797154,
      "grad_norm": 0.2947385907173157,
      "learning_rate": 0.0004888983774551665,
      "loss": 7.7725,
      "step": 157
    },
    {
      "epoch": 0.04498220640569395,
      "grad_norm": 0.33179420232772827,
      "learning_rate": 0.0004888272132080843,
      "loss": 7.4795,
      "step": 158
    },
    {
      "epoch": 0.04526690391459075,
      "grad_norm": 0.30874258279800415,
      "learning_rate": 0.000488756048961002,
      "loss": 7.667,
      "step": 159
    },
    {
      "epoch": 0.04555160142348755,
      "grad_norm": 0.37757524847984314,
      "learning_rate": 0.0004886848847139197,
      "loss": 7.1436,
      "step": 160
    },
    {
      "epoch": 0.04583629893238434,
      "grad_norm": 0.35947147011756897,
      "learning_rate": 0.0004886137204668375,
      "loss": 7.2451,
      "step": 161
    },
    {
      "epoch": 0.04612099644128114,
      "grad_norm": 0.38363611698150635,
      "learning_rate": 0.0004885425562197552,
      "loss": 7.2061,
      "step": 162
    },
    {
      "epoch": 0.046405693950177934,
      "grad_norm": 0.3002108335494995,
      "learning_rate": 0.0004884713919726729,
      "loss": 7.6328,
      "step": 163
    },
    {
      "epoch": 0.046690391459074734,
      "grad_norm": 0.36899691820144653,
      "learning_rate": 0.0004884002277255907,
      "loss": 7.2451,
      "step": 164
    },
    {
      "epoch": 0.04697508896797153,
      "grad_norm": 0.3589462637901306,
      "learning_rate": 0.0004883290634785084,
      "loss": 7.4961,
      "step": 165
    },
    {
      "epoch": 0.04725978647686833,
      "grad_norm": 0.3247394263744354,
      "learning_rate": 0.00048825789923142616,
      "loss": 7.4971,
      "step": 166
    },
    {
      "epoch": 0.04754448398576513,
      "grad_norm": 0.4397582411766052,
      "learning_rate": 0.00048818673498434383,
      "loss": 7.2158,
      "step": 167
    },
    {
      "epoch": 0.04782918149466192,
      "grad_norm": 0.34369221329689026,
      "learning_rate": 0.0004881155707372616,
      "loss": 7.3965,
      "step": 168
    },
    {
      "epoch": 0.04811387900355872,
      "grad_norm": 0.315603107213974,
      "learning_rate": 0.00048804440649017933,
      "loss": 7.4365,
      "step": 169
    },
    {
      "epoch": 0.048398576512455514,
      "grad_norm": 0.5103448033332825,
      "learning_rate": 0.0004879732422430971,
      "loss": 6.8682,
      "step": 170
    },
    {
      "epoch": 0.048683274021352314,
      "grad_norm": 0.43841269612312317,
      "learning_rate": 0.00048790207799601483,
      "loss": 7.3838,
      "step": 171
    },
    {
      "epoch": 0.04896797153024911,
      "grad_norm": 0.43405452370643616,
      "learning_rate": 0.00048783091374893255,
      "loss": 6.9912,
      "step": 172
    },
    {
      "epoch": 0.04925266903914591,
      "grad_norm": 0.4368981122970581,
      "learning_rate": 0.0004877597495018503,
      "loss": 7.1592,
      "step": 173
    },
    {
      "epoch": 0.04953736654804271,
      "grad_norm": 0.3200138211250305,
      "learning_rate": 0.000487688585254768,
      "loss": 7.5615,
      "step": 174
    },
    {
      "epoch": 0.0498220640569395,
      "grad_norm": 0.3974745571613312,
      "learning_rate": 0.0004876174210076857,
      "loss": 7.0674,
      "step": 175
    },
    {
      "epoch": 0.0501067615658363,
      "grad_norm": 0.3726997673511505,
      "learning_rate": 0.0004875462567606035,
      "loss": 7.3057,
      "step": 176
    },
    {
      "epoch": 0.050391459074733094,
      "grad_norm": 0.37156978249549866,
      "learning_rate": 0.0004874750925135212,
      "loss": 7.3672,
      "step": 177
    },
    {
      "epoch": 0.050676156583629894,
      "grad_norm": 0.3446742594242096,
      "learning_rate": 0.000487403928266439,
      "loss": 7.3379,
      "step": 178
    },
    {
      "epoch": 0.05096085409252669,
      "grad_norm": 0.3899495601654053,
      "learning_rate": 0.00048733276401935667,
      "loss": 7.0742,
      "step": 179
    },
    {
      "epoch": 0.05124555160142349,
      "grad_norm": 0.365047425031662,
      "learning_rate": 0.0004872615997722744,
      "loss": 7.2949,
      "step": 180
    },
    {
      "epoch": 0.05153024911032029,
      "grad_norm": 0.3096078336238861,
      "learning_rate": 0.00048719043552519217,
      "loss": 7.585,
      "step": 181
    },
    {
      "epoch": 0.05181494661921708,
      "grad_norm": 0.3642439842224121,
      "learning_rate": 0.0004871192712781099,
      "loss": 7.293,
      "step": 182
    },
    {
      "epoch": 0.05209964412811388,
      "grad_norm": 0.31257525086402893,
      "learning_rate": 0.00048704810703102767,
      "loss": 7.6279,
      "step": 183
    },
    {
      "epoch": 0.052384341637010674,
      "grad_norm": 0.3482113480567932,
      "learning_rate": 0.00048697694278394533,
      "loss": 7.4062,
      "step": 184
    },
    {
      "epoch": 0.052669039145907474,
      "grad_norm": 0.40061259269714355,
      "learning_rate": 0.00048690577853686306,
      "loss": 6.9482,
      "step": 185
    },
    {
      "epoch": 0.05295373665480427,
      "grad_norm": 0.4041823744773865,
      "learning_rate": 0.00048683461428978083,
      "loss": 7.1836,
      "step": 186
    },
    {
      "epoch": 0.05323843416370107,
      "grad_norm": 0.41112077236175537,
      "learning_rate": 0.00048676345004269856,
      "loss": 6.8662,
      "step": 187
    },
    {
      "epoch": 0.05352313167259787,
      "grad_norm": 0.38859066367149353,
      "learning_rate": 0.00048669228579561633,
      "loss": 7.626,
      "step": 188
    },
    {
      "epoch": 0.05380782918149466,
      "grad_norm": 0.3483869135379791,
      "learning_rate": 0.00048662112154853406,
      "loss": 7.2656,
      "step": 189
    },
    {
      "epoch": 0.05409252669039146,
      "grad_norm": 0.36553943157196045,
      "learning_rate": 0.0004865499573014517,
      "loss": 7.0918,
      "step": 190
    },
    {
      "epoch": 0.054377224199288254,
      "grad_norm": 0.45641273260116577,
      "learning_rate": 0.0004864787930543695,
      "loss": 7.1328,
      "step": 191
    },
    {
      "epoch": 0.054661921708185054,
      "grad_norm": 0.5525060296058655,
      "learning_rate": 0.0004864076288072872,
      "loss": 6.6797,
      "step": 192
    },
    {
      "epoch": 0.05494661921708185,
      "grad_norm": 0.35605090856552124,
      "learning_rate": 0.00048633646456020495,
      "loss": 7.3145,
      "step": 193
    },
    {
      "epoch": 0.05523131672597865,
      "grad_norm": 0.33720216155052185,
      "learning_rate": 0.0004862653003131227,
      "loss": 7.583,
      "step": 194
    },
    {
      "epoch": 0.05551601423487545,
      "grad_norm": 0.39028072357177734,
      "learning_rate": 0.0004861941360660404,
      "loss": 7.041,
      "step": 195
    },
    {
      "epoch": 0.05580071174377224,
      "grad_norm": 0.37324032187461853,
      "learning_rate": 0.00048612297181895817,
      "loss": 7.5771,
      "step": 196
    },
    {
      "epoch": 0.05608540925266904,
      "grad_norm": 0.3692830204963684,
      "learning_rate": 0.0004860518075718759,
      "loss": 6.9414,
      "step": 197
    },
    {
      "epoch": 0.056370106761565834,
      "grad_norm": 0.3400883674621582,
      "learning_rate": 0.0004859806433247936,
      "loss": 7.6484,
      "step": 198
    },
    {
      "epoch": 0.056654804270462635,
      "grad_norm": 0.36927610635757446,
      "learning_rate": 0.0004859094790777114,
      "loss": 7.0127,
      "step": 199
    },
    {
      "epoch": 0.05693950177935943,
      "grad_norm": 0.3512347936630249,
      "learning_rate": 0.0004858383148306291,
      "loss": 7.5596,
      "step": 200
    },
    {
      "epoch": 0.05693950177935943,
      "eval_bleu": 0.04428465159832603,
      "eval_loss": 7.046875,
      "eval_runtime": 206.9516,
      "eval_samples_per_second": 1.372,
      "eval_steps_per_second": 0.087,
      "step": 200
    },
    {
      "epoch": 0.05722419928825623,
      "grad_norm": 0.40993812680244446,
      "learning_rate": 0.00048576715058354684,
      "loss": 7.165,
      "step": 201
    },
    {
      "epoch": 0.05750889679715303,
      "grad_norm": 0.37675940990448,
      "learning_rate": 0.00048569598633646456,
      "loss": 7.5469,
      "step": 202
    },
    {
      "epoch": 0.05779359430604982,
      "grad_norm": 0.41827306151390076,
      "learning_rate": 0.0004856248220893823,
      "loss": 7.252,
      "step": 203
    },
    {
      "epoch": 0.05807829181494662,
      "grad_norm": 0.40496769547462463,
      "learning_rate": 0.00048555365784230006,
      "loss": 7.3262,
      "step": 204
    },
    {
      "epoch": 0.058362989323843414,
      "grad_norm": 0.4544559121131897,
      "learning_rate": 0.0004854824935952178,
      "loss": 7.2686,
      "step": 205
    },
    {
      "epoch": 0.058647686832740215,
      "grad_norm": 0.43348002433776855,
      "learning_rate": 0.0004854113293481355,
      "loss": 7.1436,
      "step": 206
    },
    {
      "epoch": 0.05893238434163701,
      "grad_norm": 0.5123205780982971,
      "learning_rate": 0.00048534016510105323,
      "loss": 6.9443,
      "step": 207
    },
    {
      "epoch": 0.05921708185053381,
      "grad_norm": 0.3473433554172516,
      "learning_rate": 0.00048526900085397095,
      "loss": 7.4775,
      "step": 208
    },
    {
      "epoch": 0.05950177935943061,
      "grad_norm": 0.3604001998901367,
      "learning_rate": 0.00048519783660688873,
      "loss": 7.5098,
      "step": 209
    },
    {
      "epoch": 0.0597864768683274,
      "grad_norm": 0.33064237236976624,
      "learning_rate": 0.00048512667235980645,
      "loss": 7.6191,
      "step": 210
    },
    {
      "epoch": 0.0600711743772242,
      "grad_norm": 0.5796976089477539,
      "learning_rate": 0.0004850555081127242,
      "loss": 6.7324,
      "step": 211
    },
    {
      "epoch": 0.060355871886120994,
      "grad_norm": 0.3960364758968353,
      "learning_rate": 0.0004849843438656419,
      "loss": 7.2354,
      "step": 212
    },
    {
      "epoch": 0.060640569395017795,
      "grad_norm": 0.3528068959712982,
      "learning_rate": 0.0004849131796185596,
      "loss": 7.4775,
      "step": 213
    },
    {
      "epoch": 0.06092526690391459,
      "grad_norm": 0.33591997623443604,
      "learning_rate": 0.0004848420153714774,
      "loss": 7.5264,
      "step": 214
    },
    {
      "epoch": 0.06120996441281139,
      "grad_norm": 0.3542058765888214,
      "learning_rate": 0.0004847708511243951,
      "loss": 7.4365,
      "step": 215
    },
    {
      "epoch": 0.06149466192170819,
      "grad_norm": 0.4078296422958374,
      "learning_rate": 0.00048469968687731284,
      "loss": 6.9795,
      "step": 216
    },
    {
      "epoch": 0.06177935943060498,
      "grad_norm": 0.3528203070163727,
      "learning_rate": 0.0004846285226302306,
      "loss": 7.251,
      "step": 217
    },
    {
      "epoch": 0.06206405693950178,
      "grad_norm": 0.454461932182312,
      "learning_rate": 0.0004845573583831483,
      "loss": 6.876,
      "step": 218
    },
    {
      "epoch": 0.062348754448398575,
      "grad_norm": 0.39278143644332886,
      "learning_rate": 0.00048448619413606606,
      "loss": 7.2432,
      "step": 219
    },
    {
      "epoch": 0.06263345195729537,
      "grad_norm": 0.4267103374004364,
      "learning_rate": 0.0004844150298889838,
      "loss": 6.8594,
      "step": 220
    },
    {
      "epoch": 0.06291814946619217,
      "grad_norm": 0.3892408609390259,
      "learning_rate": 0.0004843438656419015,
      "loss": 6.8838,
      "step": 221
    },
    {
      "epoch": 0.06320284697508897,
      "grad_norm": 0.3549274802207947,
      "learning_rate": 0.0004842727013948193,
      "loss": 7.2236,
      "step": 222
    },
    {
      "epoch": 0.06348754448398576,
      "grad_norm": 0.33616870641708374,
      "learning_rate": 0.000484201537147737,
      "loss": 7.7422,
      "step": 223
    },
    {
      "epoch": 0.06377224199288256,
      "grad_norm": 0.33883076906204224,
      "learning_rate": 0.0004841303729006547,
      "loss": 7.5654,
      "step": 224
    },
    {
      "epoch": 0.06405693950177936,
      "grad_norm": 0.3884200155735016,
      "learning_rate": 0.00048405920865357246,
      "loss": 7.3457,
      "step": 225
    },
    {
      "epoch": 0.06434163701067616,
      "grad_norm": 0.3521203398704529,
      "learning_rate": 0.0004839880444064902,
      "loss": 7.2559,
      "step": 226
    },
    {
      "epoch": 0.06462633451957295,
      "grad_norm": 0.5035505294799805,
      "learning_rate": 0.00048391688015940796,
      "loss": 7.1816,
      "step": 227
    },
    {
      "epoch": 0.06491103202846975,
      "grad_norm": 0.37743714451789856,
      "learning_rate": 0.0004838457159123257,
      "loss": 7.5049,
      "step": 228
    },
    {
      "epoch": 0.06519572953736655,
      "grad_norm": 0.3604282736778259,
      "learning_rate": 0.00048377455166524335,
      "loss": 7.7832,
      "step": 229
    },
    {
      "epoch": 0.06548042704626335,
      "grad_norm": 0.6729333400726318,
      "learning_rate": 0.0004837033874181611,
      "loss": 6.8105,
      "step": 230
    },
    {
      "epoch": 0.06576512455516015,
      "grad_norm": 0.41351523995399475,
      "learning_rate": 0.00048363222317107885,
      "loss": 6.7354,
      "step": 231
    },
    {
      "epoch": 0.06604982206405693,
      "grad_norm": 0.3609699308872223,
      "learning_rate": 0.0004835610589239966,
      "loss": 7.3662,
      "step": 232
    },
    {
      "epoch": 0.06633451957295373,
      "grad_norm": 0.34872451424598694,
      "learning_rate": 0.00048348989467691435,
      "loss": 7.4785,
      "step": 233
    },
    {
      "epoch": 0.06661921708185053,
      "grad_norm": 0.3711255192756653,
      "learning_rate": 0.00048341873042983207,
      "loss": 7.2041,
      "step": 234
    },
    {
      "epoch": 0.06690391459074733,
      "grad_norm": 0.4340938925743103,
      "learning_rate": 0.0004833475661827498,
      "loss": 6.8232,
      "step": 235
    },
    {
      "epoch": 0.06718861209964413,
      "grad_norm": 0.3729588985443115,
      "learning_rate": 0.0004832764019356675,
      "loss": 7.2197,
      "step": 236
    },
    {
      "epoch": 0.06747330960854092,
      "grad_norm": 0.429115355014801,
      "learning_rate": 0.00048320523768858524,
      "loss": 7.1836,
      "step": 237
    },
    {
      "epoch": 0.06775800711743772,
      "grad_norm": 0.47582298517227173,
      "learning_rate": 0.000483134073441503,
      "loss": 6.8828,
      "step": 238
    },
    {
      "epoch": 0.06804270462633452,
      "grad_norm": 0.44467341899871826,
      "learning_rate": 0.00048306290919442074,
      "loss": 7.1777,
      "step": 239
    },
    {
      "epoch": 0.06832740213523132,
      "grad_norm": 0.438344269990921,
      "learning_rate": 0.00048299174494733846,
      "loss": 6.8701,
      "step": 240
    },
    {
      "epoch": 0.06861209964412811,
      "grad_norm": 0.4138439893722534,
      "learning_rate": 0.0004829205807002562,
      "loss": 7.1123,
      "step": 241
    },
    {
      "epoch": 0.06889679715302491,
      "grad_norm": 0.39080458879470825,
      "learning_rate": 0.0004828494164531739,
      "loss": 7.3242,
      "step": 242
    },
    {
      "epoch": 0.06918149466192171,
      "grad_norm": 0.43729132413864136,
      "learning_rate": 0.0004827782522060917,
      "loss": 6.7295,
      "step": 243
    },
    {
      "epoch": 0.06946619217081851,
      "grad_norm": 0.4565962851047516,
      "learning_rate": 0.0004827070879590094,
      "loss": 7.4229,
      "step": 244
    },
    {
      "epoch": 0.06975088967971531,
      "grad_norm": 0.4301145374774933,
      "learning_rate": 0.0004826359237119272,
      "loss": 7.3975,
      "step": 245
    },
    {
      "epoch": 0.0700355871886121,
      "grad_norm": 0.4343007504940033,
      "learning_rate": 0.00048256475946484485,
      "loss": 7.1396,
      "step": 246
    },
    {
      "epoch": 0.0703202846975089,
      "grad_norm": 0.3824312388896942,
      "learning_rate": 0.0004824935952177626,
      "loss": 7.3086,
      "step": 247
    },
    {
      "epoch": 0.0706049822064057,
      "grad_norm": 0.38155433535575867,
      "learning_rate": 0.00048242243097068035,
      "loss": 7.1982,
      "step": 248
    },
    {
      "epoch": 0.0708896797153025,
      "grad_norm": 0.36901432275772095,
      "learning_rate": 0.00048235126672359807,
      "loss": 7.3613,
      "step": 249
    },
    {
      "epoch": 0.0711743772241993,
      "grad_norm": 0.368551105260849,
      "learning_rate": 0.00048228010247651585,
      "loss": 7.3828,
      "step": 250
    },
    {
      "epoch": 0.07145907473309608,
      "grad_norm": 0.3908926844596863,
      "learning_rate": 0.00048220893822943357,
      "loss": 6.9717,
      "step": 251
    },
    {
      "epoch": 0.07174377224199288,
      "grad_norm": 0.46843716502189636,
      "learning_rate": 0.00048213777398235124,
      "loss": 7.2441,
      "step": 252
    },
    {
      "epoch": 0.07202846975088968,
      "grad_norm": 0.4673958420753479,
      "learning_rate": 0.000482066609735269,
      "loss": 7.4209,
      "step": 253
    },
    {
      "epoch": 0.07231316725978648,
      "grad_norm": 0.45724111795425415,
      "learning_rate": 0.00048199544548818674,
      "loss": 6.9893,
      "step": 254
    },
    {
      "epoch": 0.07259786476868327,
      "grad_norm": 0.48041409254074097,
      "learning_rate": 0.00048192428124110446,
      "loss": 6.8398,
      "step": 255
    },
    {
      "epoch": 0.07288256227758007,
      "grad_norm": 0.3993391692638397,
      "learning_rate": 0.00048185311699402224,
      "loss": 7.3496,
      "step": 256
    },
    {
      "epoch": 0.07316725978647687,
      "grad_norm": 0.38099411129951477,
      "learning_rate": 0.0004817819527469399,
      "loss": 7.5664,
      "step": 257
    },
    {
      "epoch": 0.07345195729537367,
      "grad_norm": 0.36448702216148376,
      "learning_rate": 0.0004817107884998577,
      "loss": 7.5957,
      "step": 258
    },
    {
      "epoch": 0.07373665480427047,
      "grad_norm": 0.3705407977104187,
      "learning_rate": 0.0004816396242527754,
      "loss": 7.4092,
      "step": 259
    },
    {
      "epoch": 0.07402135231316725,
      "grad_norm": 0.4222451150417328,
      "learning_rate": 0.00048156846000569313,
      "loss": 7.2832,
      "step": 260
    },
    {
      "epoch": 0.07430604982206405,
      "grad_norm": 0.43533411622047424,
      "learning_rate": 0.0004814972957586109,
      "loss": 7.0967,
      "step": 261
    },
    {
      "epoch": 0.07459074733096085,
      "grad_norm": 0.45843270421028137,
      "learning_rate": 0.00048142613151152863,
      "loss": 7.1719,
      "step": 262
    },
    {
      "epoch": 0.07487544483985765,
      "grad_norm": 0.38327649235725403,
      "learning_rate": 0.00048135496726444635,
      "loss": 7.2725,
      "step": 263
    },
    {
      "epoch": 0.07516014234875446,
      "grad_norm": 0.4202622175216675,
      "learning_rate": 0.0004812838030173641,
      "loss": 7.4424,
      "step": 264
    },
    {
      "epoch": 0.07544483985765124,
      "grad_norm": 0.5358533263206482,
      "learning_rate": 0.0004812126387702818,
      "loss": 6.6904,
      "step": 265
    },
    {
      "epoch": 0.07572953736654804,
      "grad_norm": 0.4957423806190491,
      "learning_rate": 0.0004811414745231996,
      "loss": 6.8877,
      "step": 266
    },
    {
      "epoch": 0.07601423487544484,
      "grad_norm": 0.40162715315818787,
      "learning_rate": 0.0004810703102761173,
      "loss": 7.21,
      "step": 267
    },
    {
      "epoch": 0.07629893238434164,
      "grad_norm": 0.46528732776641846,
      "learning_rate": 0.000480999146029035,
      "loss": 7.2451,
      "step": 268
    },
    {
      "epoch": 0.07658362989323843,
      "grad_norm": 0.4028266668319702,
      "learning_rate": 0.00048092798178195274,
      "loss": 7.4043,
      "step": 269
    },
    {
      "epoch": 0.07686832740213523,
      "grad_norm": 0.46121296286582947,
      "learning_rate": 0.00048085681753487047,
      "loss": 7.2305,
      "step": 270
    },
    {
      "epoch": 0.07715302491103203,
      "grad_norm": 0.43850409984588623,
      "learning_rate": 0.00048078565328778824,
      "loss": 7.1523,
      "step": 271
    },
    {
      "epoch": 0.07743772241992883,
      "grad_norm": 0.36082831025123596,
      "learning_rate": 0.00048071448904070597,
      "loss": 7.6426,
      "step": 272
    },
    {
      "epoch": 0.07772241992882563,
      "grad_norm": 0.42451992630958557,
      "learning_rate": 0.0004806433247936237,
      "loss": 7.5566,
      "step": 273
    },
    {
      "epoch": 0.07800711743772241,
      "grad_norm": 0.5189881324768066,
      "learning_rate": 0.0004805721605465414,
      "loss": 6.9717,
      "step": 274
    },
    {
      "epoch": 0.07829181494661921,
      "grad_norm": 0.34993991255760193,
      "learning_rate": 0.00048050099629945914,
      "loss": 7.75,
      "step": 275
    },
    {
      "epoch": 0.07857651245551601,
      "grad_norm": 0.38793134689331055,
      "learning_rate": 0.0004804298320523769,
      "loss": 7.0029,
      "step": 276
    },
    {
      "epoch": 0.07886120996441282,
      "grad_norm": 0.4095827639102936,
      "learning_rate": 0.00048035866780529464,
      "loss": 7.1084,
      "step": 277
    },
    {
      "epoch": 0.07914590747330962,
      "grad_norm": 0.400534987449646,
      "learning_rate": 0.00048028750355821236,
      "loss": 7.5254,
      "step": 278
    },
    {
      "epoch": 0.0794306049822064,
      "grad_norm": 0.38447555899620056,
      "learning_rate": 0.00048021633931113013,
      "loss": 7.5615,
      "step": 279
    },
    {
      "epoch": 0.0797153024911032,
      "grad_norm": 0.4068629741668701,
      "learning_rate": 0.0004801451750640478,
      "loss": 7.459,
      "step": 280
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.39288195967674255,
      "learning_rate": 0.0004800740108169656,
      "loss": 7.1357,
      "step": 281
    },
    {
      "epoch": 0.0802846975088968,
      "grad_norm": 0.44044890999794006,
      "learning_rate": 0.0004800028465698833,
      "loss": 6.8838,
      "step": 282
    },
    {
      "epoch": 0.08056939501779359,
      "grad_norm": 0.3920339047908783,
      "learning_rate": 0.000479931682322801,
      "loss": 7.4258,
      "step": 283
    },
    {
      "epoch": 0.08085409252669039,
      "grad_norm": 0.42204195261001587,
      "learning_rate": 0.0004798605180757188,
      "loss": 7.2412,
      "step": 284
    },
    {
      "epoch": 0.08113879003558719,
      "grad_norm": 0.4086628258228302,
      "learning_rate": 0.00047978935382863647,
      "loss": 7.4141,
      "step": 285
    },
    {
      "epoch": 0.08142348754448399,
      "grad_norm": 0.48516544699668884,
      "learning_rate": 0.0004797181895815542,
      "loss": 6.7148,
      "step": 286
    },
    {
      "epoch": 0.08170818505338079,
      "grad_norm": 0.4337480366230011,
      "learning_rate": 0.00047964702533447197,
      "loss": 7.1201,
      "step": 287
    },
    {
      "epoch": 0.08199288256227757,
      "grad_norm": 0.4319247305393219,
      "learning_rate": 0.0004795758610873897,
      "loss": 7.1279,
      "step": 288
    },
    {
      "epoch": 0.08227758007117437,
      "grad_norm": 0.49747705459594727,
      "learning_rate": 0.00047950469684030747,
      "loss": 7.2217,
      "step": 289
    },
    {
      "epoch": 0.08256227758007118,
      "grad_norm": 0.39577603340148926,
      "learning_rate": 0.0004794335325932252,
      "loss": 7.4053,
      "step": 290
    },
    {
      "epoch": 0.08284697508896798,
      "grad_norm": 0.4405271112918854,
      "learning_rate": 0.00047936236834614286,
      "loss": 7.0312,
      "step": 291
    },
    {
      "epoch": 0.08313167259786478,
      "grad_norm": 0.3082965910434723,
      "learning_rate": 0.00047929120409906064,
      "loss": 7.7031,
      "step": 292
    },
    {
      "epoch": 0.08341637010676156,
      "grad_norm": 0.5239378809928894,
      "learning_rate": 0.00047922003985197836,
      "loss": 6.9287,
      "step": 293
    },
    {
      "epoch": 0.08370106761565836,
      "grad_norm": 0.38422203063964844,
      "learning_rate": 0.00047914887560489614,
      "loss": 7.251,
      "step": 294
    },
    {
      "epoch": 0.08398576512455516,
      "grad_norm": 0.44853416085243225,
      "learning_rate": 0.00047907771135781386,
      "loss": 7.0537,
      "step": 295
    },
    {
      "epoch": 0.08427046263345196,
      "grad_norm": 0.3908516466617584,
      "learning_rate": 0.0004790065471107316,
      "loss": 7.5029,
      "step": 296
    },
    {
      "epoch": 0.08455516014234875,
      "grad_norm": 0.5178523659706116,
      "learning_rate": 0.0004789353828636493,
      "loss": 6.6699,
      "step": 297
    },
    {
      "epoch": 0.08483985765124555,
      "grad_norm": 0.39277517795562744,
      "learning_rate": 0.00047886421861656703,
      "loss": 7.6855,
      "step": 298
    },
    {
      "epoch": 0.08512455516014235,
      "grad_norm": 0.4601745307445526,
      "learning_rate": 0.0004787930543694848,
      "loss": 6.6934,
      "step": 299
    },
    {
      "epoch": 0.08540925266903915,
      "grad_norm": 0.4405549168586731,
      "learning_rate": 0.00047872189012240253,
      "loss": 7.2129,
      "step": 300
    },
    {
      "epoch": 0.08569395017793595,
      "grad_norm": 0.4540632665157318,
      "learning_rate": 0.00047865072587532025,
      "loss": 6.9492,
      "step": 301
    },
    {
      "epoch": 0.08597864768683273,
      "grad_norm": 0.4265691936016083,
      "learning_rate": 0.000478579561628238,
      "loss": 7.1064,
      "step": 302
    },
    {
      "epoch": 0.08626334519572953,
      "grad_norm": 0.45279815793037415,
      "learning_rate": 0.0004785083973811557,
      "loss": 7.001,
      "step": 303
    },
    {
      "epoch": 0.08654804270462634,
      "grad_norm": 0.4508437216281891,
      "learning_rate": 0.0004784372331340734,
      "loss": 6.9727,
      "step": 304
    },
    {
      "epoch": 0.08683274021352314,
      "grad_norm": 0.39535853266716003,
      "learning_rate": 0.0004783660688869912,
      "loss": 7.4971,
      "step": 305
    },
    {
      "epoch": 0.08711743772241994,
      "grad_norm": 0.38876810669898987,
      "learning_rate": 0.0004782949046399089,
      "loss": 6.96,
      "step": 306
    },
    {
      "epoch": 0.08740213523131672,
      "grad_norm": 0.3352130055427551,
      "learning_rate": 0.0004782237403928267,
      "loss": 7.5664,
      "step": 307
    },
    {
      "epoch": 0.08768683274021352,
      "grad_norm": 0.43718674778938293,
      "learning_rate": 0.00047815257614574437,
      "loss": 7.3652,
      "step": 308
    },
    {
      "epoch": 0.08797153024911032,
      "grad_norm": 0.41846373677253723,
      "learning_rate": 0.0004780814118986621,
      "loss": 7.4209,
      "step": 309
    },
    {
      "epoch": 0.08825622775800712,
      "grad_norm": 0.35628408193588257,
      "learning_rate": 0.00047801024765157987,
      "loss": 7.665,
      "step": 310
    },
    {
      "epoch": 0.08854092526690391,
      "grad_norm": 0.43138930201530457,
      "learning_rate": 0.0004779390834044976,
      "loss": 7.0596,
      "step": 311
    },
    {
      "epoch": 0.08882562277580071,
      "grad_norm": 0.41381263732910156,
      "learning_rate": 0.00047786791915741537,
      "loss": 7.4902,
      "step": 312
    },
    {
      "epoch": 0.08911032028469751,
      "grad_norm": 0.44806408882141113,
      "learning_rate": 0.00047779675491033303,
      "loss": 7.3066,
      "step": 313
    },
    {
      "epoch": 0.08939501779359431,
      "grad_norm": 0.41572806239128113,
      "learning_rate": 0.00047772559066325076,
      "loss": 7.1211,
      "step": 314
    },
    {
      "epoch": 0.08967971530249111,
      "grad_norm": 0.4351762533187866,
      "learning_rate": 0.00047765442641616853,
      "loss": 7.1748,
      "step": 315
    },
    {
      "epoch": 0.0899644128113879,
      "grad_norm": 0.5767992734909058,
      "learning_rate": 0.00047758326216908626,
      "loss": 6.5059,
      "step": 316
    },
    {
      "epoch": 0.0902491103202847,
      "grad_norm": 0.4354422092437744,
      "learning_rate": 0.00047751209792200403,
      "loss": 6.9727,
      "step": 317
    },
    {
      "epoch": 0.0905338078291815,
      "grad_norm": 0.48386743664741516,
      "learning_rate": 0.00047744093367492176,
      "loss": 7.5859,
      "step": 318
    },
    {
      "epoch": 0.0908185053380783,
      "grad_norm": 0.447219580411911,
      "learning_rate": 0.0004773697694278394,
      "loss": 7.5195,
      "step": 319
    },
    {
      "epoch": 0.0911032028469751,
      "grad_norm": 0.4001949727535248,
      "learning_rate": 0.0004772986051807572,
      "loss": 7.3672,
      "step": 320
    },
    {
      "epoch": 0.09138790035587188,
      "grad_norm": 0.4028257131576538,
      "learning_rate": 0.0004772274409336749,
      "loss": 7.1143,
      "step": 321
    },
    {
      "epoch": 0.09167259786476868,
      "grad_norm": 0.6439388990402222,
      "learning_rate": 0.00047715627668659265,
      "loss": 6.9395,
      "step": 322
    },
    {
      "epoch": 0.09195729537366548,
      "grad_norm": 0.434939980506897,
      "learning_rate": 0.0004770851124395104,
      "loss": 7.0215,
      "step": 323
    },
    {
      "epoch": 0.09224199288256228,
      "grad_norm": 0.4516619145870209,
      "learning_rate": 0.00047701394819242815,
      "loss": 6.9014,
      "step": 324
    },
    {
      "epoch": 0.09252669039145907,
      "grad_norm": 0.4030342102050781,
      "learning_rate": 0.00047694278394534587,
      "loss": 7.5859,
      "step": 325
    },
    {
      "epoch": 0.09281138790035587,
      "grad_norm": 0.3350996673107147,
      "learning_rate": 0.0004768716196982636,
      "loss": 7.5254,
      "step": 326
    },
    {
      "epoch": 0.09309608540925267,
      "grad_norm": 0.5201140642166138,
      "learning_rate": 0.0004768004554511813,
      "loss": 7.3613,
      "step": 327
    },
    {
      "epoch": 0.09338078291814947,
      "grad_norm": 0.3434700667858124,
      "learning_rate": 0.0004767292912040991,
      "loss": 7.5518,
      "step": 328
    },
    {
      "epoch": 0.09366548042704627,
      "grad_norm": 0.39710113406181335,
      "learning_rate": 0.0004766581269570168,
      "loss": 7.3145,
      "step": 329
    },
    {
      "epoch": 0.09395017793594305,
      "grad_norm": 0.3940322995185852,
      "learning_rate": 0.00047658696270993454,
      "loss": 7.4756,
      "step": 330
    },
    {
      "epoch": 0.09423487544483986,
      "grad_norm": 0.35256409645080566,
      "learning_rate": 0.00047651579846285226,
      "loss": 7.3662,
      "step": 331
    },
    {
      "epoch": 0.09451957295373666,
      "grad_norm": 0.490875780582428,
      "learning_rate": 0.00047644463421577,
      "loss": 6.9189,
      "step": 332
    },
    {
      "epoch": 0.09480427046263346,
      "grad_norm": 0.3466689884662628,
      "learning_rate": 0.00047637346996868776,
      "loss": 7.7227,
      "step": 333
    },
    {
      "epoch": 0.09508896797153026,
      "grad_norm": 0.41715121269226074,
      "learning_rate": 0.0004763023057216055,
      "loss": 7.4795,
      "step": 334
    },
    {
      "epoch": 0.09537366548042704,
      "grad_norm": 0.42658162117004395,
      "learning_rate": 0.00047623114147452326,
      "loss": 7.2119,
      "step": 335
    },
    {
      "epoch": 0.09565836298932384,
      "grad_norm": 0.4596523344516754,
      "learning_rate": 0.00047615997722744093,
      "loss": 7.0312,
      "step": 336
    },
    {
      "epoch": 0.09594306049822064,
      "grad_norm": 0.44432130455970764,
      "learning_rate": 0.00047608881298035865,
      "loss": 7.1562,
      "step": 337
    },
    {
      "epoch": 0.09622775800711744,
      "grad_norm": 0.41700947284698486,
      "learning_rate": 0.00047601764873327643,
      "loss": 7.0879,
      "step": 338
    },
    {
      "epoch": 0.09651245551601423,
      "grad_norm": 0.41166216135025024,
      "learning_rate": 0.00047594648448619415,
      "loss": 7.2783,
      "step": 339
    },
    {
      "epoch": 0.09679715302491103,
      "grad_norm": 0.3937351107597351,
      "learning_rate": 0.0004758753202391119,
      "loss": 7.4902,
      "step": 340
    },
    {
      "epoch": 0.09708185053380783,
      "grad_norm": 0.49600276350975037,
      "learning_rate": 0.00047580415599202965,
      "loss": 7.0215,
      "step": 341
    },
    {
      "epoch": 0.09736654804270463,
      "grad_norm": 0.39852508902549744,
      "learning_rate": 0.0004757329917449473,
      "loss": 7.5752,
      "step": 342
    },
    {
      "epoch": 0.09765124555160143,
      "grad_norm": 0.43215256929397583,
      "learning_rate": 0.0004756618274978651,
      "loss": 7.3994,
      "step": 343
    },
    {
      "epoch": 0.09793594306049822,
      "grad_norm": 0.395315945148468,
      "learning_rate": 0.0004755906632507828,
      "loss": 7.4033,
      "step": 344
    },
    {
      "epoch": 0.09822064056939502,
      "grad_norm": 0.4142919182777405,
      "learning_rate": 0.00047551949900370054,
      "loss": 7.3242,
      "step": 345
    },
    {
      "epoch": 0.09850533807829182,
      "grad_norm": 0.5080282092094421,
      "learning_rate": 0.0004754483347566183,
      "loss": 6.625,
      "step": 346
    },
    {
      "epoch": 0.09879003558718862,
      "grad_norm": 0.44504308700561523,
      "learning_rate": 0.000475377170509536,
      "loss": 7.668,
      "step": 347
    },
    {
      "epoch": 0.09907473309608542,
      "grad_norm": 0.5334663391113281,
      "learning_rate": 0.00047530600626245376,
      "loss": 7.2246,
      "step": 348
    },
    {
      "epoch": 0.0993594306049822,
      "grad_norm": 0.4300445020198822,
      "learning_rate": 0.0004752348420153715,
      "loss": 7.2812,
      "step": 349
    },
    {
      "epoch": 0.099644128113879,
      "grad_norm": 0.43815192580223083,
      "learning_rate": 0.0004751636777682892,
      "loss": 6.8447,
      "step": 350
    },
    {
      "epoch": 0.0999288256227758,
      "grad_norm": 0.45553478598594666,
      "learning_rate": 0.000475092513521207,
      "loss": 7.3037,
      "step": 351
    },
    {
      "epoch": 0.1002135231316726,
      "grad_norm": 0.4697282612323761,
      "learning_rate": 0.0004750213492741247,
      "loss": 7.0762,
      "step": 352
    },
    {
      "epoch": 0.10049822064056939,
      "grad_norm": 0.5708103179931641,
      "learning_rate": 0.0004749501850270424,
      "loss": 7.0889,
      "step": 353
    },
    {
      "epoch": 0.10078291814946619,
      "grad_norm": 0.46478471159935,
      "learning_rate": 0.00047487902077996015,
      "loss": 7.2305,
      "step": 354
    },
    {
      "epoch": 0.10106761565836299,
      "grad_norm": 0.4347301721572876,
      "learning_rate": 0.0004748078565328779,
      "loss": 7.417,
      "step": 355
    },
    {
      "epoch": 0.10135231316725979,
      "grad_norm": 0.3864136040210724,
      "learning_rate": 0.00047473669228579565,
      "loss": 7.7119,
      "step": 356
    },
    {
      "epoch": 0.10163701067615659,
      "grad_norm": 0.3752633333206177,
      "learning_rate": 0.0004746655280387134,
      "loss": 7.6006,
      "step": 357
    },
    {
      "epoch": 0.10192170818505338,
      "grad_norm": 0.5481764674186707,
      "learning_rate": 0.00047459436379163105,
      "loss": 7.0039,
      "step": 358
    },
    {
      "epoch": 0.10220640569395018,
      "grad_norm": 0.40612098574638367,
      "learning_rate": 0.0004745231995445488,
      "loss": 7.4434,
      "step": 359
    },
    {
      "epoch": 0.10249110320284698,
      "grad_norm": 0.4205334484577179,
      "learning_rate": 0.00047445203529746655,
      "loss": 7.2832,
      "step": 360
    },
    {
      "epoch": 0.10277580071174378,
      "grad_norm": 0.3059813678264618,
      "learning_rate": 0.0004743808710503843,
      "loss": 7.6846,
      "step": 361
    },
    {
      "epoch": 0.10306049822064058,
      "grad_norm": 0.48755931854248047,
      "learning_rate": 0.00047430970680330205,
      "loss": 6.9141,
      "step": 362
    },
    {
      "epoch": 0.10334519572953736,
      "grad_norm": 0.42173999547958374,
      "learning_rate": 0.00047423854255621977,
      "loss": 7.5166,
      "step": 363
    },
    {
      "epoch": 0.10362989323843416,
      "grad_norm": 0.5560771226882935,
      "learning_rate": 0.0004741673783091375,
      "loss": 7.418,
      "step": 364
    },
    {
      "epoch": 0.10391459074733096,
      "grad_norm": 0.6154012680053711,
      "learning_rate": 0.0004740962140620552,
      "loss": 7.2734,
      "step": 365
    },
    {
      "epoch": 0.10419928825622776,
      "grad_norm": 0.4501911997795105,
      "learning_rate": 0.000474025049814973,
      "loss": 6.9023,
      "step": 366
    },
    {
      "epoch": 0.10448398576512455,
      "grad_norm": 0.4486687183380127,
      "learning_rate": 0.0004739538855678907,
      "loss": 7.3291,
      "step": 367
    },
    {
      "epoch": 0.10476868327402135,
      "grad_norm": 0.406693696975708,
      "learning_rate": 0.00047388272132080844,
      "loss": 7.501,
      "step": 368
    },
    {
      "epoch": 0.10505338078291815,
      "grad_norm": 0.4880397915840149,
      "learning_rate": 0.0004738115570737262,
      "loss": 6.6982,
      "step": 369
    },
    {
      "epoch": 0.10533807829181495,
      "grad_norm": 0.4266977608203888,
      "learning_rate": 0.0004737403928266439,
      "loss": 7.2031,
      "step": 370
    },
    {
      "epoch": 0.10562277580071175,
      "grad_norm": 0.4237028658390045,
      "learning_rate": 0.0004736692285795616,
      "loss": 6.9111,
      "step": 371
    },
    {
      "epoch": 0.10590747330960854,
      "grad_norm": 0.5166893601417542,
      "learning_rate": 0.0004735980643324794,
      "loss": 7.3945,
      "step": 372
    },
    {
      "epoch": 0.10619217081850534,
      "grad_norm": 0.4069686233997345,
      "learning_rate": 0.0004735269000853971,
      "loss": 7.54,
      "step": 373
    },
    {
      "epoch": 0.10647686832740214,
      "grad_norm": 0.43857699632644653,
      "learning_rate": 0.0004734557358383149,
      "loss": 7.3135,
      "step": 374
    },
    {
      "epoch": 0.10676156583629894,
      "grad_norm": 0.39594122767448425,
      "learning_rate": 0.00047338457159123255,
      "loss": 7.8369,
      "step": 375
    },
    {
      "epoch": 0.10704626334519574,
      "grad_norm": 0.4282735586166382,
      "learning_rate": 0.00047331340734415027,
      "loss": 7.4375,
      "step": 376
    },
    {
      "epoch": 0.10733096085409252,
      "grad_norm": 0.45673078298568726,
      "learning_rate": 0.00047324224309706805,
      "loss": 7.3848,
      "step": 377
    },
    {
      "epoch": 0.10761565836298932,
      "grad_norm": 0.4529609978199005,
      "learning_rate": 0.00047317107884998577,
      "loss": 7.1904,
      "step": 378
    },
    {
      "epoch": 0.10790035587188612,
      "grad_norm": 0.508343517780304,
      "learning_rate": 0.00047309991460290355,
      "loss": 7.1104,
      "step": 379
    },
    {
      "epoch": 0.10818505338078292,
      "grad_norm": 0.5695661306381226,
      "learning_rate": 0.00047302875035582127,
      "loss": 6.9775,
      "step": 380
    },
    {
      "epoch": 0.10846975088967971,
      "grad_norm": 0.4289855659008026,
      "learning_rate": 0.00047295758610873894,
      "loss": 7.3799,
      "step": 381
    },
    {
      "epoch": 0.10875444839857651,
      "grad_norm": 0.45409122109413147,
      "learning_rate": 0.0004728864218616567,
      "loss": 6.877,
      "step": 382
    },
    {
      "epoch": 0.10903914590747331,
      "grad_norm": 0.43355792760849,
      "learning_rate": 0.00047281525761457444,
      "loss": 7.1543,
      "step": 383
    },
    {
      "epoch": 0.10932384341637011,
      "grad_norm": 0.3788268268108368,
      "learning_rate": 0.00047274409336749216,
      "loss": 7.582,
      "step": 384
    },
    {
      "epoch": 0.10960854092526691,
      "grad_norm": 0.4172390103340149,
      "learning_rate": 0.00047267292912040994,
      "loss": 7.4766,
      "step": 385
    },
    {
      "epoch": 0.1098932384341637,
      "grad_norm": 0.5651355385780334,
      "learning_rate": 0.00047260176487332766,
      "loss": 6.8984,
      "step": 386
    },
    {
      "epoch": 0.1101779359430605,
      "grad_norm": 0.4061424136161804,
      "learning_rate": 0.0004725306006262454,
      "loss": 7.6387,
      "step": 387
    },
    {
      "epoch": 0.1104626334519573,
      "grad_norm": 0.41827505826950073,
      "learning_rate": 0.0004724594363791631,
      "loss": 7.2285,
      "step": 388
    },
    {
      "epoch": 0.1107473309608541,
      "grad_norm": 0.37790295481681824,
      "learning_rate": 0.00047238827213208083,
      "loss": 7.583,
      "step": 389
    },
    {
      "epoch": 0.1110320284697509,
      "grad_norm": 0.41281452775001526,
      "learning_rate": 0.0004723171078849986,
      "loss": 7.4941,
      "step": 390
    },
    {
      "epoch": 0.11131672597864768,
      "grad_norm": 0.41881147027015686,
      "learning_rate": 0.00047224594363791633,
      "loss": 7.3848,
      "step": 391
    },
    {
      "epoch": 0.11160142348754448,
      "grad_norm": 0.41661399602890015,
      "learning_rate": 0.00047217477939083405,
      "loss": 7.4121,
      "step": 392
    },
    {
      "epoch": 0.11188612099644128,
      "grad_norm": 0.4192899763584137,
      "learning_rate": 0.0004721036151437518,
      "loss": 7.0596,
      "step": 393
    },
    {
      "epoch": 0.11217081850533808,
      "grad_norm": 0.45563167333602905,
      "learning_rate": 0.0004720324508966695,
      "loss": 7.4004,
      "step": 394
    },
    {
      "epoch": 0.11245551601423487,
      "grad_norm": 0.4283228814601898,
      "learning_rate": 0.0004719612866495873,
      "loss": 7.6162,
      "step": 395
    },
    {
      "epoch": 0.11274021352313167,
      "grad_norm": 0.4331873655319214,
      "learning_rate": 0.000471890122402505,
      "loss": 7.001,
      "step": 396
    },
    {
      "epoch": 0.11302491103202847,
      "grad_norm": 0.4580132067203522,
      "learning_rate": 0.0004718189581554228,
      "loss": 7.0654,
      "step": 397
    },
    {
      "epoch": 0.11330960854092527,
      "grad_norm": 0.46618202328681946,
      "learning_rate": 0.00047174779390834044,
      "loss": 7.0742,
      "step": 398
    },
    {
      "epoch": 0.11359430604982207,
      "grad_norm": 0.4677276909351349,
      "learning_rate": 0.00047167662966125817,
      "loss": 7.1973,
      "step": 399
    },
    {
      "epoch": 0.11387900355871886,
      "grad_norm": 0.4366690516471863,
      "learning_rate": 0.00047160546541417594,
      "loss": 7.4443,
      "step": 400
    },
    {
      "epoch": 0.11387900355871886,
      "eval_bleu": 0.06055701380020064,
      "eval_loss": 7.00390625,
      "eval_runtime": 195.2568,
      "eval_samples_per_second": 1.454,
      "eval_steps_per_second": 0.092,
      "step": 400
    },
    {
      "epoch": 0.11416370106761566,
      "grad_norm": 0.3341705799102783,
      "learning_rate": 0.00047153430116709367,
      "loss": 7.7119,
      "step": 401
    },
    {
      "epoch": 0.11444839857651246,
      "grad_norm": 0.4076211154460907,
      "learning_rate": 0.0004714631369200114,
      "loss": 7.4521,
      "step": 402
    },
    {
      "epoch": 0.11473309608540926,
      "grad_norm": 0.3936387598514557,
      "learning_rate": 0.0004713919726729291,
      "loss": 7.2227,
      "step": 403
    },
    {
      "epoch": 0.11501779359430606,
      "grad_norm": 0.39969155192375183,
      "learning_rate": 0.00047132080842584683,
      "loss": 7.5381,
      "step": 404
    },
    {
      "epoch": 0.11530249110320284,
      "grad_norm": 0.46734946966171265,
      "learning_rate": 0.0004712496441787646,
      "loss": 7.4033,
      "step": 405
    },
    {
      "epoch": 0.11558718861209964,
      "grad_norm": 0.4855714738368988,
      "learning_rate": 0.00047117847993168233,
      "loss": 7.457,
      "step": 406
    },
    {
      "epoch": 0.11587188612099644,
      "grad_norm": 0.5113205313682556,
      "learning_rate": 0.00047110731568460006,
      "loss": 7.1904,
      "step": 407
    },
    {
      "epoch": 0.11615658362989324,
      "grad_norm": 0.3756895065307617,
      "learning_rate": 0.00047103615143751783,
      "loss": 7.5938,
      "step": 408
    },
    {
      "epoch": 0.11644128113879003,
      "grad_norm": 0.37924525141716003,
      "learning_rate": 0.0004709649871904355,
      "loss": 7.585,
      "step": 409
    },
    {
      "epoch": 0.11672597864768683,
      "grad_norm": 0.4761236608028412,
      "learning_rate": 0.0004708938229433533,
      "loss": 7.3105,
      "step": 410
    },
    {
      "epoch": 0.11701067615658363,
      "grad_norm": 0.4178760051727295,
      "learning_rate": 0.000470822658696271,
      "loss": 7.501,
      "step": 411
    },
    {
      "epoch": 0.11729537366548043,
      "grad_norm": 0.5172440409660339,
      "learning_rate": 0.0004707514944491887,
      "loss": 7.1963,
      "step": 412
    },
    {
      "epoch": 0.11758007117437723,
      "grad_norm": 0.7484517693519592,
      "learning_rate": 0.0004706803302021065,
      "loss": 6.7061,
      "step": 413
    },
    {
      "epoch": 0.11786476868327402,
      "grad_norm": 0.47221601009368896,
      "learning_rate": 0.0004706091659550242,
      "loss": 7.7109,
      "step": 414
    },
    {
      "epoch": 0.11814946619217082,
      "grad_norm": 0.39013606309890747,
      "learning_rate": 0.00047053800170794195,
      "loss": 7.75,
      "step": 415
    },
    {
      "epoch": 0.11843416370106762,
      "grad_norm": 0.549603283405304,
      "learning_rate": 0.00047046683746085967,
      "loss": 6.8496,
      "step": 416
    },
    {
      "epoch": 0.11871886120996442,
      "grad_norm": 0.5139209628105164,
      "learning_rate": 0.0004703956732137774,
      "loss": 7.0254,
      "step": 417
    },
    {
      "epoch": 0.11900355871886122,
      "grad_norm": 0.5115612745285034,
      "learning_rate": 0.00047032450896669517,
      "loss": 7.21,
      "step": 418
    },
    {
      "epoch": 0.119288256227758,
      "grad_norm": 0.4744890034198761,
      "learning_rate": 0.0004702533447196129,
      "loss": 7.0742,
      "step": 419
    },
    {
      "epoch": 0.1195729537366548,
      "grad_norm": 0.43311771750450134,
      "learning_rate": 0.00047018218047253056,
      "loss": 7.0781,
      "step": 420
    },
    {
      "epoch": 0.1198576512455516,
      "grad_norm": 0.4734751582145691,
      "learning_rate": 0.00047011101622544834,
      "loss": 7.2168,
      "step": 421
    },
    {
      "epoch": 0.1201423487544484,
      "grad_norm": 0.5691256523132324,
      "learning_rate": 0.00047003985197836606,
      "loss": 6.623,
      "step": 422
    },
    {
      "epoch": 0.12042704626334519,
      "grad_norm": 0.40093910694122314,
      "learning_rate": 0.00046996868773128384,
      "loss": 7.4346,
      "step": 423
    },
    {
      "epoch": 0.12071174377224199,
      "grad_norm": 0.42212653160095215,
      "learning_rate": 0.00046989752348420156,
      "loss": 7.4355,
      "step": 424
    },
    {
      "epoch": 0.12099644128113879,
      "grad_norm": 0.44689905643463135,
      "learning_rate": 0.0004698263592371193,
      "loss": 7.6377,
      "step": 425
    },
    {
      "epoch": 0.12128113879003559,
      "grad_norm": 0.4593307375907898,
      "learning_rate": 0.000469755194990037,
      "loss": 7.4443,
      "step": 426
    },
    {
      "epoch": 0.12156583629893239,
      "grad_norm": 0.5486469268798828,
      "learning_rate": 0.00046968403074295473,
      "loss": 6.8281,
      "step": 427
    },
    {
      "epoch": 0.12185053380782918,
      "grad_norm": 0.4106900990009308,
      "learning_rate": 0.0004696128664958725,
      "loss": 7.2324,
      "step": 428
    },
    {
      "epoch": 0.12213523131672598,
      "grad_norm": 0.42247626185417175,
      "learning_rate": 0.00046954170224879023,
      "loss": 7.2988,
      "step": 429
    },
    {
      "epoch": 0.12241992882562278,
      "grad_norm": 0.5033213496208191,
      "learning_rate": 0.00046947053800170795,
      "loss": 7.291,
      "step": 430
    },
    {
      "epoch": 0.12270462633451958,
      "grad_norm": 0.42371541261672974,
      "learning_rate": 0.0004693993737546257,
      "loss": 7.458,
      "step": 431
    },
    {
      "epoch": 0.12298932384341638,
      "grad_norm": 0.5228193998336792,
      "learning_rate": 0.0004693282095075434,
      "loss": 7.1699,
      "step": 432
    },
    {
      "epoch": 0.12327402135231316,
      "grad_norm": 0.3674987852573395,
      "learning_rate": 0.0004692570452604611,
      "loss": 7.5742,
      "step": 433
    },
    {
      "epoch": 0.12355871886120996,
      "grad_norm": 0.5292471051216125,
      "learning_rate": 0.0004691858810133789,
      "loss": 6.7803,
      "step": 434
    },
    {
      "epoch": 0.12384341637010676,
      "grad_norm": 0.49759700894355774,
      "learning_rate": 0.0004691147167662966,
      "loss": 7.0801,
      "step": 435
    },
    {
      "epoch": 0.12412811387900356,
      "grad_norm": 0.4338753819465637,
      "learning_rate": 0.0004690435525192144,
      "loss": 7.7227,
      "step": 436
    },
    {
      "epoch": 0.12441281138790036,
      "grad_norm": 0.4573684632778168,
      "learning_rate": 0.00046897238827213207,
      "loss": 7.3672,
      "step": 437
    },
    {
      "epoch": 0.12469750889679715,
      "grad_norm": 0.504006028175354,
      "learning_rate": 0.0004689012240250498,
      "loss": 7.1426,
      "step": 438
    },
    {
      "epoch": 0.12498220640569395,
      "grad_norm": 0.5081356167793274,
      "learning_rate": 0.00046883005977796756,
      "loss": 7.0537,
      "step": 439
    },
    {
      "epoch": 0.12526690391459075,
      "grad_norm": 0.48333534598350525,
      "learning_rate": 0.0004687588955308853,
      "loss": 7.0693,
      "step": 440
    },
    {
      "epoch": 0.12555160142348754,
      "grad_norm": 0.4130278527736664,
      "learning_rate": 0.00046868773128380306,
      "loss": 7.4668,
      "step": 441
    },
    {
      "epoch": 0.12583629893238435,
      "grad_norm": 0.4785782992839813,
      "learning_rate": 0.0004686165670367208,
      "loss": 7.0684,
      "step": 442
    },
    {
      "epoch": 0.12612099644128114,
      "grad_norm": 0.5069476366043091,
      "learning_rate": 0.00046854540278963846,
      "loss": 7.2559,
      "step": 443
    },
    {
      "epoch": 0.12640569395017795,
      "grad_norm": 0.37336277961730957,
      "learning_rate": 0.00046847423854255623,
      "loss": 7.626,
      "step": 444
    },
    {
      "epoch": 0.12669039145907474,
      "grad_norm": 0.401167631149292,
      "learning_rate": 0.00046840307429547396,
      "loss": 7.2949,
      "step": 445
    },
    {
      "epoch": 0.12697508896797152,
      "grad_norm": 0.5884152054786682,
      "learning_rate": 0.00046833191004839173,
      "loss": 7.0771,
      "step": 446
    },
    {
      "epoch": 0.12725978647686834,
      "grad_norm": 0.43068522214889526,
      "learning_rate": 0.00046826074580130946,
      "loss": 7.6777,
      "step": 447
    },
    {
      "epoch": 0.12754448398576512,
      "grad_norm": 0.44112157821655273,
      "learning_rate": 0.0004681895815542271,
      "loss": 7.376,
      "step": 448
    },
    {
      "epoch": 0.1278291814946619,
      "grad_norm": 0.46927693486213684,
      "learning_rate": 0.0004681184173071449,
      "loss": 7.2109,
      "step": 449
    },
    {
      "epoch": 0.12811387900355872,
      "grad_norm": 0.3317018449306488,
      "learning_rate": 0.0004680472530600626,
      "loss": 7.751,
      "step": 450
    },
    {
      "epoch": 0.1283985765124555,
      "grad_norm": 0.4970950782299042,
      "learning_rate": 0.00046797608881298035,
      "loss": 6.835,
      "step": 451
    },
    {
      "epoch": 0.12868327402135232,
      "grad_norm": 0.3675149381160736,
      "learning_rate": 0.0004679049245658981,
      "loss": 7.8193,
      "step": 452
    },
    {
      "epoch": 0.1289679715302491,
      "grad_norm": 0.4956851005554199,
      "learning_rate": 0.00046783376031881585,
      "loss": 6.8252,
      "step": 453
    },
    {
      "epoch": 0.1292526690391459,
      "grad_norm": 0.4222683012485504,
      "learning_rate": 0.00046776259607173357,
      "loss": 7.501,
      "step": 454
    },
    {
      "epoch": 0.1295373665480427,
      "grad_norm": 0.4823405146598816,
      "learning_rate": 0.0004676914318246513,
      "loss": 7.1631,
      "step": 455
    },
    {
      "epoch": 0.1298220640569395,
      "grad_norm": 0.525143563747406,
      "learning_rate": 0.000467620267577569,
      "loss": 7.2627,
      "step": 456
    },
    {
      "epoch": 0.1301067615658363,
      "grad_norm": 0.4748491048812866,
      "learning_rate": 0.0004675491033304868,
      "loss": 7.4766,
      "step": 457
    },
    {
      "epoch": 0.1303914590747331,
      "grad_norm": 0.4361896514892578,
      "learning_rate": 0.0004674779390834045,
      "loss": 7.2959,
      "step": 458
    },
    {
      "epoch": 0.13067615658362988,
      "grad_norm": 0.5087428689002991,
      "learning_rate": 0.0004674067748363223,
      "loss": 7.3447,
      "step": 459
    },
    {
      "epoch": 0.1309608540925267,
      "grad_norm": 0.5033571720123291,
      "learning_rate": 0.00046733561058923996,
      "loss": 7.0791,
      "step": 460
    },
    {
      "epoch": 0.13124555160142348,
      "grad_norm": 0.5781779885292053,
      "learning_rate": 0.0004672644463421577,
      "loss": 7.1611,
      "step": 461
    },
    {
      "epoch": 0.1315302491103203,
      "grad_norm": 0.45533907413482666,
      "learning_rate": 0.00046719328209507546,
      "loss": 7.1758,
      "step": 462
    },
    {
      "epoch": 0.13181494661921708,
      "grad_norm": 0.4691354036331177,
      "learning_rate": 0.0004671221178479932,
      "loss": 7.375,
      "step": 463
    },
    {
      "epoch": 0.13209964412811387,
      "grad_norm": 0.4058699905872345,
      "learning_rate": 0.00046705095360091096,
      "loss": 7.7324,
      "step": 464
    },
    {
      "epoch": 0.13238434163701068,
      "grad_norm": 0.559533953666687,
      "learning_rate": 0.00046697978935382863,
      "loss": 6.6924,
      "step": 465
    },
    {
      "epoch": 0.13266903914590747,
      "grad_norm": 0.4726496934890747,
      "learning_rate": 0.00046690862510674635,
      "loss": 6.9219,
      "step": 466
    },
    {
      "epoch": 0.13295373665480428,
      "grad_norm": 0.4936983287334442,
      "learning_rate": 0.00046683746085966413,
      "loss": 7.1094,
      "step": 467
    },
    {
      "epoch": 0.13323843416370107,
      "grad_norm": 0.4566884934902191,
      "learning_rate": 0.00046676629661258185,
      "loss": 7.5312,
      "step": 468
    },
    {
      "epoch": 0.13352313167259786,
      "grad_norm": 0.4185795187950134,
      "learning_rate": 0.00046669513236549957,
      "loss": 7.5166,
      "step": 469
    },
    {
      "epoch": 0.13380782918149467,
      "grad_norm": 0.5276864767074585,
      "learning_rate": 0.00046662396811841735,
      "loss": 7.4355,
      "step": 470
    },
    {
      "epoch": 0.13409252669039146,
      "grad_norm": 0.5576158761978149,
      "learning_rate": 0.000466552803871335,
      "loss": 7.1006,
      "step": 471
    },
    {
      "epoch": 0.13437722419928827,
      "grad_norm": 0.4316464960575104,
      "learning_rate": 0.0004664816396242528,
      "loss": 7.4736,
      "step": 472
    },
    {
      "epoch": 0.13466192170818506,
      "grad_norm": 0.5265529155731201,
      "learning_rate": 0.0004664104753771705,
      "loss": 6.8154,
      "step": 473
    },
    {
      "epoch": 0.13494661921708184,
      "grad_norm": 0.4530062973499298,
      "learning_rate": 0.00046633931113008824,
      "loss": 7.377,
      "step": 474
    },
    {
      "epoch": 0.13523131672597866,
      "grad_norm": 0.47892332077026367,
      "learning_rate": 0.000466268146883006,
      "loss": 6.8652,
      "step": 475
    },
    {
      "epoch": 0.13551601423487544,
      "grad_norm": 0.5138373970985413,
      "learning_rate": 0.0004661969826359237,
      "loss": 7.0654,
      "step": 476
    },
    {
      "epoch": 0.13580071174377223,
      "grad_norm": 0.4899928867816925,
      "learning_rate": 0.00046612581838884146,
      "loss": 7.2871,
      "step": 477
    },
    {
      "epoch": 0.13608540925266904,
      "grad_norm": 0.5416154861450195,
      "learning_rate": 0.0004660546541417592,
      "loss": 7.0,
      "step": 478
    },
    {
      "epoch": 0.13637010676156583,
      "grad_norm": 0.4114988148212433,
      "learning_rate": 0.0004659834898946769,
      "loss": 7.1953,
      "step": 479
    },
    {
      "epoch": 0.13665480427046264,
      "grad_norm": 0.480387419462204,
      "learning_rate": 0.0004659123256475947,
      "loss": 7.2236,
      "step": 480
    },
    {
      "epoch": 0.13693950177935943,
      "grad_norm": 0.4656660854816437,
      "learning_rate": 0.0004658411614005124,
      "loss": 7.5098,
      "step": 481
    },
    {
      "epoch": 0.13722419928825622,
      "grad_norm": 0.49934709072113037,
      "learning_rate": 0.0004657699971534301,
      "loss": 7.1338,
      "step": 482
    },
    {
      "epoch": 0.13750889679715303,
      "grad_norm": 0.4244711995124817,
      "learning_rate": 0.00046569883290634785,
      "loss": 7.3154,
      "step": 483
    },
    {
      "epoch": 0.13779359430604982,
      "grad_norm": 0.5288747549057007,
      "learning_rate": 0.0004656276686592656,
      "loss": 6.918,
      "step": 484
    },
    {
      "epoch": 0.13807829181494663,
      "grad_norm": 0.5109588503837585,
      "learning_rate": 0.00046555650441218335,
      "loss": 7.1914,
      "step": 485
    },
    {
      "epoch": 0.13836298932384342,
      "grad_norm": 0.4920799434185028,
      "learning_rate": 0.0004654853401651011,
      "loss": 7.457,
      "step": 486
    },
    {
      "epoch": 0.1386476868327402,
      "grad_norm": 0.3864028751850128,
      "learning_rate": 0.0004654141759180188,
      "loss": 7.877,
      "step": 487
    },
    {
      "epoch": 0.13893238434163702,
      "grad_norm": 0.41772550344467163,
      "learning_rate": 0.0004653430116709365,
      "loss": 7.5791,
      "step": 488
    },
    {
      "epoch": 0.1392170818505338,
      "grad_norm": 0.527531087398529,
      "learning_rate": 0.00046527184742385424,
      "loss": 6.834,
      "step": 489
    },
    {
      "epoch": 0.13950177935943062,
      "grad_norm": 0.4375729262828827,
      "learning_rate": 0.000465200683176772,
      "loss": 7.208,
      "step": 490
    },
    {
      "epoch": 0.1397864768683274,
      "grad_norm": 0.5000871419906616,
      "learning_rate": 0.00046512951892968974,
      "loss": 7.2734,
      "step": 491
    },
    {
      "epoch": 0.1400711743772242,
      "grad_norm": 0.39341986179351807,
      "learning_rate": 0.00046505835468260747,
      "loss": 7.627,
      "step": 492
    },
    {
      "epoch": 0.140355871886121,
      "grad_norm": 0.5892451405525208,
      "learning_rate": 0.0004649871904355252,
      "loss": 6.7764,
      "step": 493
    },
    {
      "epoch": 0.1406405693950178,
      "grad_norm": 0.4922020137310028,
      "learning_rate": 0.0004649160261884429,
      "loss": 7.3164,
      "step": 494
    },
    {
      "epoch": 0.1409252669039146,
      "grad_norm": 0.4670257568359375,
      "learning_rate": 0.0004648448619413607,
      "loss": 7.1924,
      "step": 495
    },
    {
      "epoch": 0.1412099644128114,
      "grad_norm": 0.5158605575561523,
      "learning_rate": 0.0004647736976942784,
      "loss": 7.0537,
      "step": 496
    },
    {
      "epoch": 0.14149466192170818,
      "grad_norm": 0.48508933186531067,
      "learning_rate": 0.00046470253344719614,
      "loss": 6.9873,
      "step": 497
    },
    {
      "epoch": 0.141779359430605,
      "grad_norm": 0.4340175688266754,
      "learning_rate": 0.0004646313692001139,
      "loss": 7.2236,
      "step": 498
    },
    {
      "epoch": 0.14206405693950178,
      "grad_norm": 0.4614635109901428,
      "learning_rate": 0.0004645602049530316,
      "loss": 7.4971,
      "step": 499
    },
    {
      "epoch": 0.1423487544483986,
      "grad_norm": 0.43651285767555237,
      "learning_rate": 0.0004644890407059493,
      "loss": 7.0176,
      "step": 500
    },
    {
      "epoch": 0.14263345195729538,
      "grad_norm": 0.4170750081539154,
      "learning_rate": 0.0004644178764588671,
      "loss": 7.3975,
      "step": 501
    },
    {
      "epoch": 0.14291814946619216,
      "grad_norm": 0.45592260360717773,
      "learning_rate": 0.0004643467122117848,
      "loss": 7.4209,
      "step": 502
    },
    {
      "epoch": 0.14320284697508898,
      "grad_norm": 0.5318942666053772,
      "learning_rate": 0.0004642755479647026,
      "loss": 6.6689,
      "step": 503
    },
    {
      "epoch": 0.14348754448398576,
      "grad_norm": 0.43632060289382935,
      "learning_rate": 0.0004642043837176203,
      "loss": 7.3867,
      "step": 504
    },
    {
      "epoch": 0.14377224199288255,
      "grad_norm": 0.48805883526802063,
      "learning_rate": 0.00046413321947053797,
      "loss": 7.0771,
      "step": 505
    },
    {
      "epoch": 0.14405693950177936,
      "grad_norm": 0.5213673710823059,
      "learning_rate": 0.00046406205522345575,
      "loss": 7.0059,
      "step": 506
    },
    {
      "epoch": 0.14434163701067615,
      "grad_norm": 0.4732288420200348,
      "learning_rate": 0.00046399089097637347,
      "loss": 7.2939,
      "step": 507
    },
    {
      "epoch": 0.14462633451957296,
      "grad_norm": 0.44369205832481384,
      "learning_rate": 0.00046391972672929125,
      "loss": 7.5195,
      "step": 508
    },
    {
      "epoch": 0.14491103202846975,
      "grad_norm": 0.48915088176727295,
      "learning_rate": 0.00046384856248220897,
      "loss": 7.1377,
      "step": 509
    },
    {
      "epoch": 0.14519572953736654,
      "grad_norm": 0.4987899959087372,
      "learning_rate": 0.00046377739823512664,
      "loss": 7.0449,
      "step": 510
    },
    {
      "epoch": 0.14548042704626335,
      "grad_norm": 0.48522457480430603,
      "learning_rate": 0.0004637062339880444,
      "loss": 6.9531,
      "step": 511
    },
    {
      "epoch": 0.14576512455516014,
      "grad_norm": 0.45173463225364685,
      "learning_rate": 0.00046363506974096214,
      "loss": 7.4492,
      "step": 512
    },
    {
      "epoch": 0.14604982206405695,
      "grad_norm": 0.4935166835784912,
      "learning_rate": 0.0004635639054938799,
      "loss": 7.6309,
      "step": 513
    },
    {
      "epoch": 0.14633451957295374,
      "grad_norm": 0.467246949672699,
      "learning_rate": 0.00046349274124679764,
      "loss": 7.3164,
      "step": 514
    },
    {
      "epoch": 0.14661921708185052,
      "grad_norm": 0.4763619005680084,
      "learning_rate": 0.00046342157699971536,
      "loss": 7.2002,
      "step": 515
    },
    {
      "epoch": 0.14690391459074734,
      "grad_norm": 0.43141141533851624,
      "learning_rate": 0.0004633504127526331,
      "loss": 7.6465,
      "step": 516
    },
    {
      "epoch": 0.14718861209964412,
      "grad_norm": 0.4468206763267517,
      "learning_rate": 0.0004632792485055508,
      "loss": 7.3887,
      "step": 517
    },
    {
      "epoch": 0.14747330960854094,
      "grad_norm": 0.4942794740200043,
      "learning_rate": 0.00046320808425846853,
      "loss": 6.8789,
      "step": 518
    },
    {
      "epoch": 0.14775800711743772,
      "grad_norm": 0.4572586715221405,
      "learning_rate": 0.0004631369200113863,
      "loss": 7.3047,
      "step": 519
    },
    {
      "epoch": 0.1480427046263345,
      "grad_norm": 0.47273606061935425,
      "learning_rate": 0.00046306575576430403,
      "loss": 6.9844,
      "step": 520
    },
    {
      "epoch": 0.14832740213523132,
      "grad_norm": 0.5519757866859436,
      "learning_rate": 0.00046299459151722175,
      "loss": 7.2256,
      "step": 521
    },
    {
      "epoch": 0.1486120996441281,
      "grad_norm": 0.46243593096733093,
      "learning_rate": 0.0004629234272701395,
      "loss": 7.3311,
      "step": 522
    },
    {
      "epoch": 0.14889679715302492,
      "grad_norm": 0.5186846256256104,
      "learning_rate": 0.0004628522630230572,
      "loss": 7.0801,
      "step": 523
    },
    {
      "epoch": 0.1491814946619217,
      "grad_norm": 0.47964605689048767,
      "learning_rate": 0.000462781098775975,
      "loss": 7.2568,
      "step": 524
    },
    {
      "epoch": 0.1494661921708185,
      "grad_norm": 0.48539063334465027,
      "learning_rate": 0.0004627099345288927,
      "loss": 6.9678,
      "step": 525
    },
    {
      "epoch": 0.1497508896797153,
      "grad_norm": 0.45621731877326965,
      "learning_rate": 0.0004626387702818105,
      "loss": 7.4131,
      "step": 526
    },
    {
      "epoch": 0.1500355871886121,
      "grad_norm": 0.4345969557762146,
      "learning_rate": 0.00046256760603472814,
      "loss": 7.4258,
      "step": 527
    },
    {
      "epoch": 0.1503202846975089,
      "grad_norm": 0.4568675756454468,
      "learning_rate": 0.00046249644178764587,
      "loss": 7.2705,
      "step": 528
    },
    {
      "epoch": 0.1506049822064057,
      "grad_norm": 0.6596431732177734,
      "learning_rate": 0.00046242527754056364,
      "loss": 6.4834,
      "step": 529
    },
    {
      "epoch": 0.15088967971530248,
      "grad_norm": 0.5479394197463989,
      "learning_rate": 0.00046235411329348137,
      "loss": 6.5928,
      "step": 530
    },
    {
      "epoch": 0.1511743772241993,
      "grad_norm": 0.47287479043006897,
      "learning_rate": 0.0004622829490463991,
      "loss": 7.1133,
      "step": 531
    },
    {
      "epoch": 0.15145907473309608,
      "grad_norm": 0.42825841903686523,
      "learning_rate": 0.00046221178479931687,
      "loss": 7.6221,
      "step": 532
    },
    {
      "epoch": 0.15174377224199287,
      "grad_norm": 0.6770346164703369,
      "learning_rate": 0.00046214062055223453,
      "loss": 6.1582,
      "step": 533
    },
    {
      "epoch": 0.15202846975088968,
      "grad_norm": 0.46598076820373535,
      "learning_rate": 0.0004620694563051523,
      "loss": 7.334,
      "step": 534
    },
    {
      "epoch": 0.15231316725978647,
      "grad_norm": 0.48289692401885986,
      "learning_rate": 0.00046199829205807003,
      "loss": 7.0273,
      "step": 535
    },
    {
      "epoch": 0.15259786476868328,
      "grad_norm": 0.5073385238647461,
      "learning_rate": 0.00046192712781098776,
      "loss": 6.874,
      "step": 536
    },
    {
      "epoch": 0.15288256227758007,
      "grad_norm": 0.3938785791397095,
      "learning_rate": 0.00046185596356390553,
      "loss": 7.749,
      "step": 537
    },
    {
      "epoch": 0.15316725978647686,
      "grad_norm": 0.4579390287399292,
      "learning_rate": 0.0004617847993168232,
      "loss": 7.1562,
      "step": 538
    },
    {
      "epoch": 0.15345195729537367,
      "grad_norm": 0.4372034966945648,
      "learning_rate": 0.000461713635069741,
      "loss": 7.2236,
      "step": 539
    },
    {
      "epoch": 0.15373665480427046,
      "grad_norm": 0.4431258738040924,
      "learning_rate": 0.0004616424708226587,
      "loss": 7.3242,
      "step": 540
    },
    {
      "epoch": 0.15402135231316727,
      "grad_norm": 0.43316811323165894,
      "learning_rate": 0.0004615713065755764,
      "loss": 7.3438,
      "step": 541
    },
    {
      "epoch": 0.15430604982206406,
      "grad_norm": 0.4466095566749573,
      "learning_rate": 0.0004615001423284942,
      "loss": 7.2197,
      "step": 542
    },
    {
      "epoch": 0.15459074733096084,
      "grad_norm": 0.45385703444480896,
      "learning_rate": 0.0004614289780814119,
      "loss": 7.3252,
      "step": 543
    },
    {
      "epoch": 0.15487544483985766,
      "grad_norm": 0.4653369188308716,
      "learning_rate": 0.00046135781383432965,
      "loss": 7.4531,
      "step": 544
    },
    {
      "epoch": 0.15516014234875444,
      "grad_norm": 0.4816647469997406,
      "learning_rate": 0.00046128664958724737,
      "loss": 7.3721,
      "step": 545
    },
    {
      "epoch": 0.15544483985765126,
      "grad_norm": 0.47521352767944336,
      "learning_rate": 0.0004612154853401651,
      "loss": 7.2432,
      "step": 546
    },
    {
      "epoch": 0.15572953736654804,
      "grad_norm": 0.34924232959747314,
      "learning_rate": 0.00046114432109308287,
      "loss": 7.96,
      "step": 547
    },
    {
      "epoch": 0.15601423487544483,
      "grad_norm": 0.5224582552909851,
      "learning_rate": 0.0004610731568460006,
      "loss": 6.9951,
      "step": 548
    },
    {
      "epoch": 0.15629893238434164,
      "grad_norm": 0.49455711245536804,
      "learning_rate": 0.0004610019925989183,
      "loss": 7.2002,
      "step": 549
    },
    {
      "epoch": 0.15658362989323843,
      "grad_norm": 0.46338167786598206,
      "learning_rate": 0.00046093082835183604,
      "loss": 7.1396,
      "step": 550
    },
    {
      "epoch": 0.15686832740213524,
      "grad_norm": 0.6119192838668823,
      "learning_rate": 0.00046085966410475376,
      "loss": 6.7188,
      "step": 551
    },
    {
      "epoch": 0.15715302491103203,
      "grad_norm": 0.6386239528656006,
      "learning_rate": 0.00046078849985767154,
      "loss": 6.7236,
      "step": 552
    },
    {
      "epoch": 0.15743772241992882,
      "grad_norm": 0.3954797089099884,
      "learning_rate": 0.00046071733561058926,
      "loss": 7.7266,
      "step": 553
    },
    {
      "epoch": 0.15772241992882563,
      "grad_norm": 0.5548195242881775,
      "learning_rate": 0.000460646171363507,
      "loss": 6.9756,
      "step": 554
    },
    {
      "epoch": 0.15800711743772242,
      "grad_norm": 0.6150522828102112,
      "learning_rate": 0.0004605750071164247,
      "loss": 6.5869,
      "step": 555
    },
    {
      "epoch": 0.15829181494661923,
      "grad_norm": 0.5321157574653625,
      "learning_rate": 0.00046050384286934243,
      "loss": 6.8682,
      "step": 556
    },
    {
      "epoch": 0.15857651245551602,
      "grad_norm": 0.4596554934978485,
      "learning_rate": 0.0004604326786222602,
      "loss": 7.1885,
      "step": 557
    },
    {
      "epoch": 0.1588612099644128,
      "grad_norm": 0.44151896238327026,
      "learning_rate": 0.00046036151437517793,
      "loss": 7.6816,
      "step": 558
    },
    {
      "epoch": 0.15914590747330962,
      "grad_norm": 0.5159528255462646,
      "learning_rate": 0.00046029035012809565,
      "loss": 6.8662,
      "step": 559
    },
    {
      "epoch": 0.1594306049822064,
      "grad_norm": 0.4236861765384674,
      "learning_rate": 0.00046021918588101343,
      "loss": 7.5635,
      "step": 560
    },
    {
      "epoch": 0.1597153024911032,
      "grad_norm": 0.5561614632606506,
      "learning_rate": 0.0004601480216339311,
      "loss": 6.8086,
      "step": 561
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.46602436900138855,
      "learning_rate": 0.0004600768573868488,
      "loss": 7.1611,
      "step": 562
    },
    {
      "epoch": 0.1602846975088968,
      "grad_norm": 0.43928325176239014,
      "learning_rate": 0.0004600056931397666,
      "loss": 7.5225,
      "step": 563
    },
    {
      "epoch": 0.1605693950177936,
      "grad_norm": 0.4085899293422699,
      "learning_rate": 0.0004599345288926843,
      "loss": 7.8828,
      "step": 564
    },
    {
      "epoch": 0.1608540925266904,
      "grad_norm": 0.37990400195121765,
      "learning_rate": 0.0004598633646456021,
      "loss": 7.5117,
      "step": 565
    },
    {
      "epoch": 0.16113879003558718,
      "grad_norm": 0.551302433013916,
      "learning_rate": 0.00045979220039851976,
      "loss": 7.1465,
      "step": 566
    },
    {
      "epoch": 0.161423487544484,
      "grad_norm": 0.4028521776199341,
      "learning_rate": 0.0004597210361514375,
      "loss": 7.6123,
      "step": 567
    },
    {
      "epoch": 0.16170818505338078,
      "grad_norm": 0.5788272619247437,
      "learning_rate": 0.00045964987190435526,
      "loss": 7.1279,
      "step": 568
    },
    {
      "epoch": 0.1619928825622776,
      "grad_norm": 0.5337854623794556,
      "learning_rate": 0.000459578707657273,
      "loss": 6.9785,
      "step": 569
    },
    {
      "epoch": 0.16227758007117438,
      "grad_norm": 0.5863319635391235,
      "learning_rate": 0.00045950754341019076,
      "loss": 7.1436,
      "step": 570
    },
    {
      "epoch": 0.16256227758007116,
      "grad_norm": 0.4670248031616211,
      "learning_rate": 0.0004594363791631085,
      "loss": 7.376,
      "step": 571
    },
    {
      "epoch": 0.16284697508896798,
      "grad_norm": 0.4174482524394989,
      "learning_rate": 0.00045936521491602616,
      "loss": 7.3574,
      "step": 572
    },
    {
      "epoch": 0.16313167259786476,
      "grad_norm": 0.5444307327270508,
      "learning_rate": 0.00045929405066894393,
      "loss": 7.0332,
      "step": 573
    },
    {
      "epoch": 0.16341637010676158,
      "grad_norm": 0.4734575152397156,
      "learning_rate": 0.00045922288642186165,
      "loss": 7.3379,
      "step": 574
    },
    {
      "epoch": 0.16370106761565836,
      "grad_norm": 0.43693485856056213,
      "learning_rate": 0.00045915172217477943,
      "loss": 7.3506,
      "step": 575
    },
    {
      "epoch": 0.16398576512455515,
      "grad_norm": 0.39955365657806396,
      "learning_rate": 0.00045908055792769715,
      "loss": 7.7363,
      "step": 576
    },
    {
      "epoch": 0.16427046263345196,
      "grad_norm": 0.42568933963775635,
      "learning_rate": 0.0004590093936806149,
      "loss": 7.7568,
      "step": 577
    },
    {
      "epoch": 0.16455516014234875,
      "grad_norm": 0.4571026563644409,
      "learning_rate": 0.0004589382294335326,
      "loss": 7.3809,
      "step": 578
    },
    {
      "epoch": 0.16483985765124556,
      "grad_norm": 0.45719119906425476,
      "learning_rate": 0.0004588670651864503,
      "loss": 7.4697,
      "step": 579
    },
    {
      "epoch": 0.16512455516014235,
      "grad_norm": 0.4227726459503174,
      "learning_rate": 0.00045879590093936805,
      "loss": 7.7959,
      "step": 580
    },
    {
      "epoch": 0.16540925266903914,
      "grad_norm": 0.4999239444732666,
      "learning_rate": 0.0004587247366922858,
      "loss": 7.0615,
      "step": 581
    },
    {
      "epoch": 0.16569395017793595,
      "grad_norm": 0.4378338158130646,
      "learning_rate": 0.00045865357244520355,
      "loss": 7.4492,
      "step": 582
    },
    {
      "epoch": 0.16597864768683274,
      "grad_norm": 0.4626426100730896,
      "learning_rate": 0.00045858240819812127,
      "loss": 7.6074,
      "step": 583
    },
    {
      "epoch": 0.16626334519572955,
      "grad_norm": 0.49946653842926025,
      "learning_rate": 0.000458511243951039,
      "loss": 7.5752,
      "step": 584
    },
    {
      "epoch": 0.16654804270462634,
      "grad_norm": 0.4690173864364624,
      "learning_rate": 0.0004584400797039567,
      "loss": 7.4453,
      "step": 585
    },
    {
      "epoch": 0.16683274021352312,
      "grad_norm": 0.5380011796951294,
      "learning_rate": 0.0004583689154568745,
      "loss": 7.085,
      "step": 586
    },
    {
      "epoch": 0.16711743772241994,
      "grad_norm": 0.47833171486854553,
      "learning_rate": 0.0004582977512097922,
      "loss": 7.1475,
      "step": 587
    },
    {
      "epoch": 0.16740213523131672,
      "grad_norm": 0.46506747603416443,
      "learning_rate": 0.00045822658696271,
      "loss": 6.9707,
      "step": 588
    },
    {
      "epoch": 0.1676868327402135,
      "grad_norm": 0.6595041155815125,
      "learning_rate": 0.00045815542271562766,
      "loss": 6.2119,
      "step": 589
    },
    {
      "epoch": 0.16797153024911032,
      "grad_norm": 0.47286534309387207,
      "learning_rate": 0.0004580842584685454,
      "loss": 7.2656,
      "step": 590
    },
    {
      "epoch": 0.1682562277580071,
      "grad_norm": 0.3845736086368561,
      "learning_rate": 0.00045801309422146316,
      "loss": 7.7588,
      "step": 591
    },
    {
      "epoch": 0.16854092526690392,
      "grad_norm": 0.4788035750389099,
      "learning_rate": 0.0004579419299743809,
      "loss": 7.4238,
      "step": 592
    },
    {
      "epoch": 0.1688256227758007,
      "grad_norm": 0.5757747292518616,
      "learning_rate": 0.00045787076572729866,
      "loss": 7.0264,
      "step": 593
    },
    {
      "epoch": 0.1691103202846975,
      "grad_norm": 0.4455467462539673,
      "learning_rate": 0.0004577996014802163,
      "loss": 7.6641,
      "step": 594
    },
    {
      "epoch": 0.1693950177935943,
      "grad_norm": 0.36923131346702576,
      "learning_rate": 0.00045772843723313405,
      "loss": 7.3965,
      "step": 595
    },
    {
      "epoch": 0.1696797153024911,
      "grad_norm": 0.5825194120407104,
      "learning_rate": 0.0004576572729860518,
      "loss": 7.0059,
      "step": 596
    },
    {
      "epoch": 0.1699644128113879,
      "grad_norm": 0.409291535615921,
      "learning_rate": 0.00045758610873896955,
      "loss": 7.3086,
      "step": 597
    },
    {
      "epoch": 0.1702491103202847,
      "grad_norm": 0.5457488298416138,
      "learning_rate": 0.00045751494449188727,
      "loss": 7.4639,
      "step": 598
    },
    {
      "epoch": 0.17053380782918148,
      "grad_norm": 0.5561560988426208,
      "learning_rate": 0.00045744378024480505,
      "loss": 7.0244,
      "step": 599
    },
    {
      "epoch": 0.1708185053380783,
      "grad_norm": 0.40452277660369873,
      "learning_rate": 0.0004573726159977227,
      "loss": 7.9131,
      "step": 600
    },
    {
      "epoch": 0.1708185053380783,
      "eval_bleu": 0.08136284175108972,
      "eval_loss": 7.046875,
      "eval_runtime": 175.1925,
      "eval_samples_per_second": 1.621,
      "eval_steps_per_second": 0.103,
      "step": 600
    },
    {
      "epoch": 0.17110320284697508,
      "grad_norm": 0.5157926082611084,
      "learning_rate": 0.0004573014517506405,
      "loss": 7.6035,
      "step": 601
    },
    {
      "epoch": 0.1713879003558719,
      "grad_norm": 0.5090907216072083,
      "learning_rate": 0.0004572302875035582,
      "loss": 7.3867,
      "step": 602
    },
    {
      "epoch": 0.17167259786476868,
      "grad_norm": 0.5271131992340088,
      "learning_rate": 0.00045715912325647594,
      "loss": 7.1807,
      "step": 603
    },
    {
      "epoch": 0.17195729537366547,
      "grad_norm": 0.479402631521225,
      "learning_rate": 0.0004570879590093937,
      "loss": 7.1025,
      "step": 604
    },
    {
      "epoch": 0.17224199288256228,
      "grad_norm": 0.4699629247188568,
      "learning_rate": 0.00045701679476231144,
      "loss": 7.4434,
      "step": 605
    },
    {
      "epoch": 0.17252669039145907,
      "grad_norm": 0.549485981464386,
      "learning_rate": 0.00045694563051522916,
      "loss": 7.0225,
      "step": 606
    },
    {
      "epoch": 0.17281138790035588,
      "grad_norm": 0.39508330821990967,
      "learning_rate": 0.0004568744662681469,
      "loss": 7.8623,
      "step": 607
    },
    {
      "epoch": 0.17309608540925267,
      "grad_norm": 0.5417274236679077,
      "learning_rate": 0.0004568033020210646,
      "loss": 6.9346,
      "step": 608
    },
    {
      "epoch": 0.17338078291814946,
      "grad_norm": 0.4323408901691437,
      "learning_rate": 0.0004567321377739824,
      "loss": 7.8262,
      "step": 609
    },
    {
      "epoch": 0.17366548042704627,
      "grad_norm": 0.5999831557273865,
      "learning_rate": 0.0004566609735269001,
      "loss": 7.0322,
      "step": 610
    },
    {
      "epoch": 0.17395017793594306,
      "grad_norm": 0.3982037901878357,
      "learning_rate": 0.0004565898092798178,
      "loss": 7.5947,
      "step": 611
    },
    {
      "epoch": 0.17423487544483987,
      "grad_norm": 0.41008031368255615,
      "learning_rate": 0.00045651864503273555,
      "loss": 8.0742,
      "step": 612
    },
    {
      "epoch": 0.17451957295373666,
      "grad_norm": 0.496019184589386,
      "learning_rate": 0.0004564474807856533,
      "loss": 7.5781,
      "step": 613
    },
    {
      "epoch": 0.17480427046263344,
      "grad_norm": 0.4990009665489197,
      "learning_rate": 0.00045637631653857105,
      "loss": 7.6621,
      "step": 614
    },
    {
      "epoch": 0.17508896797153026,
      "grad_norm": 0.510688304901123,
      "learning_rate": 0.0004563051522914888,
      "loss": 6.4463,
      "step": 615
    },
    {
      "epoch": 0.17537366548042704,
      "grad_norm": 0.4763341248035431,
      "learning_rate": 0.0004562339880444065,
      "loss": 7.5117,
      "step": 616
    },
    {
      "epoch": 0.17565836298932383,
      "grad_norm": 0.4208298623561859,
      "learning_rate": 0.0004561628237973242,
      "loss": 7.2637,
      "step": 617
    },
    {
      "epoch": 0.17594306049822064,
      "grad_norm": 0.4986557066440582,
      "learning_rate": 0.00045609165955024194,
      "loss": 6.9355,
      "step": 618
    },
    {
      "epoch": 0.17622775800711743,
      "grad_norm": 0.4251626431941986,
      "learning_rate": 0.0004560204953031597,
      "loss": 7.6758,
      "step": 619
    },
    {
      "epoch": 0.17651245551601424,
      "grad_norm": 0.5883548855781555,
      "learning_rate": 0.00045594933105607744,
      "loss": 6.8984,
      "step": 620
    },
    {
      "epoch": 0.17679715302491103,
      "grad_norm": 0.45804181694984436,
      "learning_rate": 0.00045587816680899517,
      "loss": 7.5889,
      "step": 621
    },
    {
      "epoch": 0.17708185053380782,
      "grad_norm": 0.5595649480819702,
      "learning_rate": 0.00045580700256191294,
      "loss": 7.3184,
      "step": 622
    },
    {
      "epoch": 0.17736654804270463,
      "grad_norm": 0.41659483313560486,
      "learning_rate": 0.0004557358383148306,
      "loss": 7.333,
      "step": 623
    },
    {
      "epoch": 0.17765124555160142,
      "grad_norm": 0.4967384934425354,
      "learning_rate": 0.0004556646740677484,
      "loss": 7.2559,
      "step": 624
    },
    {
      "epoch": 0.17793594306049823,
      "grad_norm": 0.43483781814575195,
      "learning_rate": 0.0004555935098206661,
      "loss": 7.7031,
      "step": 625
    },
    {
      "epoch": 0.17822064056939502,
      "grad_norm": 0.53510981798172,
      "learning_rate": 0.00045552234557358383,
      "loss": 7.0801,
      "step": 626
    },
    {
      "epoch": 0.1785053380782918,
      "grad_norm": 0.5476201176643372,
      "learning_rate": 0.0004554511813265016,
      "loss": 6.9561,
      "step": 627
    },
    {
      "epoch": 0.17879003558718862,
      "grad_norm": 0.5914725661277771,
      "learning_rate": 0.0004553800170794193,
      "loss": 7.0059,
      "step": 628
    },
    {
      "epoch": 0.1790747330960854,
      "grad_norm": 0.5816411972045898,
      "learning_rate": 0.000455308852832337,
      "loss": 7.1006,
      "step": 629
    },
    {
      "epoch": 0.17935943060498222,
      "grad_norm": 0.503734290599823,
      "learning_rate": 0.0004552376885852548,
      "loss": 7.2002,
      "step": 630
    },
    {
      "epoch": 0.179644128113879,
      "grad_norm": 0.4251323938369751,
      "learning_rate": 0.0004551665243381725,
      "loss": 7.6299,
      "step": 631
    },
    {
      "epoch": 0.1799288256227758,
      "grad_norm": 0.5005379319190979,
      "learning_rate": 0.0004550953600910903,
      "loss": 6.8135,
      "step": 632
    },
    {
      "epoch": 0.1802135231316726,
      "grad_norm": 0.4894733428955078,
      "learning_rate": 0.000455024195844008,
      "loss": 7.0869,
      "step": 633
    },
    {
      "epoch": 0.1804982206405694,
      "grad_norm": 0.40313103795051575,
      "learning_rate": 0.00045495303159692567,
      "loss": 7.4717,
      "step": 634
    },
    {
      "epoch": 0.1807829181494662,
      "grad_norm": 0.4881073832511902,
      "learning_rate": 0.00045488186734984345,
      "loss": 7.3096,
      "step": 635
    },
    {
      "epoch": 0.181067615658363,
      "grad_norm": 0.5019959807395935,
      "learning_rate": 0.00045481070310276117,
      "loss": 7.1348,
      "step": 636
    },
    {
      "epoch": 0.18135231316725978,
      "grad_norm": 0.5300182700157166,
      "learning_rate": 0.00045473953885567895,
      "loss": 7.3125,
      "step": 637
    },
    {
      "epoch": 0.1816370106761566,
      "grad_norm": 0.5444533824920654,
      "learning_rate": 0.00045466837460859667,
      "loss": 7.0107,
      "step": 638
    },
    {
      "epoch": 0.18192170818505338,
      "grad_norm": 0.5647503733634949,
      "learning_rate": 0.00045459721036151434,
      "loss": 7.1396,
      "step": 639
    },
    {
      "epoch": 0.1822064056939502,
      "grad_norm": 0.6619802117347717,
      "learning_rate": 0.0004545260461144321,
      "loss": 6.9766,
      "step": 640
    },
    {
      "epoch": 0.18249110320284698,
      "grad_norm": 0.4294302761554718,
      "learning_rate": 0.00045445488186734984,
      "loss": 7.6309,
      "step": 641
    },
    {
      "epoch": 0.18277580071174376,
      "grad_norm": 0.46216872334480286,
      "learning_rate": 0.0004543837176202676,
      "loss": 7.1006,
      "step": 642
    },
    {
      "epoch": 0.18306049822064058,
      "grad_norm": 0.48695653676986694,
      "learning_rate": 0.00045431255337318534,
      "loss": 7.3574,
      "step": 643
    },
    {
      "epoch": 0.18334519572953736,
      "grad_norm": 0.5963466763496399,
      "learning_rate": 0.00045424138912610306,
      "loss": 7.293,
      "step": 644
    },
    {
      "epoch": 0.18362989323843418,
      "grad_norm": 0.5873686671257019,
      "learning_rate": 0.0004541702248790208,
      "loss": 6.9521,
      "step": 645
    },
    {
      "epoch": 0.18391459074733096,
      "grad_norm": 0.368348628282547,
      "learning_rate": 0.0004540990606319385,
      "loss": 7.6436,
      "step": 646
    },
    {
      "epoch": 0.18419928825622775,
      "grad_norm": 0.37956151366233826,
      "learning_rate": 0.00045402789638485623,
      "loss": 7.9355,
      "step": 647
    },
    {
      "epoch": 0.18448398576512456,
      "grad_norm": 0.5874934792518616,
      "learning_rate": 0.000453956732137774,
      "loss": 7.1172,
      "step": 648
    },
    {
      "epoch": 0.18476868327402135,
      "grad_norm": 0.44219866394996643,
      "learning_rate": 0.00045388556789069173,
      "loss": 7.7051,
      "step": 649
    },
    {
      "epoch": 0.18505338078291814,
      "grad_norm": 0.49062198400497437,
      "learning_rate": 0.0004538144036436095,
      "loss": 7.5488,
      "step": 650
    },
    {
      "epoch": 0.18533807829181495,
      "grad_norm": 0.48055267333984375,
      "learning_rate": 0.0004537432393965272,
      "loss": 7.4219,
      "step": 651
    },
    {
      "epoch": 0.18562277580071174,
      "grad_norm": 0.477851927280426,
      "learning_rate": 0.0004536720751494449,
      "loss": 7.5186,
      "step": 652
    },
    {
      "epoch": 0.18590747330960855,
      "grad_norm": 0.44223421812057495,
      "learning_rate": 0.0004536009109023627,
      "loss": 7.4512,
      "step": 653
    },
    {
      "epoch": 0.18619217081850534,
      "grad_norm": 0.48089858889579773,
      "learning_rate": 0.0004535297466552804,
      "loss": 7.2803,
      "step": 654
    },
    {
      "epoch": 0.18647686832740212,
      "grad_norm": 0.5489603877067566,
      "learning_rate": 0.0004534585824081982,
      "loss": 6.2256,
      "step": 655
    },
    {
      "epoch": 0.18676156583629894,
      "grad_norm": 0.4798690676689148,
      "learning_rate": 0.00045338741816111584,
      "loss": 7.459,
      "step": 656
    },
    {
      "epoch": 0.18704626334519572,
      "grad_norm": 0.5676328539848328,
      "learning_rate": 0.00045331625391403357,
      "loss": 6.7197,
      "step": 657
    },
    {
      "epoch": 0.18733096085409254,
      "grad_norm": 0.5007146596908569,
      "learning_rate": 0.00045324508966695134,
      "loss": 7.3262,
      "step": 658
    },
    {
      "epoch": 0.18761565836298932,
      "grad_norm": 0.6378830075263977,
      "learning_rate": 0.00045317392541986906,
      "loss": 7.1758,
      "step": 659
    },
    {
      "epoch": 0.1879003558718861,
      "grad_norm": 0.4543790817260742,
      "learning_rate": 0.00045310276117278684,
      "loss": 7.1152,
      "step": 660
    },
    {
      "epoch": 0.18818505338078292,
      "grad_norm": 0.43836989998817444,
      "learning_rate": 0.00045303159692570456,
      "loss": 7.6982,
      "step": 661
    },
    {
      "epoch": 0.1884697508896797,
      "grad_norm": 0.48784226179122925,
      "learning_rate": 0.00045296043267862223,
      "loss": 7.4922,
      "step": 662
    },
    {
      "epoch": 0.18875444839857652,
      "grad_norm": 0.501962423324585,
      "learning_rate": 0.00045288926843154,
      "loss": 6.9375,
      "step": 663
    },
    {
      "epoch": 0.1890391459074733,
      "grad_norm": 0.4261062443256378,
      "learning_rate": 0.00045281810418445773,
      "loss": 7.4248,
      "step": 664
    },
    {
      "epoch": 0.1893238434163701,
      "grad_norm": 0.4303380250930786,
      "learning_rate": 0.00045274693993737546,
      "loss": 7.3965,
      "step": 665
    },
    {
      "epoch": 0.1896085409252669,
      "grad_norm": 0.4281022250652313,
      "learning_rate": 0.00045267577569029323,
      "loss": 7.6621,
      "step": 666
    },
    {
      "epoch": 0.1898932384341637,
      "grad_norm": 0.5295163989067078,
      "learning_rate": 0.00045260461144321096,
      "loss": 7.4043,
      "step": 667
    },
    {
      "epoch": 0.1901779359430605,
      "grad_norm": 0.48053741455078125,
      "learning_rate": 0.0004525334471961287,
      "loss": 7.4639,
      "step": 668
    },
    {
      "epoch": 0.1904626334519573,
      "grad_norm": 0.4163114130496979,
      "learning_rate": 0.0004524622829490464,
      "loss": 7.7852,
      "step": 669
    },
    {
      "epoch": 0.19074733096085408,
      "grad_norm": 0.43880659341812134,
      "learning_rate": 0.0004523911187019641,
      "loss": 7.457,
      "step": 670
    },
    {
      "epoch": 0.1910320284697509,
      "grad_norm": 0.4487917423248291,
      "learning_rate": 0.0004523199544548819,
      "loss": 7.665,
      "step": 671
    },
    {
      "epoch": 0.19131672597864768,
      "grad_norm": 0.45230862498283386,
      "learning_rate": 0.0004522487902077996,
      "loss": 7.8857,
      "step": 672
    },
    {
      "epoch": 0.1916014234875445,
      "grad_norm": 0.496181458234787,
      "learning_rate": 0.00045217762596071735,
      "loss": 7.2002,
      "step": 673
    },
    {
      "epoch": 0.19188612099644128,
      "grad_norm": 0.35912343859672546,
      "learning_rate": 0.00045210646171363507,
      "loss": 7.5195,
      "step": 674
    },
    {
      "epoch": 0.19217081850533807,
      "grad_norm": 0.462393194437027,
      "learning_rate": 0.0004520352974665528,
      "loss": 7.5742,
      "step": 675
    },
    {
      "epoch": 0.19245551601423488,
      "grad_norm": 0.5089987516403198,
      "learning_rate": 0.00045196413321947057,
      "loss": 7.1533,
      "step": 676
    },
    {
      "epoch": 0.19274021352313167,
      "grad_norm": 0.5419299602508545,
      "learning_rate": 0.0004518929689723883,
      "loss": 7.0479,
      "step": 677
    },
    {
      "epoch": 0.19302491103202846,
      "grad_norm": 0.47667187452316284,
      "learning_rate": 0.000451821804725306,
      "loss": 7.5176,
      "step": 678
    },
    {
      "epoch": 0.19330960854092527,
      "grad_norm": 0.49150925874710083,
      "learning_rate": 0.00045175064047822374,
      "loss": 7.5088,
      "step": 679
    },
    {
      "epoch": 0.19359430604982206,
      "grad_norm": 0.4631671905517578,
      "learning_rate": 0.00045167947623114146,
      "loss": 7.2822,
      "step": 680
    },
    {
      "epoch": 0.19387900355871887,
      "grad_norm": 0.6819197535514832,
      "learning_rate": 0.00045160831198405924,
      "loss": 6.5312,
      "step": 681
    },
    {
      "epoch": 0.19416370106761566,
      "grad_norm": 0.5060408115386963,
      "learning_rate": 0.00045153714773697696,
      "loss": 7.2939,
      "step": 682
    },
    {
      "epoch": 0.19444839857651244,
      "grad_norm": 0.4816899597644806,
      "learning_rate": 0.0004514659834898947,
      "loss": 7.5371,
      "step": 683
    },
    {
      "epoch": 0.19473309608540926,
      "grad_norm": 0.4419718086719513,
      "learning_rate": 0.0004513948192428124,
      "loss": 7.2891,
      "step": 684
    },
    {
      "epoch": 0.19501779359430604,
      "grad_norm": 0.5301364660263062,
      "learning_rate": 0.00045132365499573013,
      "loss": 6.8311,
      "step": 685
    },
    {
      "epoch": 0.19530249110320286,
      "grad_norm": 0.5797151327133179,
      "learning_rate": 0.0004512524907486479,
      "loss": 7.2871,
      "step": 686
    },
    {
      "epoch": 0.19558718861209964,
      "grad_norm": 0.604928195476532,
      "learning_rate": 0.00045118132650156563,
      "loss": 7.0869,
      "step": 687
    },
    {
      "epoch": 0.19587188612099643,
      "grad_norm": 0.4925019443035126,
      "learning_rate": 0.00045111016225448335,
      "loss": 7.1387,
      "step": 688
    },
    {
      "epoch": 0.19615658362989324,
      "grad_norm": 0.5087660551071167,
      "learning_rate": 0.0004510389980074011,
      "loss": 6.9668,
      "step": 689
    },
    {
      "epoch": 0.19644128113879003,
      "grad_norm": 0.4889131188392639,
      "learning_rate": 0.0004509678337603188,
      "loss": 6.9883,
      "step": 690
    },
    {
      "epoch": 0.19672597864768684,
      "grad_norm": 0.5806876420974731,
      "learning_rate": 0.00045089666951323657,
      "loss": 6.6211,
      "step": 691
    },
    {
      "epoch": 0.19701067615658363,
      "grad_norm": 0.6690099239349365,
      "learning_rate": 0.0004508255052661543,
      "loss": 6.5527,
      "step": 692
    },
    {
      "epoch": 0.19729537366548042,
      "grad_norm": 0.4910028278827667,
      "learning_rate": 0.000450754341019072,
      "loss": 7.0664,
      "step": 693
    },
    {
      "epoch": 0.19758007117437723,
      "grad_norm": 0.49179354310035706,
      "learning_rate": 0.0004506831767719898,
      "loss": 7.1865,
      "step": 694
    },
    {
      "epoch": 0.19786476868327402,
      "grad_norm": 0.638883113861084,
      "learning_rate": 0.0004506120125249075,
      "loss": 7.1006,
      "step": 695
    },
    {
      "epoch": 0.19814946619217083,
      "grad_norm": 0.400945246219635,
      "learning_rate": 0.0004505408482778252,
      "loss": 7.6387,
      "step": 696
    },
    {
      "epoch": 0.19843416370106762,
      "grad_norm": 0.5283869504928589,
      "learning_rate": 0.00045046968403074296,
      "loss": 7.2627,
      "step": 697
    },
    {
      "epoch": 0.1987188612099644,
      "grad_norm": 0.5028197765350342,
      "learning_rate": 0.0004503985197836607,
      "loss": 7.2373,
      "step": 698
    },
    {
      "epoch": 0.19900355871886122,
      "grad_norm": 0.5313175916671753,
      "learning_rate": 0.00045032735553657846,
      "loss": 7.4814,
      "step": 699
    },
    {
      "epoch": 0.199288256227758,
      "grad_norm": 0.5412628054618835,
      "learning_rate": 0.0004502561912894962,
      "loss": 7.584,
      "step": 700
    },
    {
      "epoch": 0.19957295373665482,
      "grad_norm": 0.5525862574577332,
      "learning_rate": 0.00045018502704241385,
      "loss": 6.9756,
      "step": 701
    },
    {
      "epoch": 0.1998576512455516,
      "grad_norm": 0.470053106546402,
      "learning_rate": 0.00045011386279533163,
      "loss": 7.3096,
      "step": 702
    },
    {
      "epoch": 0.2001423487544484,
      "grad_norm": 0.4619928300380707,
      "learning_rate": 0.00045004269854824935,
      "loss": 7.4365,
      "step": 703
    },
    {
      "epoch": 0.2004270462633452,
      "grad_norm": 0.511256754398346,
      "learning_rate": 0.00044997153430116713,
      "loss": 7.3418,
      "step": 704
    },
    {
      "epoch": 0.200711743772242,
      "grad_norm": 0.44021302461624146,
      "learning_rate": 0.00044990037005408485,
      "loss": 7.5215,
      "step": 705
    },
    {
      "epoch": 0.20099644128113878,
      "grad_norm": 0.45131561160087585,
      "learning_rate": 0.0004498292058070026,
      "loss": 7.0215,
      "step": 706
    },
    {
      "epoch": 0.2012811387900356,
      "grad_norm": 0.4091982841491699,
      "learning_rate": 0.0004497580415599203,
      "loss": 7.6855,
      "step": 707
    },
    {
      "epoch": 0.20156583629893238,
      "grad_norm": 0.42062363028526306,
      "learning_rate": 0.000449686877312838,
      "loss": 7.7256,
      "step": 708
    },
    {
      "epoch": 0.2018505338078292,
      "grad_norm": 0.49599114060401917,
      "learning_rate": 0.00044961571306575574,
      "loss": 7.1162,
      "step": 709
    },
    {
      "epoch": 0.20213523131672598,
      "grad_norm": 0.4859379231929779,
      "learning_rate": 0.0004495445488186735,
      "loss": 6.9316,
      "step": 710
    },
    {
      "epoch": 0.20241992882562276,
      "grad_norm": 0.40452441573143005,
      "learning_rate": 0.00044947338457159124,
      "loss": 7.4365,
      "step": 711
    },
    {
      "epoch": 0.20270462633451958,
      "grad_norm": 0.6663489937782288,
      "learning_rate": 0.000449402220324509,
      "loss": 6.9199,
      "step": 712
    },
    {
      "epoch": 0.20298932384341636,
      "grad_norm": 0.8615325093269348,
      "learning_rate": 0.0004493310560774267,
      "loss": 7.3496,
      "step": 713
    },
    {
      "epoch": 0.20327402135231318,
      "grad_norm": 0.4769529700279236,
      "learning_rate": 0.0004492598918303444,
      "loss": 7.4785,
      "step": 714
    },
    {
      "epoch": 0.20355871886120996,
      "grad_norm": 0.36396512389183044,
      "learning_rate": 0.0004491887275832622,
      "loss": 7.6992,
      "step": 715
    },
    {
      "epoch": 0.20384341637010675,
      "grad_norm": 0.43238091468811035,
      "learning_rate": 0.0004491175633361799,
      "loss": 7.6221,
      "step": 716
    },
    {
      "epoch": 0.20412811387900356,
      "grad_norm": 0.5147523283958435,
      "learning_rate": 0.0004490463990890977,
      "loss": 7.3389,
      "step": 717
    },
    {
      "epoch": 0.20441281138790035,
      "grad_norm": 0.47087812423706055,
      "learning_rate": 0.00044897523484201536,
      "loss": 7.5195,
      "step": 718
    },
    {
      "epoch": 0.20469750889679716,
      "grad_norm": 0.5744754672050476,
      "learning_rate": 0.0004489040705949331,
      "loss": 6.8584,
      "step": 719
    },
    {
      "epoch": 0.20498220640569395,
      "grad_norm": 0.7531524300575256,
      "learning_rate": 0.00044883290634785086,
      "loss": 7.6885,
      "step": 720
    },
    {
      "epoch": 0.20526690391459074,
      "grad_norm": 0.6688421964645386,
      "learning_rate": 0.0004487617421007686,
      "loss": 7.0498,
      "step": 721
    },
    {
      "epoch": 0.20555160142348755,
      "grad_norm": 0.41622069478034973,
      "learning_rate": 0.00044869057785368636,
      "loss": 7.5127,
      "step": 722
    },
    {
      "epoch": 0.20583629893238434,
      "grad_norm": 0.6788975596427917,
      "learning_rate": 0.0004486194136066041,
      "loss": 6.959,
      "step": 723
    },
    {
      "epoch": 0.20612099644128115,
      "grad_norm": 0.6270914077758789,
      "learning_rate": 0.00044854824935952175,
      "loss": 6.5908,
      "step": 724
    },
    {
      "epoch": 0.20640569395017794,
      "grad_norm": 0.42413002252578735,
      "learning_rate": 0.0004484770851124395,
      "loss": 7.3301,
      "step": 725
    },
    {
      "epoch": 0.20669039145907472,
      "grad_norm": 0.4253315031528473,
      "learning_rate": 0.00044840592086535725,
      "loss": 7.8818,
      "step": 726
    },
    {
      "epoch": 0.20697508896797154,
      "grad_norm": 0.45278969407081604,
      "learning_rate": 0.00044833475661827497,
      "loss": 7.4268,
      "step": 727
    },
    {
      "epoch": 0.20725978647686832,
      "grad_norm": 0.4045217037200928,
      "learning_rate": 0.00044826359237119275,
      "loss": 7.6025,
      "step": 728
    },
    {
      "epoch": 0.20754448398576514,
      "grad_norm": 0.5309168696403503,
      "learning_rate": 0.0004481924281241104,
      "loss": 7.1768,
      "step": 729
    },
    {
      "epoch": 0.20782918149466192,
      "grad_norm": 0.4788176715373993,
      "learning_rate": 0.0004481212638770282,
      "loss": 7.4551,
      "step": 730
    },
    {
      "epoch": 0.2081138790035587,
      "grad_norm": 0.535114586353302,
      "learning_rate": 0.0004480500996299459,
      "loss": 7.1504,
      "step": 731
    },
    {
      "epoch": 0.20839857651245552,
      "grad_norm": 0.5233950614929199,
      "learning_rate": 0.00044797893538286364,
      "loss": 7.1992,
      "step": 732
    },
    {
      "epoch": 0.2086832740213523,
      "grad_norm": 0.5039567351341248,
      "learning_rate": 0.0004479077711357814,
      "loss": 7.2734,
      "step": 733
    },
    {
      "epoch": 0.2089679715302491,
      "grad_norm": 0.5497581958770752,
      "learning_rate": 0.00044783660688869914,
      "loss": 7.126,
      "step": 734
    },
    {
      "epoch": 0.2092526690391459,
      "grad_norm": 0.5098934173583984,
      "learning_rate": 0.00044776544264161686,
      "loss": 7.2588,
      "step": 735
    },
    {
      "epoch": 0.2095373665480427,
      "grad_norm": 0.45094266533851624,
      "learning_rate": 0.0004476942783945346,
      "loss": 7.3164,
      "step": 736
    },
    {
      "epoch": 0.2098220640569395,
      "grad_norm": 0.46048614382743835,
      "learning_rate": 0.0004476231141474523,
      "loss": 7.7139,
      "step": 737
    },
    {
      "epoch": 0.2101067615658363,
      "grad_norm": 0.4291335642337799,
      "learning_rate": 0.0004475519499003701,
      "loss": 7.6035,
      "step": 738
    },
    {
      "epoch": 0.21039145907473308,
      "grad_norm": 0.3905137777328491,
      "learning_rate": 0.0004474807856532878,
      "loss": 7.9336,
      "step": 739
    },
    {
      "epoch": 0.2106761565836299,
      "grad_norm": 0.4790925085544586,
      "learning_rate": 0.0004474096214062056,
      "loss": 7.4365,
      "step": 740
    },
    {
      "epoch": 0.21096085409252668,
      "grad_norm": 0.6108134984970093,
      "learning_rate": 0.00044733845715912325,
      "loss": 6.9443,
      "step": 741
    },
    {
      "epoch": 0.2112455516014235,
      "grad_norm": 0.5870121121406555,
      "learning_rate": 0.000447267292912041,
      "loss": 7.0771,
      "step": 742
    },
    {
      "epoch": 0.21153024911032028,
      "grad_norm": 0.4234772324562073,
      "learning_rate": 0.00044719612866495875,
      "loss": 7.6436,
      "step": 743
    },
    {
      "epoch": 0.21181494661921707,
      "grad_norm": 0.4352046549320221,
      "learning_rate": 0.0004471249644178765,
      "loss": 7.7432,
      "step": 744
    },
    {
      "epoch": 0.21209964412811388,
      "grad_norm": 0.4731186032295227,
      "learning_rate": 0.0004470538001707942,
      "loss": 7.2842,
      "step": 745
    },
    {
      "epoch": 0.21238434163701067,
      "grad_norm": 0.5395245552062988,
      "learning_rate": 0.0004469826359237119,
      "loss": 6.9424,
      "step": 746
    },
    {
      "epoch": 0.21266903914590748,
      "grad_norm": 0.4573425352573395,
      "learning_rate": 0.00044691147167662964,
      "loss": 7.2559,
      "step": 747
    },
    {
      "epoch": 0.21295373665480427,
      "grad_norm": 0.4909103512763977,
      "learning_rate": 0.0004468403074295474,
      "loss": 7.2422,
      "step": 748
    },
    {
      "epoch": 0.21323843416370106,
      "grad_norm": 0.4994555115699768,
      "learning_rate": 0.00044676914318246514,
      "loss": 7.5713,
      "step": 749
    },
    {
      "epoch": 0.21352313167259787,
      "grad_norm": 0.5618712306022644,
      "learning_rate": 0.00044669797893538287,
      "loss": 6.9062,
      "step": 750
    },
    {
      "epoch": 0.21380782918149466,
      "grad_norm": 0.5334068536758423,
      "learning_rate": 0.00044662681468830064,
      "loss": 6.875,
      "step": 751
    },
    {
      "epoch": 0.21409252669039147,
      "grad_norm": 0.518782913684845,
      "learning_rate": 0.0004465556504412183,
      "loss": 7.1416,
      "step": 752
    },
    {
      "epoch": 0.21437722419928826,
      "grad_norm": 0.48912790417671204,
      "learning_rate": 0.0004464844861941361,
      "loss": 7.5664,
      "step": 753
    },
    {
      "epoch": 0.21466192170818504,
      "grad_norm": 0.38679495453834534,
      "learning_rate": 0.0004464133219470538,
      "loss": 7.5771,
      "step": 754
    },
    {
      "epoch": 0.21494661921708186,
      "grad_norm": 0.4669499099254608,
      "learning_rate": 0.00044634215769997153,
      "loss": 7.0508,
      "step": 755
    },
    {
      "epoch": 0.21523131672597864,
      "grad_norm": 0.5104237794876099,
      "learning_rate": 0.0004462709934528893,
      "loss": 6.9834,
      "step": 756
    },
    {
      "epoch": 0.21551601423487546,
      "grad_norm": 0.5343145728111267,
      "learning_rate": 0.000446199829205807,
      "loss": 7.3018,
      "step": 757
    },
    {
      "epoch": 0.21580071174377224,
      "grad_norm": 2.3170905113220215,
      "learning_rate": 0.0004461286649587247,
      "loss": 6.9307,
      "step": 758
    },
    {
      "epoch": 0.21608540925266903,
      "grad_norm": 0.45728206634521484,
      "learning_rate": 0.0004460575007116425,
      "loss": 7.5264,
      "step": 759
    },
    {
      "epoch": 0.21637010676156584,
      "grad_norm": 0.6728972792625427,
      "learning_rate": 0.0004459863364645602,
      "loss": 6.4639,
      "step": 760
    },
    {
      "epoch": 0.21665480427046263,
      "grad_norm": 0.5580065846443176,
      "learning_rate": 0.000445915172217478,
      "loss": 7.2715,
      "step": 761
    },
    {
      "epoch": 0.21693950177935942,
      "grad_norm": 0.5601818561553955,
      "learning_rate": 0.0004458440079703957,
      "loss": 7.0195,
      "step": 762
    },
    {
      "epoch": 0.21722419928825623,
      "grad_norm": 0.53824383020401,
      "learning_rate": 0.00044577284372331337,
      "loss": 7.1748,
      "step": 763
    },
    {
      "epoch": 0.21750889679715302,
      "grad_norm": 0.5673179030418396,
      "learning_rate": 0.00044570167947623115,
      "loss": 7.3262,
      "step": 764
    },
    {
      "epoch": 0.21779359430604983,
      "grad_norm": 0.5143839120864868,
      "learning_rate": 0.00044563051522914887,
      "loss": 7.2246,
      "step": 765
    },
    {
      "epoch": 0.21807829181494662,
      "grad_norm": 0.479536771774292,
      "learning_rate": 0.00044555935098206665,
      "loss": 7.5,
      "step": 766
    },
    {
      "epoch": 0.2183629893238434,
      "grad_norm": 0.5423471927642822,
      "learning_rate": 0.00044548818673498437,
      "loss": 7.626,
      "step": 767
    },
    {
      "epoch": 0.21864768683274022,
      "grad_norm": 0.43286991119384766,
      "learning_rate": 0.0004454170224879021,
      "loss": 7.3652,
      "step": 768
    },
    {
      "epoch": 0.218932384341637,
      "grad_norm": 0.5857230424880981,
      "learning_rate": 0.0004453458582408198,
      "loss": 7.0664,
      "step": 769
    },
    {
      "epoch": 0.21921708185053382,
      "grad_norm": 0.5000907778739929,
      "learning_rate": 0.00044527469399373754,
      "loss": 7.4561,
      "step": 770
    },
    {
      "epoch": 0.2195017793594306,
      "grad_norm": 0.43097177147865295,
      "learning_rate": 0.0004452035297466553,
      "loss": 7.3828,
      "step": 771
    },
    {
      "epoch": 0.2197864768683274,
      "grad_norm": 0.42178913950920105,
      "learning_rate": 0.00044513236549957304,
      "loss": 7.5723,
      "step": 772
    },
    {
      "epoch": 0.2200711743772242,
      "grad_norm": 0.6063342690467834,
      "learning_rate": 0.00044506120125249076,
      "loss": 7.0381,
      "step": 773
    },
    {
      "epoch": 0.220355871886121,
      "grad_norm": 0.5009235143661499,
      "learning_rate": 0.0004449900370054085,
      "loss": 7.2695,
      "step": 774
    },
    {
      "epoch": 0.2206405693950178,
      "grad_norm": 0.5648031830787659,
      "learning_rate": 0.0004449188727583262,
      "loss": 6.8516,
      "step": 775
    },
    {
      "epoch": 0.2209252669039146,
      "grad_norm": 0.5607783794403076,
      "learning_rate": 0.00044484770851124393,
      "loss": 6.6475,
      "step": 776
    },
    {
      "epoch": 0.22120996441281138,
      "grad_norm": 0.5817128419876099,
      "learning_rate": 0.0004447765442641617,
      "loss": 6.7178,
      "step": 777
    },
    {
      "epoch": 0.2214946619217082,
      "grad_norm": 0.5132507085800171,
      "learning_rate": 0.00044470538001707943,
      "loss": 7.5273,
      "step": 778
    },
    {
      "epoch": 0.22177935943060498,
      "grad_norm": 0.491405725479126,
      "learning_rate": 0.0004446342157699972,
      "loss": 7.3223,
      "step": 779
    },
    {
      "epoch": 0.2220640569395018,
      "grad_norm": 0.49990856647491455,
      "learning_rate": 0.0004445630515229149,
      "loss": 7.0146,
      "step": 780
    },
    {
      "epoch": 0.22234875444839858,
      "grad_norm": 0.598653256893158,
      "learning_rate": 0.0004444918872758326,
      "loss": 6.3633,
      "step": 781
    },
    {
      "epoch": 0.22263345195729536,
      "grad_norm": 0.5837319493293762,
      "learning_rate": 0.0004444207230287504,
      "loss": 7.0635,
      "step": 782
    },
    {
      "epoch": 0.22291814946619218,
      "grad_norm": 0.3937888443470001,
      "learning_rate": 0.0004443495587816681,
      "loss": 7.7715,
      "step": 783
    },
    {
      "epoch": 0.22320284697508896,
      "grad_norm": 0.44455066323280334,
      "learning_rate": 0.0004442783945345859,
      "loss": 7.417,
      "step": 784
    },
    {
      "epoch": 0.22348754448398578,
      "grad_norm": 0.4986377954483032,
      "learning_rate": 0.0004442072302875036,
      "loss": 7.3779,
      "step": 785
    },
    {
      "epoch": 0.22377224199288256,
      "grad_norm": 0.5064935088157654,
      "learning_rate": 0.00044413606604042126,
      "loss": 7.123,
      "step": 786
    },
    {
      "epoch": 0.22405693950177935,
      "grad_norm": 0.44009876251220703,
      "learning_rate": 0.00044406490179333904,
      "loss": 7.709,
      "step": 787
    },
    {
      "epoch": 0.22434163701067616,
      "grad_norm": 0.483054518699646,
      "learning_rate": 0.00044399373754625676,
      "loss": 7.5605,
      "step": 788
    },
    {
      "epoch": 0.22462633451957295,
      "grad_norm": 0.5088704228401184,
      "learning_rate": 0.00044392257329917454,
      "loss": 7.2334,
      "step": 789
    },
    {
      "epoch": 0.22491103202846974,
      "grad_norm": 0.41016024351119995,
      "learning_rate": 0.00044385140905209226,
      "loss": 7.791,
      "step": 790
    },
    {
      "epoch": 0.22519572953736655,
      "grad_norm": 0.5502153038978577,
      "learning_rate": 0.00044378024480500993,
      "loss": 7.3965,
      "step": 791
    },
    {
      "epoch": 0.22548042704626334,
      "grad_norm": 0.543286144733429,
      "learning_rate": 0.0004437090805579277,
      "loss": 7.2197,
      "step": 792
    },
    {
      "epoch": 0.22576512455516015,
      "grad_norm": 0.4524809718132019,
      "learning_rate": 0.00044363791631084543,
      "loss": 7.7568,
      "step": 793
    },
    {
      "epoch": 0.22604982206405694,
      "grad_norm": 0.46553486585617065,
      "learning_rate": 0.00044356675206376315,
      "loss": 7.4297,
      "step": 794
    },
    {
      "epoch": 0.22633451957295372,
      "grad_norm": 0.4544578492641449,
      "learning_rate": 0.00044349558781668093,
      "loss": 7.6367,
      "step": 795
    },
    {
      "epoch": 0.22661921708185054,
      "grad_norm": 0.4988901615142822,
      "learning_rate": 0.00044342442356959865,
      "loss": 7.5498,
      "step": 796
    },
    {
      "epoch": 0.22690391459074732,
      "grad_norm": 0.5461361408233643,
      "learning_rate": 0.0004433532593225164,
      "loss": 7.2773,
      "step": 797
    },
    {
      "epoch": 0.22718861209964414,
      "grad_norm": 0.5461740493774414,
      "learning_rate": 0.0004432820950754341,
      "loss": 7.0732,
      "step": 798
    },
    {
      "epoch": 0.22747330960854092,
      "grad_norm": 0.48787885904312134,
      "learning_rate": 0.0004432109308283518,
      "loss": 7.1338,
      "step": 799
    },
    {
      "epoch": 0.2277580071174377,
      "grad_norm": 0.4876340329647064,
      "learning_rate": 0.0004431397665812696,
      "loss": 7.0635,
      "step": 800
    },
    {
      "epoch": 0.2277580071174377,
      "eval_bleu": 0.10199226196567952,
      "eval_loss": 7.03515625,
      "eval_runtime": 179.5646,
      "eval_samples_per_second": 1.582,
      "eval_steps_per_second": 0.1,
      "step": 800
    },
    {
      "epoch": 0.22804270462633452,
      "grad_norm": 0.4812236428260803,
      "learning_rate": 0.0004430686023341873,
      "loss": 7.4922,
      "step": 801
    },
    {
      "epoch": 0.2283274021352313,
      "grad_norm": 0.5275574922561646,
      "learning_rate": 0.00044299743808710505,
      "loss": 7.2969,
      "step": 802
    },
    {
      "epoch": 0.22861209964412813,
      "grad_norm": 0.6468873620033264,
      "learning_rate": 0.00044292627384002277,
      "loss": 7.0674,
      "step": 803
    },
    {
      "epoch": 0.2288967971530249,
      "grad_norm": 0.5731150507926941,
      "learning_rate": 0.0004428551095929405,
      "loss": 6.8281,
      "step": 804
    },
    {
      "epoch": 0.2291814946619217,
      "grad_norm": 0.3813442587852478,
      "learning_rate": 0.00044278394534585827,
      "loss": 7.7334,
      "step": 805
    },
    {
      "epoch": 0.2294661921708185,
      "grad_norm": 0.4659816026687622,
      "learning_rate": 0.000442712781098776,
      "loss": 7.6846,
      "step": 806
    },
    {
      "epoch": 0.2297508896797153,
      "grad_norm": 0.4053337574005127,
      "learning_rate": 0.0004426416168516937,
      "loss": 7.9707,
      "step": 807
    },
    {
      "epoch": 0.2300355871886121,
      "grad_norm": 0.642076849937439,
      "learning_rate": 0.00044257045260461144,
      "loss": 7.5889,
      "step": 808
    },
    {
      "epoch": 0.2303202846975089,
      "grad_norm": 0.5518573522567749,
      "learning_rate": 0.00044249928835752916,
      "loss": 7.4805,
      "step": 809
    },
    {
      "epoch": 0.23060498220640568,
      "grad_norm": 0.5629279017448425,
      "learning_rate": 0.00044242812411044694,
      "loss": 7.0342,
      "step": 810
    },
    {
      "epoch": 0.2308896797153025,
      "grad_norm": 0.5347673892974854,
      "learning_rate": 0.00044235695986336466,
      "loss": 7.0684,
      "step": 811
    },
    {
      "epoch": 0.23117437722419928,
      "grad_norm": 0.5217247605323792,
      "learning_rate": 0.0004422857956162824,
      "loss": 7.2383,
      "step": 812
    },
    {
      "epoch": 0.2314590747330961,
      "grad_norm": 0.6054062843322754,
      "learning_rate": 0.00044221463136920016,
      "loss": 7.4854,
      "step": 813
    },
    {
      "epoch": 0.23174377224199288,
      "grad_norm": 0.6359828114509583,
      "learning_rate": 0.0004421434671221178,
      "loss": 6.7822,
      "step": 814
    },
    {
      "epoch": 0.23202846975088967,
      "grad_norm": 0.508417010307312,
      "learning_rate": 0.0004420723028750356,
      "loss": 7.4092,
      "step": 815
    },
    {
      "epoch": 0.23231316725978648,
      "grad_norm": 0.47506824135780334,
      "learning_rate": 0.0004420011386279533,
      "loss": 7.3555,
      "step": 816
    },
    {
      "epoch": 0.23259786476868327,
      "grad_norm": 0.5379090905189514,
      "learning_rate": 0.00044192997438087105,
      "loss": 6.8398,
      "step": 817
    },
    {
      "epoch": 0.23288256227758006,
      "grad_norm": 0.49462756514549255,
      "learning_rate": 0.0004418588101337888,
      "loss": 7.1006,
      "step": 818
    },
    {
      "epoch": 0.23316725978647687,
      "grad_norm": 0.4736199676990509,
      "learning_rate": 0.0004417876458867065,
      "loss": 7.082,
      "step": 819
    },
    {
      "epoch": 0.23345195729537366,
      "grad_norm": 0.46984589099884033,
      "learning_rate": 0.00044171648163962427,
      "loss": 7.0547,
      "step": 820
    },
    {
      "epoch": 0.23373665480427047,
      "grad_norm": 0.42229676246643066,
      "learning_rate": 0.000441645317392542,
      "loss": 7.5547,
      "step": 821
    },
    {
      "epoch": 0.23402135231316726,
      "grad_norm": 0.5318613052368164,
      "learning_rate": 0.0004415741531454597,
      "loss": 7.2295,
      "step": 822
    },
    {
      "epoch": 0.23430604982206404,
      "grad_norm": 0.5012297034263611,
      "learning_rate": 0.0004415029888983775,
      "loss": 7.3096,
      "step": 823
    },
    {
      "epoch": 0.23459074733096086,
      "grad_norm": 0.4093872606754303,
      "learning_rate": 0.0004414318246512952,
      "loss": 7.4219,
      "step": 824
    },
    {
      "epoch": 0.23487544483985764,
      "grad_norm": 0.542306661605835,
      "learning_rate": 0.0004413606604042129,
      "loss": 7.1299,
      "step": 825
    },
    {
      "epoch": 0.23516014234875446,
      "grad_norm": 0.5459290742874146,
      "learning_rate": 0.00044128949615713066,
      "loss": 7.2705,
      "step": 826
    },
    {
      "epoch": 0.23544483985765124,
      "grad_norm": 0.4683960974216461,
      "learning_rate": 0.0004412183319100484,
      "loss": 7.1016,
      "step": 827
    },
    {
      "epoch": 0.23572953736654803,
      "grad_norm": 0.4583689570426941,
      "learning_rate": 0.00044114716766296616,
      "loss": 7.5137,
      "step": 828
    },
    {
      "epoch": 0.23601423487544484,
      "grad_norm": 0.48082277178764343,
      "learning_rate": 0.0004410760034158839,
      "loss": 7.4746,
      "step": 829
    },
    {
      "epoch": 0.23629893238434163,
      "grad_norm": 0.4595732092857361,
      "learning_rate": 0.0004410048391688016,
      "loss": 7.3242,
      "step": 830
    },
    {
      "epoch": 0.23658362989323845,
      "grad_norm": 0.8203401565551758,
      "learning_rate": 0.00044093367492171933,
      "loss": 7.3457,
      "step": 831
    },
    {
      "epoch": 0.23686832740213523,
      "grad_norm": 0.6309863924980164,
      "learning_rate": 0.00044086251067463705,
      "loss": 6.5508,
      "step": 832
    },
    {
      "epoch": 0.23715302491103202,
      "grad_norm": 0.5663462281227112,
      "learning_rate": 0.00044079134642755483,
      "loss": 6.9541,
      "step": 833
    },
    {
      "epoch": 0.23743772241992883,
      "grad_norm": 0.5051040649414062,
      "learning_rate": 0.00044072018218047255,
      "loss": 6.9756,
      "step": 834
    },
    {
      "epoch": 0.23772241992882562,
      "grad_norm": 0.47865554690361023,
      "learning_rate": 0.0004406490179333903,
      "loss": 7.3057,
      "step": 835
    },
    {
      "epoch": 0.23800711743772243,
      "grad_norm": 0.43697893619537354,
      "learning_rate": 0.000440577853686308,
      "loss": 7.6318,
      "step": 836
    },
    {
      "epoch": 0.23829181494661922,
      "grad_norm": 0.534851610660553,
      "learning_rate": 0.0004405066894392257,
      "loss": 7.0742,
      "step": 837
    },
    {
      "epoch": 0.238576512455516,
      "grad_norm": 0.5143926739692688,
      "learning_rate": 0.0004404355251921435,
      "loss": 6.8213,
      "step": 838
    },
    {
      "epoch": 0.23886120996441282,
      "grad_norm": 0.5173239707946777,
      "learning_rate": 0.0004403643609450612,
      "loss": 7.1689,
      "step": 839
    },
    {
      "epoch": 0.2391459074733096,
      "grad_norm": 0.44212883710861206,
      "learning_rate": 0.00044029319669797894,
      "loss": 7.6211,
      "step": 840
    },
    {
      "epoch": 0.23943060498220642,
      "grad_norm": 0.4778367877006531,
      "learning_rate": 0.0004402220324508967,
      "loss": 7.2168,
      "step": 841
    },
    {
      "epoch": 0.2397153024911032,
      "grad_norm": 0.6036151051521301,
      "learning_rate": 0.0004401508682038144,
      "loss": 6.9395,
      "step": 842
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.5227299928665161,
      "learning_rate": 0.0004400797039567321,
      "loss": 7.4209,
      "step": 843
    },
    {
      "epoch": 0.2402846975088968,
      "grad_norm": 0.4932726323604584,
      "learning_rate": 0.0004400085397096499,
      "loss": 7.5088,
      "step": 844
    },
    {
      "epoch": 0.2405693950177936,
      "grad_norm": 0.41230401396751404,
      "learning_rate": 0.0004399373754625676,
      "loss": 7.6035,
      "step": 845
    },
    {
      "epoch": 0.24085409252669038,
      "grad_norm": 0.6228237748146057,
      "learning_rate": 0.0004398662112154854,
      "loss": 6.9785,
      "step": 846
    },
    {
      "epoch": 0.2411387900355872,
      "grad_norm": 0.5555920004844666,
      "learning_rate": 0.00043979504696840306,
      "loss": 7.0986,
      "step": 847
    },
    {
      "epoch": 0.24142348754448398,
      "grad_norm": 0.5252820253372192,
      "learning_rate": 0.0004397238827213208,
      "loss": 7.1113,
      "step": 848
    },
    {
      "epoch": 0.2417081850533808,
      "grad_norm": 0.5262016654014587,
      "learning_rate": 0.00043965271847423856,
      "loss": 7.3691,
      "step": 849
    },
    {
      "epoch": 0.24199288256227758,
      "grad_norm": 0.5109695196151733,
      "learning_rate": 0.0004395815542271563,
      "loss": 7.3809,
      "step": 850
    },
    {
      "epoch": 0.24227758007117436,
      "grad_norm": 0.4192237854003906,
      "learning_rate": 0.00043951038998007406,
      "loss": 7.791,
      "step": 851
    },
    {
      "epoch": 0.24256227758007118,
      "grad_norm": 0.5321003794670105,
      "learning_rate": 0.0004394392257329918,
      "loss": 7.1904,
      "step": 852
    },
    {
      "epoch": 0.24284697508896796,
      "grad_norm": 0.5241415500640869,
      "learning_rate": 0.00043936806148590945,
      "loss": 7.2236,
      "step": 853
    },
    {
      "epoch": 0.24313167259786478,
      "grad_norm": 0.4347819685935974,
      "learning_rate": 0.0004392968972388272,
      "loss": 7.6279,
      "step": 854
    },
    {
      "epoch": 0.24341637010676156,
      "grad_norm": 0.5250548720359802,
      "learning_rate": 0.00043922573299174495,
      "loss": 7.4512,
      "step": 855
    },
    {
      "epoch": 0.24370106761565835,
      "grad_norm": 0.5096853971481323,
      "learning_rate": 0.00043915456874466267,
      "loss": 7.3613,
      "step": 856
    },
    {
      "epoch": 0.24398576512455517,
      "grad_norm": 0.5170667767524719,
      "learning_rate": 0.00043908340449758045,
      "loss": 7.2148,
      "step": 857
    },
    {
      "epoch": 0.24427046263345195,
      "grad_norm": 0.4182034730911255,
      "learning_rate": 0.00043901224025049817,
      "loss": 7.5996,
      "step": 858
    },
    {
      "epoch": 0.24455516014234877,
      "grad_norm": 0.47495993971824646,
      "learning_rate": 0.0004389410760034159,
      "loss": 7.1172,
      "step": 859
    },
    {
      "epoch": 0.24483985765124555,
      "grad_norm": 0.425849050283432,
      "learning_rate": 0.0004388699117563336,
      "loss": 7.5752,
      "step": 860
    },
    {
      "epoch": 0.24512455516014234,
      "grad_norm": 0.5022419095039368,
      "learning_rate": 0.00043879874750925134,
      "loss": 7.2393,
      "step": 861
    },
    {
      "epoch": 0.24540925266903915,
      "grad_norm": 0.5683399438858032,
      "learning_rate": 0.0004387275832621691,
      "loss": 7.1514,
      "step": 862
    },
    {
      "epoch": 0.24569395017793594,
      "grad_norm": 0.3528391122817993,
      "learning_rate": 0.00043865641901508684,
      "loss": 7.8232,
      "step": 863
    },
    {
      "epoch": 0.24597864768683275,
      "grad_norm": 0.4852282404899597,
      "learning_rate": 0.00043858525476800456,
      "loss": 7.5166,
      "step": 864
    },
    {
      "epoch": 0.24626334519572954,
      "grad_norm": 0.6018944382667542,
      "learning_rate": 0.0004385140905209223,
      "loss": 7.3467,
      "step": 865
    },
    {
      "epoch": 0.24654804270462632,
      "grad_norm": 0.6870088577270508,
      "learning_rate": 0.00043844292627384,
      "loss": 6.6152,
      "step": 866
    },
    {
      "epoch": 0.24683274021352314,
      "grad_norm": 0.4139854907989502,
      "learning_rate": 0.0004383717620267578,
      "loss": 7.709,
      "step": 867
    },
    {
      "epoch": 0.24711743772241992,
      "grad_norm": 0.576387882232666,
      "learning_rate": 0.0004383005977796755,
      "loss": 6.8809,
      "step": 868
    },
    {
      "epoch": 0.24740213523131674,
      "grad_norm": 0.4755402207374573,
      "learning_rate": 0.0004382294335325933,
      "loss": 7.293,
      "step": 869
    },
    {
      "epoch": 0.24768683274021353,
      "grad_norm": 0.5647282600402832,
      "learning_rate": 0.00043815826928551095,
      "loss": 6.9727,
      "step": 870
    },
    {
      "epoch": 0.2479715302491103,
      "grad_norm": 0.6776756644248962,
      "learning_rate": 0.0004380871050384287,
      "loss": 5.9951,
      "step": 871
    },
    {
      "epoch": 0.24825622775800713,
      "grad_norm": 0.45100125670433044,
      "learning_rate": 0.00043801594079134645,
      "loss": 7.3037,
      "step": 872
    },
    {
      "epoch": 0.2485409252669039,
      "grad_norm": 0.4857039749622345,
      "learning_rate": 0.0004379447765442642,
      "loss": 7.4775,
      "step": 873
    },
    {
      "epoch": 0.24882562277580073,
      "grad_norm": 0.5256150960922241,
      "learning_rate": 0.0004378736122971819,
      "loss": 7.2344,
      "step": 874
    },
    {
      "epoch": 0.2491103202846975,
      "grad_norm": 0.5394523739814758,
      "learning_rate": 0.0004378024480500997,
      "loss": 7.0469,
      "step": 875
    },
    {
      "epoch": 0.2493950177935943,
      "grad_norm": 0.4427834153175354,
      "learning_rate": 0.00043773128380301734,
      "loss": 7.4883,
      "step": 876
    },
    {
      "epoch": 0.2496797153024911,
      "grad_norm": 0.5576911568641663,
      "learning_rate": 0.0004376601195559351,
      "loss": 6.9834,
      "step": 877
    },
    {
      "epoch": 0.2499644128113879,
      "grad_norm": 0.5288885235786438,
      "learning_rate": 0.00043758895530885284,
      "loss": 7.5918,
      "step": 878
    },
    {
      "epoch": 0.2502491103202847,
      "grad_norm": 0.8065580129623413,
      "learning_rate": 0.00043751779106177056,
      "loss": 6.6963,
      "step": 879
    },
    {
      "epoch": 0.2505338078291815,
      "grad_norm": 0.6014062762260437,
      "learning_rate": 0.00043744662681468834,
      "loss": 6.7549,
      "step": 880
    },
    {
      "epoch": 0.2508185053380783,
      "grad_norm": 0.4984685480594635,
      "learning_rate": 0.000437375462567606,
      "loss": 7.5518,
      "step": 881
    },
    {
      "epoch": 0.25110320284697507,
      "grad_norm": 0.47743654251098633,
      "learning_rate": 0.0004373042983205238,
      "loss": 7.4434,
      "step": 882
    },
    {
      "epoch": 0.2513879003558719,
      "grad_norm": 0.5659381151199341,
      "learning_rate": 0.0004372331340734415,
      "loss": 7.1982,
      "step": 883
    },
    {
      "epoch": 0.2516725978647687,
      "grad_norm": 0.521162748336792,
      "learning_rate": 0.00043716196982635923,
      "loss": 7.4102,
      "step": 884
    },
    {
      "epoch": 0.25195729537366546,
      "grad_norm": 0.5566650629043579,
      "learning_rate": 0.000437090805579277,
      "loss": 6.7998,
      "step": 885
    },
    {
      "epoch": 0.25224199288256227,
      "grad_norm": 0.5467177629470825,
      "learning_rate": 0.00043701964133219473,
      "loss": 7.2295,
      "step": 886
    },
    {
      "epoch": 0.2525266903914591,
      "grad_norm": 0.5133892297744751,
      "learning_rate": 0.0004369484770851124,
      "loss": 6.835,
      "step": 887
    },
    {
      "epoch": 0.2528113879003559,
      "grad_norm": 0.4159110486507416,
      "learning_rate": 0.0004368773128380302,
      "loss": 7.7285,
      "step": 888
    },
    {
      "epoch": 0.25309608540925266,
      "grad_norm": 0.443999320268631,
      "learning_rate": 0.0004368061485909479,
      "loss": 7.4121,
      "step": 889
    },
    {
      "epoch": 0.25338078291814947,
      "grad_norm": 0.5295789837837219,
      "learning_rate": 0.0004367349843438657,
      "loss": 7.1299,
      "step": 890
    },
    {
      "epoch": 0.2536654804270463,
      "grad_norm": 0.42882344126701355,
      "learning_rate": 0.0004366638200967834,
      "loss": 7.7754,
      "step": 891
    },
    {
      "epoch": 0.25395017793594304,
      "grad_norm": 0.5075104236602783,
      "learning_rate": 0.00043659265584970107,
      "loss": 6.7295,
      "step": 892
    },
    {
      "epoch": 0.25423487544483986,
      "grad_norm": 0.5219612121582031,
      "learning_rate": 0.00043652149160261885,
      "loss": 7.6309,
      "step": 893
    },
    {
      "epoch": 0.2545195729537367,
      "grad_norm": 0.5781932473182678,
      "learning_rate": 0.00043645032735553657,
      "loss": 6.8701,
      "step": 894
    },
    {
      "epoch": 0.25480427046263343,
      "grad_norm": 0.4656875431537628,
      "learning_rate": 0.00043637916310845435,
      "loss": 7.5107,
      "step": 895
    },
    {
      "epoch": 0.25508896797153024,
      "grad_norm": 0.512679934501648,
      "learning_rate": 0.00043630799886137207,
      "loss": 7.2812,
      "step": 896
    },
    {
      "epoch": 0.25537366548042706,
      "grad_norm": 0.530357301235199,
      "learning_rate": 0.0004362368346142898,
      "loss": 6.9248,
      "step": 897
    },
    {
      "epoch": 0.2556583629893238,
      "grad_norm": 0.44879037141799927,
      "learning_rate": 0.0004361656703672075,
      "loss": 7.5576,
      "step": 898
    },
    {
      "epoch": 0.25594306049822063,
      "grad_norm": 0.4848640263080597,
      "learning_rate": 0.00043609450612012524,
      "loss": 7.3281,
      "step": 899
    },
    {
      "epoch": 0.25622775800711745,
      "grad_norm": 0.43455377221107483,
      "learning_rate": 0.000436023341873043,
      "loss": 7.5635,
      "step": 900
    },
    {
      "epoch": 0.25651245551601426,
      "grad_norm": 0.41088178753852844,
      "learning_rate": 0.00043595217762596074,
      "loss": 7.5537,
      "step": 901
    },
    {
      "epoch": 0.256797153024911,
      "grad_norm": 0.5732548832893372,
      "learning_rate": 0.00043588101337887846,
      "loss": 6.7061,
      "step": 902
    },
    {
      "epoch": 0.25708185053380783,
      "grad_norm": 0.5296939015388489,
      "learning_rate": 0.00043580984913179624,
      "loss": 7.0996,
      "step": 903
    },
    {
      "epoch": 0.25736654804270465,
      "grad_norm": 0.44330182671546936,
      "learning_rate": 0.0004357386848847139,
      "loss": 7.5117,
      "step": 904
    },
    {
      "epoch": 0.2576512455516014,
      "grad_norm": 0.625610888004303,
      "learning_rate": 0.00043566752063763163,
      "loss": 7.1572,
      "step": 905
    },
    {
      "epoch": 0.2579359430604982,
      "grad_norm": 0.5074996948242188,
      "learning_rate": 0.0004355963563905494,
      "loss": 7.4404,
      "step": 906
    },
    {
      "epoch": 0.25822064056939503,
      "grad_norm": 0.4755004942417145,
      "learning_rate": 0.00043552519214346713,
      "loss": 7.374,
      "step": 907
    },
    {
      "epoch": 0.2585053380782918,
      "grad_norm": 0.6408856511116028,
      "learning_rate": 0.0004354540278963849,
      "loss": 7.1172,
      "step": 908
    },
    {
      "epoch": 0.2587900355871886,
      "grad_norm": 0.5064488053321838,
      "learning_rate": 0.00043538286364930257,
      "loss": 7.2129,
      "step": 909
    },
    {
      "epoch": 0.2590747330960854,
      "grad_norm": 0.6024318933486938,
      "learning_rate": 0.0004353116994022203,
      "loss": 7.0586,
      "step": 910
    },
    {
      "epoch": 0.25935943060498223,
      "grad_norm": 0.5472372174263,
      "learning_rate": 0.00043524053515513807,
      "loss": 6.8779,
      "step": 911
    },
    {
      "epoch": 0.259644128113879,
      "grad_norm": 0.4426235258579254,
      "learning_rate": 0.0004351693709080558,
      "loss": 7.7656,
      "step": 912
    },
    {
      "epoch": 0.2599288256227758,
      "grad_norm": 0.47683799266815186,
      "learning_rate": 0.00043509820666097357,
      "loss": 7.9033,
      "step": 913
    },
    {
      "epoch": 0.2602135231316726,
      "grad_norm": 0.4714563488960266,
      "learning_rate": 0.0004350270424138913,
      "loss": 7.6914,
      "step": 914
    },
    {
      "epoch": 0.2604982206405694,
      "grad_norm": 0.5874764323234558,
      "learning_rate": 0.00043495587816680896,
      "loss": 6.9482,
      "step": 915
    },
    {
      "epoch": 0.2607829181494662,
      "grad_norm": 0.46627673506736755,
      "learning_rate": 0.00043488471391972674,
      "loss": 7.5713,
      "step": 916
    },
    {
      "epoch": 0.261067615658363,
      "grad_norm": 0.5269532203674316,
      "learning_rate": 0.00043481354967264446,
      "loss": 7.1367,
      "step": 917
    },
    {
      "epoch": 0.26135231316725976,
      "grad_norm": 0.49821069836616516,
      "learning_rate": 0.00043474238542556224,
      "loss": 7.6846,
      "step": 918
    },
    {
      "epoch": 0.2616370106761566,
      "grad_norm": 0.47218114137649536,
      "learning_rate": 0.00043467122117847996,
      "loss": 7.5557,
      "step": 919
    },
    {
      "epoch": 0.2619217081850534,
      "grad_norm": 0.5291839241981506,
      "learning_rate": 0.0004346000569313977,
      "loss": 7.4102,
      "step": 920
    },
    {
      "epoch": 0.26220640569395015,
      "grad_norm": 0.5340070724487305,
      "learning_rate": 0.0004345288926843154,
      "loss": 6.9697,
      "step": 921
    },
    {
      "epoch": 0.26249110320284696,
      "grad_norm": 0.5339959859848022,
      "learning_rate": 0.00043445772843723313,
      "loss": 7.0928,
      "step": 922
    },
    {
      "epoch": 0.2627758007117438,
      "grad_norm": 0.580223798751831,
      "learning_rate": 0.00043438656419015085,
      "loss": 6.8213,
      "step": 923
    },
    {
      "epoch": 0.2630604982206406,
      "grad_norm": 0.5580481290817261,
      "learning_rate": 0.00043431539994306863,
      "loss": 6.8799,
      "step": 924
    },
    {
      "epoch": 0.26334519572953735,
      "grad_norm": 0.4867836534976959,
      "learning_rate": 0.00043424423569598635,
      "loss": 7.8086,
      "step": 925
    },
    {
      "epoch": 0.26362989323843417,
      "grad_norm": 0.4634561538696289,
      "learning_rate": 0.0004341730714489041,
      "loss": 7.4473,
      "step": 926
    },
    {
      "epoch": 0.263914590747331,
      "grad_norm": 0.5524905920028687,
      "learning_rate": 0.0004341019072018218,
      "loss": 6.9121,
      "step": 927
    },
    {
      "epoch": 0.26419928825622774,
      "grad_norm": 0.45816829800605774,
      "learning_rate": 0.0004340307429547395,
      "loss": 7.3428,
      "step": 928
    },
    {
      "epoch": 0.26448398576512455,
      "grad_norm": 0.6225414276123047,
      "learning_rate": 0.0004339595787076573,
      "loss": 6.8535,
      "step": 929
    },
    {
      "epoch": 0.26476868327402137,
      "grad_norm": 0.4236343801021576,
      "learning_rate": 0.000433888414460575,
      "loss": 8.0156,
      "step": 930
    },
    {
      "epoch": 0.2650533807829181,
      "grad_norm": 0.4699235260486603,
      "learning_rate": 0.0004338172502134928,
      "loss": 7.5518,
      "step": 931
    },
    {
      "epoch": 0.26533807829181494,
      "grad_norm": 0.4837797284126282,
      "learning_rate": 0.00043374608596641047,
      "loss": 7.3555,
      "step": 932
    },
    {
      "epoch": 0.26562277580071175,
      "grad_norm": 0.5280996561050415,
      "learning_rate": 0.0004336749217193282,
      "loss": 6.7998,
      "step": 933
    },
    {
      "epoch": 0.26590747330960857,
      "grad_norm": 0.5825293064117432,
      "learning_rate": 0.00043360375747224597,
      "loss": 7.0225,
      "step": 934
    },
    {
      "epoch": 0.2661921708185053,
      "grad_norm": 0.38843557238578796,
      "learning_rate": 0.0004335325932251637,
      "loss": 7.8369,
      "step": 935
    },
    {
      "epoch": 0.26647686832740214,
      "grad_norm": 0.41565942764282227,
      "learning_rate": 0.00043346142897808147,
      "loss": 7.6914,
      "step": 936
    },
    {
      "epoch": 0.26676156583629895,
      "grad_norm": 0.5718416571617126,
      "learning_rate": 0.00043339026473099914,
      "loss": 6.6875,
      "step": 937
    },
    {
      "epoch": 0.2670462633451957,
      "grad_norm": 0.5091272592544556,
      "learning_rate": 0.00043331910048391686,
      "loss": 7.2822,
      "step": 938
    },
    {
      "epoch": 0.2673309608540925,
      "grad_norm": 0.47761550545692444,
      "learning_rate": 0.00043324793623683463,
      "loss": 7.2646,
      "step": 939
    },
    {
      "epoch": 0.26761565836298934,
      "grad_norm": 0.4958757758140564,
      "learning_rate": 0.00043317677198975236,
      "loss": 7.3691,
      "step": 940
    },
    {
      "epoch": 0.2679003558718861,
      "grad_norm": 0.5092728137969971,
      "learning_rate": 0.0004331056077426701,
      "loss": 7.3887,
      "step": 941
    },
    {
      "epoch": 0.2681850533807829,
      "grad_norm": 0.5320535898208618,
      "learning_rate": 0.00043303444349558786,
      "loss": 7.2588,
      "step": 942
    },
    {
      "epoch": 0.2684697508896797,
      "grad_norm": 0.4988536834716797,
      "learning_rate": 0.0004329632792485055,
      "loss": 7.1562,
      "step": 943
    },
    {
      "epoch": 0.26875444839857654,
      "grad_norm": 0.5935326814651489,
      "learning_rate": 0.0004328921150014233,
      "loss": 6.0732,
      "step": 944
    },
    {
      "epoch": 0.2690391459074733,
      "grad_norm": 0.4439448416233063,
      "learning_rate": 0.000432820950754341,
      "loss": 7.1514,
      "step": 945
    },
    {
      "epoch": 0.2693238434163701,
      "grad_norm": 0.41855907440185547,
      "learning_rate": 0.00043274978650725875,
      "loss": 7.793,
      "step": 946
    },
    {
      "epoch": 0.2696085409252669,
      "grad_norm": 0.4971495568752289,
      "learning_rate": 0.0004326786222601765,
      "loss": 7.2061,
      "step": 947
    },
    {
      "epoch": 0.2698932384341637,
      "grad_norm": 0.5047661662101746,
      "learning_rate": 0.00043260745801309425,
      "loss": 6.791,
      "step": 948
    },
    {
      "epoch": 0.2701779359430605,
      "grad_norm": 0.4491565525531769,
      "learning_rate": 0.00043253629376601197,
      "loss": 7.3984,
      "step": 949
    },
    {
      "epoch": 0.2704626334519573,
      "grad_norm": 0.5155214071273804,
      "learning_rate": 0.0004324651295189297,
      "loss": 7.3164,
      "step": 950
    },
    {
      "epoch": 0.27074733096085407,
      "grad_norm": 0.46427953243255615,
      "learning_rate": 0.0004323939652718474,
      "loss": 7.3857,
      "step": 951
    },
    {
      "epoch": 0.2710320284697509,
      "grad_norm": 0.5414927005767822,
      "learning_rate": 0.0004323228010247652,
      "loss": 7.0195,
      "step": 952
    },
    {
      "epoch": 0.2713167259786477,
      "grad_norm": 0.5723718404769897,
      "learning_rate": 0.0004322516367776829,
      "loss": 7.0186,
      "step": 953
    },
    {
      "epoch": 0.27160142348754446,
      "grad_norm": 0.5131714940071106,
      "learning_rate": 0.0004321804725306006,
      "loss": 7.1494,
      "step": 954
    },
    {
      "epoch": 0.27188612099644127,
      "grad_norm": 0.6219976544380188,
      "learning_rate": 0.00043210930828351836,
      "loss": 6.8535,
      "step": 955
    },
    {
      "epoch": 0.2721708185053381,
      "grad_norm": 0.4529915452003479,
      "learning_rate": 0.0004320381440364361,
      "loss": 7.4473,
      "step": 956
    },
    {
      "epoch": 0.2724555160142349,
      "grad_norm": 0.5033783912658691,
      "learning_rate": 0.00043196697978935386,
      "loss": 7.4102,
      "step": 957
    },
    {
      "epoch": 0.27274021352313166,
      "grad_norm": 0.44644036889076233,
      "learning_rate": 0.0004318958155422716,
      "loss": 7.7686,
      "step": 958
    },
    {
      "epoch": 0.27302491103202847,
      "grad_norm": 0.5473158359527588,
      "learning_rate": 0.0004318246512951893,
      "loss": 7.25,
      "step": 959
    },
    {
      "epoch": 0.2733096085409253,
      "grad_norm": 0.4492653012275696,
      "learning_rate": 0.00043175348704810703,
      "loss": 7.8701,
      "step": 960
    },
    {
      "epoch": 0.27359430604982204,
      "grad_norm": 0.47338056564331055,
      "learning_rate": 0.00043168232280102475,
      "loss": 7.2969,
      "step": 961
    },
    {
      "epoch": 0.27387900355871886,
      "grad_norm": 0.5589231848716736,
      "learning_rate": 0.00043161115855394253,
      "loss": 7.4736,
      "step": 962
    },
    {
      "epoch": 0.2741637010676157,
      "grad_norm": 0.4764990210533142,
      "learning_rate": 0.00043153999430686025,
      "loss": 7.3125,
      "step": 963
    },
    {
      "epoch": 0.27444839857651243,
      "grad_norm": 0.5841454863548279,
      "learning_rate": 0.000431468830059778,
      "loss": 7.0547,
      "step": 964
    },
    {
      "epoch": 0.27473309608540925,
      "grad_norm": 0.52353835105896,
      "learning_rate": 0.0004313976658126957,
      "loss": 7.459,
      "step": 965
    },
    {
      "epoch": 0.27501779359430606,
      "grad_norm": 0.523868978023529,
      "learning_rate": 0.0004313265015656134,
      "loss": 7.3135,
      "step": 966
    },
    {
      "epoch": 0.2753024911032029,
      "grad_norm": 0.4624759554862976,
      "learning_rate": 0.0004312553373185312,
      "loss": 7.6846,
      "step": 967
    },
    {
      "epoch": 0.27558718861209963,
      "grad_norm": 0.5388631224632263,
      "learning_rate": 0.0004311841730714489,
      "loss": 7.3652,
      "step": 968
    },
    {
      "epoch": 0.27587188612099645,
      "grad_norm": 0.5112046599388123,
      "learning_rate": 0.00043111300882436664,
      "loss": 7.6865,
      "step": 969
    },
    {
      "epoch": 0.27615658362989326,
      "grad_norm": 0.40607786178588867,
      "learning_rate": 0.0004310418445772844,
      "loss": 7.4023,
      "step": 970
    },
    {
      "epoch": 0.27644128113879,
      "grad_norm": 0.44282764196395874,
      "learning_rate": 0.0004309706803302021,
      "loss": 7.7432,
      "step": 971
    },
    {
      "epoch": 0.27672597864768683,
      "grad_norm": 0.4683508574962616,
      "learning_rate": 0.0004308995160831198,
      "loss": 7.6504,
      "step": 972
    },
    {
      "epoch": 0.27701067615658365,
      "grad_norm": 0.5277723073959351,
      "learning_rate": 0.0004308283518360376,
      "loss": 7.3008,
      "step": 973
    },
    {
      "epoch": 0.2772953736654804,
      "grad_norm": 0.48368245363235474,
      "learning_rate": 0.0004307571875889553,
      "loss": 7.2158,
      "step": 974
    },
    {
      "epoch": 0.2775800711743772,
      "grad_norm": 0.5163562893867493,
      "learning_rate": 0.0004306860233418731,
      "loss": 7.3232,
      "step": 975
    },
    {
      "epoch": 0.27786476868327403,
      "grad_norm": 0.5331327319145203,
      "learning_rate": 0.0004306148590947908,
      "loss": 6.9707,
      "step": 976
    },
    {
      "epoch": 0.2781494661921708,
      "grad_norm": 0.5096694231033325,
      "learning_rate": 0.0004305436948477085,
      "loss": 7.1387,
      "step": 977
    },
    {
      "epoch": 0.2784341637010676,
      "grad_norm": 0.6267108917236328,
      "learning_rate": 0.00043047253060062626,
      "loss": 6.917,
      "step": 978
    },
    {
      "epoch": 0.2787188612099644,
      "grad_norm": 0.4161131680011749,
      "learning_rate": 0.000430401366353544,
      "loss": 7.3604,
      "step": 979
    },
    {
      "epoch": 0.27900355871886123,
      "grad_norm": 0.41714102029800415,
      "learning_rate": 0.00043033020210646176,
      "loss": 7.6514,
      "step": 980
    },
    {
      "epoch": 0.279288256227758,
      "grad_norm": 0.5368282794952393,
      "learning_rate": 0.0004302590378593795,
      "loss": 7.1318,
      "step": 981
    },
    {
      "epoch": 0.2795729537366548,
      "grad_norm": 0.42313826084136963,
      "learning_rate": 0.00043018787361229715,
      "loss": 7.5117,
      "step": 982
    },
    {
      "epoch": 0.2798576512455516,
      "grad_norm": 0.4829246997833252,
      "learning_rate": 0.0004301167093652149,
      "loss": 7.4912,
      "step": 983
    },
    {
      "epoch": 0.2801423487544484,
      "grad_norm": 0.6406773924827576,
      "learning_rate": 0.00043004554511813265,
      "loss": 6.4014,
      "step": 984
    },
    {
      "epoch": 0.2804270462633452,
      "grad_norm": 0.3736668527126312,
      "learning_rate": 0.0004299743808710504,
      "loss": 7.9062,
      "step": 985
    },
    {
      "epoch": 0.280711743772242,
      "grad_norm": 0.522930383682251,
      "learning_rate": 0.00042990321662396815,
      "loss": 7.0312,
      "step": 986
    },
    {
      "epoch": 0.28099644128113876,
      "grad_norm": 0.4965778887271881,
      "learning_rate": 0.00042983205237688587,
      "loss": 7.1846,
      "step": 987
    },
    {
      "epoch": 0.2812811387900356,
      "grad_norm": 0.45272135734558105,
      "learning_rate": 0.0004297608881298036,
      "loss": 7.2119,
      "step": 988
    },
    {
      "epoch": 0.2815658362989324,
      "grad_norm": 0.5090800523757935,
      "learning_rate": 0.0004296897238827213,
      "loss": 7.0088,
      "step": 989
    },
    {
      "epoch": 0.2818505338078292,
      "grad_norm": 0.6086441874504089,
      "learning_rate": 0.00042961855963563904,
      "loss": 7.2715,
      "step": 990
    },
    {
      "epoch": 0.28213523131672597,
      "grad_norm": 0.7149702906608582,
      "learning_rate": 0.0004295473953885568,
      "loss": 6.3223,
      "step": 991
    },
    {
      "epoch": 0.2824199288256228,
      "grad_norm": 0.574139416217804,
      "learning_rate": 0.00042947623114147454,
      "loss": 7.1348,
      "step": 992
    },
    {
      "epoch": 0.2827046263345196,
      "grad_norm": 0.4520958960056305,
      "learning_rate": 0.0004294050668943923,
      "loss": 7.4697,
      "step": 993
    },
    {
      "epoch": 0.28298932384341635,
      "grad_norm": 0.4637325406074524,
      "learning_rate": 0.00042933390264731,
      "loss": 7.2773,
      "step": 994
    },
    {
      "epoch": 0.28327402135231317,
      "grad_norm": 0.4676058888435364,
      "learning_rate": 0.0004292627384002277,
      "loss": 7.3975,
      "step": 995
    },
    {
      "epoch": 0.28355871886121,
      "grad_norm": 0.42332184314727783,
      "learning_rate": 0.0004291915741531455,
      "loss": 7.6719,
      "step": 996
    },
    {
      "epoch": 0.28384341637010674,
      "grad_norm": 0.47916945815086365,
      "learning_rate": 0.0004291204099060632,
      "loss": 7.3799,
      "step": 997
    },
    {
      "epoch": 0.28412811387900355,
      "grad_norm": 0.5687378644943237,
      "learning_rate": 0.000429049245658981,
      "loss": 7.1699,
      "step": 998
    },
    {
      "epoch": 0.28441281138790037,
      "grad_norm": 0.38771921396255493,
      "learning_rate": 0.00042897808141189865,
      "loss": 7.5986,
      "step": 999
    },
    {
      "epoch": 0.2846975088967972,
      "grad_norm": 0.43714073300361633,
      "learning_rate": 0.0004289069171648164,
      "loss": 7.5859,
      "step": 1000
    },
    {
      "epoch": 0.2846975088967972,
      "eval_bleu": 0.10196535812756952,
      "eval_loss": 7.04296875,
      "eval_runtime": 121.1357,
      "eval_samples_per_second": 2.344,
      "eval_steps_per_second": 0.149,
      "step": 1000
    },
    {
      "epoch": 0.28498220640569394,
      "grad_norm": 0.4797312617301941,
      "learning_rate": 0.00042883575291773415,
      "loss": 7.6777,
      "step": 1001
    },
    {
      "epoch": 0.28526690391459075,
      "grad_norm": 0.47663864493370056,
      "learning_rate": 0.0004287645886706519,
      "loss": 7.6465,
      "step": 1002
    },
    {
      "epoch": 0.28555160142348757,
      "grad_norm": 0.4880309998989105,
      "learning_rate": 0.0004286934244235696,
      "loss": 7.5508,
      "step": 1003
    },
    {
      "epoch": 0.2858362989323843,
      "grad_norm": 0.4995867908000946,
      "learning_rate": 0.0004286222601764874,
      "loss": 7.2236,
      "step": 1004
    },
    {
      "epoch": 0.28612099644128114,
      "grad_norm": 0.5235399603843689,
      "learning_rate": 0.00042855109592940504,
      "loss": 6.8809,
      "step": 1005
    },
    {
      "epoch": 0.28640569395017795,
      "grad_norm": 0.4574708938598633,
      "learning_rate": 0.0004284799316823228,
      "loss": 7.4971,
      "step": 1006
    },
    {
      "epoch": 0.2866903914590747,
      "grad_norm": 0.6360961198806763,
      "learning_rate": 0.00042840876743524054,
      "loss": 7.5049,
      "step": 1007
    },
    {
      "epoch": 0.2869750889679715,
      "grad_norm": 0.5685173869132996,
      "learning_rate": 0.00042833760318815826,
      "loss": 6.958,
      "step": 1008
    },
    {
      "epoch": 0.28725978647686834,
      "grad_norm": 0.4308505654335022,
      "learning_rate": 0.00042826643894107604,
      "loss": 7.6084,
      "step": 1009
    },
    {
      "epoch": 0.2875444839857651,
      "grad_norm": 0.4639454483985901,
      "learning_rate": 0.0004281952746939937,
      "loss": 7.4463,
      "step": 1010
    },
    {
      "epoch": 0.2878291814946619,
      "grad_norm": 0.4709155857563019,
      "learning_rate": 0.0004281241104469115,
      "loss": 7.5322,
      "step": 1011
    },
    {
      "epoch": 0.2881138790035587,
      "grad_norm": 0.43574628233909607,
      "learning_rate": 0.0004280529461998292,
      "loss": 7.5186,
      "step": 1012
    },
    {
      "epoch": 0.28839857651245554,
      "grad_norm": 0.4731388986110687,
      "learning_rate": 0.00042798178195274693,
      "loss": 7.2949,
      "step": 1013
    },
    {
      "epoch": 0.2886832740213523,
      "grad_norm": 0.4204134941101074,
      "learning_rate": 0.0004279106177056647,
      "loss": 7.5859,
      "step": 1014
    },
    {
      "epoch": 0.2889679715302491,
      "grad_norm": 0.5122082829475403,
      "learning_rate": 0.00042783945345858243,
      "loss": 7.2627,
      "step": 1015
    },
    {
      "epoch": 0.2892526690391459,
      "grad_norm": 0.3954521119594574,
      "learning_rate": 0.00042776828921150015,
      "loss": 7.543,
      "step": 1016
    },
    {
      "epoch": 0.2895373665480427,
      "grad_norm": 0.4996376037597656,
      "learning_rate": 0.0004276971249644179,
      "loss": 7.4268,
      "step": 1017
    },
    {
      "epoch": 0.2898220640569395,
      "grad_norm": 0.48199692368507385,
      "learning_rate": 0.0004276259607173356,
      "loss": 7.4111,
      "step": 1018
    },
    {
      "epoch": 0.2901067615658363,
      "grad_norm": 0.4896969497203827,
      "learning_rate": 0.0004275547964702534,
      "loss": 7.5732,
      "step": 1019
    },
    {
      "epoch": 0.29039145907473307,
      "grad_norm": 0.5372595191001892,
      "learning_rate": 0.0004274836322231711,
      "loss": 6.9336,
      "step": 1020
    },
    {
      "epoch": 0.2906761565836299,
      "grad_norm": 0.46799662709236145,
      "learning_rate": 0.0004274124679760888,
      "loss": 7.6201,
      "step": 1021
    },
    {
      "epoch": 0.2909608540925267,
      "grad_norm": 0.47574394941329956,
      "learning_rate": 0.00042734130372900655,
      "loss": 7.2734,
      "step": 1022
    },
    {
      "epoch": 0.2912455516014235,
      "grad_norm": 0.4846518337726593,
      "learning_rate": 0.00042727013948192427,
      "loss": 7.4453,
      "step": 1023
    },
    {
      "epoch": 0.29153024911032027,
      "grad_norm": 0.5557963848114014,
      "learning_rate": 0.00042719897523484205,
      "loss": 7.0713,
      "step": 1024
    },
    {
      "epoch": 0.2918149466192171,
      "grad_norm": 0.4947628080844879,
      "learning_rate": 0.00042712781098775977,
      "loss": 7.5947,
      "step": 1025
    },
    {
      "epoch": 0.2920996441281139,
      "grad_norm": 0.492767333984375,
      "learning_rate": 0.0004270566467406775,
      "loss": 7.252,
      "step": 1026
    },
    {
      "epoch": 0.29238434163701066,
      "grad_norm": 0.6265051960945129,
      "learning_rate": 0.0004269854824935952,
      "loss": 6.3564,
      "step": 1027
    },
    {
      "epoch": 0.2926690391459075,
      "grad_norm": 0.5161956548690796,
      "learning_rate": 0.00042691431824651294,
      "loss": 7.5752,
      "step": 1028
    },
    {
      "epoch": 0.2929537366548043,
      "grad_norm": 0.5737975239753723,
      "learning_rate": 0.0004268431539994307,
      "loss": 7.0615,
      "step": 1029
    },
    {
      "epoch": 0.29323843416370104,
      "grad_norm": 0.4803426265716553,
      "learning_rate": 0.00042677198975234844,
      "loss": 7.5,
      "step": 1030
    },
    {
      "epoch": 0.29352313167259786,
      "grad_norm": 0.4499126076698303,
      "learning_rate": 0.00042670082550526616,
      "loss": 7.2568,
      "step": 1031
    },
    {
      "epoch": 0.2938078291814947,
      "grad_norm": 0.4502011239528656,
      "learning_rate": 0.00042662966125818394,
      "loss": 7.7002,
      "step": 1032
    },
    {
      "epoch": 0.29409252669039143,
      "grad_norm": 0.43873316049575806,
      "learning_rate": 0.0004265584970111016,
      "loss": 7.4287,
      "step": 1033
    },
    {
      "epoch": 0.29437722419928825,
      "grad_norm": 0.4562997817993164,
      "learning_rate": 0.0004264873327640193,
      "loss": 7.5312,
      "step": 1034
    },
    {
      "epoch": 0.29466192170818506,
      "grad_norm": 0.5061038136482239,
      "learning_rate": 0.0004264161685169371,
      "loss": 7.4629,
      "step": 1035
    },
    {
      "epoch": 0.2949466192170819,
      "grad_norm": 0.4653073847293854,
      "learning_rate": 0.0004263450042698548,
      "loss": 7.5996,
      "step": 1036
    },
    {
      "epoch": 0.29523131672597863,
      "grad_norm": 0.6139973402023315,
      "learning_rate": 0.0004262738400227726,
      "loss": 6.3779,
      "step": 1037
    },
    {
      "epoch": 0.29551601423487545,
      "grad_norm": 0.5320224761962891,
      "learning_rate": 0.0004262026757756903,
      "loss": 7.127,
      "step": 1038
    },
    {
      "epoch": 0.29580071174377226,
      "grad_norm": 0.5026519298553467,
      "learning_rate": 0.000426131511528608,
      "loss": 7.3281,
      "step": 1039
    },
    {
      "epoch": 0.296085409252669,
      "grad_norm": 0.5016393065452576,
      "learning_rate": 0.00042606034728152577,
      "loss": 7.0791,
      "step": 1040
    },
    {
      "epoch": 0.29637010676156583,
      "grad_norm": 0.516268253326416,
      "learning_rate": 0.0004259891830344435,
      "loss": 7.041,
      "step": 1041
    },
    {
      "epoch": 0.29665480427046265,
      "grad_norm": 0.48828786611557007,
      "learning_rate": 0.00042591801878736127,
      "loss": 7.4434,
      "step": 1042
    },
    {
      "epoch": 0.2969395017793594,
      "grad_norm": 0.4357978105545044,
      "learning_rate": 0.000425846854540279,
      "loss": 7.1211,
      "step": 1043
    },
    {
      "epoch": 0.2972241992882562,
      "grad_norm": 0.5390711426734924,
      "learning_rate": 0.00042577569029319666,
      "loss": 6.9951,
      "step": 1044
    },
    {
      "epoch": 0.29750889679715303,
      "grad_norm": 0.43850216269493103,
      "learning_rate": 0.00042570452604611444,
      "loss": 7.4346,
      "step": 1045
    },
    {
      "epoch": 0.29779359430604985,
      "grad_norm": 0.5152047276496887,
      "learning_rate": 0.00042563336179903216,
      "loss": 7.2969,
      "step": 1046
    },
    {
      "epoch": 0.2980782918149466,
      "grad_norm": 0.4733029305934906,
      "learning_rate": 0.00042556219755194994,
      "loss": 7.626,
      "step": 1047
    },
    {
      "epoch": 0.2983629893238434,
      "grad_norm": 0.5408203601837158,
      "learning_rate": 0.00042549103330486766,
      "loss": 7.0078,
      "step": 1048
    },
    {
      "epoch": 0.29864768683274023,
      "grad_norm": 0.39213523268699646,
      "learning_rate": 0.0004254198690577854,
      "loss": 7.5322,
      "step": 1049
    },
    {
      "epoch": 0.298932384341637,
      "grad_norm": 0.4816019833087921,
      "learning_rate": 0.0004253487048107031,
      "loss": 7.7979,
      "step": 1050
    },
    {
      "epoch": 0.2992170818505338,
      "grad_norm": 0.47788047790527344,
      "learning_rate": 0.00042527754056362083,
      "loss": 7.3506,
      "step": 1051
    },
    {
      "epoch": 0.2995017793594306,
      "grad_norm": 0.5580766201019287,
      "learning_rate": 0.00042520637631653855,
      "loss": 6.7529,
      "step": 1052
    },
    {
      "epoch": 0.2997864768683274,
      "grad_norm": 0.4852105379104614,
      "learning_rate": 0.00042513521206945633,
      "loss": 7.3311,
      "step": 1053
    },
    {
      "epoch": 0.3000711743772242,
      "grad_norm": 0.5395891070365906,
      "learning_rate": 0.00042506404782237405,
      "loss": 6.998,
      "step": 1054
    },
    {
      "epoch": 0.300355871886121,
      "grad_norm": 0.5457566976547241,
      "learning_rate": 0.0004249928835752918,
      "loss": 7.085,
      "step": 1055
    },
    {
      "epoch": 0.3006405693950178,
      "grad_norm": 0.5181650519371033,
      "learning_rate": 0.0004249217193282095,
      "loss": 7.3096,
      "step": 1056
    },
    {
      "epoch": 0.3009252669039146,
      "grad_norm": 0.5144093036651611,
      "learning_rate": 0.0004248505550811272,
      "loss": 7.0566,
      "step": 1057
    },
    {
      "epoch": 0.3012099644128114,
      "grad_norm": 0.6092381477355957,
      "learning_rate": 0.000424779390834045,
      "loss": 7.3281,
      "step": 1058
    },
    {
      "epoch": 0.3014946619217082,
      "grad_norm": 0.5332916975021362,
      "learning_rate": 0.0004247082265869627,
      "loss": 7.002,
      "step": 1059
    },
    {
      "epoch": 0.30177935943060497,
      "grad_norm": 0.5226554274559021,
      "learning_rate": 0.0004246370623398805,
      "loss": 7.1387,
      "step": 1060
    },
    {
      "epoch": 0.3020640569395018,
      "grad_norm": 0.5610384941101074,
      "learning_rate": 0.00042456589809279817,
      "loss": 7.3506,
      "step": 1061
    },
    {
      "epoch": 0.3023487544483986,
      "grad_norm": 0.42654889822006226,
      "learning_rate": 0.0004244947338457159,
      "loss": 7.5635,
      "step": 1062
    },
    {
      "epoch": 0.30263345195729535,
      "grad_norm": 0.5280149579048157,
      "learning_rate": 0.00042442356959863367,
      "loss": 7.29,
      "step": 1063
    },
    {
      "epoch": 0.30291814946619217,
      "grad_norm": 0.4202621579170227,
      "learning_rate": 0.0004243524053515514,
      "loss": 7.5176,
      "step": 1064
    },
    {
      "epoch": 0.303202846975089,
      "grad_norm": 0.5048372745513916,
      "learning_rate": 0.00042428124110446917,
      "loss": 6.957,
      "step": 1065
    },
    {
      "epoch": 0.30348754448398574,
      "grad_norm": 0.6739668846130371,
      "learning_rate": 0.0004242100768573869,
      "loss": 6.7461,
      "step": 1066
    },
    {
      "epoch": 0.30377224199288255,
      "grad_norm": 0.74714595079422,
      "learning_rate": 0.00042413891261030456,
      "loss": 7.7529,
      "step": 1067
    },
    {
      "epoch": 0.30405693950177937,
      "grad_norm": 0.6046257019042969,
      "learning_rate": 0.00042406774836322233,
      "loss": 7.0068,
      "step": 1068
    },
    {
      "epoch": 0.3043416370106762,
      "grad_norm": 0.5528740882873535,
      "learning_rate": 0.00042399658411614006,
      "loss": 7.2773,
      "step": 1069
    },
    {
      "epoch": 0.30462633451957294,
      "grad_norm": 0.47163739800453186,
      "learning_rate": 0.0004239254198690578,
      "loss": 7.4395,
      "step": 1070
    },
    {
      "epoch": 0.30491103202846975,
      "grad_norm": 0.4603082239627838,
      "learning_rate": 0.00042385425562197556,
      "loss": 7.54,
      "step": 1071
    },
    {
      "epoch": 0.30519572953736657,
      "grad_norm": 0.5182943344116211,
      "learning_rate": 0.0004237830913748932,
      "loss": 7.0479,
      "step": 1072
    },
    {
      "epoch": 0.3054804270462633,
      "grad_norm": 0.4548114240169525,
      "learning_rate": 0.000423711927127811,
      "loss": 7.7168,
      "step": 1073
    },
    {
      "epoch": 0.30576512455516014,
      "grad_norm": 0.5856614708900452,
      "learning_rate": 0.0004236407628807287,
      "loss": 7.1514,
      "step": 1074
    },
    {
      "epoch": 0.30604982206405695,
      "grad_norm": 0.4826880395412445,
      "learning_rate": 0.00042356959863364645,
      "loss": 7.3438,
      "step": 1075
    },
    {
      "epoch": 0.3063345195729537,
      "grad_norm": 0.5586170554161072,
      "learning_rate": 0.0004234984343865642,
      "loss": 7.1416,
      "step": 1076
    },
    {
      "epoch": 0.3066192170818505,
      "grad_norm": 0.4764145612716675,
      "learning_rate": 0.00042342727013948195,
      "loss": 7.0732,
      "step": 1077
    },
    {
      "epoch": 0.30690391459074734,
      "grad_norm": 0.6197493076324463,
      "learning_rate": 0.00042335610589239967,
      "loss": 7.1006,
      "step": 1078
    },
    {
      "epoch": 0.30718861209964415,
      "grad_norm": 0.47980010509490967,
      "learning_rate": 0.0004232849416453174,
      "loss": 7.5361,
      "step": 1079
    },
    {
      "epoch": 0.3074733096085409,
      "grad_norm": 0.5147501826286316,
      "learning_rate": 0.0004232137773982351,
      "loss": 7.2217,
      "step": 1080
    },
    {
      "epoch": 0.3077580071174377,
      "grad_norm": 0.6229455471038818,
      "learning_rate": 0.0004231426131511529,
      "loss": 6.8916,
      "step": 1081
    },
    {
      "epoch": 0.30804270462633454,
      "grad_norm": 0.519205629825592,
      "learning_rate": 0.0004230714489040706,
      "loss": 7.1543,
      "step": 1082
    },
    {
      "epoch": 0.3083274021352313,
      "grad_norm": 0.5824966430664062,
      "learning_rate": 0.0004230002846569884,
      "loss": 6.1338,
      "step": 1083
    },
    {
      "epoch": 0.3086120996441281,
      "grad_norm": 0.5626207590103149,
      "learning_rate": 0.00042292912040990606,
      "loss": 6.9707,
      "step": 1084
    },
    {
      "epoch": 0.3088967971530249,
      "grad_norm": 0.5577024817466736,
      "learning_rate": 0.0004228579561628238,
      "loss": 6.8984,
      "step": 1085
    },
    {
      "epoch": 0.3091814946619217,
      "grad_norm": 0.48237547278404236,
      "learning_rate": 0.00042278679191574156,
      "loss": 7.4697,
      "step": 1086
    },
    {
      "epoch": 0.3094661921708185,
      "grad_norm": 0.4760837256908417,
      "learning_rate": 0.0004227156276686593,
      "loss": 7.3721,
      "step": 1087
    },
    {
      "epoch": 0.3097508896797153,
      "grad_norm": 0.466458797454834,
      "learning_rate": 0.000422644463421577,
      "loss": 7.5654,
      "step": 1088
    },
    {
      "epoch": 0.3100355871886121,
      "grad_norm": 0.5326457023620605,
      "learning_rate": 0.00042257329917449473,
      "loss": 7.2549,
      "step": 1089
    },
    {
      "epoch": 0.3103202846975089,
      "grad_norm": 0.6053488850593567,
      "learning_rate": 0.00042250213492741245,
      "loss": 6.918,
      "step": 1090
    },
    {
      "epoch": 0.3106049822064057,
      "grad_norm": 0.4849885404109955,
      "learning_rate": 0.00042243097068033023,
      "loss": 6.9521,
      "step": 1091
    },
    {
      "epoch": 0.3108896797153025,
      "grad_norm": 0.46493789553642273,
      "learning_rate": 0.00042235980643324795,
      "loss": 7.5,
      "step": 1092
    },
    {
      "epoch": 0.3111743772241993,
      "grad_norm": 0.5597470998764038,
      "learning_rate": 0.0004222886421861657,
      "loss": 7.1045,
      "step": 1093
    },
    {
      "epoch": 0.3114590747330961,
      "grad_norm": 0.46983328461647034,
      "learning_rate": 0.00042221747793908345,
      "loss": 7.376,
      "step": 1094
    },
    {
      "epoch": 0.3117437722419929,
      "grad_norm": 0.7661401033401489,
      "learning_rate": 0.0004221463136920011,
      "loss": 7.9004,
      "step": 1095
    },
    {
      "epoch": 0.31202846975088966,
      "grad_norm": 0.5901150703430176,
      "learning_rate": 0.0004220751494449189,
      "loss": 7.0488,
      "step": 1096
    },
    {
      "epoch": 0.3123131672597865,
      "grad_norm": 0.49586740136146545,
      "learning_rate": 0.0004220039851978366,
      "loss": 7.3682,
      "step": 1097
    },
    {
      "epoch": 0.3125978647686833,
      "grad_norm": 0.42957544326782227,
      "learning_rate": 0.00042193282095075434,
      "loss": 7.3447,
      "step": 1098
    },
    {
      "epoch": 0.31288256227758005,
      "grad_norm": 0.6224203705787659,
      "learning_rate": 0.0004218616567036721,
      "loss": 6.999,
      "step": 1099
    },
    {
      "epoch": 0.31316725978647686,
      "grad_norm": 0.5391501188278198,
      "learning_rate": 0.0004217904924565898,
      "loss": 7.1846,
      "step": 1100
    },
    {
      "epoch": 0.3134519572953737,
      "grad_norm": 0.4944549798965454,
      "learning_rate": 0.0004217193282095075,
      "loss": 7.5273,
      "step": 1101
    },
    {
      "epoch": 0.3137366548042705,
      "grad_norm": 0.5197928547859192,
      "learning_rate": 0.0004216481639624253,
      "loss": 7.1572,
      "step": 1102
    },
    {
      "epoch": 0.31402135231316725,
      "grad_norm": 0.45653462409973145,
      "learning_rate": 0.000421576999715343,
      "loss": 7.1807,
      "step": 1103
    },
    {
      "epoch": 0.31430604982206406,
      "grad_norm": 0.5307984352111816,
      "learning_rate": 0.0004215058354682608,
      "loss": 7.4922,
      "step": 1104
    },
    {
      "epoch": 0.3145907473309609,
      "grad_norm": 0.4564886689186096,
      "learning_rate": 0.0004214346712211785,
      "loss": 7.3174,
      "step": 1105
    },
    {
      "epoch": 0.31487544483985763,
      "grad_norm": 0.5012796521186829,
      "learning_rate": 0.0004213635069740962,
      "loss": 7.165,
      "step": 1106
    },
    {
      "epoch": 0.31516014234875445,
      "grad_norm": 3.3040928840637207,
      "learning_rate": 0.00042129234272701396,
      "loss": 7.4414,
      "step": 1107
    },
    {
      "epoch": 0.31544483985765126,
      "grad_norm": 0.5748804211616516,
      "learning_rate": 0.0004212211784799317,
      "loss": 7.124,
      "step": 1108
    },
    {
      "epoch": 0.315729537366548,
      "grad_norm": 0.565574586391449,
      "learning_rate": 0.00042115001423284946,
      "loss": 6.9238,
      "step": 1109
    },
    {
      "epoch": 0.31601423487544483,
      "grad_norm": 0.5565568208694458,
      "learning_rate": 0.0004210788499857672,
      "loss": 7.1934,
      "step": 1110
    },
    {
      "epoch": 0.31629893238434165,
      "grad_norm": 0.4593667685985565,
      "learning_rate": 0.0004210076857386849,
      "loss": 7.4541,
      "step": 1111
    },
    {
      "epoch": 0.31658362989323846,
      "grad_norm": 0.4457915127277374,
      "learning_rate": 0.0004209365214916026,
      "loss": 7.6582,
      "step": 1112
    },
    {
      "epoch": 0.3168683274021352,
      "grad_norm": 0.40129971504211426,
      "learning_rate": 0.00042086535724452035,
      "loss": 7.7979,
      "step": 1113
    },
    {
      "epoch": 0.31715302491103203,
      "grad_norm": 0.5046826004981995,
      "learning_rate": 0.0004207941929974381,
      "loss": 7.2188,
      "step": 1114
    },
    {
      "epoch": 0.31743772241992885,
      "grad_norm": 0.4885587990283966,
      "learning_rate": 0.00042072302875035585,
      "loss": 7.6162,
      "step": 1115
    },
    {
      "epoch": 0.3177224199288256,
      "grad_norm": 0.4405319392681122,
      "learning_rate": 0.00042065186450327357,
      "loss": 7.4834,
      "step": 1116
    },
    {
      "epoch": 0.3180071174377224,
      "grad_norm": 0.5957425236701965,
      "learning_rate": 0.0004205807002561913,
      "loss": 6.667,
      "step": 1117
    },
    {
      "epoch": 0.31829181494661923,
      "grad_norm": 0.5472882986068726,
      "learning_rate": 0.000420509536009109,
      "loss": 7.4531,
      "step": 1118
    },
    {
      "epoch": 0.318576512455516,
      "grad_norm": 0.4806768596172333,
      "learning_rate": 0.00042043837176202674,
      "loss": 7.4805,
      "step": 1119
    },
    {
      "epoch": 0.3188612099644128,
      "grad_norm": 0.4944848120212555,
      "learning_rate": 0.0004203672075149445,
      "loss": 7.5342,
      "step": 1120
    },
    {
      "epoch": 0.3191459074733096,
      "grad_norm": 0.5867344737052917,
      "learning_rate": 0.00042029604326786224,
      "loss": 6.6504,
      "step": 1121
    },
    {
      "epoch": 0.3194306049822064,
      "grad_norm": 0.45524725317955017,
      "learning_rate": 0.00042022487902078,
      "loss": 7.334,
      "step": 1122
    },
    {
      "epoch": 0.3197153024911032,
      "grad_norm": 0.47168710827827454,
      "learning_rate": 0.0004201537147736977,
      "loss": 7.3643,
      "step": 1123
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.46018290519714355,
      "learning_rate": 0.0004200825505266154,
      "loss": 7.2725,
      "step": 1124
    },
    {
      "epoch": 0.3202846975088968,
      "grad_norm": 0.49334147572517395,
      "learning_rate": 0.0004200113862795332,
      "loss": 7.3291,
      "step": 1125
    },
    {
      "epoch": 0.3205693950177936,
      "grad_norm": 0.6619085073471069,
      "learning_rate": 0.0004199402220324509,
      "loss": 6.3311,
      "step": 1126
    },
    {
      "epoch": 0.3208540925266904,
      "grad_norm": 0.4803929626941681,
      "learning_rate": 0.0004198690577853687,
      "loss": 7.1162,
      "step": 1127
    },
    {
      "epoch": 0.3211387900355872,
      "grad_norm": 0.4872837960720062,
      "learning_rate": 0.00041979789353828635,
      "loss": 7.2471,
      "step": 1128
    },
    {
      "epoch": 0.32142348754448397,
      "grad_norm": 0.5039362907409668,
      "learning_rate": 0.00041972672929120407,
      "loss": 7.9453,
      "step": 1129
    },
    {
      "epoch": 0.3217081850533808,
      "grad_norm": 0.4333154559135437,
      "learning_rate": 0.00041965556504412185,
      "loss": 7.4307,
      "step": 1130
    },
    {
      "epoch": 0.3219928825622776,
      "grad_norm": 0.6478785276412964,
      "learning_rate": 0.00041958440079703957,
      "loss": 7.7002,
      "step": 1131
    },
    {
      "epoch": 0.32227758007117435,
      "grad_norm": 0.5212644934654236,
      "learning_rate": 0.0004195132365499573,
      "loss": 7.1113,
      "step": 1132
    },
    {
      "epoch": 0.32256227758007117,
      "grad_norm": 0.4614885449409485,
      "learning_rate": 0.00041944207230287507,
      "loss": 7.6172,
      "step": 1133
    },
    {
      "epoch": 0.322846975088968,
      "grad_norm": 0.5155771374702454,
      "learning_rate": 0.00041937090805579274,
      "loss": 7.1846,
      "step": 1134
    },
    {
      "epoch": 0.3231316725978648,
      "grad_norm": 0.5528173446655273,
      "learning_rate": 0.0004192997438087105,
      "loss": 7.2109,
      "step": 1135
    },
    {
      "epoch": 0.32341637010676155,
      "grad_norm": 0.505190372467041,
      "learning_rate": 0.00041922857956162824,
      "loss": 6.9854,
      "step": 1136
    },
    {
      "epoch": 0.32370106761565837,
      "grad_norm": 0.4246407747268677,
      "learning_rate": 0.00041915741531454596,
      "loss": 7.6191,
      "step": 1137
    },
    {
      "epoch": 0.3239857651245552,
      "grad_norm": 0.5137282609939575,
      "learning_rate": 0.00041908625106746374,
      "loss": 6.8467,
      "step": 1138
    },
    {
      "epoch": 0.32427046263345194,
      "grad_norm": 0.5498955845832825,
      "learning_rate": 0.00041901508682038146,
      "loss": 7.3867,
      "step": 1139
    },
    {
      "epoch": 0.32455516014234875,
      "grad_norm": 0.46873560547828674,
      "learning_rate": 0.0004189439225732992,
      "loss": 7.5898,
      "step": 1140
    },
    {
      "epoch": 0.32483985765124557,
      "grad_norm": 0.4682499170303345,
      "learning_rate": 0.0004188727583262169,
      "loss": 7.0762,
      "step": 1141
    },
    {
      "epoch": 0.3251245551601423,
      "grad_norm": 0.6929412484169006,
      "learning_rate": 0.00041880159407913463,
      "loss": 6.9707,
      "step": 1142
    },
    {
      "epoch": 0.32540925266903914,
      "grad_norm": 0.5219119787216187,
      "learning_rate": 0.0004187304298320524,
      "loss": 6.9746,
      "step": 1143
    },
    {
      "epoch": 0.32569395017793595,
      "grad_norm": 0.6330713629722595,
      "learning_rate": 0.00041865926558497013,
      "loss": 6.1309,
      "step": 1144
    },
    {
      "epoch": 0.32597864768683277,
      "grad_norm": 0.6545981168746948,
      "learning_rate": 0.00041858810133788785,
      "loss": 6.5879,
      "step": 1145
    },
    {
      "epoch": 0.3262633451957295,
      "grad_norm": 0.5922051668167114,
      "learning_rate": 0.0004185169370908056,
      "loss": 7.0771,
      "step": 1146
    },
    {
      "epoch": 0.32654804270462634,
      "grad_norm": 0.6108818054199219,
      "learning_rate": 0.0004184457728437233,
      "loss": 7.0137,
      "step": 1147
    },
    {
      "epoch": 0.32683274021352315,
      "grad_norm": 0.5289431810379028,
      "learning_rate": 0.0004183746085966411,
      "loss": 7.001,
      "step": 1148
    },
    {
      "epoch": 0.3271174377224199,
      "grad_norm": 0.4609707295894623,
      "learning_rate": 0.0004183034443495588,
      "loss": 7.4082,
      "step": 1149
    },
    {
      "epoch": 0.3274021352313167,
      "grad_norm": 0.4477441608905792,
      "learning_rate": 0.0004182322801024765,
      "loss": 7.6816,
      "step": 1150
    },
    {
      "epoch": 0.32768683274021354,
      "grad_norm": 0.4503907859325409,
      "learning_rate": 0.00041816111585539424,
      "loss": 7.873,
      "step": 1151
    },
    {
      "epoch": 0.3279715302491103,
      "grad_norm": 0.972274661064148,
      "learning_rate": 0.00041808995160831197,
      "loss": 7.2227,
      "step": 1152
    },
    {
      "epoch": 0.3282562277580071,
      "grad_norm": 0.5258875489234924,
      "learning_rate": 0.00041801878736122974,
      "loss": 7.418,
      "step": 1153
    },
    {
      "epoch": 0.3285409252669039,
      "grad_norm": 0.47450560331344604,
      "learning_rate": 0.00041794762311414747,
      "loss": 7.2832,
      "step": 1154
    },
    {
      "epoch": 0.3288256227758007,
      "grad_norm": 0.4881042540073395,
      "learning_rate": 0.0004178764588670652,
      "loss": 7.5781,
      "step": 1155
    },
    {
      "epoch": 0.3291103202846975,
      "grad_norm": 0.5091273784637451,
      "learning_rate": 0.00041780529461998297,
      "loss": 7.2471,
      "step": 1156
    },
    {
      "epoch": 0.3293950177935943,
      "grad_norm": 0.5281254053115845,
      "learning_rate": 0.00041773413037290064,
      "loss": 7.1455,
      "step": 1157
    },
    {
      "epoch": 0.3296797153024911,
      "grad_norm": 0.44808077812194824,
      "learning_rate": 0.0004176629661258184,
      "loss": 7.7734,
      "step": 1158
    },
    {
      "epoch": 0.3299644128113879,
      "grad_norm": 0.49891433119773865,
      "learning_rate": 0.00041759180187873613,
      "loss": 7.4795,
      "step": 1159
    },
    {
      "epoch": 0.3302491103202847,
      "grad_norm": 0.5285683274269104,
      "learning_rate": 0.00041752063763165386,
      "loss": 7.4639,
      "step": 1160
    },
    {
      "epoch": 0.3305338078291815,
      "grad_norm": 0.4684009253978729,
      "learning_rate": 0.00041744947338457163,
      "loss": 7.6631,
      "step": 1161
    },
    {
      "epoch": 0.3308185053380783,
      "grad_norm": 0.711145281791687,
      "learning_rate": 0.0004173783091374893,
      "loss": 7.2764,
      "step": 1162
    },
    {
      "epoch": 0.3311032028469751,
      "grad_norm": 0.49265673756599426,
      "learning_rate": 0.0004173071448904071,
      "loss": 7.21,
      "step": 1163
    },
    {
      "epoch": 0.3313879003558719,
      "grad_norm": 0.6499678492546082,
      "learning_rate": 0.0004172359806433248,
      "loss": 6.4902,
      "step": 1164
    },
    {
      "epoch": 0.33167259786476866,
      "grad_norm": 0.48859643936157227,
      "learning_rate": 0.0004171648163962425,
      "loss": 7.4043,
      "step": 1165
    },
    {
      "epoch": 0.3319572953736655,
      "grad_norm": 0.43188872933387756,
      "learning_rate": 0.0004170936521491603,
      "loss": 7.8252,
      "step": 1166
    },
    {
      "epoch": 0.3322419928825623,
      "grad_norm": 0.4691556990146637,
      "learning_rate": 0.000417022487902078,
      "loss": 7.4482,
      "step": 1167
    },
    {
      "epoch": 0.3325266903914591,
      "grad_norm": 0.582195520401001,
      "learning_rate": 0.0004169513236549957,
      "loss": 7.5371,
      "step": 1168
    },
    {
      "epoch": 0.33281138790035586,
      "grad_norm": 0.4443216919898987,
      "learning_rate": 0.00041688015940791347,
      "loss": 7.7871,
      "step": 1169
    },
    {
      "epoch": 0.3330960854092527,
      "grad_norm": 0.6017194390296936,
      "learning_rate": 0.0004168089951608312,
      "loss": 6.9932,
      "step": 1170
    },
    {
      "epoch": 0.3333807829181495,
      "grad_norm": 0.5376923680305481,
      "learning_rate": 0.00041673783091374897,
      "loss": 7.125,
      "step": 1171
    },
    {
      "epoch": 0.33366548042704625,
      "grad_norm": 0.4595070779323578,
      "learning_rate": 0.0004166666666666667,
      "loss": 7.8262,
      "step": 1172
    },
    {
      "epoch": 0.33395017793594306,
      "grad_norm": 0.5157293081283569,
      "learning_rate": 0.00041659550241958436,
      "loss": 7.165,
      "step": 1173
    },
    {
      "epoch": 0.3342348754448399,
      "grad_norm": 0.6389365196228027,
      "learning_rate": 0.00041652433817250214,
      "loss": 6.8311,
      "step": 1174
    },
    {
      "epoch": 0.33451957295373663,
      "grad_norm": 0.4487728476524353,
      "learning_rate": 0.00041645317392541986,
      "loss": 7.7686,
      "step": 1175
    },
    {
      "epoch": 0.33480427046263345,
      "grad_norm": 0.4286770224571228,
      "learning_rate": 0.00041638200967833764,
      "loss": 7.6963,
      "step": 1176
    },
    {
      "epoch": 0.33508896797153026,
      "grad_norm": 0.4808689057826996,
      "learning_rate": 0.00041631084543125536,
      "loss": 7.1982,
      "step": 1177
    },
    {
      "epoch": 0.335373665480427,
      "grad_norm": 0.5197522044181824,
      "learning_rate": 0.0004162396811841731,
      "loss": 7.3545,
      "step": 1178
    },
    {
      "epoch": 0.33565836298932383,
      "grad_norm": 0.4670189917087555,
      "learning_rate": 0.0004161685169370908,
      "loss": 7.1934,
      "step": 1179
    },
    {
      "epoch": 0.33594306049822065,
      "grad_norm": 0.5172544121742249,
      "learning_rate": 0.00041609735269000853,
      "loss": 7.1943,
      "step": 1180
    },
    {
      "epoch": 0.33622775800711746,
      "grad_norm": 0.4959327280521393,
      "learning_rate": 0.00041602618844292625,
      "loss": 7.3457,
      "step": 1181
    },
    {
      "epoch": 0.3365124555160142,
      "grad_norm": 0.5002378225326538,
      "learning_rate": 0.00041595502419584403,
      "loss": 7.502,
      "step": 1182
    },
    {
      "epoch": 0.33679715302491103,
      "grad_norm": 0.4305514097213745,
      "learning_rate": 0.00041588385994876175,
      "loss": 7.751,
      "step": 1183
    },
    {
      "epoch": 0.33708185053380785,
      "grad_norm": 0.46878859400749207,
      "learning_rate": 0.00041581269570167953,
      "loss": 7.2695,
      "step": 1184
    },
    {
      "epoch": 0.3373665480427046,
      "grad_norm": 0.4970056712627411,
      "learning_rate": 0.0004157415314545972,
      "loss": 7.2119,
      "step": 1185
    },
    {
      "epoch": 0.3376512455516014,
      "grad_norm": 0.5278494358062744,
      "learning_rate": 0.0004156703672075149,
      "loss": 7.2178,
      "step": 1186
    },
    {
      "epoch": 0.33793594306049823,
      "grad_norm": 0.6189301013946533,
      "learning_rate": 0.0004155992029604327,
      "loss": 7.0967,
      "step": 1187
    },
    {
      "epoch": 0.338220640569395,
      "grad_norm": 0.5286704301834106,
      "learning_rate": 0.0004155280387133504,
      "loss": 7.4258,
      "step": 1188
    },
    {
      "epoch": 0.3385053380782918,
      "grad_norm": 0.5079885721206665,
      "learning_rate": 0.0004154568744662682,
      "loss": 7.3896,
      "step": 1189
    },
    {
      "epoch": 0.3387900355871886,
      "grad_norm": 0.6349208950996399,
      "learning_rate": 0.00041538571021918587,
      "loss": 7.4512,
      "step": 1190
    },
    {
      "epoch": 0.33907473309608543,
      "grad_norm": 0.6474052667617798,
      "learning_rate": 0.0004153145459721036,
      "loss": 6.4209,
      "step": 1191
    },
    {
      "epoch": 0.3393594306049822,
      "grad_norm": 0.3994763195514679,
      "learning_rate": 0.00041524338172502137,
      "loss": 7.8379,
      "step": 1192
    },
    {
      "epoch": 0.339644128113879,
      "grad_norm": 0.46896281838417053,
      "learning_rate": 0.0004151722174779391,
      "loss": 7.4766,
      "step": 1193
    },
    {
      "epoch": 0.3399288256227758,
      "grad_norm": 4.865256309509277,
      "learning_rate": 0.00041510105323085687,
      "loss": 7.6611,
      "step": 1194
    },
    {
      "epoch": 0.3402135231316726,
      "grad_norm": 0.5058532953262329,
      "learning_rate": 0.0004150298889837746,
      "loss": 7.6475,
      "step": 1195
    },
    {
      "epoch": 0.3404982206405694,
      "grad_norm": 0.4236146807670593,
      "learning_rate": 0.00041495872473669226,
      "loss": 7.4639,
      "step": 1196
    },
    {
      "epoch": 0.3407829181494662,
      "grad_norm": 0.5200099945068359,
      "learning_rate": 0.00041488756048961003,
      "loss": 6.7969,
      "step": 1197
    },
    {
      "epoch": 0.34106761565836297,
      "grad_norm": 0.5131163597106934,
      "learning_rate": 0.00041481639624252776,
      "loss": 7.0908,
      "step": 1198
    },
    {
      "epoch": 0.3413523131672598,
      "grad_norm": 0.42460203170776367,
      "learning_rate": 0.0004147452319954455,
      "loss": 7.6074,
      "step": 1199
    },
    {
      "epoch": 0.3416370106761566,
      "grad_norm": 0.4523850977420807,
      "learning_rate": 0.00041467406774836326,
      "loss": 7.1504,
      "step": 1200
    },
    {
      "epoch": 0.3416370106761566,
      "eval_bleu": 0.10037787970140617,
      "eval_loss": 7.05078125,
      "eval_runtime": 151.2375,
      "eval_samples_per_second": 1.878,
      "eval_steps_per_second": 0.119,
      "step": 1200
    },
    {
      "epoch": 0.3419217081850534,
      "grad_norm": 0.5223976373672485,
      "learning_rate": 0.000414602903501281,
      "loss": 7.1885,
      "step": 1201
    },
    {
      "epoch": 0.34220640569395017,
      "grad_norm": 0.5459167957305908,
      "learning_rate": 0.0004145317392541987,
      "loss": 7.1562,
      "step": 1202
    },
    {
      "epoch": 0.342491103202847,
      "grad_norm": 0.416970431804657,
      "learning_rate": 0.0004144605750071164,
      "loss": 7.335,
      "step": 1203
    },
    {
      "epoch": 0.3427758007117438,
      "grad_norm": 0.4617096781730652,
      "learning_rate": 0.00041438941076003415,
      "loss": 7.2158,
      "step": 1204
    },
    {
      "epoch": 0.34306049822064055,
      "grad_norm": 0.48640942573547363,
      "learning_rate": 0.0004143182465129519,
      "loss": 7.3984,
      "step": 1205
    },
    {
      "epoch": 0.34334519572953737,
      "grad_norm": 0.5603982210159302,
      "learning_rate": 0.00041424708226586965,
      "loss": 6.6523,
      "step": 1206
    },
    {
      "epoch": 0.3436298932384342,
      "grad_norm": 0.652839720249176,
      "learning_rate": 0.00041417591801878737,
      "loss": 6.833,
      "step": 1207
    },
    {
      "epoch": 0.34391459074733094,
      "grad_norm": 0.5112550258636475,
      "learning_rate": 0.0004141047537717051,
      "loss": 7.3613,
      "step": 1208
    },
    {
      "epoch": 0.34419928825622775,
      "grad_norm": 0.46459585428237915,
      "learning_rate": 0.0004140335895246228,
      "loss": 7.2656,
      "step": 1209
    },
    {
      "epoch": 0.34448398576512457,
      "grad_norm": 0.58054119348526,
      "learning_rate": 0.0004139624252775406,
      "loss": 7.0889,
      "step": 1210
    },
    {
      "epoch": 0.3447686832740213,
      "grad_norm": 0.5896499156951904,
      "learning_rate": 0.0004138912610304583,
      "loss": 6.834,
      "step": 1211
    },
    {
      "epoch": 0.34505338078291814,
      "grad_norm": 0.3972550630569458,
      "learning_rate": 0.0004138200967833761,
      "loss": 7.4492,
      "step": 1212
    },
    {
      "epoch": 0.34533807829181495,
      "grad_norm": 0.5355075597763062,
      "learning_rate": 0.00041374893253629376,
      "loss": 7.2773,
      "step": 1213
    },
    {
      "epoch": 0.34562277580071177,
      "grad_norm": 0.5021422505378723,
      "learning_rate": 0.0004136777682892115,
      "loss": 7.5195,
      "step": 1214
    },
    {
      "epoch": 0.3459074733096085,
      "grad_norm": 0.5513196587562561,
      "learning_rate": 0.00041360660404212926,
      "loss": 7.165,
      "step": 1215
    },
    {
      "epoch": 0.34619217081850534,
      "grad_norm": 0.4936729073524475,
      "learning_rate": 0.000413535439795047,
      "loss": 6.9385,
      "step": 1216
    },
    {
      "epoch": 0.34647686832740215,
      "grad_norm": 0.4948480725288391,
      "learning_rate": 0.0004134642755479647,
      "loss": 7.3184,
      "step": 1217
    },
    {
      "epoch": 0.3467615658362989,
      "grad_norm": 0.49627193808555603,
      "learning_rate": 0.00041339311130088243,
      "loss": 7.248,
      "step": 1218
    },
    {
      "epoch": 0.3470462633451957,
      "grad_norm": 0.4766037166118622,
      "learning_rate": 0.00041332194705380015,
      "loss": 7.3076,
      "step": 1219
    },
    {
      "epoch": 0.34733096085409254,
      "grad_norm": 0.4347938299179077,
      "learning_rate": 0.00041325078280671793,
      "loss": 7.3867,
      "step": 1220
    },
    {
      "epoch": 0.3476156583629893,
      "grad_norm": 0.4287054240703583,
      "learning_rate": 0.00041317961855963565,
      "loss": 7.6777,
      "step": 1221
    },
    {
      "epoch": 0.3479003558718861,
      "grad_norm": 0.5360247492790222,
      "learning_rate": 0.0004131084543125534,
      "loss": 7.0908,
      "step": 1222
    },
    {
      "epoch": 0.3481850533807829,
      "grad_norm": 0.504662811756134,
      "learning_rate": 0.00041303729006547115,
      "loss": 7.0732,
      "step": 1223
    },
    {
      "epoch": 0.34846975088967974,
      "grad_norm": 0.5329994559288025,
      "learning_rate": 0.0004129661258183888,
      "loss": 7.2832,
      "step": 1224
    },
    {
      "epoch": 0.3487544483985765,
      "grad_norm": 0.49108269810676575,
      "learning_rate": 0.0004128949615713066,
      "loss": 7.3379,
      "step": 1225
    },
    {
      "epoch": 0.3490391459074733,
      "grad_norm": 0.43490204215049744,
      "learning_rate": 0.0004128237973242243,
      "loss": 7.5186,
      "step": 1226
    },
    {
      "epoch": 0.34932384341637013,
      "grad_norm": 0.4555596113204956,
      "learning_rate": 0.00041275263307714204,
      "loss": 7.5586,
      "step": 1227
    },
    {
      "epoch": 0.3496085409252669,
      "grad_norm": 0.43048095703125,
      "learning_rate": 0.0004126814688300598,
      "loss": 7.499,
      "step": 1228
    },
    {
      "epoch": 0.3498932384341637,
      "grad_norm": 0.5056287050247192,
      "learning_rate": 0.00041261030458297754,
      "loss": 7.2539,
      "step": 1229
    },
    {
      "epoch": 0.3501779359430605,
      "grad_norm": 0.5323185920715332,
      "learning_rate": 0.0004125391403358952,
      "loss": 7.4971,
      "step": 1230
    },
    {
      "epoch": 0.3504626334519573,
      "grad_norm": 0.4007214307785034,
      "learning_rate": 0.000412467976088813,
      "loss": 8.1367,
      "step": 1231
    },
    {
      "epoch": 0.3507473309608541,
      "grad_norm": 0.49167752265930176,
      "learning_rate": 0.0004123968118417307,
      "loss": 7.5352,
      "step": 1232
    },
    {
      "epoch": 0.3510320284697509,
      "grad_norm": 0.4917040467262268,
      "learning_rate": 0.0004123256475946485,
      "loss": 7.1221,
      "step": 1233
    },
    {
      "epoch": 0.35131672597864766,
      "grad_norm": 0.5261372327804565,
      "learning_rate": 0.0004122544833475662,
      "loss": 6.8887,
      "step": 1234
    },
    {
      "epoch": 0.3516014234875445,
      "grad_norm": 0.43594714999198914,
      "learning_rate": 0.0004121833191004839,
      "loss": 7.6035,
      "step": 1235
    },
    {
      "epoch": 0.3518861209964413,
      "grad_norm": 0.5169447064399719,
      "learning_rate": 0.00041211215485340165,
      "loss": 7.2676,
      "step": 1236
    },
    {
      "epoch": 0.3521708185053381,
      "grad_norm": 0.4773392081260681,
      "learning_rate": 0.0004120409906063194,
      "loss": 7.6768,
      "step": 1237
    },
    {
      "epoch": 0.35245551601423486,
      "grad_norm": 0.4493792951107025,
      "learning_rate": 0.00041196982635923715,
      "loss": 7.4033,
      "step": 1238
    },
    {
      "epoch": 0.3527402135231317,
      "grad_norm": 0.4882626533508301,
      "learning_rate": 0.0004118986621121549,
      "loss": 7.3887,
      "step": 1239
    },
    {
      "epoch": 0.3530249110320285,
      "grad_norm": 0.6581220030784607,
      "learning_rate": 0.0004118274978650726,
      "loss": 6.6924,
      "step": 1240
    },
    {
      "epoch": 0.35330960854092525,
      "grad_norm": 0.541993260383606,
      "learning_rate": 0.0004117563336179903,
      "loss": 6.876,
      "step": 1241
    },
    {
      "epoch": 0.35359430604982206,
      "grad_norm": 0.46581751108169556,
      "learning_rate": 0.00041168516937090805,
      "loss": 7.5488,
      "step": 1242
    },
    {
      "epoch": 0.3538790035587189,
      "grad_norm": 0.46979838609695435,
      "learning_rate": 0.0004116140051238258,
      "loss": 7.6562,
      "step": 1243
    },
    {
      "epoch": 0.35416370106761563,
      "grad_norm": 0.5168439745903015,
      "learning_rate": 0.00041154284087674354,
      "loss": 7.3828,
      "step": 1244
    },
    {
      "epoch": 0.35444839857651245,
      "grad_norm": 0.4394519627094269,
      "learning_rate": 0.00041147167662966127,
      "loss": 7.6738,
      "step": 1245
    },
    {
      "epoch": 0.35473309608540926,
      "grad_norm": 0.41233596205711365,
      "learning_rate": 0.00041140051238257904,
      "loss": 7.7783,
      "step": 1246
    },
    {
      "epoch": 0.3550177935943061,
      "grad_norm": 0.5092318058013916,
      "learning_rate": 0.0004113293481354967,
      "loss": 7.4785,
      "step": 1247
    },
    {
      "epoch": 0.35530249110320283,
      "grad_norm": 0.5250484347343445,
      "learning_rate": 0.00041125818388841444,
      "loss": 6.3848,
      "step": 1248
    },
    {
      "epoch": 0.35558718861209965,
      "grad_norm": 0.46243852376937866,
      "learning_rate": 0.0004111870196413322,
      "loss": 7.7559,
      "step": 1249
    },
    {
      "epoch": 0.35587188612099646,
      "grad_norm": 0.5164723992347717,
      "learning_rate": 0.00041111585539424994,
      "loss": 7.1367,
      "step": 1250
    },
    {
      "epoch": 0.3561565836298932,
      "grad_norm": 0.5358831882476807,
      "learning_rate": 0.0004110446911471677,
      "loss": 7.1572,
      "step": 1251
    },
    {
      "epoch": 0.35644128113879003,
      "grad_norm": 0.5281264185905457,
      "learning_rate": 0.0004109735269000854,
      "loss": 7.5811,
      "step": 1252
    },
    {
      "epoch": 0.35672597864768685,
      "grad_norm": 0.5329515337944031,
      "learning_rate": 0.0004109023626530031,
      "loss": 7.0713,
      "step": 1253
    },
    {
      "epoch": 0.3570106761565836,
      "grad_norm": 0.5931563377380371,
      "learning_rate": 0.0004108311984059209,
      "loss": 7.1826,
      "step": 1254
    },
    {
      "epoch": 0.3572953736654804,
      "grad_norm": 0.481064110994339,
      "learning_rate": 0.0004107600341588386,
      "loss": 6.9893,
      "step": 1255
    },
    {
      "epoch": 0.35758007117437723,
      "grad_norm": 0.45632845163345337,
      "learning_rate": 0.0004106888699117564,
      "loss": 7.4512,
      "step": 1256
    },
    {
      "epoch": 0.35786476868327405,
      "grad_norm": 0.5430977940559387,
      "learning_rate": 0.0004106177056646741,
      "loss": 7.2402,
      "step": 1257
    },
    {
      "epoch": 0.3581494661921708,
      "grad_norm": 1.1921885013580322,
      "learning_rate": 0.00041054654141759177,
      "loss": 7.5723,
      "step": 1258
    },
    {
      "epoch": 0.3584341637010676,
      "grad_norm": 0.5579004287719727,
      "learning_rate": 0.00041047537717050955,
      "loss": 7.0098,
      "step": 1259
    },
    {
      "epoch": 0.35871886120996443,
      "grad_norm": 0.474059522151947,
      "learning_rate": 0.00041040421292342727,
      "loss": 7.6367,
      "step": 1260
    },
    {
      "epoch": 0.3590035587188612,
      "grad_norm": 0.6036648154258728,
      "learning_rate": 0.00041033304867634505,
      "loss": 7.3857,
      "step": 1261
    },
    {
      "epoch": 0.359288256227758,
      "grad_norm": 0.4579026699066162,
      "learning_rate": 0.00041026188442926277,
      "loss": 7.79,
      "step": 1262
    },
    {
      "epoch": 0.3595729537366548,
      "grad_norm": 0.5366966724395752,
      "learning_rate": 0.00041019072018218044,
      "loss": 6.7188,
      "step": 1263
    },
    {
      "epoch": 0.3598576512455516,
      "grad_norm": 0.6088496446609497,
      "learning_rate": 0.0004101195559350982,
      "loss": 6.8174,
      "step": 1264
    },
    {
      "epoch": 0.3601423487544484,
      "grad_norm": 0.5021112561225891,
      "learning_rate": 0.00041004839168801594,
      "loss": 7.3369,
      "step": 1265
    },
    {
      "epoch": 0.3604270462633452,
      "grad_norm": 0.4617200195789337,
      "learning_rate": 0.00040997722744093366,
      "loss": 7.6934,
      "step": 1266
    },
    {
      "epoch": 0.36071174377224197,
      "grad_norm": 0.4734245240688324,
      "learning_rate": 0.00040990606319385144,
      "loss": 7.3154,
      "step": 1267
    },
    {
      "epoch": 0.3609964412811388,
      "grad_norm": 0.621208667755127,
      "learning_rate": 0.00040983489894676916,
      "loss": 7.1494,
      "step": 1268
    },
    {
      "epoch": 0.3612811387900356,
      "grad_norm": 0.58305823802948,
      "learning_rate": 0.0004097637346996869,
      "loss": 6.7168,
      "step": 1269
    },
    {
      "epoch": 0.3615658362989324,
      "grad_norm": 0.4475565254688263,
      "learning_rate": 0.0004096925704526046,
      "loss": 7.6641,
      "step": 1270
    },
    {
      "epoch": 0.36185053380782917,
      "grad_norm": 0.476866215467453,
      "learning_rate": 0.00040962140620552233,
      "loss": 7.5732,
      "step": 1271
    },
    {
      "epoch": 0.362135231316726,
      "grad_norm": 0.4611436426639557,
      "learning_rate": 0.0004095502419584401,
      "loss": 7.8398,
      "step": 1272
    },
    {
      "epoch": 0.3624199288256228,
      "grad_norm": 0.4453016519546509,
      "learning_rate": 0.00040947907771135783,
      "loss": 7.583,
      "step": 1273
    },
    {
      "epoch": 0.36270462633451955,
      "grad_norm": 0.490815132856369,
      "learning_rate": 0.0004094079134642756,
      "loss": 7.207,
      "step": 1274
    },
    {
      "epoch": 0.36298932384341637,
      "grad_norm": 0.48163872957229614,
      "learning_rate": 0.0004093367492171933,
      "loss": 7.8252,
      "step": 1275
    },
    {
      "epoch": 0.3632740213523132,
      "grad_norm": 0.5621169209480286,
      "learning_rate": 0.000409265584970111,
      "loss": 7.1377,
      "step": 1276
    },
    {
      "epoch": 0.36355871886120994,
      "grad_norm": 0.48964807391166687,
      "learning_rate": 0.0004091944207230288,
      "loss": 7.5391,
      "step": 1277
    },
    {
      "epoch": 0.36384341637010675,
      "grad_norm": 0.6166073679924011,
      "learning_rate": 0.0004091232564759465,
      "loss": 6.9941,
      "step": 1278
    },
    {
      "epoch": 0.36412811387900357,
      "grad_norm": 0.5375643968582153,
      "learning_rate": 0.0004090520922288642,
      "loss": 6.9863,
      "step": 1279
    },
    {
      "epoch": 0.3644128113879004,
      "grad_norm": 0.6133733987808228,
      "learning_rate": 0.00040898092798178194,
      "loss": 6.7217,
      "step": 1280
    },
    {
      "epoch": 0.36469750889679714,
      "grad_norm": 0.4984293580055237,
      "learning_rate": 0.00040890976373469967,
      "loss": 7.4521,
      "step": 1281
    },
    {
      "epoch": 0.36498220640569395,
      "grad_norm": 0.4901769459247589,
      "learning_rate": 0.00040883859948761744,
      "loss": 7.166,
      "step": 1282
    },
    {
      "epoch": 0.36526690391459077,
      "grad_norm": 0.48212358355522156,
      "learning_rate": 0.00040876743524053517,
      "loss": 7.2979,
      "step": 1283
    },
    {
      "epoch": 0.3655516014234875,
      "grad_norm": 0.5306196212768555,
      "learning_rate": 0.0004086962709934529,
      "loss": 6.5479,
      "step": 1284
    },
    {
      "epoch": 0.36583629893238434,
      "grad_norm": 0.3485376238822937,
      "learning_rate": 0.00040862510674637067,
      "loss": 7.6035,
      "step": 1285
    },
    {
      "epoch": 0.36612099644128115,
      "grad_norm": 0.6489346623420715,
      "learning_rate": 0.00040855394249928833,
      "loss": 6.4941,
      "step": 1286
    },
    {
      "epoch": 0.3664056939501779,
      "grad_norm": 0.5040377974510193,
      "learning_rate": 0.0004084827782522061,
      "loss": 7.2979,
      "step": 1287
    },
    {
      "epoch": 0.3666903914590747,
      "grad_norm": 0.4830140769481659,
      "learning_rate": 0.00040841161400512383,
      "loss": 7.7559,
      "step": 1288
    },
    {
      "epoch": 0.36697508896797154,
      "grad_norm": 0.44353169202804565,
      "learning_rate": 0.00040834044975804156,
      "loss": 7.3809,
      "step": 1289
    },
    {
      "epoch": 0.36725978647686836,
      "grad_norm": 0.5763711333274841,
      "learning_rate": 0.00040826928551095933,
      "loss": 7.2432,
      "step": 1290
    },
    {
      "epoch": 0.3675444839857651,
      "grad_norm": 0.5355408191680908,
      "learning_rate": 0.000408198121263877,
      "loss": 7.2393,
      "step": 1291
    },
    {
      "epoch": 0.3678291814946619,
      "grad_norm": 0.534460723400116,
      "learning_rate": 0.0004081269570167948,
      "loss": 7.4102,
      "step": 1292
    },
    {
      "epoch": 0.36811387900355874,
      "grad_norm": 0.49930524826049805,
      "learning_rate": 0.0004080557927697125,
      "loss": 7.374,
      "step": 1293
    },
    {
      "epoch": 0.3683985765124555,
      "grad_norm": 0.5417733192443848,
      "learning_rate": 0.0004079846285226302,
      "loss": 7.4746,
      "step": 1294
    },
    {
      "epoch": 0.3686832740213523,
      "grad_norm": 0.48457103967666626,
      "learning_rate": 0.000407913464275548,
      "loss": 7.7334,
      "step": 1295
    },
    {
      "epoch": 0.36896797153024913,
      "grad_norm": 0.5501992106437683,
      "learning_rate": 0.0004078423000284657,
      "loss": 6.957,
      "step": 1296
    },
    {
      "epoch": 0.3692526690391459,
      "grad_norm": 0.49789419770240784,
      "learning_rate": 0.0004077711357813834,
      "loss": 7.3945,
      "step": 1297
    },
    {
      "epoch": 0.3695373665480427,
      "grad_norm": 0.5354185104370117,
      "learning_rate": 0.00040769997153430117,
      "loss": 7.375,
      "step": 1298
    },
    {
      "epoch": 0.3698220640569395,
      "grad_norm": 0.49562764167785645,
      "learning_rate": 0.0004076288072872189,
      "loss": 7.4277,
      "step": 1299
    },
    {
      "epoch": 0.3701067615658363,
      "grad_norm": 0.5128252506256104,
      "learning_rate": 0.00040755764304013667,
      "loss": 7.0391,
      "step": 1300
    },
    {
      "epoch": 0.3703914590747331,
      "grad_norm": 0.5912569165229797,
      "learning_rate": 0.0004074864787930544,
      "loss": 6.8955,
      "step": 1301
    },
    {
      "epoch": 0.3706761565836299,
      "grad_norm": 0.4700371026992798,
      "learning_rate": 0.0004074153145459721,
      "loss": 7.7754,
      "step": 1302
    },
    {
      "epoch": 0.3709608540925267,
      "grad_norm": 0.4822196960449219,
      "learning_rate": 0.00040734415029888984,
      "loss": 7.3359,
      "step": 1303
    },
    {
      "epoch": 0.3712455516014235,
      "grad_norm": 0.49563339352607727,
      "learning_rate": 0.00040727298605180756,
      "loss": 7.3438,
      "step": 1304
    },
    {
      "epoch": 0.3715302491103203,
      "grad_norm": 0.5423407554626465,
      "learning_rate": 0.00040720182180472534,
      "loss": 6.4746,
      "step": 1305
    },
    {
      "epoch": 0.3718149466192171,
      "grad_norm": 0.48336294293403625,
      "learning_rate": 0.00040713065755764306,
      "loss": 7.5684,
      "step": 1306
    },
    {
      "epoch": 0.37209964412811386,
      "grad_norm": 0.4428510069847107,
      "learning_rate": 0.0004070594933105608,
      "loss": 7.6387,
      "step": 1307
    },
    {
      "epoch": 0.3723843416370107,
      "grad_norm": 0.592888355255127,
      "learning_rate": 0.0004069883290634785,
      "loss": 6.9248,
      "step": 1308
    },
    {
      "epoch": 0.3726690391459075,
      "grad_norm": 0.42219093441963196,
      "learning_rate": 0.00040691716481639623,
      "loss": 7.6143,
      "step": 1309
    },
    {
      "epoch": 0.37295373665480425,
      "grad_norm": 0.43672579526901245,
      "learning_rate": 0.000406846000569314,
      "loss": 7.9111,
      "step": 1310
    },
    {
      "epoch": 0.37323843416370106,
      "grad_norm": 0.5268492698669434,
      "learning_rate": 0.00040677483632223173,
      "loss": 7.4766,
      "step": 1311
    },
    {
      "epoch": 0.3735231316725979,
      "grad_norm": 0.5420467853546143,
      "learning_rate": 0.00040670367207514945,
      "loss": 7.2422,
      "step": 1312
    },
    {
      "epoch": 0.3738078291814947,
      "grad_norm": 0.48100414872169495,
      "learning_rate": 0.00040663250782806723,
      "loss": 7.667,
      "step": 1313
    },
    {
      "epoch": 0.37409252669039145,
      "grad_norm": 0.4420747458934784,
      "learning_rate": 0.0004065613435809849,
      "loss": 7.709,
      "step": 1314
    },
    {
      "epoch": 0.37437722419928826,
      "grad_norm": 0.4994819164276123,
      "learning_rate": 0.0004064901793339026,
      "loss": 7.459,
      "step": 1315
    },
    {
      "epoch": 0.3746619217081851,
      "grad_norm": 0.4126765727996826,
      "learning_rate": 0.0004064190150868204,
      "loss": 7.9648,
      "step": 1316
    },
    {
      "epoch": 0.37494661921708183,
      "grad_norm": 0.4801805317401886,
      "learning_rate": 0.0004063478508397381,
      "loss": 7.3936,
      "step": 1317
    },
    {
      "epoch": 0.37523131672597865,
      "grad_norm": 0.5441536903381348,
      "learning_rate": 0.0004062766865926559,
      "loss": 7.6826,
      "step": 1318
    },
    {
      "epoch": 0.37551601423487546,
      "grad_norm": 0.41975390911102295,
      "learning_rate": 0.0004062055223455736,
      "loss": 7.8721,
      "step": 1319
    },
    {
      "epoch": 0.3758007117437722,
      "grad_norm": 0.43222537636756897,
      "learning_rate": 0.0004061343580984913,
      "loss": 7.668,
      "step": 1320
    },
    {
      "epoch": 0.37608540925266903,
      "grad_norm": 0.4959360361099243,
      "learning_rate": 0.00040606319385140906,
      "loss": 7.2529,
      "step": 1321
    },
    {
      "epoch": 0.37637010676156585,
      "grad_norm": 0.5001708269119263,
      "learning_rate": 0.0004059920296043268,
      "loss": 7.8594,
      "step": 1322
    },
    {
      "epoch": 0.3766548042704626,
      "grad_norm": 0.5112770795822144,
      "learning_rate": 0.00040592086535724456,
      "loss": 7.1445,
      "step": 1323
    },
    {
      "epoch": 0.3769395017793594,
      "grad_norm": 0.40957221388816833,
      "learning_rate": 0.0004058497011101623,
      "loss": 7.9492,
      "step": 1324
    },
    {
      "epoch": 0.37722419928825623,
      "grad_norm": 0.4229575991630554,
      "learning_rate": 0.00040577853686307996,
      "loss": 7.4902,
      "step": 1325
    },
    {
      "epoch": 0.37750889679715305,
      "grad_norm": 0.5905504822731018,
      "learning_rate": 0.00040570737261599773,
      "loss": 6.8193,
      "step": 1326
    },
    {
      "epoch": 0.3777935943060498,
      "grad_norm": 0.5057560205459595,
      "learning_rate": 0.00040563620836891546,
      "loss": 7.5215,
      "step": 1327
    },
    {
      "epoch": 0.3780782918149466,
      "grad_norm": 0.536011278629303,
      "learning_rate": 0.0004055650441218332,
      "loss": 7.1104,
      "step": 1328
    },
    {
      "epoch": 0.37836298932384343,
      "grad_norm": 0.46675094962120056,
      "learning_rate": 0.00040549387987475096,
      "loss": 7.6035,
      "step": 1329
    },
    {
      "epoch": 0.3786476868327402,
      "grad_norm": 0.43799978494644165,
      "learning_rate": 0.0004054227156276687,
      "loss": 7.1494,
      "step": 1330
    },
    {
      "epoch": 0.378932384341637,
      "grad_norm": 0.5438700914382935,
      "learning_rate": 0.0004053515513805864,
      "loss": 6.7471,
      "step": 1331
    },
    {
      "epoch": 0.3792170818505338,
      "grad_norm": 0.374488890171051,
      "learning_rate": 0.0004052803871335041,
      "loss": 8.1045,
      "step": 1332
    },
    {
      "epoch": 0.3795017793594306,
      "grad_norm": 0.5037118792533875,
      "learning_rate": 0.00040520922288642185,
      "loss": 7.5791,
      "step": 1333
    },
    {
      "epoch": 0.3797864768683274,
      "grad_norm": 0.6229990720748901,
      "learning_rate": 0.0004051380586393396,
      "loss": 7.2334,
      "step": 1334
    },
    {
      "epoch": 0.3800711743772242,
      "grad_norm": 0.4150471091270447,
      "learning_rate": 0.00040506689439225735,
      "loss": 7.8135,
      "step": 1335
    },
    {
      "epoch": 0.380355871886121,
      "grad_norm": 0.4916723966598511,
      "learning_rate": 0.00040499573014517507,
      "loss": 7.1611,
      "step": 1336
    },
    {
      "epoch": 0.3806405693950178,
      "grad_norm": 0.5903331637382507,
      "learning_rate": 0.0004049245658980928,
      "loss": 7.3105,
      "step": 1337
    },
    {
      "epoch": 0.3809252669039146,
      "grad_norm": 0.5368372797966003,
      "learning_rate": 0.0004048534016510105,
      "loss": 6.752,
      "step": 1338
    },
    {
      "epoch": 0.3812099644128114,
      "grad_norm": 0.570995032787323,
      "learning_rate": 0.0004047822374039283,
      "loss": 7.1406,
      "step": 1339
    },
    {
      "epoch": 0.38149466192170817,
      "grad_norm": 0.5715189576148987,
      "learning_rate": 0.000404711073156846,
      "loss": 7.2061,
      "step": 1340
    },
    {
      "epoch": 0.381779359430605,
      "grad_norm": 0.543644368648529,
      "learning_rate": 0.0004046399089097638,
      "loss": 6.8984,
      "step": 1341
    },
    {
      "epoch": 0.3820640569395018,
      "grad_norm": 0.4701455533504486,
      "learning_rate": 0.00040456874466268146,
      "loss": 7.3135,
      "step": 1342
    },
    {
      "epoch": 0.38234875444839855,
      "grad_norm": 0.47678303718566895,
      "learning_rate": 0.0004044975804155992,
      "loss": 7.6211,
      "step": 1343
    },
    {
      "epoch": 0.38263345195729537,
      "grad_norm": 0.46302834153175354,
      "learning_rate": 0.00040442641616851696,
      "loss": 7.2256,
      "step": 1344
    },
    {
      "epoch": 0.3829181494661922,
      "grad_norm": 0.45586827397346497,
      "learning_rate": 0.0004043552519214347,
      "loss": 8.0332,
      "step": 1345
    },
    {
      "epoch": 0.383202846975089,
      "grad_norm": 0.7867341041564941,
      "learning_rate": 0.0004042840876743524,
      "loss": 6.6299,
      "step": 1346
    },
    {
      "epoch": 0.38348754448398575,
      "grad_norm": 0.5549026131629944,
      "learning_rate": 0.0004042129234272702,
      "loss": 6.9746,
      "step": 1347
    },
    {
      "epoch": 0.38377224199288257,
      "grad_norm": 0.5253206491470337,
      "learning_rate": 0.00040414175918018785,
      "loss": 7.167,
      "step": 1348
    },
    {
      "epoch": 0.3840569395017794,
      "grad_norm": 0.5383652448654175,
      "learning_rate": 0.0004040705949331056,
      "loss": 6.873,
      "step": 1349
    },
    {
      "epoch": 0.38434163701067614,
      "grad_norm": 0.450445294380188,
      "learning_rate": 0.00040399943068602335,
      "loss": 7.584,
      "step": 1350
    },
    {
      "epoch": 0.38462633451957295,
      "grad_norm": 0.5621605515480042,
      "learning_rate": 0.00040392826643894107,
      "loss": 7.1729,
      "step": 1351
    },
    {
      "epoch": 0.38491103202846977,
      "grad_norm": 0.5089342594146729,
      "learning_rate": 0.00040385710219185885,
      "loss": 7.0508,
      "step": 1352
    },
    {
      "epoch": 0.3851957295373665,
      "grad_norm": 0.49221551418304443,
      "learning_rate": 0.0004037859379447765,
      "loss": 7.3994,
      "step": 1353
    },
    {
      "epoch": 0.38548042704626334,
      "grad_norm": 0.49137023091316223,
      "learning_rate": 0.0004037147736976943,
      "loss": 7.2207,
      "step": 1354
    },
    {
      "epoch": 0.38576512455516015,
      "grad_norm": 0.4768388867378235,
      "learning_rate": 0.000403643609450612,
      "loss": 7.3447,
      "step": 1355
    },
    {
      "epoch": 0.3860498220640569,
      "grad_norm": 0.5809038281440735,
      "learning_rate": 0.00040357244520352974,
      "loss": 7.1562,
      "step": 1356
    },
    {
      "epoch": 0.3863345195729537,
      "grad_norm": 0.4786946475505829,
      "learning_rate": 0.0004035012809564475,
      "loss": 7.6953,
      "step": 1357
    },
    {
      "epoch": 0.38661921708185054,
      "grad_norm": 0.48583534359931946,
      "learning_rate": 0.00040343011670936524,
      "loss": 7.4375,
      "step": 1358
    },
    {
      "epoch": 0.38690391459074736,
      "grad_norm": 0.5095043778419495,
      "learning_rate": 0.0004033589524622829,
      "loss": 7.2275,
      "step": 1359
    },
    {
      "epoch": 0.3871886120996441,
      "grad_norm": 0.5062644481658936,
      "learning_rate": 0.0004032877882152007,
      "loss": 7.3193,
      "step": 1360
    },
    {
      "epoch": 0.38747330960854093,
      "grad_norm": 0.5594438314437866,
      "learning_rate": 0.0004032166239681184,
      "loss": 7.0176,
      "step": 1361
    },
    {
      "epoch": 0.38775800711743774,
      "grad_norm": 0.46305960416793823,
      "learning_rate": 0.0004031454597210362,
      "loss": 7.415,
      "step": 1362
    },
    {
      "epoch": 0.3880427046263345,
      "grad_norm": 0.5114954710006714,
      "learning_rate": 0.0004030742954739539,
      "loss": 7.3076,
      "step": 1363
    },
    {
      "epoch": 0.3883274021352313,
      "grad_norm": 0.5575315952301025,
      "learning_rate": 0.00040300313122687163,
      "loss": 7.3623,
      "step": 1364
    },
    {
      "epoch": 0.38861209964412813,
      "grad_norm": 0.5885306596755981,
      "learning_rate": 0.00040293196697978935,
      "loss": 7.3291,
      "step": 1365
    },
    {
      "epoch": 0.3888967971530249,
      "grad_norm": 0.5181400179862976,
      "learning_rate": 0.0004028608027327071,
      "loss": 7.0488,
      "step": 1366
    },
    {
      "epoch": 0.3891814946619217,
      "grad_norm": 0.4886661171913147,
      "learning_rate": 0.00040278963848562485,
      "loss": 7.4795,
      "step": 1367
    },
    {
      "epoch": 0.3894661921708185,
      "grad_norm": 0.49588173627853394,
      "learning_rate": 0.0004027184742385426,
      "loss": 7.75,
      "step": 1368
    },
    {
      "epoch": 0.38975088967971533,
      "grad_norm": 0.44115424156188965,
      "learning_rate": 0.0004026473099914603,
      "loss": 7.5293,
      "step": 1369
    },
    {
      "epoch": 0.3900355871886121,
      "grad_norm": 0.44835931062698364,
      "learning_rate": 0.000402576145744378,
      "loss": 7.5801,
      "step": 1370
    },
    {
      "epoch": 0.3903202846975089,
      "grad_norm": 0.4747038781642914,
      "learning_rate": 0.00040250498149729574,
      "loss": 7.252,
      "step": 1371
    },
    {
      "epoch": 0.3906049822064057,
      "grad_norm": 0.5122354030609131,
      "learning_rate": 0.0004024338172502135,
      "loss": 6.8496,
      "step": 1372
    },
    {
      "epoch": 0.3908896797153025,
      "grad_norm": 0.45732030272483826,
      "learning_rate": 0.00040236265300313124,
      "loss": 7.418,
      "step": 1373
    },
    {
      "epoch": 0.3911743772241993,
      "grad_norm": 0.5770905613899231,
      "learning_rate": 0.00040229148875604897,
      "loss": 7.0566,
      "step": 1374
    },
    {
      "epoch": 0.3914590747330961,
      "grad_norm": 0.4752143621444702,
      "learning_rate": 0.00040222032450896674,
      "loss": 7.585,
      "step": 1375
    },
    {
      "epoch": 0.39174377224199286,
      "grad_norm": 0.5004891753196716,
      "learning_rate": 0.0004021491602618844,
      "loss": 7.1553,
      "step": 1376
    },
    {
      "epoch": 0.3920284697508897,
      "grad_norm": 0.42816469073295593,
      "learning_rate": 0.00040207799601480214,
      "loss": 7.3057,
      "step": 1377
    },
    {
      "epoch": 0.3923131672597865,
      "grad_norm": 0.5445579290390015,
      "learning_rate": 0.0004020068317677199,
      "loss": 7.0078,
      "step": 1378
    },
    {
      "epoch": 0.39259786476868325,
      "grad_norm": 0.492911159992218,
      "learning_rate": 0.00040193566752063763,
      "loss": 7.1426,
      "step": 1379
    },
    {
      "epoch": 0.39288256227758006,
      "grad_norm": 0.5625669956207275,
      "learning_rate": 0.0004018645032735554,
      "loss": 7.3311,
      "step": 1380
    },
    {
      "epoch": 0.3931672597864769,
      "grad_norm": 0.4951213300228119,
      "learning_rate": 0.0004017933390264731,
      "loss": 7.54,
      "step": 1381
    },
    {
      "epoch": 0.3934519572953737,
      "grad_norm": 0.4944069981575012,
      "learning_rate": 0.0004017221747793908,
      "loss": 7.4336,
      "step": 1382
    },
    {
      "epoch": 0.39373665480427045,
      "grad_norm": 0.5445288419723511,
      "learning_rate": 0.0004016510105323086,
      "loss": 6.4658,
      "step": 1383
    },
    {
      "epoch": 0.39402135231316726,
      "grad_norm": 0.4639240801334381,
      "learning_rate": 0.0004015798462852263,
      "loss": 7.458,
      "step": 1384
    },
    {
      "epoch": 0.3943060498220641,
      "grad_norm": 0.8487449884414673,
      "learning_rate": 0.0004015086820381441,
      "loss": 5.751,
      "step": 1385
    },
    {
      "epoch": 0.39459074733096083,
      "grad_norm": 0.5100080370903015,
      "learning_rate": 0.0004014375177910618,
      "loss": 7.7383,
      "step": 1386
    },
    {
      "epoch": 0.39487544483985765,
      "grad_norm": 0.467268168926239,
      "learning_rate": 0.00040136635354397947,
      "loss": 7.4971,
      "step": 1387
    },
    {
      "epoch": 0.39516014234875446,
      "grad_norm": 0.5072725415229797,
      "learning_rate": 0.00040129518929689725,
      "loss": 7.5605,
      "step": 1388
    },
    {
      "epoch": 0.3954448398576512,
      "grad_norm": 0.5850234031677246,
      "learning_rate": 0.00040122402504981497,
      "loss": 7.2188,
      "step": 1389
    },
    {
      "epoch": 0.39572953736654803,
      "grad_norm": 0.39694520831108093,
      "learning_rate": 0.00040115286080273275,
      "loss": 7.916,
      "step": 1390
    },
    {
      "epoch": 0.39601423487544485,
      "grad_norm": 0.5012302398681641,
      "learning_rate": 0.00040108169655565047,
      "loss": 7.2686,
      "step": 1391
    },
    {
      "epoch": 0.39629893238434166,
      "grad_norm": 0.5060039162635803,
      "learning_rate": 0.0004010105323085682,
      "loss": 7.0459,
      "step": 1392
    },
    {
      "epoch": 0.3965836298932384,
      "grad_norm": 0.5460544228553772,
      "learning_rate": 0.0004009393680614859,
      "loss": 7.3105,
      "step": 1393
    },
    {
      "epoch": 0.39686832740213523,
      "grad_norm": 0.5348192453384399,
      "learning_rate": 0.00040086820381440364,
      "loss": 7.2607,
      "step": 1394
    },
    {
      "epoch": 0.39715302491103205,
      "grad_norm": 0.5724747180938721,
      "learning_rate": 0.00040079703956732136,
      "loss": 6.8809,
      "step": 1395
    },
    {
      "epoch": 0.3974377224199288,
      "grad_norm": 0.5098087787628174,
      "learning_rate": 0.00040072587532023914,
      "loss": 7.5469,
      "step": 1396
    },
    {
      "epoch": 0.3977224199288256,
      "grad_norm": 0.4509059488773346,
      "learning_rate": 0.00040065471107315686,
      "loss": 7.6543,
      "step": 1397
    },
    {
      "epoch": 0.39800711743772244,
      "grad_norm": 0.5082734823226929,
      "learning_rate": 0.0004005835468260746,
      "loss": 7.2939,
      "step": 1398
    },
    {
      "epoch": 0.3982918149466192,
      "grad_norm": 0.5169782042503357,
      "learning_rate": 0.0004005123825789923,
      "loss": 7.0498,
      "step": 1399
    },
    {
      "epoch": 0.398576512455516,
      "grad_norm": 0.48001834750175476,
      "learning_rate": 0.00040044121833191003,
      "loss": 7.1455,
      "step": 1400
    },
    {
      "epoch": 0.398576512455516,
      "eval_bleu": 0.10806696460928729,
      "eval_loss": 7.0703125,
      "eval_runtime": 146.4757,
      "eval_samples_per_second": 1.939,
      "eval_steps_per_second": 0.123,
      "step": 1400
    },
    {
      "epoch": 0.3988612099644128,
      "grad_norm": 0.4626038670539856,
      "learning_rate": 0.0004003700540848278,
      "loss": 7.0742,
      "step": 1401
    },
    {
      "epoch": 0.39914590747330964,
      "grad_norm": 0.5733367204666138,
      "learning_rate": 0.00040029888983774553,
      "loss": 7.918,
      "step": 1402
    },
    {
      "epoch": 0.3994306049822064,
      "grad_norm": 0.531499981880188,
      "learning_rate": 0.0004002277255906633,
      "loss": 7.1992,
      "step": 1403
    },
    {
      "epoch": 0.3997153024911032,
      "grad_norm": 0.49822041392326355,
      "learning_rate": 0.000400156561343581,
      "loss": 7.0742,
      "step": 1404
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.5700138211250305,
      "learning_rate": 0.0004000853970964987,
      "loss": 6.8828,
      "step": 1405
    },
    {
      "epoch": 0.4002846975088968,
      "grad_norm": 0.4932331442832947,
      "learning_rate": 0.0004000142328494165,
      "loss": 7.584,
      "step": 1406
    },
    {
      "epoch": 0.4005693950177936,
      "grad_norm": 0.5428732633590698,
      "learning_rate": 0.0003999430686023342,
      "loss": 7.1328,
      "step": 1407
    },
    {
      "epoch": 0.4008540925266904,
      "grad_norm": 0.704805850982666,
      "learning_rate": 0.000399871904355252,
      "loss": 6.8857,
      "step": 1408
    },
    {
      "epoch": 0.40113879003558717,
      "grad_norm": 0.580433189868927,
      "learning_rate": 0.0003998007401081697,
      "loss": 6.9805,
      "step": 1409
    },
    {
      "epoch": 0.401423487544484,
      "grad_norm": 0.42145293951034546,
      "learning_rate": 0.00039972957586108737,
      "loss": 7.6494,
      "step": 1410
    },
    {
      "epoch": 0.4017081850533808,
      "grad_norm": 0.448978066444397,
      "learning_rate": 0.00039965841161400514,
      "loss": 7.7148,
      "step": 1411
    },
    {
      "epoch": 0.40199288256227755,
      "grad_norm": 0.6176373958587646,
      "learning_rate": 0.00039958724736692287,
      "loss": 7.2891,
      "step": 1412
    },
    {
      "epoch": 0.40227758007117437,
      "grad_norm": 0.5092046856880188,
      "learning_rate": 0.0003995160831198406,
      "loss": 7.459,
      "step": 1413
    },
    {
      "epoch": 0.4025622775800712,
      "grad_norm": 0.4859773814678192,
      "learning_rate": 0.00039944491887275837,
      "loss": 7.2979,
      "step": 1414
    },
    {
      "epoch": 0.402846975088968,
      "grad_norm": 0.5634656548500061,
      "learning_rate": 0.00039937375462567603,
      "loss": 7.2861,
      "step": 1415
    },
    {
      "epoch": 0.40313167259786475,
      "grad_norm": 0.5251878499984741,
      "learning_rate": 0.0003993025903785938,
      "loss": 7.1133,
      "step": 1416
    },
    {
      "epoch": 0.40341637010676157,
      "grad_norm": 0.44515395164489746,
      "learning_rate": 0.00039923142613151153,
      "loss": 7.585,
      "step": 1417
    },
    {
      "epoch": 0.4037010676156584,
      "grad_norm": 0.5295915603637695,
      "learning_rate": 0.00039916026188442926,
      "loss": 6.9727,
      "step": 1418
    },
    {
      "epoch": 0.40398576512455514,
      "grad_norm": 0.49705609679222107,
      "learning_rate": 0.00039908909763734703,
      "loss": 7.2217,
      "step": 1419
    },
    {
      "epoch": 0.40427046263345195,
      "grad_norm": 0.5276616811752319,
      "learning_rate": 0.00039901793339026476,
      "loss": 7.3721,
      "step": 1420
    },
    {
      "epoch": 0.40455516014234877,
      "grad_norm": 0.6657406687736511,
      "learning_rate": 0.0003989467691431825,
      "loss": 6.4639,
      "step": 1421
    },
    {
      "epoch": 0.4048398576512455,
      "grad_norm": 0.5318674445152283,
      "learning_rate": 0.0003988756048961002,
      "loss": 7.0811,
      "step": 1422
    },
    {
      "epoch": 0.40512455516014234,
      "grad_norm": 0.5530655980110168,
      "learning_rate": 0.0003988044406490179,
      "loss": 7.0322,
      "step": 1423
    },
    {
      "epoch": 0.40540925266903916,
      "grad_norm": 0.5155173540115356,
      "learning_rate": 0.0003987332764019357,
      "loss": 7.3486,
      "step": 1424
    },
    {
      "epoch": 0.40569395017793597,
      "grad_norm": 0.5686538219451904,
      "learning_rate": 0.0003986621121548534,
      "loss": 7.5049,
      "step": 1425
    },
    {
      "epoch": 0.4059786476868327,
      "grad_norm": 0.5946599841117859,
      "learning_rate": 0.0003985909479077711,
      "loss": 6.7871,
      "step": 1426
    },
    {
      "epoch": 0.40626334519572954,
      "grad_norm": 0.5096083283424377,
      "learning_rate": 0.00039851978366068887,
      "loss": 7.21,
      "step": 1427
    },
    {
      "epoch": 0.40654804270462636,
      "grad_norm": 0.4696735739707947,
      "learning_rate": 0.0003984486194136066,
      "loss": 7.3418,
      "step": 1428
    },
    {
      "epoch": 0.4068327402135231,
      "grad_norm": 0.49570679664611816,
      "learning_rate": 0.00039837745516652437,
      "loss": 7.376,
      "step": 1429
    },
    {
      "epoch": 0.40711743772241993,
      "grad_norm": 0.5150629878044128,
      "learning_rate": 0.0003983062909194421,
      "loss": 7.2188,
      "step": 1430
    },
    {
      "epoch": 0.40740213523131674,
      "grad_norm": 0.49344584345817566,
      "learning_rate": 0.0003982351266723598,
      "loss": 7.7285,
      "step": 1431
    },
    {
      "epoch": 0.4076868327402135,
      "grad_norm": 0.47263434529304504,
      "learning_rate": 0.00039816396242527754,
      "loss": 7.3428,
      "step": 1432
    },
    {
      "epoch": 0.4079715302491103,
      "grad_norm": 0.45633506774902344,
      "learning_rate": 0.00039809279817819526,
      "loss": 7.4014,
      "step": 1433
    },
    {
      "epoch": 0.40825622775800713,
      "grad_norm": 0.45884770154953003,
      "learning_rate": 0.00039802163393111304,
      "loss": 7.8037,
      "step": 1434
    },
    {
      "epoch": 0.4085409252669039,
      "grad_norm": 0.5151818990707397,
      "learning_rate": 0.00039795046968403076,
      "loss": 7.4043,
      "step": 1435
    },
    {
      "epoch": 0.4088256227758007,
      "grad_norm": 0.45263946056365967,
      "learning_rate": 0.0003978793054369485,
      "loss": 7.3467,
      "step": 1436
    },
    {
      "epoch": 0.4091103202846975,
      "grad_norm": 0.47288239002227783,
      "learning_rate": 0.00039780814118986626,
      "loss": 7.1445,
      "step": 1437
    },
    {
      "epoch": 0.40939501779359433,
      "grad_norm": 0.522957444190979,
      "learning_rate": 0.00039773697694278393,
      "loss": 7.4648,
      "step": 1438
    },
    {
      "epoch": 0.4096797153024911,
      "grad_norm": 0.425659716129303,
      "learning_rate": 0.0003976658126957017,
      "loss": 7.6475,
      "step": 1439
    },
    {
      "epoch": 0.4099644128113879,
      "grad_norm": 0.5347933769226074,
      "learning_rate": 0.00039759464844861943,
      "loss": 7.3047,
      "step": 1440
    },
    {
      "epoch": 0.4102491103202847,
      "grad_norm": 0.521828830242157,
      "learning_rate": 0.00039752348420153715,
      "loss": 7.4268,
      "step": 1441
    },
    {
      "epoch": 0.4105338078291815,
      "grad_norm": 0.3580774664878845,
      "learning_rate": 0.00039745231995445493,
      "loss": 8.0146,
      "step": 1442
    },
    {
      "epoch": 0.4108185053380783,
      "grad_norm": 0.4850667417049408,
      "learning_rate": 0.0003973811557073726,
      "loss": 7.6045,
      "step": 1443
    },
    {
      "epoch": 0.4111032028469751,
      "grad_norm": 0.5015649795532227,
      "learning_rate": 0.0003973099914602903,
      "loss": 7.3203,
      "step": 1444
    },
    {
      "epoch": 0.41138790035587186,
      "grad_norm": 0.6262863278388977,
      "learning_rate": 0.0003972388272132081,
      "loss": 6.9189,
      "step": 1445
    },
    {
      "epoch": 0.4116725978647687,
      "grad_norm": 0.7990656495094299,
      "learning_rate": 0.0003971676629661258,
      "loss": 7.4863,
      "step": 1446
    },
    {
      "epoch": 0.4119572953736655,
      "grad_norm": 0.4794697165489197,
      "learning_rate": 0.0003970964987190436,
      "loss": 7.5576,
      "step": 1447
    },
    {
      "epoch": 0.4122419928825623,
      "grad_norm": 0.3931448459625244,
      "learning_rate": 0.0003970253344719613,
      "loss": 7.5928,
      "step": 1448
    },
    {
      "epoch": 0.41252669039145906,
      "grad_norm": 0.5144194960594177,
      "learning_rate": 0.000396954170224879,
      "loss": 7.3867,
      "step": 1449
    },
    {
      "epoch": 0.4128113879003559,
      "grad_norm": 0.4392128586769104,
      "learning_rate": 0.00039688300597779676,
      "loss": 7.7227,
      "step": 1450
    },
    {
      "epoch": 0.4130960854092527,
      "grad_norm": 0.5938143134117126,
      "learning_rate": 0.0003968118417307145,
      "loss": 6.9053,
      "step": 1451
    },
    {
      "epoch": 0.41338078291814945,
      "grad_norm": 0.47164639830589294,
      "learning_rate": 0.00039674067748363226,
      "loss": 7.7539,
      "step": 1452
    },
    {
      "epoch": 0.41366548042704626,
      "grad_norm": 0.5650911927223206,
      "learning_rate": 0.00039666951323655,
      "loss": 6.7578,
      "step": 1453
    },
    {
      "epoch": 0.4139501779359431,
      "grad_norm": 0.5679348111152649,
      "learning_rate": 0.00039659834898946765,
      "loss": 6.8457,
      "step": 1454
    },
    {
      "epoch": 0.41423487544483983,
      "grad_norm": 0.4574567675590515,
      "learning_rate": 0.00039652718474238543,
      "loss": 7.9424,
      "step": 1455
    },
    {
      "epoch": 0.41451957295373665,
      "grad_norm": 0.47063425183296204,
      "learning_rate": 0.00039645602049530315,
      "loss": 7.5273,
      "step": 1456
    },
    {
      "epoch": 0.41480427046263346,
      "grad_norm": 0.5154306888580322,
      "learning_rate": 0.0003963848562482209,
      "loss": 6.8311,
      "step": 1457
    },
    {
      "epoch": 0.4150889679715303,
      "grad_norm": 0.5713111162185669,
      "learning_rate": 0.00039631369200113865,
      "loss": 7.0635,
      "step": 1458
    },
    {
      "epoch": 0.41537366548042703,
      "grad_norm": 0.6168776154518127,
      "learning_rate": 0.0003962425277540564,
      "loss": 7.1387,
      "step": 1459
    },
    {
      "epoch": 0.41565836298932385,
      "grad_norm": 0.6416788697242737,
      "learning_rate": 0.0003961713635069741,
      "loss": 6.4355,
      "step": 1460
    },
    {
      "epoch": 0.41594306049822066,
      "grad_norm": 0.5416299700737,
      "learning_rate": 0.0003961001992598918,
      "loss": 6.8975,
      "step": 1461
    },
    {
      "epoch": 0.4162277580071174,
      "grad_norm": 0.4209768772125244,
      "learning_rate": 0.00039602903501280955,
      "loss": 7.6387,
      "step": 1462
    },
    {
      "epoch": 0.41651245551601424,
      "grad_norm": 0.6386048793792725,
      "learning_rate": 0.0003959578707657273,
      "loss": 7.0059,
      "step": 1463
    },
    {
      "epoch": 0.41679715302491105,
      "grad_norm": 0.46949610114097595,
      "learning_rate": 0.00039588670651864504,
      "loss": 7.5059,
      "step": 1464
    },
    {
      "epoch": 0.4170818505338078,
      "grad_norm": 0.45663750171661377,
      "learning_rate": 0.0003958155422715628,
      "loss": 7.5117,
      "step": 1465
    },
    {
      "epoch": 0.4173665480427046,
      "grad_norm": 0.4578010141849518,
      "learning_rate": 0.0003957443780244805,
      "loss": 7.6816,
      "step": 1466
    },
    {
      "epoch": 0.41765124555160144,
      "grad_norm": 0.5094571113586426,
      "learning_rate": 0.0003956732137773982,
      "loss": 7.5137,
      "step": 1467
    },
    {
      "epoch": 0.4179359430604982,
      "grad_norm": 0.6814231872558594,
      "learning_rate": 0.000395602049530316,
      "loss": 6.5576,
      "step": 1468
    },
    {
      "epoch": 0.418220640569395,
      "grad_norm": 0.5959497094154358,
      "learning_rate": 0.0003955308852832337,
      "loss": 7.0986,
      "step": 1469
    },
    {
      "epoch": 0.4185053380782918,
      "grad_norm": 0.508515477180481,
      "learning_rate": 0.0003954597210361515,
      "loss": 7.166,
      "step": 1470
    },
    {
      "epoch": 0.41879003558718864,
      "grad_norm": 0.5078320503234863,
      "learning_rate": 0.00039538855678906916,
      "loss": 7.4453,
      "step": 1471
    },
    {
      "epoch": 0.4190747330960854,
      "grad_norm": 0.4999483525753021,
      "learning_rate": 0.0003953173925419869,
      "loss": 7.167,
      "step": 1472
    },
    {
      "epoch": 0.4193594306049822,
      "grad_norm": 0.6152355670928955,
      "learning_rate": 0.00039524622829490466,
      "loss": 6.6787,
      "step": 1473
    },
    {
      "epoch": 0.419644128113879,
      "grad_norm": 0.4665818512439728,
      "learning_rate": 0.0003951750640478224,
      "loss": 7.7871,
      "step": 1474
    },
    {
      "epoch": 0.4199288256227758,
      "grad_norm": 0.7211839556694031,
      "learning_rate": 0.0003951038998007401,
      "loss": 6.9883,
      "step": 1475
    },
    {
      "epoch": 0.4202135231316726,
      "grad_norm": 0.5218536257743835,
      "learning_rate": 0.0003950327355536579,
      "loss": 7.1084,
      "step": 1476
    },
    {
      "epoch": 0.4204982206405694,
      "grad_norm": 0.4066094756126404,
      "learning_rate": 0.00039496157130657555,
      "loss": 7.5684,
      "step": 1477
    },
    {
      "epoch": 0.42078291814946617,
      "grad_norm": 0.5480060577392578,
      "learning_rate": 0.0003948904070594933,
      "loss": 7.5,
      "step": 1478
    },
    {
      "epoch": 0.421067615658363,
      "grad_norm": 0.4934498071670532,
      "learning_rate": 0.00039481924281241105,
      "loss": 7.6016,
      "step": 1479
    },
    {
      "epoch": 0.4213523131672598,
      "grad_norm": 0.5548122525215149,
      "learning_rate": 0.00039474807856532877,
      "loss": 7.3652,
      "step": 1480
    },
    {
      "epoch": 0.4216370106761566,
      "grad_norm": 0.45546260476112366,
      "learning_rate": 0.00039467691431824655,
      "loss": 7.5967,
      "step": 1481
    },
    {
      "epoch": 0.42192170818505337,
      "grad_norm": 0.5817190408706665,
      "learning_rate": 0.00039460575007116427,
      "loss": 7.3926,
      "step": 1482
    },
    {
      "epoch": 0.4222064056939502,
      "grad_norm": 0.5597066879272461,
      "learning_rate": 0.000394534585824082,
      "loss": 7.1064,
      "step": 1483
    },
    {
      "epoch": 0.422491103202847,
      "grad_norm": 0.4652703106403351,
      "learning_rate": 0.0003944634215769997,
      "loss": 7.6709,
      "step": 1484
    },
    {
      "epoch": 0.42277580071174375,
      "grad_norm": 0.47021421790122986,
      "learning_rate": 0.00039439225732991744,
      "loss": 7.5459,
      "step": 1485
    },
    {
      "epoch": 0.42306049822064057,
      "grad_norm": 0.49439412355422974,
      "learning_rate": 0.0003943210930828352,
      "loss": 7.3359,
      "step": 1486
    },
    {
      "epoch": 0.4233451957295374,
      "grad_norm": 0.6628631949424744,
      "learning_rate": 0.00039424992883575294,
      "loss": 6.8301,
      "step": 1487
    },
    {
      "epoch": 0.42362989323843414,
      "grad_norm": 0.7028085589408875,
      "learning_rate": 0.00039417876458867066,
      "loss": 6.502,
      "step": 1488
    },
    {
      "epoch": 0.42391459074733095,
      "grad_norm": 0.42867743968963623,
      "learning_rate": 0.0003941076003415884,
      "loss": 7.5254,
      "step": 1489
    },
    {
      "epoch": 0.42419928825622777,
      "grad_norm": 0.48032599687576294,
      "learning_rate": 0.0003940364360945061,
      "loss": 7.3857,
      "step": 1490
    },
    {
      "epoch": 0.4244839857651246,
      "grad_norm": 0.5163261890411377,
      "learning_rate": 0.0003939652718474239,
      "loss": 7.1885,
      "step": 1491
    },
    {
      "epoch": 0.42476868327402134,
      "grad_norm": 0.5880164504051208,
      "learning_rate": 0.0003938941076003416,
      "loss": 6.9902,
      "step": 1492
    },
    {
      "epoch": 0.42505338078291816,
      "grad_norm": 0.5400647521018982,
      "learning_rate": 0.00039382294335325933,
      "loss": 7.2158,
      "step": 1493
    },
    {
      "epoch": 0.42533807829181497,
      "grad_norm": 0.5012080073356628,
      "learning_rate": 0.00039375177910617705,
      "loss": 7.5469,
      "step": 1494
    },
    {
      "epoch": 0.42562277580071173,
      "grad_norm": 0.4507448971271515,
      "learning_rate": 0.0003936806148590948,
      "loss": 7.5957,
      "step": 1495
    },
    {
      "epoch": 0.42590747330960854,
      "grad_norm": 0.4821513295173645,
      "learning_rate": 0.00039360945061201255,
      "loss": 7.6807,
      "step": 1496
    },
    {
      "epoch": 0.42619217081850536,
      "grad_norm": 0.6177121996879578,
      "learning_rate": 0.0003935382863649303,
      "loss": 6.666,
      "step": 1497
    },
    {
      "epoch": 0.4264768683274021,
      "grad_norm": 0.5181967616081238,
      "learning_rate": 0.000393467122117848,
      "loss": 7.3809,
      "step": 1498
    },
    {
      "epoch": 0.42676156583629893,
      "grad_norm": 0.5240229368209839,
      "learning_rate": 0.0003933959578707657,
      "loss": 7.0859,
      "step": 1499
    },
    {
      "epoch": 0.42704626334519574,
      "grad_norm": 0.4757440984249115,
      "learning_rate": 0.00039332479362368344,
      "loss": 7.4932,
      "step": 1500
    },
    {
      "epoch": 0.4273309608540925,
      "grad_norm": 0.4744635224342346,
      "learning_rate": 0.0003932536293766012,
      "loss": 7.5098,
      "step": 1501
    },
    {
      "epoch": 0.4276156583629893,
      "grad_norm": 0.4695112109184265,
      "learning_rate": 0.00039318246512951894,
      "loss": 7.4092,
      "step": 1502
    },
    {
      "epoch": 0.42790035587188613,
      "grad_norm": 0.5622231364250183,
      "learning_rate": 0.00039311130088243667,
      "loss": 7.3037,
      "step": 1503
    },
    {
      "epoch": 0.42818505338078294,
      "grad_norm": 0.5066624283790588,
      "learning_rate": 0.00039304013663535444,
      "loss": 7.25,
      "step": 1504
    },
    {
      "epoch": 0.4284697508896797,
      "grad_norm": 0.5502866506576538,
      "learning_rate": 0.0003929689723882721,
      "loss": 6.9746,
      "step": 1505
    },
    {
      "epoch": 0.4287544483985765,
      "grad_norm": 0.6086217164993286,
      "learning_rate": 0.00039289780814118983,
      "loss": 7.1367,
      "step": 1506
    },
    {
      "epoch": 0.42903914590747333,
      "grad_norm": 0.42815905809402466,
      "learning_rate": 0.0003928266438941076,
      "loss": 7.7324,
      "step": 1507
    },
    {
      "epoch": 0.4293238434163701,
      "grad_norm": 0.6567767858505249,
      "learning_rate": 0.00039275547964702533,
      "loss": 6.5312,
      "step": 1508
    },
    {
      "epoch": 0.4296085409252669,
      "grad_norm": 0.5427162647247314,
      "learning_rate": 0.0003926843153999431,
      "loss": 7.1055,
      "step": 1509
    },
    {
      "epoch": 0.4298932384341637,
      "grad_norm": 0.5855767726898193,
      "learning_rate": 0.00039261315115286083,
      "loss": 7.3848,
      "step": 1510
    },
    {
      "epoch": 0.4301779359430605,
      "grad_norm": 0.5894039273262024,
      "learning_rate": 0.0003925419869057785,
      "loss": 7.0264,
      "step": 1511
    },
    {
      "epoch": 0.4304626334519573,
      "grad_norm": 0.607792317867279,
      "learning_rate": 0.0003924708226586963,
      "loss": 7.2588,
      "step": 1512
    },
    {
      "epoch": 0.4307473309608541,
      "grad_norm": 0.4645512104034424,
      "learning_rate": 0.000392399658411614,
      "loss": 7.5645,
      "step": 1513
    },
    {
      "epoch": 0.4310320284697509,
      "grad_norm": 0.42966175079345703,
      "learning_rate": 0.0003923284941645318,
      "loss": 7.7715,
      "step": 1514
    },
    {
      "epoch": 0.4313167259786477,
      "grad_norm": 0.5350296497344971,
      "learning_rate": 0.0003922573299174495,
      "loss": 7.5469,
      "step": 1515
    },
    {
      "epoch": 0.4316014234875445,
      "grad_norm": 0.6260858774185181,
      "learning_rate": 0.00039218616567036717,
      "loss": 7.2988,
      "step": 1516
    },
    {
      "epoch": 0.4318861209964413,
      "grad_norm": 0.48744967579841614,
      "learning_rate": 0.00039211500142328495,
      "loss": 7.2822,
      "step": 1517
    },
    {
      "epoch": 0.43217081850533806,
      "grad_norm": 0.49149957299232483,
      "learning_rate": 0.00039204383717620267,
      "loss": 7.3467,
      "step": 1518
    },
    {
      "epoch": 0.4324555160142349,
      "grad_norm": 0.455314964056015,
      "learning_rate": 0.00039197267292912045,
      "loss": 7.1279,
      "step": 1519
    },
    {
      "epoch": 0.4327402135231317,
      "grad_norm": 0.7547210454940796,
      "learning_rate": 0.00039190150868203817,
      "loss": 6.4346,
      "step": 1520
    },
    {
      "epoch": 0.43302491103202845,
      "grad_norm": 0.46810033917427063,
      "learning_rate": 0.0003918303444349559,
      "loss": 7.1934,
      "step": 1521
    },
    {
      "epoch": 0.43330960854092526,
      "grad_norm": 0.645000696182251,
      "learning_rate": 0.0003917591801878736,
      "loss": 6.6836,
      "step": 1522
    },
    {
      "epoch": 0.4335943060498221,
      "grad_norm": 0.4427584111690521,
      "learning_rate": 0.00039168801594079134,
      "loss": 7.6494,
      "step": 1523
    },
    {
      "epoch": 0.43387900355871883,
      "grad_norm": 0.5031856894493103,
      "learning_rate": 0.00039161685169370906,
      "loss": 7.501,
      "step": 1524
    },
    {
      "epoch": 0.43416370106761565,
      "grad_norm": 0.5083847045898438,
      "learning_rate": 0.00039154568744662684,
      "loss": 7.3271,
      "step": 1525
    },
    {
      "epoch": 0.43444839857651246,
      "grad_norm": 0.5034371018409729,
      "learning_rate": 0.00039147452319954456,
      "loss": 7.4863,
      "step": 1526
    },
    {
      "epoch": 0.4347330960854093,
      "grad_norm": 0.5857424736022949,
      "learning_rate": 0.00039140335895246234,
      "loss": 6.9111,
      "step": 1527
    },
    {
      "epoch": 0.43501779359430603,
      "grad_norm": 0.5124900341033936,
      "learning_rate": 0.00039133219470538,
      "loss": 7.4219,
      "step": 1528
    },
    {
      "epoch": 0.43530249110320285,
      "grad_norm": 0.5162451863288879,
      "learning_rate": 0.00039126103045829773,
      "loss": 7.8906,
      "step": 1529
    },
    {
      "epoch": 0.43558718861209966,
      "grad_norm": 0.5202774405479431,
      "learning_rate": 0.0003911898662112155,
      "loss": 7.2754,
      "step": 1530
    },
    {
      "epoch": 0.4358718861209964,
      "grad_norm": 0.470385879278183,
      "learning_rate": 0.00039111870196413323,
      "loss": 7.3652,
      "step": 1531
    },
    {
      "epoch": 0.43615658362989324,
      "grad_norm": 0.6605905890464783,
      "learning_rate": 0.000391047537717051,
      "loss": 6.9004,
      "step": 1532
    },
    {
      "epoch": 0.43644128113879005,
      "grad_norm": 0.6666309237480164,
      "learning_rate": 0.0003909763734699687,
      "loss": 6.8594,
      "step": 1533
    },
    {
      "epoch": 0.4367259786476868,
      "grad_norm": 0.5365574359893799,
      "learning_rate": 0.0003909052092228864,
      "loss": 7.0547,
      "step": 1534
    },
    {
      "epoch": 0.4370106761565836,
      "grad_norm": 0.4991326332092285,
      "learning_rate": 0.0003908340449758042,
      "loss": 7.8955,
      "step": 1535
    },
    {
      "epoch": 0.43729537366548044,
      "grad_norm": 0.4271744191646576,
      "learning_rate": 0.0003907628807287219,
      "loss": 7.7021,
      "step": 1536
    },
    {
      "epoch": 0.43758007117437725,
      "grad_norm": 0.5445852875709534,
      "learning_rate": 0.0003906917164816397,
      "loss": 7.208,
      "step": 1537
    },
    {
      "epoch": 0.437864768683274,
      "grad_norm": 0.5331472754478455,
      "learning_rate": 0.0003906205522345574,
      "loss": 6.9561,
      "step": 1538
    },
    {
      "epoch": 0.4381494661921708,
      "grad_norm": 0.5976002812385559,
      "learning_rate": 0.00039054938798747506,
      "loss": 7.3389,
      "step": 1539
    },
    {
      "epoch": 0.43843416370106764,
      "grad_norm": 0.5062530636787415,
      "learning_rate": 0.00039047822374039284,
      "loss": 7.3623,
      "step": 1540
    },
    {
      "epoch": 0.4387188612099644,
      "grad_norm": 0.5322418808937073,
      "learning_rate": 0.00039040705949331056,
      "loss": 7.1709,
      "step": 1541
    },
    {
      "epoch": 0.4390035587188612,
      "grad_norm": 0.5205498337745667,
      "learning_rate": 0.0003903358952462283,
      "loss": 7.2744,
      "step": 1542
    },
    {
      "epoch": 0.439288256227758,
      "grad_norm": 0.45532262325286865,
      "learning_rate": 0.00039026473099914606,
      "loss": 7.79,
      "step": 1543
    },
    {
      "epoch": 0.4395729537366548,
      "grad_norm": 0.48682892322540283,
      "learning_rate": 0.00039019356675206373,
      "loss": 7.4336,
      "step": 1544
    },
    {
      "epoch": 0.4398576512455516,
      "grad_norm": 0.579499363899231,
      "learning_rate": 0.0003901224025049815,
      "loss": 7.6016,
      "step": 1545
    },
    {
      "epoch": 0.4401423487544484,
      "grad_norm": 0.7260848879814148,
      "learning_rate": 0.00039005123825789923,
      "loss": 6.5781,
      "step": 1546
    },
    {
      "epoch": 0.4404270462633452,
      "grad_norm": 0.5014261603355408,
      "learning_rate": 0.00038998007401081696,
      "loss": 7.0049,
      "step": 1547
    },
    {
      "epoch": 0.440711743772242,
      "grad_norm": 0.4954442083835602,
      "learning_rate": 0.00038990890976373473,
      "loss": 7.2334,
      "step": 1548
    },
    {
      "epoch": 0.4409964412811388,
      "grad_norm": 0.6689090728759766,
      "learning_rate": 0.00038983774551665246,
      "loss": 7.0127,
      "step": 1549
    },
    {
      "epoch": 0.4412811387900356,
      "grad_norm": 0.4490669369697571,
      "learning_rate": 0.0003897665812695702,
      "loss": 7.6104,
      "step": 1550
    },
    {
      "epoch": 0.44156583629893237,
      "grad_norm": 0.532595694065094,
      "learning_rate": 0.0003896954170224879,
      "loss": 7.668,
      "step": 1551
    },
    {
      "epoch": 0.4418505338078292,
      "grad_norm": 0.4709828495979309,
      "learning_rate": 0.0003896242527754056,
      "loss": 7.4961,
      "step": 1552
    },
    {
      "epoch": 0.442135231316726,
      "grad_norm": 0.40507403016090393,
      "learning_rate": 0.0003895530885283234,
      "loss": 7.627,
      "step": 1553
    },
    {
      "epoch": 0.44241992882562275,
      "grad_norm": 0.6264590620994568,
      "learning_rate": 0.0003894819242812411,
      "loss": 6.8613,
      "step": 1554
    },
    {
      "epoch": 0.44270462633451957,
      "grad_norm": 0.5518805384635925,
      "learning_rate": 0.0003894107600341589,
      "loss": 6.7607,
      "step": 1555
    },
    {
      "epoch": 0.4429893238434164,
      "grad_norm": 0.5427720546722412,
      "learning_rate": 0.00038933959578707657,
      "loss": 7.5371,
      "step": 1556
    },
    {
      "epoch": 0.44327402135231314,
      "grad_norm": 0.533429741859436,
      "learning_rate": 0.0003892684315399943,
      "loss": 7.2744,
      "step": 1557
    },
    {
      "epoch": 0.44355871886120996,
      "grad_norm": 0.4740807116031647,
      "learning_rate": 0.00038919726729291207,
      "loss": 7.752,
      "step": 1558
    },
    {
      "epoch": 0.44384341637010677,
      "grad_norm": 0.613650381565094,
      "learning_rate": 0.0003891261030458298,
      "loss": 6.8564,
      "step": 1559
    },
    {
      "epoch": 0.4441281138790036,
      "grad_norm": 0.49877262115478516,
      "learning_rate": 0.0003890549387987475,
      "loss": 7.0342,
      "step": 1560
    },
    {
      "epoch": 0.44441281138790034,
      "grad_norm": 0.5290449261665344,
      "learning_rate": 0.00038898377455166524,
      "loss": 7.1807,
      "step": 1561
    },
    {
      "epoch": 0.44469750889679716,
      "grad_norm": 0.44291484355926514,
      "learning_rate": 0.00038891261030458296,
      "loss": 7.6875,
      "step": 1562
    },
    {
      "epoch": 0.44498220640569397,
      "grad_norm": 0.5553268194198608,
      "learning_rate": 0.00038884144605750074,
      "loss": 7.2188,
      "step": 1563
    },
    {
      "epoch": 0.44526690391459073,
      "grad_norm": 0.5795010328292847,
      "learning_rate": 0.00038877028181041846,
      "loss": 7.207,
      "step": 1564
    },
    {
      "epoch": 0.44555160142348754,
      "grad_norm": 0.5112793445587158,
      "learning_rate": 0.0003886991175633362,
      "loss": 7.2705,
      "step": 1565
    },
    {
      "epoch": 0.44583629893238436,
      "grad_norm": 0.48628485202789307,
      "learning_rate": 0.00038862795331625396,
      "loss": 8.0352,
      "step": 1566
    },
    {
      "epoch": 0.4461209964412811,
      "grad_norm": 0.48861849308013916,
      "learning_rate": 0.00038855678906917163,
      "loss": 7.5625,
      "step": 1567
    },
    {
      "epoch": 0.44640569395017793,
      "grad_norm": 0.6439377665519714,
      "learning_rate": 0.0003884856248220894,
      "loss": 6.5352,
      "step": 1568
    },
    {
      "epoch": 0.44669039145907474,
      "grad_norm": 0.569162905216217,
      "learning_rate": 0.0003884144605750071,
      "loss": 7.5254,
      "step": 1569
    },
    {
      "epoch": 0.44697508896797156,
      "grad_norm": 0.4747311770915985,
      "learning_rate": 0.00038834329632792485,
      "loss": 7.7129,
      "step": 1570
    },
    {
      "epoch": 0.4472597864768683,
      "grad_norm": 0.600536584854126,
      "learning_rate": 0.0003882721320808426,
      "loss": 7.1709,
      "step": 1571
    },
    {
      "epoch": 0.44754448398576513,
      "grad_norm": 0.54041588306427,
      "learning_rate": 0.00038820096783376035,
      "loss": 7.3242,
      "step": 1572
    },
    {
      "epoch": 0.44782918149466194,
      "grad_norm": 0.6432653665542603,
      "learning_rate": 0.000388129803586678,
      "loss": 6.4092,
      "step": 1573
    },
    {
      "epoch": 0.4481138790035587,
      "grad_norm": 0.49148261547088623,
      "learning_rate": 0.0003880586393395958,
      "loss": 8.0059,
      "step": 1574
    },
    {
      "epoch": 0.4483985765124555,
      "grad_norm": 0.5798601508140564,
      "learning_rate": 0.0003879874750925135,
      "loss": 6.9951,
      "step": 1575
    },
    {
      "epoch": 0.44868327402135233,
      "grad_norm": 0.5091765522956848,
      "learning_rate": 0.0003879163108454313,
      "loss": 7.8105,
      "step": 1576
    },
    {
      "epoch": 0.4489679715302491,
      "grad_norm": 0.5045468211174011,
      "learning_rate": 0.000387845146598349,
      "loss": 7.3867,
      "step": 1577
    },
    {
      "epoch": 0.4492526690391459,
      "grad_norm": 0.49986931681632996,
      "learning_rate": 0.0003877739823512667,
      "loss": 7.3086,
      "step": 1578
    },
    {
      "epoch": 0.4495373665480427,
      "grad_norm": 0.5203457474708557,
      "learning_rate": 0.00038770281810418446,
      "loss": 7.582,
      "step": 1579
    },
    {
      "epoch": 0.4498220640569395,
      "grad_norm": 0.4249582290649414,
      "learning_rate": 0.0003876316538571022,
      "loss": 7.7676,
      "step": 1580
    },
    {
      "epoch": 0.4501067615658363,
      "grad_norm": 0.4739021360874176,
      "learning_rate": 0.00038756048961001996,
      "loss": 7.583,
      "step": 1581
    },
    {
      "epoch": 0.4503914590747331,
      "grad_norm": 0.6087887287139893,
      "learning_rate": 0.0003874893253629377,
      "loss": 6.8906,
      "step": 1582
    },
    {
      "epoch": 0.4506761565836299,
      "grad_norm": 0.550430417060852,
      "learning_rate": 0.0003874181611158554,
      "loss": 7.0439,
      "step": 1583
    },
    {
      "epoch": 0.4509608540925267,
      "grad_norm": 0.4809916615486145,
      "learning_rate": 0.00038734699686877313,
      "loss": 7.4092,
      "step": 1584
    },
    {
      "epoch": 0.4512455516014235,
      "grad_norm": 0.40640443563461304,
      "learning_rate": 0.00038727583262169085,
      "loss": 8.0088,
      "step": 1585
    },
    {
      "epoch": 0.4515302491103203,
      "grad_norm": 0.5101658701896667,
      "learning_rate": 0.00038720466837460863,
      "loss": 7.2812,
      "step": 1586
    },
    {
      "epoch": 0.45181494661921706,
      "grad_norm": 0.46021097898483276,
      "learning_rate": 0.00038713350412752635,
      "loss": 7.6826,
      "step": 1587
    },
    {
      "epoch": 0.4520996441281139,
      "grad_norm": 0.5049157738685608,
      "learning_rate": 0.0003870623398804441,
      "loss": 6.8799,
      "step": 1588
    },
    {
      "epoch": 0.4523843416370107,
      "grad_norm": 0.48057305812835693,
      "learning_rate": 0.0003869911756333618,
      "loss": 7.1172,
      "step": 1589
    },
    {
      "epoch": 0.45266903914590745,
      "grad_norm": 0.5754631161689758,
      "learning_rate": 0.0003869200113862795,
      "loss": 7.0146,
      "step": 1590
    },
    {
      "epoch": 0.45295373665480426,
      "grad_norm": 0.5763919353485107,
      "learning_rate": 0.00038684884713919724,
      "loss": 6.6592,
      "step": 1591
    },
    {
      "epoch": 0.4532384341637011,
      "grad_norm": 0.4379666745662689,
      "learning_rate": 0.000386777682892115,
      "loss": 7.7998,
      "step": 1592
    },
    {
      "epoch": 0.4535231316725979,
      "grad_norm": 0.4938500225543976,
      "learning_rate": 0.00038670651864503274,
      "loss": 7.5752,
      "step": 1593
    },
    {
      "epoch": 0.45380782918149465,
      "grad_norm": 0.5131168961524963,
      "learning_rate": 0.0003866353543979505,
      "loss": 7.5557,
      "step": 1594
    },
    {
      "epoch": 0.45409252669039146,
      "grad_norm": 0.5030447840690613,
      "learning_rate": 0.0003865641901508682,
      "loss": 7.2998,
      "step": 1595
    },
    {
      "epoch": 0.4543772241992883,
      "grad_norm": 0.4757480025291443,
      "learning_rate": 0.0003864930259037859,
      "loss": 7.2979,
      "step": 1596
    },
    {
      "epoch": 0.45466192170818504,
      "grad_norm": 0.495022714138031,
      "learning_rate": 0.0003864218616567037,
      "loss": 7.6387,
      "step": 1597
    },
    {
      "epoch": 0.45494661921708185,
      "grad_norm": 0.5130190253257751,
      "learning_rate": 0.0003863506974096214,
      "loss": 7.1055,
      "step": 1598
    },
    {
      "epoch": 0.45523131672597866,
      "grad_norm": 0.5624128580093384,
      "learning_rate": 0.0003862795331625392,
      "loss": 7.293,
      "step": 1599
    },
    {
      "epoch": 0.4555160142348754,
      "grad_norm": 0.5088523626327515,
      "learning_rate": 0.0003862083689154569,
      "loss": 7.1807,
      "step": 1600
    },
    {
      "epoch": 0.4555160142348754,
      "eval_bleu": 0.10999045970759502,
      "eval_loss": 7.0859375,
      "eval_runtime": 125.4975,
      "eval_samples_per_second": 2.263,
      "eval_steps_per_second": 0.143,
      "step": 1600
    },
    {
      "epoch": 0.45580071174377224,
      "grad_norm": 0.4664638340473175,
      "learning_rate": 0.0003861372046683746,
      "loss": 7.7344,
      "step": 1601
    },
    {
      "epoch": 0.45608540925266905,
      "grad_norm": 0.6145609617233276,
      "learning_rate": 0.00038606604042129236,
      "loss": 7.334,
      "step": 1602
    },
    {
      "epoch": 0.45637010676156586,
      "grad_norm": 0.5187442898750305,
      "learning_rate": 0.0003859948761742101,
      "loss": 7.3242,
      "step": 1603
    },
    {
      "epoch": 0.4566548042704626,
      "grad_norm": 0.5328884720802307,
      "learning_rate": 0.0003859237119271278,
      "loss": 7.293,
      "step": 1604
    },
    {
      "epoch": 0.45693950177935944,
      "grad_norm": 0.5432795286178589,
      "learning_rate": 0.0003858525476800456,
      "loss": 6.9834,
      "step": 1605
    },
    {
      "epoch": 0.45722419928825625,
      "grad_norm": 0.4709322154521942,
      "learning_rate": 0.00038578138343296325,
      "loss": 7.627,
      "step": 1606
    },
    {
      "epoch": 0.457508896797153,
      "grad_norm": 0.7863417267799377,
      "learning_rate": 0.000385710219185881,
      "loss": 6.5352,
      "step": 1607
    },
    {
      "epoch": 0.4577935943060498,
      "grad_norm": 0.4933783710002899,
      "learning_rate": 0.00038563905493879875,
      "loss": 7.1816,
      "step": 1608
    },
    {
      "epoch": 0.45807829181494664,
      "grad_norm": 0.4733075797557831,
      "learning_rate": 0.00038556789069171647,
      "loss": 7.4834,
      "step": 1609
    },
    {
      "epoch": 0.4583629893238434,
      "grad_norm": 0.5065616369247437,
      "learning_rate": 0.00038549672644463425,
      "loss": 7.6025,
      "step": 1610
    },
    {
      "epoch": 0.4586476868327402,
      "grad_norm": 0.5557234287261963,
      "learning_rate": 0.00038542556219755197,
      "loss": 7.5264,
      "step": 1611
    },
    {
      "epoch": 0.458932384341637,
      "grad_norm": 0.5921558737754822,
      "learning_rate": 0.0003853543979504697,
      "loss": 7.2539,
      "step": 1612
    },
    {
      "epoch": 0.4592170818505338,
      "grad_norm": 0.5364576578140259,
      "learning_rate": 0.0003852832337033874,
      "loss": 7.6895,
      "step": 1613
    },
    {
      "epoch": 0.4595017793594306,
      "grad_norm": 0.53724205493927,
      "learning_rate": 0.00038521206945630514,
      "loss": 7.1143,
      "step": 1614
    },
    {
      "epoch": 0.4597864768683274,
      "grad_norm": 0.44680672883987427,
      "learning_rate": 0.0003851409052092229,
      "loss": 7.8203,
      "step": 1615
    },
    {
      "epoch": 0.4600711743772242,
      "grad_norm": 0.4798159599304199,
      "learning_rate": 0.00038506974096214064,
      "loss": 7.5967,
      "step": 1616
    },
    {
      "epoch": 0.460355871886121,
      "grad_norm": 0.5024503469467163,
      "learning_rate": 0.00038499857671505836,
      "loss": 7.6113,
      "step": 1617
    },
    {
      "epoch": 0.4606405693950178,
      "grad_norm": 0.4705354869365692,
      "learning_rate": 0.0003849274124679761,
      "loss": 7.5576,
      "step": 1618
    },
    {
      "epoch": 0.4609252669039146,
      "grad_norm": 0.47796255350112915,
      "learning_rate": 0.0003848562482208938,
      "loss": 7.627,
      "step": 1619
    },
    {
      "epoch": 0.46120996441281137,
      "grad_norm": 0.5365163683891296,
      "learning_rate": 0.0003847850839738116,
      "loss": 7.332,
      "step": 1620
    },
    {
      "epoch": 0.4614946619217082,
      "grad_norm": 0.607476532459259,
      "learning_rate": 0.0003847139197267293,
      "loss": 6.9092,
      "step": 1621
    },
    {
      "epoch": 0.461779359430605,
      "grad_norm": 0.6723403334617615,
      "learning_rate": 0.00038464275547964703,
      "loss": 7.25,
      "step": 1622
    },
    {
      "epoch": 0.46206405693950175,
      "grad_norm": 0.5916773080825806,
      "learning_rate": 0.00038457159123256475,
      "loss": 7.1523,
      "step": 1623
    },
    {
      "epoch": 0.46234875444839857,
      "grad_norm": 0.6792402267456055,
      "learning_rate": 0.0003845004269854825,
      "loss": 6.9072,
      "step": 1624
    },
    {
      "epoch": 0.4626334519572954,
      "grad_norm": 0.5384190678596497,
      "learning_rate": 0.00038442926273840025,
      "loss": 6.8887,
      "step": 1625
    },
    {
      "epoch": 0.4629181494661922,
      "grad_norm": 0.49065250158309937,
      "learning_rate": 0.000384358098491318,
      "loss": 7.7617,
      "step": 1626
    },
    {
      "epoch": 0.46320284697508896,
      "grad_norm": 0.5000590085983276,
      "learning_rate": 0.0003842869342442357,
      "loss": 7.2842,
      "step": 1627
    },
    {
      "epoch": 0.46348754448398577,
      "grad_norm": 0.5557690858840942,
      "learning_rate": 0.0003842157699971535,
      "loss": 7.0381,
      "step": 1628
    },
    {
      "epoch": 0.4637722419928826,
      "grad_norm": 0.5086784362792969,
      "learning_rate": 0.00038414460575007114,
      "loss": 7.4062,
      "step": 1629
    },
    {
      "epoch": 0.46405693950177934,
      "grad_norm": 0.43616223335266113,
      "learning_rate": 0.0003840734415029889,
      "loss": 7.4316,
      "step": 1630
    },
    {
      "epoch": 0.46434163701067616,
      "grad_norm": 0.582670271396637,
      "learning_rate": 0.00038400227725590664,
      "loss": 6.8467,
      "step": 1631
    },
    {
      "epoch": 0.46462633451957297,
      "grad_norm": 0.42955803871154785,
      "learning_rate": 0.00038393111300882437,
      "loss": 7.8594,
      "step": 1632
    },
    {
      "epoch": 0.46491103202846973,
      "grad_norm": 0.45816558599472046,
      "learning_rate": 0.00038385994876174214,
      "loss": 7.2773,
      "step": 1633
    },
    {
      "epoch": 0.46519572953736654,
      "grad_norm": 0.571272075176239,
      "learning_rate": 0.0003837887845146598,
      "loss": 7.5977,
      "step": 1634
    },
    {
      "epoch": 0.46548042704626336,
      "grad_norm": 0.5401399731636047,
      "learning_rate": 0.00038371762026757753,
      "loss": 7.1367,
      "step": 1635
    },
    {
      "epoch": 0.4657651245551601,
      "grad_norm": 0.5665181875228882,
      "learning_rate": 0.0003836464560204953,
      "loss": 6.7285,
      "step": 1636
    },
    {
      "epoch": 0.46604982206405693,
      "grad_norm": 0.4491924047470093,
      "learning_rate": 0.00038357529177341303,
      "loss": 7.5703,
      "step": 1637
    },
    {
      "epoch": 0.46633451957295374,
      "grad_norm": 0.424701452255249,
      "learning_rate": 0.0003835041275263308,
      "loss": 7.5449,
      "step": 1638
    },
    {
      "epoch": 0.46661921708185056,
      "grad_norm": 0.6199308633804321,
      "learning_rate": 0.00038343296327924853,
      "loss": 6.6602,
      "step": 1639
    },
    {
      "epoch": 0.4669039145907473,
      "grad_norm": 0.4524313807487488,
      "learning_rate": 0.0003833617990321662,
      "loss": 7.5479,
      "step": 1640
    },
    {
      "epoch": 0.46718861209964413,
      "grad_norm": 0.5219374895095825,
      "learning_rate": 0.000383290634785084,
      "loss": 7.3516,
      "step": 1641
    },
    {
      "epoch": 0.46747330960854094,
      "grad_norm": 0.5036229491233826,
      "learning_rate": 0.0003832194705380017,
      "loss": 7.2031,
      "step": 1642
    },
    {
      "epoch": 0.4677580071174377,
      "grad_norm": 0.5226691961288452,
      "learning_rate": 0.0003831483062909195,
      "loss": 7.0127,
      "step": 1643
    },
    {
      "epoch": 0.4680427046263345,
      "grad_norm": 0.47675246000289917,
      "learning_rate": 0.0003830771420438372,
      "loss": 7.7695,
      "step": 1644
    },
    {
      "epoch": 0.46832740213523133,
      "grad_norm": 0.5894948840141296,
      "learning_rate": 0.0003830059777967549,
      "loss": 6.9688,
      "step": 1645
    },
    {
      "epoch": 0.4686120996441281,
      "grad_norm": 0.4946599304676056,
      "learning_rate": 0.00038293481354967265,
      "loss": 6.9648,
      "step": 1646
    },
    {
      "epoch": 0.4688967971530249,
      "grad_norm": 0.49283403158187866,
      "learning_rate": 0.00038286364930259037,
      "loss": 7.6992,
      "step": 1647
    },
    {
      "epoch": 0.4691814946619217,
      "grad_norm": 0.494332492351532,
      "learning_rate": 0.00038279248505550815,
      "loss": 7.5107,
      "step": 1648
    },
    {
      "epoch": 0.46946619217081853,
      "grad_norm": 0.39411231875419617,
      "learning_rate": 0.00038272132080842587,
      "loss": 7.6553,
      "step": 1649
    },
    {
      "epoch": 0.4697508896797153,
      "grad_norm": 0.5392952561378479,
      "learning_rate": 0.0003826501565613436,
      "loss": 7.2207,
      "step": 1650
    },
    {
      "epoch": 0.4700355871886121,
      "grad_norm": 0.5077725052833557,
      "learning_rate": 0.0003825789923142613,
      "loss": 7.1738,
      "step": 1651
    },
    {
      "epoch": 0.4703202846975089,
      "grad_norm": 0.4558878242969513,
      "learning_rate": 0.00038250782806717904,
      "loss": 7.7305,
      "step": 1652
    },
    {
      "epoch": 0.4706049822064057,
      "grad_norm": 0.5433383584022522,
      "learning_rate": 0.00038243666382009676,
      "loss": 6.9561,
      "step": 1653
    },
    {
      "epoch": 0.4708896797153025,
      "grad_norm": 0.5727225542068481,
      "learning_rate": 0.00038236549957301454,
      "loss": 7.082,
      "step": 1654
    },
    {
      "epoch": 0.4711743772241993,
      "grad_norm": 0.5149667859077454,
      "learning_rate": 0.00038229433532593226,
      "loss": 7.3779,
      "step": 1655
    },
    {
      "epoch": 0.47145907473309606,
      "grad_norm": 0.5559431314468384,
      "learning_rate": 0.00038222317107885004,
      "loss": 6.8467,
      "step": 1656
    },
    {
      "epoch": 0.4717437722419929,
      "grad_norm": 0.5958291888237,
      "learning_rate": 0.0003821520068317677,
      "loss": 7.0537,
      "step": 1657
    },
    {
      "epoch": 0.4720284697508897,
      "grad_norm": 0.3918918967247009,
      "learning_rate": 0.00038208084258468543,
      "loss": 7.9854,
      "step": 1658
    },
    {
      "epoch": 0.4723131672597865,
      "grad_norm": 0.4432237148284912,
      "learning_rate": 0.0003820096783376032,
      "loss": 7.4531,
      "step": 1659
    },
    {
      "epoch": 0.47259786476868326,
      "grad_norm": 0.5828916430473328,
      "learning_rate": 0.00038193851409052093,
      "loss": 7.4658,
      "step": 1660
    },
    {
      "epoch": 0.4728825622775801,
      "grad_norm": 0.5251741409301758,
      "learning_rate": 0.0003818673498434387,
      "loss": 7.6777,
      "step": 1661
    },
    {
      "epoch": 0.4731672597864769,
      "grad_norm": 0.5652466416358948,
      "learning_rate": 0.0003817961855963564,
      "loss": 7.1152,
      "step": 1662
    },
    {
      "epoch": 0.47345195729537365,
      "grad_norm": 0.43186524510383606,
      "learning_rate": 0.0003817250213492741,
      "loss": 7.9424,
      "step": 1663
    },
    {
      "epoch": 0.47373665480427046,
      "grad_norm": 0.4190675914287567,
      "learning_rate": 0.0003816538571021919,
      "loss": 7.9873,
      "step": 1664
    },
    {
      "epoch": 0.4740213523131673,
      "grad_norm": 0.35119152069091797,
      "learning_rate": 0.0003815826928551096,
      "loss": 8.0195,
      "step": 1665
    },
    {
      "epoch": 0.47430604982206404,
      "grad_norm": 0.5248192548751831,
      "learning_rate": 0.00038151152860802737,
      "loss": 7.5029,
      "step": 1666
    },
    {
      "epoch": 0.47459074733096085,
      "grad_norm": 0.5171993374824524,
      "learning_rate": 0.0003814403643609451,
      "loss": 7.8926,
      "step": 1667
    },
    {
      "epoch": 0.47487544483985766,
      "grad_norm": 0.5714148879051208,
      "learning_rate": 0.00038136920011386276,
      "loss": 6.8262,
      "step": 1668
    },
    {
      "epoch": 0.4751601423487544,
      "grad_norm": 0.5456072092056274,
      "learning_rate": 0.00038129803586678054,
      "loss": 6.9072,
      "step": 1669
    },
    {
      "epoch": 0.47544483985765124,
      "grad_norm": 0.6678564548492432,
      "learning_rate": 0.00038122687161969826,
      "loss": 6.8818,
      "step": 1670
    },
    {
      "epoch": 0.47572953736654805,
      "grad_norm": 0.4060457646846771,
      "learning_rate": 0.000381155707372616,
      "loss": 7.6504,
      "step": 1671
    },
    {
      "epoch": 0.47601423487544486,
      "grad_norm": 0.5769488215446472,
      "learning_rate": 0.00038108454312553376,
      "loss": 7.1602,
      "step": 1672
    },
    {
      "epoch": 0.4762989323843416,
      "grad_norm": 0.5093549489974976,
      "learning_rate": 0.0003810133788784515,
      "loss": 7.5371,
      "step": 1673
    },
    {
      "epoch": 0.47658362989323844,
      "grad_norm": 0.5345686674118042,
      "learning_rate": 0.0003809422146313692,
      "loss": 7.4883,
      "step": 1674
    },
    {
      "epoch": 0.47686832740213525,
      "grad_norm": 0.6932775378227234,
      "learning_rate": 0.00038087105038428693,
      "loss": 6.6279,
      "step": 1675
    },
    {
      "epoch": 0.477153024911032,
      "grad_norm": 0.5208515524864197,
      "learning_rate": 0.00038079988613720465,
      "loss": 7.4189,
      "step": 1676
    },
    {
      "epoch": 0.4774377224199288,
      "grad_norm": 0.6215210556983948,
      "learning_rate": 0.00038072872189012243,
      "loss": 6.8779,
      "step": 1677
    },
    {
      "epoch": 0.47772241992882564,
      "grad_norm": 0.48564231395721436,
      "learning_rate": 0.00038065755764304015,
      "loss": 7.7676,
      "step": 1678
    },
    {
      "epoch": 0.4780071174377224,
      "grad_norm": 0.5211162567138672,
      "learning_rate": 0.0003805863933959579,
      "loss": 7.375,
      "step": 1679
    },
    {
      "epoch": 0.4782918149466192,
      "grad_norm": 0.634146511554718,
      "learning_rate": 0.0003805152291488756,
      "loss": 6.2031,
      "step": 1680
    },
    {
      "epoch": 0.478576512455516,
      "grad_norm": 0.57793790102005,
      "learning_rate": 0.0003804440649017933,
      "loss": 7.4961,
      "step": 1681
    },
    {
      "epoch": 0.47886120996441284,
      "grad_norm": 0.613333523273468,
      "learning_rate": 0.0003803729006547111,
      "loss": 6.4297,
      "step": 1682
    },
    {
      "epoch": 0.4791459074733096,
      "grad_norm": 0.5153689384460449,
      "learning_rate": 0.0003803017364076288,
      "loss": 7.415,
      "step": 1683
    },
    {
      "epoch": 0.4794306049822064,
      "grad_norm": 0.4538441300392151,
      "learning_rate": 0.0003802305721605466,
      "loss": 7.1904,
      "step": 1684
    },
    {
      "epoch": 0.4797153024911032,
      "grad_norm": 0.5165327191352844,
      "learning_rate": 0.00038015940791346427,
      "loss": 7.7002,
      "step": 1685
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.45390772819519043,
      "learning_rate": 0.000380088243666382,
      "loss": 7.4102,
      "step": 1686
    },
    {
      "epoch": 0.4802846975088968,
      "grad_norm": 0.502204179763794,
      "learning_rate": 0.00038001707941929977,
      "loss": 7.25,
      "step": 1687
    },
    {
      "epoch": 0.4805693950177936,
      "grad_norm": 0.5534747838973999,
      "learning_rate": 0.0003799459151722175,
      "loss": 7.2061,
      "step": 1688
    },
    {
      "epoch": 0.48085409252669037,
      "grad_norm": 0.5134608149528503,
      "learning_rate": 0.0003798747509251352,
      "loss": 7.4482,
      "step": 1689
    },
    {
      "epoch": 0.4811387900355872,
      "grad_norm": 0.5647716522216797,
      "learning_rate": 0.000379803586678053,
      "loss": 7.4502,
      "step": 1690
    },
    {
      "epoch": 0.481423487544484,
      "grad_norm": 0.5849151611328125,
      "learning_rate": 0.00037973242243097066,
      "loss": 7.2373,
      "step": 1691
    },
    {
      "epoch": 0.48170818505338076,
      "grad_norm": 0.5531435608863831,
      "learning_rate": 0.00037966125818388844,
      "loss": 6.8877,
      "step": 1692
    },
    {
      "epoch": 0.48199288256227757,
      "grad_norm": 0.4758262634277344,
      "learning_rate": 0.00037959009393680616,
      "loss": 7.3408,
      "step": 1693
    },
    {
      "epoch": 0.4822775800711744,
      "grad_norm": 0.5585567951202393,
      "learning_rate": 0.0003795189296897239,
      "loss": 7.0674,
      "step": 1694
    },
    {
      "epoch": 0.4825622775800712,
      "grad_norm": 0.463369756937027,
      "learning_rate": 0.00037944776544264166,
      "loss": 7.5537,
      "step": 1695
    },
    {
      "epoch": 0.48284697508896796,
      "grad_norm": 0.5086468458175659,
      "learning_rate": 0.0003793766011955593,
      "loss": 7.6162,
      "step": 1696
    },
    {
      "epoch": 0.48313167259786477,
      "grad_norm": 0.5536462068557739,
      "learning_rate": 0.0003793054369484771,
      "loss": 7.332,
      "step": 1697
    },
    {
      "epoch": 0.4834163701067616,
      "grad_norm": 0.4514314532279968,
      "learning_rate": 0.0003792342727013948,
      "loss": 7.6406,
      "step": 1698
    },
    {
      "epoch": 0.48370106761565834,
      "grad_norm": 0.4876021444797516,
      "learning_rate": 0.00037916310845431255,
      "loss": 7.4336,
      "step": 1699
    },
    {
      "epoch": 0.48398576512455516,
      "grad_norm": 0.5244501829147339,
      "learning_rate": 0.0003790919442072303,
      "loss": 7.7041,
      "step": 1700
    },
    {
      "epoch": 0.48427046263345197,
      "grad_norm": 0.6171659827232361,
      "learning_rate": 0.00037902077996014805,
      "loss": 7.0674,
      "step": 1701
    },
    {
      "epoch": 0.48455516014234873,
      "grad_norm": 0.5354804992675781,
      "learning_rate": 0.0003789496157130657,
      "loss": 7.0459,
      "step": 1702
    },
    {
      "epoch": 0.48483985765124554,
      "grad_norm": 0.49222564697265625,
      "learning_rate": 0.0003788784514659835,
      "loss": 7.5684,
      "step": 1703
    },
    {
      "epoch": 0.48512455516014236,
      "grad_norm": 0.5722427368164062,
      "learning_rate": 0.0003788072872189012,
      "loss": 6.9424,
      "step": 1704
    },
    {
      "epoch": 0.48540925266903917,
      "grad_norm": 0.3971741497516632,
      "learning_rate": 0.000378736122971819,
      "loss": 7.8623,
      "step": 1705
    },
    {
      "epoch": 0.48569395017793593,
      "grad_norm": 0.5367334485054016,
      "learning_rate": 0.0003786649587247367,
      "loss": 7.6836,
      "step": 1706
    },
    {
      "epoch": 0.48597864768683274,
      "grad_norm": 0.458075612783432,
      "learning_rate": 0.0003785937944776544,
      "loss": 7.7031,
      "step": 1707
    },
    {
      "epoch": 0.48626334519572956,
      "grad_norm": 0.611355721950531,
      "learning_rate": 0.00037852263023057216,
      "loss": 7.7764,
      "step": 1708
    },
    {
      "epoch": 0.4865480427046263,
      "grad_norm": 0.5967645645141602,
      "learning_rate": 0.0003784514659834899,
      "loss": 6.9072,
      "step": 1709
    },
    {
      "epoch": 0.48683274021352313,
      "grad_norm": 0.5037733316421509,
      "learning_rate": 0.00037838030173640766,
      "loss": 7.5635,
      "step": 1710
    },
    {
      "epoch": 0.48711743772241994,
      "grad_norm": 0.5649775862693787,
      "learning_rate": 0.0003783091374893254,
      "loss": 7.2119,
      "step": 1711
    },
    {
      "epoch": 0.4874021352313167,
      "grad_norm": 0.6592603325843811,
      "learning_rate": 0.0003782379732422431,
      "loss": 7.0742,
      "step": 1712
    },
    {
      "epoch": 0.4876868327402135,
      "grad_norm": 0.41307011246681213,
      "learning_rate": 0.00037816680899516083,
      "loss": 7.9648,
      "step": 1713
    },
    {
      "epoch": 0.48797153024911033,
      "grad_norm": 0.5472896099090576,
      "learning_rate": 0.00037809564474807855,
      "loss": 7.0195,
      "step": 1714
    },
    {
      "epoch": 0.48825622775800714,
      "grad_norm": 0.5462066531181335,
      "learning_rate": 0.00037802448050099633,
      "loss": 7.6846,
      "step": 1715
    },
    {
      "epoch": 0.4885409252669039,
      "grad_norm": 0.5576370358467102,
      "learning_rate": 0.00037795331625391405,
      "loss": 7.0869,
      "step": 1716
    },
    {
      "epoch": 0.4888256227758007,
      "grad_norm": 0.500810980796814,
      "learning_rate": 0.0003778821520068318,
      "loss": 7.4414,
      "step": 1717
    },
    {
      "epoch": 0.48911032028469753,
      "grad_norm": 0.48971080780029297,
      "learning_rate": 0.00037781098775974955,
      "loss": 7.5107,
      "step": 1718
    },
    {
      "epoch": 0.4893950177935943,
      "grad_norm": 0.566116452217102,
      "learning_rate": 0.0003777398235126672,
      "loss": 7.4121,
      "step": 1719
    },
    {
      "epoch": 0.4896797153024911,
      "grad_norm": 0.42671483755111694,
      "learning_rate": 0.00037766865926558494,
      "loss": 7.6367,
      "step": 1720
    },
    {
      "epoch": 0.4899644128113879,
      "grad_norm": 0.590766966342926,
      "learning_rate": 0.0003775974950185027,
      "loss": 7.1406,
      "step": 1721
    },
    {
      "epoch": 0.4902491103202847,
      "grad_norm": 0.5169826149940491,
      "learning_rate": 0.00037752633077142044,
      "loss": 7.3428,
      "step": 1722
    },
    {
      "epoch": 0.4905338078291815,
      "grad_norm": 0.6001588702201843,
      "learning_rate": 0.0003774551665243382,
      "loss": 7.1094,
      "step": 1723
    },
    {
      "epoch": 0.4908185053380783,
      "grad_norm": 0.5002423524856567,
      "learning_rate": 0.0003773840022772559,
      "loss": 7.2109,
      "step": 1724
    },
    {
      "epoch": 0.49110320284697506,
      "grad_norm": 0.5491111278533936,
      "learning_rate": 0.0003773128380301736,
      "loss": 6.79,
      "step": 1725
    },
    {
      "epoch": 0.4913879003558719,
      "grad_norm": 0.5625193119049072,
      "learning_rate": 0.0003772416737830914,
      "loss": 7.3906,
      "step": 1726
    },
    {
      "epoch": 0.4916725978647687,
      "grad_norm": 0.522409200668335,
      "learning_rate": 0.0003771705095360091,
      "loss": 7.3896,
      "step": 1727
    },
    {
      "epoch": 0.4919572953736655,
      "grad_norm": 0.5947089791297913,
      "learning_rate": 0.0003770993452889269,
      "loss": 7.7861,
      "step": 1728
    },
    {
      "epoch": 0.49224199288256226,
      "grad_norm": 0.441991925239563,
      "learning_rate": 0.0003770281810418446,
      "loss": 7.9551,
      "step": 1729
    },
    {
      "epoch": 0.4925266903914591,
      "grad_norm": 0.49497219920158386,
      "learning_rate": 0.0003769570167947623,
      "loss": 7.6113,
      "step": 1730
    },
    {
      "epoch": 0.4928113879003559,
      "grad_norm": 0.4308784306049347,
      "learning_rate": 0.00037688585254768006,
      "loss": 7.9043,
      "step": 1731
    },
    {
      "epoch": 0.49309608540925265,
      "grad_norm": 0.5549896955490112,
      "learning_rate": 0.0003768146883005978,
      "loss": 7.252,
      "step": 1732
    },
    {
      "epoch": 0.49338078291814946,
      "grad_norm": 0.6144887804985046,
      "learning_rate": 0.00037674352405351556,
      "loss": 7.1465,
      "step": 1733
    },
    {
      "epoch": 0.4936654804270463,
      "grad_norm": 0.6344668865203857,
      "learning_rate": 0.0003766723598064333,
      "loss": 6.4443,
      "step": 1734
    },
    {
      "epoch": 0.49395017793594304,
      "grad_norm": 0.5163561105728149,
      "learning_rate": 0.000376601195559351,
      "loss": 7.4385,
      "step": 1735
    },
    {
      "epoch": 0.49423487544483985,
      "grad_norm": 0.511461615562439,
      "learning_rate": 0.0003765300313122687,
      "loss": 7.7744,
      "step": 1736
    },
    {
      "epoch": 0.49451957295373666,
      "grad_norm": 0.5111595392227173,
      "learning_rate": 0.00037645886706518645,
      "loss": 7.3789,
      "step": 1737
    },
    {
      "epoch": 0.4948042704626335,
      "grad_norm": 0.49170342087745667,
      "learning_rate": 0.00037638770281810417,
      "loss": 7.4453,
      "step": 1738
    },
    {
      "epoch": 0.49508896797153024,
      "grad_norm": 0.508344292640686,
      "learning_rate": 0.00037631653857102195,
      "loss": 7.5684,
      "step": 1739
    },
    {
      "epoch": 0.49537366548042705,
      "grad_norm": 0.511880099773407,
      "learning_rate": 0.00037624537432393967,
      "loss": 7.5498,
      "step": 1740
    },
    {
      "epoch": 0.49565836298932386,
      "grad_norm": 0.6036074757575989,
      "learning_rate": 0.0003761742100768574,
      "loss": 6.7861,
      "step": 1741
    },
    {
      "epoch": 0.4959430604982206,
      "grad_norm": 0.5811192989349365,
      "learning_rate": 0.0003761030458297751,
      "loss": 6.9082,
      "step": 1742
    },
    {
      "epoch": 0.49622775800711744,
      "grad_norm": 0.5931008458137512,
      "learning_rate": 0.00037603188158269284,
      "loss": 6.9551,
      "step": 1743
    },
    {
      "epoch": 0.49651245551601425,
      "grad_norm": 0.5401119589805603,
      "learning_rate": 0.0003759607173356106,
      "loss": 7.4736,
      "step": 1744
    },
    {
      "epoch": 0.496797153024911,
      "grad_norm": 0.4910326898097992,
      "learning_rate": 0.00037588955308852834,
      "loss": 7.3223,
      "step": 1745
    },
    {
      "epoch": 0.4970818505338078,
      "grad_norm": 0.5661277770996094,
      "learning_rate": 0.0003758183888414461,
      "loss": 7.1172,
      "step": 1746
    },
    {
      "epoch": 0.49736654804270464,
      "grad_norm": 0.5483824014663696,
      "learning_rate": 0.0003757472245943638,
      "loss": 7.3525,
      "step": 1747
    },
    {
      "epoch": 0.49765124555160145,
      "grad_norm": 0.5940328240394592,
      "learning_rate": 0.0003756760603472815,
      "loss": 7.2393,
      "step": 1748
    },
    {
      "epoch": 0.4979359430604982,
      "grad_norm": 0.6712566614151001,
      "learning_rate": 0.0003756048961001993,
      "loss": 6.5723,
      "step": 1749
    },
    {
      "epoch": 0.498220640569395,
      "grad_norm": 0.5339024066925049,
      "learning_rate": 0.000375533731853117,
      "loss": 7.2012,
      "step": 1750
    },
    {
      "epoch": 0.49850533807829184,
      "grad_norm": 0.492465615272522,
      "learning_rate": 0.00037546256760603473,
      "loss": 7.5957,
      "step": 1751
    },
    {
      "epoch": 0.4987900355871886,
      "grad_norm": 0.5669850707054138,
      "learning_rate": 0.00037539140335895245,
      "loss": 6.9922,
      "step": 1752
    },
    {
      "epoch": 0.4990747330960854,
      "grad_norm": 0.5506566166877747,
      "learning_rate": 0.0003753202391118702,
      "loss": 7.2881,
      "step": 1753
    },
    {
      "epoch": 0.4993594306049822,
      "grad_norm": 0.45686304569244385,
      "learning_rate": 0.00037524907486478795,
      "loss": 7.4141,
      "step": 1754
    },
    {
      "epoch": 0.499644128113879,
      "grad_norm": 0.5495540499687195,
      "learning_rate": 0.0003751779106177057,
      "loss": 7.1133,
      "step": 1755
    },
    {
      "epoch": 0.4999288256227758,
      "grad_norm": 0.5432637333869934,
      "learning_rate": 0.0003751067463706234,
      "loss": 7.041,
      "step": 1756
    },
    {
      "epoch": 0.5002135231316726,
      "grad_norm": 0.4298688769340515,
      "learning_rate": 0.0003750355821235412,
      "loss": 7.498,
      "step": 1757
    },
    {
      "epoch": 0.5004982206405694,
      "grad_norm": 0.5609787106513977,
      "learning_rate": 0.00037496441787645884,
      "loss": 7.1523,
      "step": 1758
    },
    {
      "epoch": 0.5007829181494662,
      "grad_norm": 0.6223969459533691,
      "learning_rate": 0.0003748932536293766,
      "loss": 6.4463,
      "step": 1759
    },
    {
      "epoch": 0.501067615658363,
      "grad_norm": 0.6767446398735046,
      "learning_rate": 0.00037482208938229434,
      "loss": 6.4258,
      "step": 1760
    },
    {
      "epoch": 0.5013523131672598,
      "grad_norm": 0.5012683868408203,
      "learning_rate": 0.00037475092513521206,
      "loss": 7.6631,
      "step": 1761
    },
    {
      "epoch": 0.5016370106761566,
      "grad_norm": 0.545363187789917,
      "learning_rate": 0.00037467976088812984,
      "loss": 7.1758,
      "step": 1762
    },
    {
      "epoch": 0.5019217081850533,
      "grad_norm": 0.5195574760437012,
      "learning_rate": 0.00037460859664104756,
      "loss": 7.4082,
      "step": 1763
    },
    {
      "epoch": 0.5022064056939501,
      "grad_norm": 0.6033876538276672,
      "learning_rate": 0.0003745374323939653,
      "loss": 6.9688,
      "step": 1764
    },
    {
      "epoch": 0.502491103202847,
      "grad_norm": 0.5066131353378296,
      "learning_rate": 0.000374466268146883,
      "loss": 7.2031,
      "step": 1765
    },
    {
      "epoch": 0.5027758007117438,
      "grad_norm": 0.5280950665473938,
      "learning_rate": 0.00037439510389980073,
      "loss": 7.5674,
      "step": 1766
    },
    {
      "epoch": 0.5030604982206406,
      "grad_norm": 0.4802970886230469,
      "learning_rate": 0.0003743239396527185,
      "loss": 7.6758,
      "step": 1767
    },
    {
      "epoch": 0.5033451957295374,
      "grad_norm": 0.4784156084060669,
      "learning_rate": 0.00037425277540563623,
      "loss": 7.4297,
      "step": 1768
    },
    {
      "epoch": 0.5036298932384342,
      "grad_norm": 0.6867071986198425,
      "learning_rate": 0.0003741816111585539,
      "loss": 6.541,
      "step": 1769
    },
    {
      "epoch": 0.5039145907473309,
      "grad_norm": 0.5525267720222473,
      "learning_rate": 0.0003741104469114717,
      "loss": 7.4209,
      "step": 1770
    },
    {
      "epoch": 0.5041992882562277,
      "grad_norm": 0.44265449047088623,
      "learning_rate": 0.0003740392826643894,
      "loss": 7.5107,
      "step": 1771
    },
    {
      "epoch": 0.5044839857651245,
      "grad_norm": 0.4428139925003052,
      "learning_rate": 0.0003739681184173072,
      "loss": 7.5918,
      "step": 1772
    },
    {
      "epoch": 0.5047686832740214,
      "grad_norm": 0.6008185744285583,
      "learning_rate": 0.0003738969541702249,
      "loss": 7.0234,
      "step": 1773
    },
    {
      "epoch": 0.5050533807829182,
      "grad_norm": 0.5214946866035461,
      "learning_rate": 0.0003738257899231426,
      "loss": 7.1064,
      "step": 1774
    },
    {
      "epoch": 0.505338078291815,
      "grad_norm": 0.49460625648498535,
      "learning_rate": 0.00037375462567606035,
      "loss": 7.6562,
      "step": 1775
    },
    {
      "epoch": 0.5056227758007118,
      "grad_norm": 0.5446761250495911,
      "learning_rate": 0.00037368346142897807,
      "loss": 7.3613,
      "step": 1776
    },
    {
      "epoch": 0.5059074733096085,
      "grad_norm": 0.5687732100486755,
      "learning_rate": 0.00037361229718189585,
      "loss": 6.4512,
      "step": 1777
    },
    {
      "epoch": 0.5061921708185053,
      "grad_norm": 0.5303898453712463,
      "learning_rate": 0.00037354113293481357,
      "loss": 7.7227,
      "step": 1778
    },
    {
      "epoch": 0.5064768683274021,
      "grad_norm": 0.4860377013683319,
      "learning_rate": 0.0003734699686877313,
      "loss": 7.4395,
      "step": 1779
    },
    {
      "epoch": 0.5067615658362989,
      "grad_norm": 0.4877166748046875,
      "learning_rate": 0.000373398804440649,
      "loss": 7.7168,
      "step": 1780
    },
    {
      "epoch": 0.5070462633451958,
      "grad_norm": 0.6578726768493652,
      "learning_rate": 0.00037332764019356674,
      "loss": 7.2344,
      "step": 1781
    },
    {
      "epoch": 0.5073309608540926,
      "grad_norm": 0.436906635761261,
      "learning_rate": 0.00037325647594648446,
      "loss": 7.8311,
      "step": 1782
    },
    {
      "epoch": 0.5076156583629893,
      "grad_norm": 0.5282929539680481,
      "learning_rate": 0.00037318531169940224,
      "loss": 7.1514,
      "step": 1783
    },
    {
      "epoch": 0.5079003558718861,
      "grad_norm": 0.5723035335540771,
      "learning_rate": 0.00037311414745231996,
      "loss": 7.4482,
      "step": 1784
    },
    {
      "epoch": 0.5081850533807829,
      "grad_norm": 0.5254671573638916,
      "learning_rate": 0.00037304298320523774,
      "loss": 7.542,
      "step": 1785
    },
    {
      "epoch": 0.5084697508896797,
      "grad_norm": 0.506375789642334,
      "learning_rate": 0.0003729718189581554,
      "loss": 7.1572,
      "step": 1786
    },
    {
      "epoch": 0.5087544483985765,
      "grad_norm": 0.5270366668701172,
      "learning_rate": 0.00037290065471107313,
      "loss": 7.7188,
      "step": 1787
    },
    {
      "epoch": 0.5090391459074733,
      "grad_norm": 0.43990278244018555,
      "learning_rate": 0.0003728294904639909,
      "loss": 7.5264,
      "step": 1788
    },
    {
      "epoch": 0.5093238434163702,
      "grad_norm": 0.5385646224021912,
      "learning_rate": 0.0003727583262169086,
      "loss": 7.2461,
      "step": 1789
    },
    {
      "epoch": 0.5096085409252669,
      "grad_norm": 0.3960351347923279,
      "learning_rate": 0.0003726871619698264,
      "loss": 7.8682,
      "step": 1790
    },
    {
      "epoch": 0.5098932384341637,
      "grad_norm": 0.5219964385032654,
      "learning_rate": 0.0003726159977227441,
      "loss": 7.375,
      "step": 1791
    },
    {
      "epoch": 0.5101779359430605,
      "grad_norm": 0.5305919647216797,
      "learning_rate": 0.0003725448334756618,
      "loss": 7.4053,
      "step": 1792
    },
    {
      "epoch": 0.5104626334519573,
      "grad_norm": 0.5393206477165222,
      "learning_rate": 0.00037247366922857957,
      "loss": 7.4043,
      "step": 1793
    },
    {
      "epoch": 0.5107473309608541,
      "grad_norm": 0.5216227173805237,
      "learning_rate": 0.0003724025049814973,
      "loss": 7.3125,
      "step": 1794
    },
    {
      "epoch": 0.5110320284697509,
      "grad_norm": 0.4426933526992798,
      "learning_rate": 0.00037233134073441507,
      "loss": 7.4648,
      "step": 1795
    },
    {
      "epoch": 0.5113167259786476,
      "grad_norm": 0.5463060736656189,
      "learning_rate": 0.0003722601764873328,
      "loss": 7.6865,
      "step": 1796
    },
    {
      "epoch": 0.5116014234875444,
      "grad_norm": 0.41957664489746094,
      "learning_rate": 0.00037218901224025046,
      "loss": 7.5635,
      "step": 1797
    },
    {
      "epoch": 0.5118861209964413,
      "grad_norm": 0.5480870008468628,
      "learning_rate": 0.00037211784799316824,
      "loss": 6.6904,
      "step": 1798
    },
    {
      "epoch": 0.5121708185053381,
      "grad_norm": 0.6035405397415161,
      "learning_rate": 0.00037204668374608596,
      "loss": 5.7183,
      "step": 1799
    },
    {
      "epoch": 0.5124555160142349,
      "grad_norm": 0.5898758769035339,
      "learning_rate": 0.0003719755194990037,
      "loss": 6.9951,
      "step": 1800
    },
    {
      "epoch": 0.5124555160142349,
      "eval_bleu": 0.10834418317524194,
      "eval_loss": 7.09765625,
      "eval_runtime": 150.1062,
      "eval_samples_per_second": 1.892,
      "eval_steps_per_second": 0.12,
      "step": 1800
    }
  ],
  "logging_steps": 1,
  "max_steps": 7026,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 200,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 3920357228544000.0,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
