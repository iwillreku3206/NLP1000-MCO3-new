{
  "best_global_step": 400,
  "best_metric": 7.00390625,
  "best_model_checkpoint": "trained-nllb-en-to-bicol\\checkpoint-400",
  "epoch": 0.3416370106761566,
  "eval_steps": 200,
  "global_step": 1200,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.00028469750889679714,
      "grad_norm": 0.13050267100334167,
      "learning_rate": 0.0005,
      "loss": 11.8242,
      "step": 1
    },
    {
      "epoch": 0.0005693950177935943,
      "grad_norm": 0.16353872418403625,
      "learning_rate": 0.0004999288357529178,
      "loss": 12.7012,
      "step": 2
    },
    {
      "epoch": 0.0008540925266903915,
      "grad_norm": 0.1825779378414154,
      "learning_rate": 0.0004998576715058355,
      "loss": 11.4883,
      "step": 3
    },
    {
      "epoch": 0.0011387900355871886,
      "grad_norm": 0.2419777810573578,
      "learning_rate": 0.0004997865072587532,
      "loss": 11.9531,
      "step": 4
    },
    {
      "epoch": 0.0014234875444839859,
      "grad_norm": 0.31029146909713745,
      "learning_rate": 0.0004997153430116709,
      "loss": 11.7852,
      "step": 5
    },
    {
      "epoch": 0.001708185053380783,
      "grad_norm": 0.3981788754463196,
      "learning_rate": 0.0004996441787645887,
      "loss": 11.8242,
      "step": 6
    },
    {
      "epoch": 0.00199288256227758,
      "grad_norm": 0.44912195205688477,
      "learning_rate": 0.0004995730145175065,
      "loss": 11.1836,
      "step": 7
    },
    {
      "epoch": 0.002277580071174377,
      "grad_norm": 0.5424309372901917,
      "learning_rate": 0.0004995018502704241,
      "loss": 11.6504,
      "step": 8
    },
    {
      "epoch": 0.002562277580071174,
      "grad_norm": 0.6373918056488037,
      "learning_rate": 0.0004994306860233419,
      "loss": 11.3926,
      "step": 9
    },
    {
      "epoch": 0.0028469750889679717,
      "grad_norm": 0.7767484188079834,
      "learning_rate": 0.0004993595217762596,
      "loss": 11.5098,
      "step": 10
    },
    {
      "epoch": 0.003131672597864769,
      "grad_norm": 0.7795925736427307,
      "learning_rate": 0.0004992883575291773,
      "loss": 12.1816,
      "step": 11
    },
    {
      "epoch": 0.003416370106761566,
      "grad_norm": 0.5827488303184509,
      "learning_rate": 0.0004992171932820951,
      "loss": 11.4355,
      "step": 12
    },
    {
      "epoch": 0.003701067615658363,
      "grad_norm": 0.5459573864936829,
      "learning_rate": 0.0004991460290350128,
      "loss": 11.0156,
      "step": 13
    },
    {
      "epoch": 0.00398576512455516,
      "grad_norm": 0.513734757900238,
      "learning_rate": 0.0004990748647879306,
      "loss": 11.6016,
      "step": 14
    },
    {
      "epoch": 0.004270462633451958,
      "grad_norm": 0.5204881429672241,
      "learning_rate": 0.0004990037005408483,
      "loss": 11.1738,
      "step": 15
    },
    {
      "epoch": 0.004555160142348754,
      "grad_norm": 0.5095188021659851,
      "learning_rate": 0.000498932536293766,
      "loss": 11.6738,
      "step": 16
    },
    {
      "epoch": 0.004839857651245552,
      "grad_norm": 0.5002894997596741,
      "learning_rate": 0.0004988613720466838,
      "loss": 11.1172,
      "step": 17
    },
    {
      "epoch": 0.005124555160142348,
      "grad_norm": 0.5175882577896118,
      "learning_rate": 0.0004987902077996015,
      "loss": 11.709,
      "step": 18
    },
    {
      "epoch": 0.005409252669039146,
      "grad_norm": 0.481122225522995,
      "learning_rate": 0.0004987190435525192,
      "loss": 11.2383,
      "step": 19
    },
    {
      "epoch": 0.0056939501779359435,
      "grad_norm": 0.4916481077671051,
      "learning_rate": 0.000498647879305437,
      "loss": 11.1875,
      "step": 20
    },
    {
      "epoch": 0.00597864768683274,
      "grad_norm": 0.4639431834220886,
      "learning_rate": 0.0004985767150583547,
      "loss": 10.959,
      "step": 21
    },
    {
      "epoch": 0.006263345195729538,
      "grad_norm": 0.471029669046402,
      "learning_rate": 0.0004985055508112725,
      "loss": 10.5039,
      "step": 22
    },
    {
      "epoch": 0.006548042704626334,
      "grad_norm": 0.5494607090950012,
      "learning_rate": 0.0004984343865641901,
      "loss": 10.7871,
      "step": 23
    },
    {
      "epoch": 0.006832740213523132,
      "grad_norm": 0.542377769947052,
      "learning_rate": 0.0004983632223171079,
      "loss": 10.7402,
      "step": 24
    },
    {
      "epoch": 0.0071174377224199285,
      "grad_norm": 0.6290743947029114,
      "learning_rate": 0.0004982920580700257,
      "loss": 11.1191,
      "step": 25
    },
    {
      "epoch": 0.007402135231316726,
      "grad_norm": 0.5208401083946228,
      "learning_rate": 0.0004982208938229434,
      "loss": 10.3535,
      "step": 26
    },
    {
      "epoch": 0.0076868327402135235,
      "grad_norm": 0.5653201937675476,
      "learning_rate": 0.0004981497295758611,
      "loss": 10.2676,
      "step": 27
    },
    {
      "epoch": 0.00797153024911032,
      "grad_norm": 0.5279284119606018,
      "learning_rate": 0.0004980785653287788,
      "loss": 10.6504,
      "step": 28
    },
    {
      "epoch": 0.008256227758007117,
      "grad_norm": 0.5287833213806152,
      "learning_rate": 0.0004980074010816966,
      "loss": 9.9648,
      "step": 29
    },
    {
      "epoch": 0.008540925266903915,
      "grad_norm": 0.4779149293899536,
      "learning_rate": 0.0004979362368346143,
      "loss": 11.1738,
      "step": 30
    },
    {
      "epoch": 0.008825622775800712,
      "grad_norm": 0.4311399459838867,
      "learning_rate": 0.000497865072587532,
      "loss": 10.7617,
      "step": 31
    },
    {
      "epoch": 0.009110320284697508,
      "grad_norm": 0.4880855083465576,
      "learning_rate": 0.0004977939083404498,
      "loss": 10.6113,
      "step": 32
    },
    {
      "epoch": 0.009395017793594307,
      "grad_norm": 0.4771440029144287,
      "learning_rate": 0.0004977227440933675,
      "loss": 10.2031,
      "step": 33
    },
    {
      "epoch": 0.009679715302491104,
      "grad_norm": 0.4335767924785614,
      "learning_rate": 0.0004976515798462852,
      "loss": 9.9766,
      "step": 34
    },
    {
      "epoch": 0.0099644128113879,
      "grad_norm": 0.47250261902809143,
      "learning_rate": 0.000497580415599203,
      "loss": 10.582,
      "step": 35
    },
    {
      "epoch": 0.010249110320284697,
      "grad_norm": 0.5027048587799072,
      "learning_rate": 0.0004975092513521207,
      "loss": 9.252,
      "step": 36
    },
    {
      "epoch": 0.010533807829181495,
      "grad_norm": 0.4936491847038269,
      "learning_rate": 0.0004974380871050385,
      "loss": 9.9648,
      "step": 37
    },
    {
      "epoch": 0.010818505338078292,
      "grad_norm": 0.4429067373275757,
      "learning_rate": 0.0004973669228579561,
      "loss": 10.1699,
      "step": 38
    },
    {
      "epoch": 0.011103202846975089,
      "grad_norm": 0.43701812624931335,
      "learning_rate": 0.0004972957586108739,
      "loss": 10.1875,
      "step": 39
    },
    {
      "epoch": 0.011387900355871887,
      "grad_norm": 0.44829320907592773,
      "learning_rate": 0.0004972245943637917,
      "loss": 9.7109,
      "step": 40
    },
    {
      "epoch": 0.011672597864768684,
      "grad_norm": 0.44973376393318176,
      "learning_rate": 0.0004971534301167094,
      "loss": 9.9023,
      "step": 41
    },
    {
      "epoch": 0.01195729537366548,
      "grad_norm": 0.4918513894081116,
      "learning_rate": 0.0004970822658696271,
      "loss": 10.1934,
      "step": 42
    },
    {
      "epoch": 0.012241992882562277,
      "grad_norm": 0.4607585668563843,
      "learning_rate": 0.0004970111016225449,
      "loss": 9.9941,
      "step": 43
    },
    {
      "epoch": 0.012526690391459075,
      "grad_norm": 0.45157185196876526,
      "learning_rate": 0.0004969399373754626,
      "loss": 9.6797,
      "step": 44
    },
    {
      "epoch": 0.012811387900355872,
      "grad_norm": 0.45155540108680725,
      "learning_rate": 0.0004968687731283804,
      "loss": 9.8535,
      "step": 45
    },
    {
      "epoch": 0.013096085409252669,
      "grad_norm": 0.45471465587615967,
      "learning_rate": 0.000496797608881298,
      "loss": 9.6699,
      "step": 46
    },
    {
      "epoch": 0.013380782918149467,
      "grad_norm": 0.45609229803085327,
      "learning_rate": 0.0004967264446342158,
      "loss": 9.834,
      "step": 47
    },
    {
      "epoch": 0.013665480427046264,
      "grad_norm": 0.46065598726272583,
      "learning_rate": 0.0004966552803871336,
      "loss": 9.1973,
      "step": 48
    },
    {
      "epoch": 0.01395017793594306,
      "grad_norm": 0.4395595192909241,
      "learning_rate": 0.0004965841161400512,
      "loss": 9.3379,
      "step": 49
    },
    {
      "epoch": 0.014234875444839857,
      "grad_norm": 0.42804303765296936,
      "learning_rate": 0.0004965129518929689,
      "loss": 8.1885,
      "step": 50
    },
    {
      "epoch": 0.014519572953736655,
      "grad_norm": 0.4336685538291931,
      "learning_rate": 0.0004964417876458867,
      "loss": 9.4141,
      "step": 51
    },
    {
      "epoch": 0.014804270462633452,
      "grad_norm": 0.4083915054798126,
      "learning_rate": 0.0004963706233988045,
      "loss": 9.2363,
      "step": 52
    },
    {
      "epoch": 0.015088967971530249,
      "grad_norm": 0.39727815985679626,
      "learning_rate": 0.0004962994591517222,
      "loss": 8.5439,
      "step": 53
    },
    {
      "epoch": 0.015373665480427047,
      "grad_norm": 0.4038327634334564,
      "learning_rate": 0.0004962282949046399,
      "loss": 9.0723,
      "step": 54
    },
    {
      "epoch": 0.015658362989323844,
      "grad_norm": 0.4088612198829651,
      "learning_rate": 0.0004961571306575576,
      "loss": 8.5332,
      "step": 55
    },
    {
      "epoch": 0.01594306049822064,
      "grad_norm": 0.39859700202941895,
      "learning_rate": 0.0004960859664104754,
      "loss": 9.1191,
      "step": 56
    },
    {
      "epoch": 0.016227758007117437,
      "grad_norm": 0.39439693093299866,
      "learning_rate": 0.0004960148021633931,
      "loss": 8.0508,
      "step": 57
    },
    {
      "epoch": 0.016512455516014234,
      "grad_norm": 0.41306084394454956,
      "learning_rate": 0.0004959436379163109,
      "loss": 8.2998,
      "step": 58
    },
    {
      "epoch": 0.016797153024911034,
      "grad_norm": 0.4390506148338318,
      "learning_rate": 0.0004958724736692286,
      "loss": 8.1816,
      "step": 59
    },
    {
      "epoch": 0.01708185053380783,
      "grad_norm": 0.40014925599098206,
      "learning_rate": 0.0004958013094221464,
      "loss": 8.248,
      "step": 60
    },
    {
      "epoch": 0.017366548042704627,
      "grad_norm": 0.3710031807422638,
      "learning_rate": 0.000495730145175064,
      "loss": 7.7432,
      "step": 61
    },
    {
      "epoch": 0.017651245551601424,
      "grad_norm": 0.3743155002593994,
      "learning_rate": 0.0004956589809279818,
      "loss": 8.9805,
      "step": 62
    },
    {
      "epoch": 0.01793594306049822,
      "grad_norm": 0.4065592288970947,
      "learning_rate": 0.0004955878166808996,
      "loss": 8.9219,
      "step": 63
    },
    {
      "epoch": 0.018220640569395017,
      "grad_norm": 0.3826305866241455,
      "learning_rate": 0.0004955166524338172,
      "loss": 8.377,
      "step": 64
    },
    {
      "epoch": 0.018505338078291814,
      "grad_norm": 0.3792859613895416,
      "learning_rate": 0.000495445488186735,
      "loss": 7.8789,
      "step": 65
    },
    {
      "epoch": 0.018790035587188614,
      "grad_norm": 0.3933613896369934,
      "learning_rate": 0.0004953743239396527,
      "loss": 8.4092,
      "step": 66
    },
    {
      "epoch": 0.01907473309608541,
      "grad_norm": 0.39127129316329956,
      "learning_rate": 0.0004953031596925705,
      "loss": 7.9512,
      "step": 67
    },
    {
      "epoch": 0.019359430604982207,
      "grad_norm": 0.34493568539619446,
      "learning_rate": 0.0004952319954454881,
      "loss": 8.3105,
      "step": 68
    },
    {
      "epoch": 0.019644128113879004,
      "grad_norm": 0.3507670760154724,
      "learning_rate": 0.0004951608311984059,
      "loss": 7.9043,
      "step": 69
    },
    {
      "epoch": 0.0199288256227758,
      "grad_norm": 0.3754495084285736,
      "learning_rate": 0.0004950896669513237,
      "loss": 7.9131,
      "step": 70
    },
    {
      "epoch": 0.020213523131672597,
      "grad_norm": 0.3600054383277893,
      "learning_rate": 0.0004950185027042415,
      "loss": 8.1426,
      "step": 71
    },
    {
      "epoch": 0.020498220640569394,
      "grad_norm": 0.3194250166416168,
      "learning_rate": 0.0004949473384571591,
      "loss": 7.8213,
      "step": 72
    },
    {
      "epoch": 0.020782918149466194,
      "grad_norm": 0.3438234329223633,
      "learning_rate": 0.0004948761742100768,
      "loss": 8.1318,
      "step": 73
    },
    {
      "epoch": 0.02106761565836299,
      "grad_norm": 0.34680306911468506,
      "learning_rate": 0.0004948050099629946,
      "loss": 8.1611,
      "step": 74
    },
    {
      "epoch": 0.021352313167259787,
      "grad_norm": 0.3298954665660858,
      "learning_rate": 0.0004947338457159124,
      "loss": 7.6816,
      "step": 75
    },
    {
      "epoch": 0.021637010676156584,
      "grad_norm": 0.35546237230300903,
      "learning_rate": 0.0004946626814688301,
      "loss": 7.9482,
      "step": 76
    },
    {
      "epoch": 0.02192170818505338,
      "grad_norm": 0.31350335478782654,
      "learning_rate": 0.0004945915172217478,
      "loss": 7.916,
      "step": 77
    },
    {
      "epoch": 0.022206405693950177,
      "grad_norm": 0.2875521183013916,
      "learning_rate": 0.0004945203529746655,
      "loss": 8.1787,
      "step": 78
    },
    {
      "epoch": 0.022491103202846974,
      "grad_norm": 0.2984481453895569,
      "learning_rate": 0.0004944491887275833,
      "loss": 8.0068,
      "step": 79
    },
    {
      "epoch": 0.022775800711743774,
      "grad_norm": 0.32493481040000916,
      "learning_rate": 0.000494378024480501,
      "loss": 7.7959,
      "step": 80
    },
    {
      "epoch": 0.02306049822064057,
      "grad_norm": 0.3053205907344818,
      "learning_rate": 0.0004943068602334188,
      "loss": 7.7959,
      "step": 81
    },
    {
      "epoch": 0.023345195729537367,
      "grad_norm": 0.29084035754203796,
      "learning_rate": 0.0004942356959863365,
      "loss": 7.9629,
      "step": 82
    },
    {
      "epoch": 0.023629893238434164,
      "grad_norm": 0.3073493242263794,
      "learning_rate": 0.0004941645317392541,
      "loss": 7.7549,
      "step": 83
    },
    {
      "epoch": 0.02391459074733096,
      "grad_norm": 0.30308839678764343,
      "learning_rate": 0.0004940933674921719,
      "loss": 8.0791,
      "step": 84
    },
    {
      "epoch": 0.024199288256227757,
      "grad_norm": 0.38702166080474854,
      "learning_rate": 0.0004940222032450897,
      "loss": 7.2637,
      "step": 85
    },
    {
      "epoch": 0.024483985765124554,
      "grad_norm": 0.33084896206855774,
      "learning_rate": 0.0004939510389980074,
      "loss": 7.833,
      "step": 86
    },
    {
      "epoch": 0.024768683274021354,
      "grad_norm": 0.2839316129684448,
      "learning_rate": 0.0004938798747509251,
      "loss": 7.9053,
      "step": 87
    },
    {
      "epoch": 0.02505338078291815,
      "grad_norm": 0.35194167494773865,
      "learning_rate": 0.0004938087105038429,
      "loss": 7.4053,
      "step": 88
    },
    {
      "epoch": 0.025338078291814947,
      "grad_norm": 0.34102603793144226,
      "learning_rate": 0.0004937375462567606,
      "loss": 7.6406,
      "step": 89
    },
    {
      "epoch": 0.025622775800711744,
      "grad_norm": 0.34196290373802185,
      "learning_rate": 0.0004936663820096784,
      "loss": 7.7129,
      "step": 90
    },
    {
      "epoch": 0.02590747330960854,
      "grad_norm": 0.33342525362968445,
      "learning_rate": 0.000493595217762596,
      "loss": 7.1934,
      "step": 91
    },
    {
      "epoch": 0.026192170818505337,
      "grad_norm": 0.2916184365749359,
      "learning_rate": 0.0004935240535155138,
      "loss": 7.7559,
      "step": 92
    },
    {
      "epoch": 0.026476868327402134,
      "grad_norm": 0.3170425295829773,
      "learning_rate": 0.0004934528892684316,
      "loss": 7.7617,
      "step": 93
    },
    {
      "epoch": 0.026761565836298934,
      "grad_norm": 0.3077421188354492,
      "learning_rate": 0.0004933817250213493,
      "loss": 7.7852,
      "step": 94
    },
    {
      "epoch": 0.02704626334519573,
      "grad_norm": 0.34794363379478455,
      "learning_rate": 0.000493310560774267,
      "loss": 7.5391,
      "step": 95
    },
    {
      "epoch": 0.027330960854092527,
      "grad_norm": 0.27851757407188416,
      "learning_rate": 0.0004932393965271847,
      "loss": 8.0039,
      "step": 96
    },
    {
      "epoch": 0.027615658362989324,
      "grad_norm": 0.3033468723297119,
      "learning_rate": 0.0004931682322801025,
      "loss": 7.8076,
      "step": 97
    },
    {
      "epoch": 0.02790035587188612,
      "grad_norm": 0.29290416836738586,
      "learning_rate": 0.0004930970680330203,
      "loss": 7.8281,
      "step": 98
    },
    {
      "epoch": 0.028185053380782917,
      "grad_norm": 0.27407997846603394,
      "learning_rate": 0.000493025903785938,
      "loss": 7.8564,
      "step": 99
    },
    {
      "epoch": 0.028469750889679714,
      "grad_norm": 0.32197993993759155,
      "learning_rate": 0.0004929547395388557,
      "loss": 7.3594,
      "step": 100
    },
    {
      "epoch": 0.028754448398576514,
      "grad_norm": 0.27607426047325134,
      "learning_rate": 0.0004928835752917734,
      "loss": 7.9219,
      "step": 101
    },
    {
      "epoch": 0.02903914590747331,
      "grad_norm": 0.25733911991119385,
      "learning_rate": 0.0004928124110446911,
      "loss": 7.6963,
      "step": 102
    },
    {
      "epoch": 0.029323843416370107,
      "grad_norm": 0.33134591579437256,
      "learning_rate": 0.0004927412467976089,
      "loss": 7.5918,
      "step": 103
    },
    {
      "epoch": 0.029608540925266904,
      "grad_norm": 0.3447519838809967,
      "learning_rate": 0.0004926700825505266,
      "loss": 7.3262,
      "step": 104
    },
    {
      "epoch": 0.0298932384341637,
      "grad_norm": 0.30481886863708496,
      "learning_rate": 0.0004925989183034444,
      "loss": 7.5986,
      "step": 105
    },
    {
      "epoch": 0.030177935943060497,
      "grad_norm": 0.3810344338417053,
      "learning_rate": 0.000492527754056362,
      "loss": 7.2734,
      "step": 106
    },
    {
      "epoch": 0.030462633451957294,
      "grad_norm": 0.26661789417266846,
      "learning_rate": 0.0004924565898092798,
      "loss": 7.8818,
      "step": 107
    },
    {
      "epoch": 0.030747330960854094,
      "grad_norm": 0.3144116997718811,
      "learning_rate": 0.0004923854255621976,
      "loss": 7.4697,
      "step": 108
    },
    {
      "epoch": 0.03103202846975089,
      "grad_norm": 0.3234042525291443,
      "learning_rate": 0.0004923142613151153,
      "loss": 7.6611,
      "step": 109
    },
    {
      "epoch": 0.03131672597864769,
      "grad_norm": 0.2908277213573456,
      "learning_rate": 0.000492243097068033,
      "loss": 7.6924,
      "step": 110
    },
    {
      "epoch": 0.03160142348754449,
      "grad_norm": 0.282621830701828,
      "learning_rate": 0.0004921719328209507,
      "loss": 7.6748,
      "step": 111
    },
    {
      "epoch": 0.03188612099644128,
      "grad_norm": 0.25899139046669006,
      "learning_rate": 0.0004921007685738685,
      "loss": 7.7725,
      "step": 112
    },
    {
      "epoch": 0.03217081850533808,
      "grad_norm": 0.30150163173675537,
      "learning_rate": 0.0004920296043267863,
      "loss": 7.6016,
      "step": 113
    },
    {
      "epoch": 0.032455516014234874,
      "grad_norm": 0.3718966245651245,
      "learning_rate": 0.0004919584400797039,
      "loss": 7.0254,
      "step": 114
    },
    {
      "epoch": 0.032740213523131674,
      "grad_norm": 0.30984169244766235,
      "learning_rate": 0.0004918872758326217,
      "loss": 7.4707,
      "step": 115
    },
    {
      "epoch": 0.03302491103202847,
      "grad_norm": 0.25567907094955444,
      "learning_rate": 0.0004918161115855395,
      "loss": 7.668,
      "step": 116
    },
    {
      "epoch": 0.03330960854092527,
      "grad_norm": 0.38791143894195557,
      "learning_rate": 0.0004917449473384572,
      "loss": 7.3613,
      "step": 117
    },
    {
      "epoch": 0.03359430604982207,
      "grad_norm": 0.28487956523895264,
      "learning_rate": 0.0004916737830913749,
      "loss": 7.5039,
      "step": 118
    },
    {
      "epoch": 0.03387900355871886,
      "grad_norm": 0.4281052350997925,
      "learning_rate": 0.0004916026188442926,
      "loss": 7.0996,
      "step": 119
    },
    {
      "epoch": 0.03416370106761566,
      "grad_norm": 0.30284109711647034,
      "learning_rate": 0.0004915314545972104,
      "loss": 7.2402,
      "step": 120
    },
    {
      "epoch": 0.034448398576512454,
      "grad_norm": 0.3210189938545227,
      "learning_rate": 0.0004914602903501282,
      "loss": 7.3271,
      "step": 121
    },
    {
      "epoch": 0.034733096085409254,
      "grad_norm": 0.373404324054718,
      "learning_rate": 0.0004913891261030458,
      "loss": 6.876,
      "step": 122
    },
    {
      "epoch": 0.03501779359430605,
      "grad_norm": 0.34432628750801086,
      "learning_rate": 0.0004913179618559636,
      "loss": 7.5469,
      "step": 123
    },
    {
      "epoch": 0.03530249110320285,
      "grad_norm": 0.3198752999305725,
      "learning_rate": 0.0004912467976088813,
      "loss": 7.6924,
      "step": 124
    },
    {
      "epoch": 0.03558718861209965,
      "grad_norm": 0.38547608256340027,
      "learning_rate": 0.000491175633361799,
      "loss": 7.0049,
      "step": 125
    },
    {
      "epoch": 0.03587188612099644,
      "grad_norm": 0.34361645579338074,
      "learning_rate": 0.0004911044691147168,
      "loss": 7.3477,
      "step": 126
    },
    {
      "epoch": 0.03615658362989324,
      "grad_norm": 0.32878419756889343,
      "learning_rate": 0.0004910333048676345,
      "loss": 7.167,
      "step": 127
    },
    {
      "epoch": 0.036441281138790034,
      "grad_norm": 0.2640332579612732,
      "learning_rate": 0.0004909621406205523,
      "loss": 7.5791,
      "step": 128
    },
    {
      "epoch": 0.036725978647686834,
      "grad_norm": 0.34086892008781433,
      "learning_rate": 0.0004908909763734699,
      "loss": 7.5127,
      "step": 129
    },
    {
      "epoch": 0.03701067615658363,
      "grad_norm": 0.3057088851928711,
      "learning_rate": 0.0004908198121263877,
      "loss": 7.6523,
      "step": 130
    },
    {
      "epoch": 0.03729537366548043,
      "grad_norm": 0.325603187084198,
      "learning_rate": 0.0004907486478793055,
      "loss": 7.2979,
      "step": 131
    },
    {
      "epoch": 0.03758007117437723,
      "grad_norm": 0.3464660942554474,
      "learning_rate": 0.0004906774836322232,
      "loss": 7.4541,
      "step": 132
    },
    {
      "epoch": 0.03786476868327402,
      "grad_norm": 0.3628098964691162,
      "learning_rate": 0.0004906063193851409,
      "loss": 7.2969,
      "step": 133
    },
    {
      "epoch": 0.03814946619217082,
      "grad_norm": 0.33785781264305115,
      "learning_rate": 0.0004905351551380586,
      "loss": 7.4209,
      "step": 134
    },
    {
      "epoch": 0.038434163701067614,
      "grad_norm": 0.3329750895500183,
      "learning_rate": 0.0004904639908909764,
      "loss": 7.5674,
      "step": 135
    },
    {
      "epoch": 0.038718861209964414,
      "grad_norm": 0.3496299088001251,
      "learning_rate": 0.0004903928266438942,
      "loss": 7.4629,
      "step": 136
    },
    {
      "epoch": 0.03900355871886121,
      "grad_norm": 0.3395816385746002,
      "learning_rate": 0.0004903216623968118,
      "loss": 7.5713,
      "step": 137
    },
    {
      "epoch": 0.03928825622775801,
      "grad_norm": 0.3240024447441101,
      "learning_rate": 0.0004902504981497296,
      "loss": 7.6055,
      "step": 138
    },
    {
      "epoch": 0.03957295373665481,
      "grad_norm": 0.2986396253108978,
      "learning_rate": 0.0004901793339026473,
      "loss": 7.4951,
      "step": 139
    },
    {
      "epoch": 0.0398576512455516,
      "grad_norm": 0.30460289120674133,
      "learning_rate": 0.000490108169655565,
      "loss": 7.7354,
      "step": 140
    },
    {
      "epoch": 0.0401423487544484,
      "grad_norm": 0.2872254252433777,
      "learning_rate": 0.0004900370054084828,
      "loss": 7.7559,
      "step": 141
    },
    {
      "epoch": 0.040427046263345194,
      "grad_norm": 0.3015432059764862,
      "learning_rate": 0.0004899658411614005,
      "loss": 7.7129,
      "step": 142
    },
    {
      "epoch": 0.040711743772241994,
      "grad_norm": 0.33976587653160095,
      "learning_rate": 0.0004898946769143183,
      "loss": 7.4961,
      "step": 143
    },
    {
      "epoch": 0.04099644128113879,
      "grad_norm": 0.3869113028049469,
      "learning_rate": 0.000489823512667236,
      "loss": 7.2979,
      "step": 144
    },
    {
      "epoch": 0.04128113879003559,
      "grad_norm": 0.3606325685977936,
      "learning_rate": 0.0004897523484201537,
      "loss": 7.4873,
      "step": 145
    },
    {
      "epoch": 0.04156583629893239,
      "grad_norm": 0.3285134434700012,
      "learning_rate": 0.0004896811841730715,
      "loss": 7.8018,
      "step": 146
    },
    {
      "epoch": 0.04185053380782918,
      "grad_norm": 0.371952086687088,
      "learning_rate": 0.0004896100199259892,
      "loss": 7.2168,
      "step": 147
    },
    {
      "epoch": 0.04213523131672598,
      "grad_norm": 0.3381865620613098,
      "learning_rate": 0.0004895388556789069,
      "loss": 7.3682,
      "step": 148
    },
    {
      "epoch": 0.042419928825622774,
      "grad_norm": 0.30638259649276733,
      "learning_rate": 0.0004894676914318247,
      "loss": 7.459,
      "step": 149
    },
    {
      "epoch": 0.042704626334519574,
      "grad_norm": 0.3618326187133789,
      "learning_rate": 0.0004893965271847424,
      "loss": 7.293,
      "step": 150
    },
    {
      "epoch": 0.04298932384341637,
      "grad_norm": 0.34743648767471313,
      "learning_rate": 0.0004893253629376602,
      "loss": 7.2275,
      "step": 151
    },
    {
      "epoch": 0.04327402135231317,
      "grad_norm": 0.24103522300720215,
      "learning_rate": 0.0004892541986905778,
      "loss": 7.6436,
      "step": 152
    },
    {
      "epoch": 0.04355871886120997,
      "grad_norm": 0.47605815529823303,
      "learning_rate": 0.0004891830344434956,
      "loss": 6.8848,
      "step": 153
    },
    {
      "epoch": 0.04384341637010676,
      "grad_norm": 0.2737746834754944,
      "learning_rate": 0.0004891118701964134,
      "loss": 7.5234,
      "step": 154
    },
    {
      "epoch": 0.04412811387900356,
      "grad_norm": 0.34337320923805237,
      "learning_rate": 0.000489040705949331,
      "loss": 7.3691,
      "step": 155
    },
    {
      "epoch": 0.044412811387900354,
      "grad_norm": 0.29171985387802124,
      "learning_rate": 0.0004889695417022488,
      "loss": 7.7021,
      "step": 156
    },
    {
      "epoch": 0.044697508896797154,
      "grad_norm": 0.2947385907173157,
      "learning_rate": 0.0004888983774551665,
      "loss": 7.7725,
      "step": 157
    },
    {
      "epoch": 0.04498220640569395,
      "grad_norm": 0.33179420232772827,
      "learning_rate": 0.0004888272132080843,
      "loss": 7.4795,
      "step": 158
    },
    {
      "epoch": 0.04526690391459075,
      "grad_norm": 0.30874258279800415,
      "learning_rate": 0.000488756048961002,
      "loss": 7.667,
      "step": 159
    },
    {
      "epoch": 0.04555160142348755,
      "grad_norm": 0.37757524847984314,
      "learning_rate": 0.0004886848847139197,
      "loss": 7.1436,
      "step": 160
    },
    {
      "epoch": 0.04583629893238434,
      "grad_norm": 0.35947147011756897,
      "learning_rate": 0.0004886137204668375,
      "loss": 7.2451,
      "step": 161
    },
    {
      "epoch": 0.04612099644128114,
      "grad_norm": 0.38363611698150635,
      "learning_rate": 0.0004885425562197552,
      "loss": 7.2061,
      "step": 162
    },
    {
      "epoch": 0.046405693950177934,
      "grad_norm": 0.3002108335494995,
      "learning_rate": 0.0004884713919726729,
      "loss": 7.6328,
      "step": 163
    },
    {
      "epoch": 0.046690391459074734,
      "grad_norm": 0.36899691820144653,
      "learning_rate": 0.0004884002277255907,
      "loss": 7.2451,
      "step": 164
    },
    {
      "epoch": 0.04697508896797153,
      "grad_norm": 0.3589462637901306,
      "learning_rate": 0.0004883290634785084,
      "loss": 7.4961,
      "step": 165
    },
    {
      "epoch": 0.04725978647686833,
      "grad_norm": 0.3247394263744354,
      "learning_rate": 0.00048825789923142616,
      "loss": 7.4971,
      "step": 166
    },
    {
      "epoch": 0.04754448398576513,
      "grad_norm": 0.4397582411766052,
      "learning_rate": 0.00048818673498434383,
      "loss": 7.2158,
      "step": 167
    },
    {
      "epoch": 0.04782918149466192,
      "grad_norm": 0.34369221329689026,
      "learning_rate": 0.0004881155707372616,
      "loss": 7.3965,
      "step": 168
    },
    {
      "epoch": 0.04811387900355872,
      "grad_norm": 0.315603107213974,
      "learning_rate": 0.00048804440649017933,
      "loss": 7.4365,
      "step": 169
    },
    {
      "epoch": 0.048398576512455514,
      "grad_norm": 0.5103448033332825,
      "learning_rate": 0.0004879732422430971,
      "loss": 6.8682,
      "step": 170
    },
    {
      "epoch": 0.048683274021352314,
      "grad_norm": 0.43841269612312317,
      "learning_rate": 0.00048790207799601483,
      "loss": 7.3838,
      "step": 171
    },
    {
      "epoch": 0.04896797153024911,
      "grad_norm": 0.43405452370643616,
      "learning_rate": 0.00048783091374893255,
      "loss": 6.9912,
      "step": 172
    },
    {
      "epoch": 0.04925266903914591,
      "grad_norm": 0.4368981122970581,
      "learning_rate": 0.0004877597495018503,
      "loss": 7.1592,
      "step": 173
    },
    {
      "epoch": 0.04953736654804271,
      "grad_norm": 0.3200138211250305,
      "learning_rate": 0.000487688585254768,
      "loss": 7.5615,
      "step": 174
    },
    {
      "epoch": 0.0498220640569395,
      "grad_norm": 0.3974745571613312,
      "learning_rate": 0.0004876174210076857,
      "loss": 7.0674,
      "step": 175
    },
    {
      "epoch": 0.0501067615658363,
      "grad_norm": 0.3726997673511505,
      "learning_rate": 0.0004875462567606035,
      "loss": 7.3057,
      "step": 176
    },
    {
      "epoch": 0.050391459074733094,
      "grad_norm": 0.37156978249549866,
      "learning_rate": 0.0004874750925135212,
      "loss": 7.3672,
      "step": 177
    },
    {
      "epoch": 0.050676156583629894,
      "grad_norm": 0.3446742594242096,
      "learning_rate": 0.000487403928266439,
      "loss": 7.3379,
      "step": 178
    },
    {
      "epoch": 0.05096085409252669,
      "grad_norm": 0.3899495601654053,
      "learning_rate": 0.00048733276401935667,
      "loss": 7.0742,
      "step": 179
    },
    {
      "epoch": 0.05124555160142349,
      "grad_norm": 0.365047425031662,
      "learning_rate": 0.0004872615997722744,
      "loss": 7.2949,
      "step": 180
    },
    {
      "epoch": 0.05153024911032029,
      "grad_norm": 0.3096078336238861,
      "learning_rate": 0.00048719043552519217,
      "loss": 7.585,
      "step": 181
    },
    {
      "epoch": 0.05181494661921708,
      "grad_norm": 0.3642439842224121,
      "learning_rate": 0.0004871192712781099,
      "loss": 7.293,
      "step": 182
    },
    {
      "epoch": 0.05209964412811388,
      "grad_norm": 0.31257525086402893,
      "learning_rate": 0.00048704810703102767,
      "loss": 7.6279,
      "step": 183
    },
    {
      "epoch": 0.052384341637010674,
      "grad_norm": 0.3482113480567932,
      "learning_rate": 0.00048697694278394533,
      "loss": 7.4062,
      "step": 184
    },
    {
      "epoch": 0.052669039145907474,
      "grad_norm": 0.40061259269714355,
      "learning_rate": 0.00048690577853686306,
      "loss": 6.9482,
      "step": 185
    },
    {
      "epoch": 0.05295373665480427,
      "grad_norm": 0.4041823744773865,
      "learning_rate": 0.00048683461428978083,
      "loss": 7.1836,
      "step": 186
    },
    {
      "epoch": 0.05323843416370107,
      "grad_norm": 0.41112077236175537,
      "learning_rate": 0.00048676345004269856,
      "loss": 6.8662,
      "step": 187
    },
    {
      "epoch": 0.05352313167259787,
      "grad_norm": 0.38859066367149353,
      "learning_rate": 0.00048669228579561633,
      "loss": 7.626,
      "step": 188
    },
    {
      "epoch": 0.05380782918149466,
      "grad_norm": 0.3483869135379791,
      "learning_rate": 0.00048662112154853406,
      "loss": 7.2656,
      "step": 189
    },
    {
      "epoch": 0.05409252669039146,
      "grad_norm": 0.36553943157196045,
      "learning_rate": 0.0004865499573014517,
      "loss": 7.0918,
      "step": 190
    },
    {
      "epoch": 0.054377224199288254,
      "grad_norm": 0.45641273260116577,
      "learning_rate": 0.0004864787930543695,
      "loss": 7.1328,
      "step": 191
    },
    {
      "epoch": 0.054661921708185054,
      "grad_norm": 0.5525060296058655,
      "learning_rate": 0.0004864076288072872,
      "loss": 6.6797,
      "step": 192
    },
    {
      "epoch": 0.05494661921708185,
      "grad_norm": 0.35605090856552124,
      "learning_rate": 0.00048633646456020495,
      "loss": 7.3145,
      "step": 193
    },
    {
      "epoch": 0.05523131672597865,
      "grad_norm": 0.33720216155052185,
      "learning_rate": 0.0004862653003131227,
      "loss": 7.583,
      "step": 194
    },
    {
      "epoch": 0.05551601423487545,
      "grad_norm": 0.39028072357177734,
      "learning_rate": 0.0004861941360660404,
      "loss": 7.041,
      "step": 195
    },
    {
      "epoch": 0.05580071174377224,
      "grad_norm": 0.37324032187461853,
      "learning_rate": 0.00048612297181895817,
      "loss": 7.5771,
      "step": 196
    },
    {
      "epoch": 0.05608540925266904,
      "grad_norm": 0.3692830204963684,
      "learning_rate": 0.0004860518075718759,
      "loss": 6.9414,
      "step": 197
    },
    {
      "epoch": 0.056370106761565834,
      "grad_norm": 0.3400883674621582,
      "learning_rate": 0.0004859806433247936,
      "loss": 7.6484,
      "step": 198
    },
    {
      "epoch": 0.056654804270462635,
      "grad_norm": 0.36927610635757446,
      "learning_rate": 0.0004859094790777114,
      "loss": 7.0127,
      "step": 199
    },
    {
      "epoch": 0.05693950177935943,
      "grad_norm": 0.3512347936630249,
      "learning_rate": 0.0004858383148306291,
      "loss": 7.5596,
      "step": 200
    },
    {
      "epoch": 0.05693950177935943,
      "eval_bleu": 0.04428465159832603,
      "eval_loss": 7.046875,
      "eval_runtime": 206.9516,
      "eval_samples_per_second": 1.372,
      "eval_steps_per_second": 0.087,
      "step": 200
    },
    {
      "epoch": 0.05722419928825623,
      "grad_norm": 0.40993812680244446,
      "learning_rate": 0.00048576715058354684,
      "loss": 7.165,
      "step": 201
    },
    {
      "epoch": 0.05750889679715303,
      "grad_norm": 0.37675940990448,
      "learning_rate": 0.00048569598633646456,
      "loss": 7.5469,
      "step": 202
    },
    {
      "epoch": 0.05779359430604982,
      "grad_norm": 0.41827306151390076,
      "learning_rate": 0.0004856248220893823,
      "loss": 7.252,
      "step": 203
    },
    {
      "epoch": 0.05807829181494662,
      "grad_norm": 0.40496769547462463,
      "learning_rate": 0.00048555365784230006,
      "loss": 7.3262,
      "step": 204
    },
    {
      "epoch": 0.058362989323843414,
      "grad_norm": 0.4544559121131897,
      "learning_rate": 0.0004854824935952178,
      "loss": 7.2686,
      "step": 205
    },
    {
      "epoch": 0.058647686832740215,
      "grad_norm": 0.43348002433776855,
      "learning_rate": 0.0004854113293481355,
      "loss": 7.1436,
      "step": 206
    },
    {
      "epoch": 0.05893238434163701,
      "grad_norm": 0.5123205780982971,
      "learning_rate": 0.00048534016510105323,
      "loss": 6.9443,
      "step": 207
    },
    {
      "epoch": 0.05921708185053381,
      "grad_norm": 0.3473433554172516,
      "learning_rate": 0.00048526900085397095,
      "loss": 7.4775,
      "step": 208
    },
    {
      "epoch": 0.05950177935943061,
      "grad_norm": 0.3604001998901367,
      "learning_rate": 0.00048519783660688873,
      "loss": 7.5098,
      "step": 209
    },
    {
      "epoch": 0.0597864768683274,
      "grad_norm": 0.33064237236976624,
      "learning_rate": 0.00048512667235980645,
      "loss": 7.6191,
      "step": 210
    },
    {
      "epoch": 0.0600711743772242,
      "grad_norm": 0.5796976089477539,
      "learning_rate": 0.0004850555081127242,
      "loss": 6.7324,
      "step": 211
    },
    {
      "epoch": 0.060355871886120994,
      "grad_norm": 0.3960364758968353,
      "learning_rate": 0.0004849843438656419,
      "loss": 7.2354,
      "step": 212
    },
    {
      "epoch": 0.060640569395017795,
      "grad_norm": 0.3528068959712982,
      "learning_rate": 0.0004849131796185596,
      "loss": 7.4775,
      "step": 213
    },
    {
      "epoch": 0.06092526690391459,
      "grad_norm": 0.33591997623443604,
      "learning_rate": 0.0004848420153714774,
      "loss": 7.5264,
      "step": 214
    },
    {
      "epoch": 0.06120996441281139,
      "grad_norm": 0.3542058765888214,
      "learning_rate": 0.0004847708511243951,
      "loss": 7.4365,
      "step": 215
    },
    {
      "epoch": 0.06149466192170819,
      "grad_norm": 0.4078296422958374,
      "learning_rate": 0.00048469968687731284,
      "loss": 6.9795,
      "step": 216
    },
    {
      "epoch": 0.06177935943060498,
      "grad_norm": 0.3528203070163727,
      "learning_rate": 0.0004846285226302306,
      "loss": 7.251,
      "step": 217
    },
    {
      "epoch": 0.06206405693950178,
      "grad_norm": 0.454461932182312,
      "learning_rate": 0.0004845573583831483,
      "loss": 6.876,
      "step": 218
    },
    {
      "epoch": 0.062348754448398575,
      "grad_norm": 0.39278143644332886,
      "learning_rate": 0.00048448619413606606,
      "loss": 7.2432,
      "step": 219
    },
    {
      "epoch": 0.06263345195729537,
      "grad_norm": 0.4267103374004364,
      "learning_rate": 0.0004844150298889838,
      "loss": 6.8594,
      "step": 220
    },
    {
      "epoch": 0.06291814946619217,
      "grad_norm": 0.3892408609390259,
      "learning_rate": 0.0004843438656419015,
      "loss": 6.8838,
      "step": 221
    },
    {
      "epoch": 0.06320284697508897,
      "grad_norm": 0.3549274802207947,
      "learning_rate": 0.0004842727013948193,
      "loss": 7.2236,
      "step": 222
    },
    {
      "epoch": 0.06348754448398576,
      "grad_norm": 0.33616870641708374,
      "learning_rate": 0.000484201537147737,
      "loss": 7.7422,
      "step": 223
    },
    {
      "epoch": 0.06377224199288256,
      "grad_norm": 0.33883076906204224,
      "learning_rate": 0.0004841303729006547,
      "loss": 7.5654,
      "step": 224
    },
    {
      "epoch": 0.06405693950177936,
      "grad_norm": 0.3884200155735016,
      "learning_rate": 0.00048405920865357246,
      "loss": 7.3457,
      "step": 225
    },
    {
      "epoch": 0.06434163701067616,
      "grad_norm": 0.3521203398704529,
      "learning_rate": 0.0004839880444064902,
      "loss": 7.2559,
      "step": 226
    },
    {
      "epoch": 0.06462633451957295,
      "grad_norm": 0.5035505294799805,
      "learning_rate": 0.00048391688015940796,
      "loss": 7.1816,
      "step": 227
    },
    {
      "epoch": 0.06491103202846975,
      "grad_norm": 0.37743714451789856,
      "learning_rate": 0.0004838457159123257,
      "loss": 7.5049,
      "step": 228
    },
    {
      "epoch": 0.06519572953736655,
      "grad_norm": 0.3604282736778259,
      "learning_rate": 0.00048377455166524335,
      "loss": 7.7832,
      "step": 229
    },
    {
      "epoch": 0.06548042704626335,
      "grad_norm": 0.6729333400726318,
      "learning_rate": 0.0004837033874181611,
      "loss": 6.8105,
      "step": 230
    },
    {
      "epoch": 0.06576512455516015,
      "grad_norm": 0.41351523995399475,
      "learning_rate": 0.00048363222317107885,
      "loss": 6.7354,
      "step": 231
    },
    {
      "epoch": 0.06604982206405693,
      "grad_norm": 0.3609699308872223,
      "learning_rate": 0.0004835610589239966,
      "loss": 7.3662,
      "step": 232
    },
    {
      "epoch": 0.06633451957295373,
      "grad_norm": 0.34872451424598694,
      "learning_rate": 0.00048348989467691435,
      "loss": 7.4785,
      "step": 233
    },
    {
      "epoch": 0.06661921708185053,
      "grad_norm": 0.3711255192756653,
      "learning_rate": 0.00048341873042983207,
      "loss": 7.2041,
      "step": 234
    },
    {
      "epoch": 0.06690391459074733,
      "grad_norm": 0.4340938925743103,
      "learning_rate": 0.0004833475661827498,
      "loss": 6.8232,
      "step": 235
    },
    {
      "epoch": 0.06718861209964413,
      "grad_norm": 0.3729588985443115,
      "learning_rate": 0.0004832764019356675,
      "loss": 7.2197,
      "step": 236
    },
    {
      "epoch": 0.06747330960854092,
      "grad_norm": 0.429115355014801,
      "learning_rate": 0.00048320523768858524,
      "loss": 7.1836,
      "step": 237
    },
    {
      "epoch": 0.06775800711743772,
      "grad_norm": 0.47582298517227173,
      "learning_rate": 0.000483134073441503,
      "loss": 6.8828,
      "step": 238
    },
    {
      "epoch": 0.06804270462633452,
      "grad_norm": 0.44467341899871826,
      "learning_rate": 0.00048306290919442074,
      "loss": 7.1777,
      "step": 239
    },
    {
      "epoch": 0.06832740213523132,
      "grad_norm": 0.438344269990921,
      "learning_rate": 0.00048299174494733846,
      "loss": 6.8701,
      "step": 240
    },
    {
      "epoch": 0.06861209964412811,
      "grad_norm": 0.4138439893722534,
      "learning_rate": 0.0004829205807002562,
      "loss": 7.1123,
      "step": 241
    },
    {
      "epoch": 0.06889679715302491,
      "grad_norm": 0.39080458879470825,
      "learning_rate": 0.0004828494164531739,
      "loss": 7.3242,
      "step": 242
    },
    {
      "epoch": 0.06918149466192171,
      "grad_norm": 0.43729132413864136,
      "learning_rate": 0.0004827782522060917,
      "loss": 6.7295,
      "step": 243
    },
    {
      "epoch": 0.06946619217081851,
      "grad_norm": 0.4565962851047516,
      "learning_rate": 0.0004827070879590094,
      "loss": 7.4229,
      "step": 244
    },
    {
      "epoch": 0.06975088967971531,
      "grad_norm": 0.4301145374774933,
      "learning_rate": 0.0004826359237119272,
      "loss": 7.3975,
      "step": 245
    },
    {
      "epoch": 0.0700355871886121,
      "grad_norm": 0.4343007504940033,
      "learning_rate": 0.00048256475946484485,
      "loss": 7.1396,
      "step": 246
    },
    {
      "epoch": 0.0703202846975089,
      "grad_norm": 0.3824312388896942,
      "learning_rate": 0.0004824935952177626,
      "loss": 7.3086,
      "step": 247
    },
    {
      "epoch": 0.0706049822064057,
      "grad_norm": 0.38155433535575867,
      "learning_rate": 0.00048242243097068035,
      "loss": 7.1982,
      "step": 248
    },
    {
      "epoch": 0.0708896797153025,
      "grad_norm": 0.36901432275772095,
      "learning_rate": 0.00048235126672359807,
      "loss": 7.3613,
      "step": 249
    },
    {
      "epoch": 0.0711743772241993,
      "grad_norm": 0.368551105260849,
      "learning_rate": 0.00048228010247651585,
      "loss": 7.3828,
      "step": 250
    },
    {
      "epoch": 0.07145907473309608,
      "grad_norm": 0.3908926844596863,
      "learning_rate": 0.00048220893822943357,
      "loss": 6.9717,
      "step": 251
    },
    {
      "epoch": 0.07174377224199288,
      "grad_norm": 0.46843716502189636,
      "learning_rate": 0.00048213777398235124,
      "loss": 7.2441,
      "step": 252
    },
    {
      "epoch": 0.07202846975088968,
      "grad_norm": 0.4673958420753479,
      "learning_rate": 0.000482066609735269,
      "loss": 7.4209,
      "step": 253
    },
    {
      "epoch": 0.07231316725978648,
      "grad_norm": 0.45724111795425415,
      "learning_rate": 0.00048199544548818674,
      "loss": 6.9893,
      "step": 254
    },
    {
      "epoch": 0.07259786476868327,
      "grad_norm": 0.48041409254074097,
      "learning_rate": 0.00048192428124110446,
      "loss": 6.8398,
      "step": 255
    },
    {
      "epoch": 0.07288256227758007,
      "grad_norm": 0.3993391692638397,
      "learning_rate": 0.00048185311699402224,
      "loss": 7.3496,
      "step": 256
    },
    {
      "epoch": 0.07316725978647687,
      "grad_norm": 0.38099411129951477,
      "learning_rate": 0.0004817819527469399,
      "loss": 7.5664,
      "step": 257
    },
    {
      "epoch": 0.07345195729537367,
      "grad_norm": 0.36448702216148376,
      "learning_rate": 0.0004817107884998577,
      "loss": 7.5957,
      "step": 258
    },
    {
      "epoch": 0.07373665480427047,
      "grad_norm": 0.3705407977104187,
      "learning_rate": 0.0004816396242527754,
      "loss": 7.4092,
      "step": 259
    },
    {
      "epoch": 0.07402135231316725,
      "grad_norm": 0.4222451150417328,
      "learning_rate": 0.00048156846000569313,
      "loss": 7.2832,
      "step": 260
    },
    {
      "epoch": 0.07430604982206405,
      "grad_norm": 0.43533411622047424,
      "learning_rate": 0.0004814972957586109,
      "loss": 7.0967,
      "step": 261
    },
    {
      "epoch": 0.07459074733096085,
      "grad_norm": 0.45843270421028137,
      "learning_rate": 0.00048142613151152863,
      "loss": 7.1719,
      "step": 262
    },
    {
      "epoch": 0.07487544483985765,
      "grad_norm": 0.38327649235725403,
      "learning_rate": 0.00048135496726444635,
      "loss": 7.2725,
      "step": 263
    },
    {
      "epoch": 0.07516014234875446,
      "grad_norm": 0.4202622175216675,
      "learning_rate": 0.0004812838030173641,
      "loss": 7.4424,
      "step": 264
    },
    {
      "epoch": 0.07544483985765124,
      "grad_norm": 0.5358533263206482,
      "learning_rate": 0.0004812126387702818,
      "loss": 6.6904,
      "step": 265
    },
    {
      "epoch": 0.07572953736654804,
      "grad_norm": 0.4957423806190491,
      "learning_rate": 0.0004811414745231996,
      "loss": 6.8877,
      "step": 266
    },
    {
      "epoch": 0.07601423487544484,
      "grad_norm": 0.40162715315818787,
      "learning_rate": 0.0004810703102761173,
      "loss": 7.21,
      "step": 267
    },
    {
      "epoch": 0.07629893238434164,
      "grad_norm": 0.46528732776641846,
      "learning_rate": 0.000480999146029035,
      "loss": 7.2451,
      "step": 268
    },
    {
      "epoch": 0.07658362989323843,
      "grad_norm": 0.4028266668319702,
      "learning_rate": 0.00048092798178195274,
      "loss": 7.4043,
      "step": 269
    },
    {
      "epoch": 0.07686832740213523,
      "grad_norm": 0.46121296286582947,
      "learning_rate": 0.00048085681753487047,
      "loss": 7.2305,
      "step": 270
    },
    {
      "epoch": 0.07715302491103203,
      "grad_norm": 0.43850409984588623,
      "learning_rate": 0.00048078565328778824,
      "loss": 7.1523,
      "step": 271
    },
    {
      "epoch": 0.07743772241992883,
      "grad_norm": 0.36082831025123596,
      "learning_rate": 0.00048071448904070597,
      "loss": 7.6426,
      "step": 272
    },
    {
      "epoch": 0.07772241992882563,
      "grad_norm": 0.42451992630958557,
      "learning_rate": 0.0004806433247936237,
      "loss": 7.5566,
      "step": 273
    },
    {
      "epoch": 0.07800711743772241,
      "grad_norm": 0.5189881324768066,
      "learning_rate": 0.0004805721605465414,
      "loss": 6.9717,
      "step": 274
    },
    {
      "epoch": 0.07829181494661921,
      "grad_norm": 0.34993991255760193,
      "learning_rate": 0.00048050099629945914,
      "loss": 7.75,
      "step": 275
    },
    {
      "epoch": 0.07857651245551601,
      "grad_norm": 0.38793134689331055,
      "learning_rate": 0.0004804298320523769,
      "loss": 7.0029,
      "step": 276
    },
    {
      "epoch": 0.07886120996441282,
      "grad_norm": 0.4095827639102936,
      "learning_rate": 0.00048035866780529464,
      "loss": 7.1084,
      "step": 277
    },
    {
      "epoch": 0.07914590747330962,
      "grad_norm": 0.400534987449646,
      "learning_rate": 0.00048028750355821236,
      "loss": 7.5254,
      "step": 278
    },
    {
      "epoch": 0.0794306049822064,
      "grad_norm": 0.38447555899620056,
      "learning_rate": 0.00048021633931113013,
      "loss": 7.5615,
      "step": 279
    },
    {
      "epoch": 0.0797153024911032,
      "grad_norm": 0.4068629741668701,
      "learning_rate": 0.0004801451750640478,
      "loss": 7.459,
      "step": 280
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.39288195967674255,
      "learning_rate": 0.0004800740108169656,
      "loss": 7.1357,
      "step": 281
    },
    {
      "epoch": 0.0802846975088968,
      "grad_norm": 0.44044890999794006,
      "learning_rate": 0.0004800028465698833,
      "loss": 6.8838,
      "step": 282
    },
    {
      "epoch": 0.08056939501779359,
      "grad_norm": 0.3920339047908783,
      "learning_rate": 0.000479931682322801,
      "loss": 7.4258,
      "step": 283
    },
    {
      "epoch": 0.08085409252669039,
      "grad_norm": 0.42204195261001587,
      "learning_rate": 0.0004798605180757188,
      "loss": 7.2412,
      "step": 284
    },
    {
      "epoch": 0.08113879003558719,
      "grad_norm": 0.4086628258228302,
      "learning_rate": 0.00047978935382863647,
      "loss": 7.4141,
      "step": 285
    },
    {
      "epoch": 0.08142348754448399,
      "grad_norm": 0.48516544699668884,
      "learning_rate": 0.0004797181895815542,
      "loss": 6.7148,
      "step": 286
    },
    {
      "epoch": 0.08170818505338079,
      "grad_norm": 0.4337480366230011,
      "learning_rate": 0.00047964702533447197,
      "loss": 7.1201,
      "step": 287
    },
    {
      "epoch": 0.08199288256227757,
      "grad_norm": 0.4319247305393219,
      "learning_rate": 0.0004795758610873897,
      "loss": 7.1279,
      "step": 288
    },
    {
      "epoch": 0.08227758007117437,
      "grad_norm": 0.49747705459594727,
      "learning_rate": 0.00047950469684030747,
      "loss": 7.2217,
      "step": 289
    },
    {
      "epoch": 0.08256227758007118,
      "grad_norm": 0.39577603340148926,
      "learning_rate": 0.0004794335325932252,
      "loss": 7.4053,
      "step": 290
    },
    {
      "epoch": 0.08284697508896798,
      "grad_norm": 0.4405271112918854,
      "learning_rate": 0.00047936236834614286,
      "loss": 7.0312,
      "step": 291
    },
    {
      "epoch": 0.08313167259786478,
      "grad_norm": 0.3082965910434723,
      "learning_rate": 0.00047929120409906064,
      "loss": 7.7031,
      "step": 292
    },
    {
      "epoch": 0.08341637010676156,
      "grad_norm": 0.5239378809928894,
      "learning_rate": 0.00047922003985197836,
      "loss": 6.9287,
      "step": 293
    },
    {
      "epoch": 0.08370106761565836,
      "grad_norm": 0.38422203063964844,
      "learning_rate": 0.00047914887560489614,
      "loss": 7.251,
      "step": 294
    },
    {
      "epoch": 0.08398576512455516,
      "grad_norm": 0.44853416085243225,
      "learning_rate": 0.00047907771135781386,
      "loss": 7.0537,
      "step": 295
    },
    {
      "epoch": 0.08427046263345196,
      "grad_norm": 0.3908516466617584,
      "learning_rate": 0.0004790065471107316,
      "loss": 7.5029,
      "step": 296
    },
    {
      "epoch": 0.08455516014234875,
      "grad_norm": 0.5178523659706116,
      "learning_rate": 0.0004789353828636493,
      "loss": 6.6699,
      "step": 297
    },
    {
      "epoch": 0.08483985765124555,
      "grad_norm": 0.39277517795562744,
      "learning_rate": 0.00047886421861656703,
      "loss": 7.6855,
      "step": 298
    },
    {
      "epoch": 0.08512455516014235,
      "grad_norm": 0.4601745307445526,
      "learning_rate": 0.0004787930543694848,
      "loss": 6.6934,
      "step": 299
    },
    {
      "epoch": 0.08540925266903915,
      "grad_norm": 0.4405549168586731,
      "learning_rate": 0.00047872189012240253,
      "loss": 7.2129,
      "step": 300
    },
    {
      "epoch": 0.08569395017793595,
      "grad_norm": 0.4540632665157318,
      "learning_rate": 0.00047865072587532025,
      "loss": 6.9492,
      "step": 301
    },
    {
      "epoch": 0.08597864768683273,
      "grad_norm": 0.4265691936016083,
      "learning_rate": 0.000478579561628238,
      "loss": 7.1064,
      "step": 302
    },
    {
      "epoch": 0.08626334519572953,
      "grad_norm": 0.45279815793037415,
      "learning_rate": 0.0004785083973811557,
      "loss": 7.001,
      "step": 303
    },
    {
      "epoch": 0.08654804270462634,
      "grad_norm": 0.4508437216281891,
      "learning_rate": 0.0004784372331340734,
      "loss": 6.9727,
      "step": 304
    },
    {
      "epoch": 0.08683274021352314,
      "grad_norm": 0.39535853266716003,
      "learning_rate": 0.0004783660688869912,
      "loss": 7.4971,
      "step": 305
    },
    {
      "epoch": 0.08711743772241994,
      "grad_norm": 0.38876810669898987,
      "learning_rate": 0.0004782949046399089,
      "loss": 6.96,
      "step": 306
    },
    {
      "epoch": 0.08740213523131672,
      "grad_norm": 0.3352130055427551,
      "learning_rate": 0.0004782237403928267,
      "loss": 7.5664,
      "step": 307
    },
    {
      "epoch": 0.08768683274021352,
      "grad_norm": 0.43718674778938293,
      "learning_rate": 0.00047815257614574437,
      "loss": 7.3652,
      "step": 308
    },
    {
      "epoch": 0.08797153024911032,
      "grad_norm": 0.41846373677253723,
      "learning_rate": 0.0004780814118986621,
      "loss": 7.4209,
      "step": 309
    },
    {
      "epoch": 0.08825622775800712,
      "grad_norm": 0.35628408193588257,
      "learning_rate": 0.00047801024765157987,
      "loss": 7.665,
      "step": 310
    },
    {
      "epoch": 0.08854092526690391,
      "grad_norm": 0.43138930201530457,
      "learning_rate": 0.0004779390834044976,
      "loss": 7.0596,
      "step": 311
    },
    {
      "epoch": 0.08882562277580071,
      "grad_norm": 0.41381263732910156,
      "learning_rate": 0.00047786791915741537,
      "loss": 7.4902,
      "step": 312
    },
    {
      "epoch": 0.08911032028469751,
      "grad_norm": 0.44806408882141113,
      "learning_rate": 0.00047779675491033303,
      "loss": 7.3066,
      "step": 313
    },
    {
      "epoch": 0.08939501779359431,
      "grad_norm": 0.41572806239128113,
      "learning_rate": 0.00047772559066325076,
      "loss": 7.1211,
      "step": 314
    },
    {
      "epoch": 0.08967971530249111,
      "grad_norm": 0.4351762533187866,
      "learning_rate": 0.00047765442641616853,
      "loss": 7.1748,
      "step": 315
    },
    {
      "epoch": 0.0899644128113879,
      "grad_norm": 0.5767992734909058,
      "learning_rate": 0.00047758326216908626,
      "loss": 6.5059,
      "step": 316
    },
    {
      "epoch": 0.0902491103202847,
      "grad_norm": 0.4354422092437744,
      "learning_rate": 0.00047751209792200403,
      "loss": 6.9727,
      "step": 317
    },
    {
      "epoch": 0.0905338078291815,
      "grad_norm": 0.48386743664741516,
      "learning_rate": 0.00047744093367492176,
      "loss": 7.5859,
      "step": 318
    },
    {
      "epoch": 0.0908185053380783,
      "grad_norm": 0.447219580411911,
      "learning_rate": 0.0004773697694278394,
      "loss": 7.5195,
      "step": 319
    },
    {
      "epoch": 0.0911032028469751,
      "grad_norm": 0.4001949727535248,
      "learning_rate": 0.0004772986051807572,
      "loss": 7.3672,
      "step": 320
    },
    {
      "epoch": 0.09138790035587188,
      "grad_norm": 0.4028257131576538,
      "learning_rate": 0.0004772274409336749,
      "loss": 7.1143,
      "step": 321
    },
    {
      "epoch": 0.09167259786476868,
      "grad_norm": 0.6439388990402222,
      "learning_rate": 0.00047715627668659265,
      "loss": 6.9395,
      "step": 322
    },
    {
      "epoch": 0.09195729537366548,
      "grad_norm": 0.434939980506897,
      "learning_rate": 0.0004770851124395104,
      "loss": 7.0215,
      "step": 323
    },
    {
      "epoch": 0.09224199288256228,
      "grad_norm": 0.4516619145870209,
      "learning_rate": 0.00047701394819242815,
      "loss": 6.9014,
      "step": 324
    },
    {
      "epoch": 0.09252669039145907,
      "grad_norm": 0.4030342102050781,
      "learning_rate": 0.00047694278394534587,
      "loss": 7.5859,
      "step": 325
    },
    {
      "epoch": 0.09281138790035587,
      "grad_norm": 0.3350996673107147,
      "learning_rate": 0.0004768716196982636,
      "loss": 7.5254,
      "step": 326
    },
    {
      "epoch": 0.09309608540925267,
      "grad_norm": 0.5201140642166138,
      "learning_rate": 0.0004768004554511813,
      "loss": 7.3613,
      "step": 327
    },
    {
      "epoch": 0.09338078291814947,
      "grad_norm": 0.3434700667858124,
      "learning_rate": 0.0004767292912040991,
      "loss": 7.5518,
      "step": 328
    },
    {
      "epoch": 0.09366548042704627,
      "grad_norm": 0.39710113406181335,
      "learning_rate": 0.0004766581269570168,
      "loss": 7.3145,
      "step": 329
    },
    {
      "epoch": 0.09395017793594305,
      "grad_norm": 0.3940322995185852,
      "learning_rate": 0.00047658696270993454,
      "loss": 7.4756,
      "step": 330
    },
    {
      "epoch": 0.09423487544483986,
      "grad_norm": 0.35256409645080566,
      "learning_rate": 0.00047651579846285226,
      "loss": 7.3662,
      "step": 331
    },
    {
      "epoch": 0.09451957295373666,
      "grad_norm": 0.490875780582428,
      "learning_rate": 0.00047644463421577,
      "loss": 6.9189,
      "step": 332
    },
    {
      "epoch": 0.09480427046263346,
      "grad_norm": 0.3466689884662628,
      "learning_rate": 0.00047637346996868776,
      "loss": 7.7227,
      "step": 333
    },
    {
      "epoch": 0.09508896797153026,
      "grad_norm": 0.41715121269226074,
      "learning_rate": 0.0004763023057216055,
      "loss": 7.4795,
      "step": 334
    },
    {
      "epoch": 0.09537366548042704,
      "grad_norm": 0.42658162117004395,
      "learning_rate": 0.00047623114147452326,
      "loss": 7.2119,
      "step": 335
    },
    {
      "epoch": 0.09565836298932384,
      "grad_norm": 0.4596523344516754,
      "learning_rate": 0.00047615997722744093,
      "loss": 7.0312,
      "step": 336
    },
    {
      "epoch": 0.09594306049822064,
      "grad_norm": 0.44432130455970764,
      "learning_rate": 0.00047608881298035865,
      "loss": 7.1562,
      "step": 337
    },
    {
      "epoch": 0.09622775800711744,
      "grad_norm": 0.41700947284698486,
      "learning_rate": 0.00047601764873327643,
      "loss": 7.0879,
      "step": 338
    },
    {
      "epoch": 0.09651245551601423,
      "grad_norm": 0.41166216135025024,
      "learning_rate": 0.00047594648448619415,
      "loss": 7.2783,
      "step": 339
    },
    {
      "epoch": 0.09679715302491103,
      "grad_norm": 0.3937351107597351,
      "learning_rate": 0.0004758753202391119,
      "loss": 7.4902,
      "step": 340
    },
    {
      "epoch": 0.09708185053380783,
      "grad_norm": 0.49600276350975037,
      "learning_rate": 0.00047580415599202965,
      "loss": 7.0215,
      "step": 341
    },
    {
      "epoch": 0.09736654804270463,
      "grad_norm": 0.39852508902549744,
      "learning_rate": 0.0004757329917449473,
      "loss": 7.5752,
      "step": 342
    },
    {
      "epoch": 0.09765124555160143,
      "grad_norm": 0.43215256929397583,
      "learning_rate": 0.0004756618274978651,
      "loss": 7.3994,
      "step": 343
    },
    {
      "epoch": 0.09793594306049822,
      "grad_norm": 0.395315945148468,
      "learning_rate": 0.0004755906632507828,
      "loss": 7.4033,
      "step": 344
    },
    {
      "epoch": 0.09822064056939502,
      "grad_norm": 0.4142919182777405,
      "learning_rate": 0.00047551949900370054,
      "loss": 7.3242,
      "step": 345
    },
    {
      "epoch": 0.09850533807829182,
      "grad_norm": 0.5080282092094421,
      "learning_rate": 0.0004754483347566183,
      "loss": 6.625,
      "step": 346
    },
    {
      "epoch": 0.09879003558718862,
      "grad_norm": 0.44504308700561523,
      "learning_rate": 0.000475377170509536,
      "loss": 7.668,
      "step": 347
    },
    {
      "epoch": 0.09907473309608542,
      "grad_norm": 0.5334663391113281,
      "learning_rate": 0.00047530600626245376,
      "loss": 7.2246,
      "step": 348
    },
    {
      "epoch": 0.0993594306049822,
      "grad_norm": 0.4300445020198822,
      "learning_rate": 0.0004752348420153715,
      "loss": 7.2812,
      "step": 349
    },
    {
      "epoch": 0.099644128113879,
      "grad_norm": 0.43815192580223083,
      "learning_rate": 0.0004751636777682892,
      "loss": 6.8447,
      "step": 350
    },
    {
      "epoch": 0.0999288256227758,
      "grad_norm": 0.45553478598594666,
      "learning_rate": 0.000475092513521207,
      "loss": 7.3037,
      "step": 351
    },
    {
      "epoch": 0.1002135231316726,
      "grad_norm": 0.4697282612323761,
      "learning_rate": 0.0004750213492741247,
      "loss": 7.0762,
      "step": 352
    },
    {
      "epoch": 0.10049822064056939,
      "grad_norm": 0.5708103179931641,
      "learning_rate": 0.0004749501850270424,
      "loss": 7.0889,
      "step": 353
    },
    {
      "epoch": 0.10078291814946619,
      "grad_norm": 0.46478471159935,
      "learning_rate": 0.00047487902077996015,
      "loss": 7.2305,
      "step": 354
    },
    {
      "epoch": 0.10106761565836299,
      "grad_norm": 0.4347301721572876,
      "learning_rate": 0.0004748078565328779,
      "loss": 7.417,
      "step": 355
    },
    {
      "epoch": 0.10135231316725979,
      "grad_norm": 0.3864136040210724,
      "learning_rate": 0.00047473669228579565,
      "loss": 7.7119,
      "step": 356
    },
    {
      "epoch": 0.10163701067615659,
      "grad_norm": 0.3752633333206177,
      "learning_rate": 0.0004746655280387134,
      "loss": 7.6006,
      "step": 357
    },
    {
      "epoch": 0.10192170818505338,
      "grad_norm": 0.5481764674186707,
      "learning_rate": 0.00047459436379163105,
      "loss": 7.0039,
      "step": 358
    },
    {
      "epoch": 0.10220640569395018,
      "grad_norm": 0.40612098574638367,
      "learning_rate": 0.0004745231995445488,
      "loss": 7.4434,
      "step": 359
    },
    {
      "epoch": 0.10249110320284698,
      "grad_norm": 0.4205334484577179,
      "learning_rate": 0.00047445203529746655,
      "loss": 7.2832,
      "step": 360
    },
    {
      "epoch": 0.10277580071174378,
      "grad_norm": 0.3059813678264618,
      "learning_rate": 0.0004743808710503843,
      "loss": 7.6846,
      "step": 361
    },
    {
      "epoch": 0.10306049822064058,
      "grad_norm": 0.48755931854248047,
      "learning_rate": 0.00047430970680330205,
      "loss": 6.9141,
      "step": 362
    },
    {
      "epoch": 0.10334519572953736,
      "grad_norm": 0.42173999547958374,
      "learning_rate": 0.00047423854255621977,
      "loss": 7.5166,
      "step": 363
    },
    {
      "epoch": 0.10362989323843416,
      "grad_norm": 0.5560771226882935,
      "learning_rate": 0.0004741673783091375,
      "loss": 7.418,
      "step": 364
    },
    {
      "epoch": 0.10391459074733096,
      "grad_norm": 0.6154012680053711,
      "learning_rate": 0.0004740962140620552,
      "loss": 7.2734,
      "step": 365
    },
    {
      "epoch": 0.10419928825622776,
      "grad_norm": 0.4501911997795105,
      "learning_rate": 0.000474025049814973,
      "loss": 6.9023,
      "step": 366
    },
    {
      "epoch": 0.10448398576512455,
      "grad_norm": 0.4486687183380127,
      "learning_rate": 0.0004739538855678907,
      "loss": 7.3291,
      "step": 367
    },
    {
      "epoch": 0.10476868327402135,
      "grad_norm": 0.406693696975708,
      "learning_rate": 0.00047388272132080844,
      "loss": 7.501,
      "step": 368
    },
    {
      "epoch": 0.10505338078291815,
      "grad_norm": 0.4880397915840149,
      "learning_rate": 0.0004738115570737262,
      "loss": 6.6982,
      "step": 369
    },
    {
      "epoch": 0.10533807829181495,
      "grad_norm": 0.4266977608203888,
      "learning_rate": 0.0004737403928266439,
      "loss": 7.2031,
      "step": 370
    },
    {
      "epoch": 0.10562277580071175,
      "grad_norm": 0.4237028658390045,
      "learning_rate": 0.0004736692285795616,
      "loss": 6.9111,
      "step": 371
    },
    {
      "epoch": 0.10590747330960854,
      "grad_norm": 0.5166893601417542,
      "learning_rate": 0.0004735980643324794,
      "loss": 7.3945,
      "step": 372
    },
    {
      "epoch": 0.10619217081850534,
      "grad_norm": 0.4069686233997345,
      "learning_rate": 0.0004735269000853971,
      "loss": 7.54,
      "step": 373
    },
    {
      "epoch": 0.10647686832740214,
      "grad_norm": 0.43857699632644653,
      "learning_rate": 0.0004734557358383149,
      "loss": 7.3135,
      "step": 374
    },
    {
      "epoch": 0.10676156583629894,
      "grad_norm": 0.39594122767448425,
      "learning_rate": 0.00047338457159123255,
      "loss": 7.8369,
      "step": 375
    },
    {
      "epoch": 0.10704626334519574,
      "grad_norm": 0.4282735586166382,
      "learning_rate": 0.00047331340734415027,
      "loss": 7.4375,
      "step": 376
    },
    {
      "epoch": 0.10733096085409252,
      "grad_norm": 0.45673078298568726,
      "learning_rate": 0.00047324224309706805,
      "loss": 7.3848,
      "step": 377
    },
    {
      "epoch": 0.10761565836298932,
      "grad_norm": 0.4529609978199005,
      "learning_rate": 0.00047317107884998577,
      "loss": 7.1904,
      "step": 378
    },
    {
      "epoch": 0.10790035587188612,
      "grad_norm": 0.508343517780304,
      "learning_rate": 0.00047309991460290355,
      "loss": 7.1104,
      "step": 379
    },
    {
      "epoch": 0.10818505338078292,
      "grad_norm": 0.5695661306381226,
      "learning_rate": 0.00047302875035582127,
      "loss": 6.9775,
      "step": 380
    },
    {
      "epoch": 0.10846975088967971,
      "grad_norm": 0.4289855659008026,
      "learning_rate": 0.00047295758610873894,
      "loss": 7.3799,
      "step": 381
    },
    {
      "epoch": 0.10875444839857651,
      "grad_norm": 0.45409122109413147,
      "learning_rate": 0.0004728864218616567,
      "loss": 6.877,
      "step": 382
    },
    {
      "epoch": 0.10903914590747331,
      "grad_norm": 0.43355792760849,
      "learning_rate": 0.00047281525761457444,
      "loss": 7.1543,
      "step": 383
    },
    {
      "epoch": 0.10932384341637011,
      "grad_norm": 0.3788268268108368,
      "learning_rate": 0.00047274409336749216,
      "loss": 7.582,
      "step": 384
    },
    {
      "epoch": 0.10960854092526691,
      "grad_norm": 0.4172390103340149,
      "learning_rate": 0.00047267292912040994,
      "loss": 7.4766,
      "step": 385
    },
    {
      "epoch": 0.1098932384341637,
      "grad_norm": 0.5651355385780334,
      "learning_rate": 0.00047260176487332766,
      "loss": 6.8984,
      "step": 386
    },
    {
      "epoch": 0.1101779359430605,
      "grad_norm": 0.4061424136161804,
      "learning_rate": 0.0004725306006262454,
      "loss": 7.6387,
      "step": 387
    },
    {
      "epoch": 0.1104626334519573,
      "grad_norm": 0.41827505826950073,
      "learning_rate": 0.0004724594363791631,
      "loss": 7.2285,
      "step": 388
    },
    {
      "epoch": 0.1107473309608541,
      "grad_norm": 0.37790295481681824,
      "learning_rate": 0.00047238827213208083,
      "loss": 7.583,
      "step": 389
    },
    {
      "epoch": 0.1110320284697509,
      "grad_norm": 0.41281452775001526,
      "learning_rate": 0.0004723171078849986,
      "loss": 7.4941,
      "step": 390
    },
    {
      "epoch": 0.11131672597864768,
      "grad_norm": 0.41881147027015686,
      "learning_rate": 0.00047224594363791633,
      "loss": 7.3848,
      "step": 391
    },
    {
      "epoch": 0.11160142348754448,
      "grad_norm": 0.41661399602890015,
      "learning_rate": 0.00047217477939083405,
      "loss": 7.4121,
      "step": 392
    },
    {
      "epoch": 0.11188612099644128,
      "grad_norm": 0.4192899763584137,
      "learning_rate": 0.0004721036151437518,
      "loss": 7.0596,
      "step": 393
    },
    {
      "epoch": 0.11217081850533808,
      "grad_norm": 0.45563167333602905,
      "learning_rate": 0.0004720324508966695,
      "loss": 7.4004,
      "step": 394
    },
    {
      "epoch": 0.11245551601423487,
      "grad_norm": 0.4283228814601898,
      "learning_rate": 0.0004719612866495873,
      "loss": 7.6162,
      "step": 395
    },
    {
      "epoch": 0.11274021352313167,
      "grad_norm": 0.4331873655319214,
      "learning_rate": 0.000471890122402505,
      "loss": 7.001,
      "step": 396
    },
    {
      "epoch": 0.11302491103202847,
      "grad_norm": 0.4580132067203522,
      "learning_rate": 0.0004718189581554228,
      "loss": 7.0654,
      "step": 397
    },
    {
      "epoch": 0.11330960854092527,
      "grad_norm": 0.46618202328681946,
      "learning_rate": 0.00047174779390834044,
      "loss": 7.0742,
      "step": 398
    },
    {
      "epoch": 0.11359430604982207,
      "grad_norm": 0.4677276909351349,
      "learning_rate": 0.00047167662966125817,
      "loss": 7.1973,
      "step": 399
    },
    {
      "epoch": 0.11387900355871886,
      "grad_norm": 0.4366690516471863,
      "learning_rate": 0.00047160546541417594,
      "loss": 7.4443,
      "step": 400
    },
    {
      "epoch": 0.11387900355871886,
      "eval_bleu": 0.06055701380020064,
      "eval_loss": 7.00390625,
      "eval_runtime": 195.2568,
      "eval_samples_per_second": 1.454,
      "eval_steps_per_second": 0.092,
      "step": 400
    },
    {
      "epoch": 0.11416370106761566,
      "grad_norm": 0.3341705799102783,
      "learning_rate": 0.00047153430116709367,
      "loss": 7.7119,
      "step": 401
    },
    {
      "epoch": 0.11444839857651246,
      "grad_norm": 0.4076211154460907,
      "learning_rate": 0.0004714631369200114,
      "loss": 7.4521,
      "step": 402
    },
    {
      "epoch": 0.11473309608540926,
      "grad_norm": 0.3936387598514557,
      "learning_rate": 0.0004713919726729291,
      "loss": 7.2227,
      "step": 403
    },
    {
      "epoch": 0.11501779359430606,
      "grad_norm": 0.39969155192375183,
      "learning_rate": 0.00047132080842584683,
      "loss": 7.5381,
      "step": 404
    },
    {
      "epoch": 0.11530249110320284,
      "grad_norm": 0.46734946966171265,
      "learning_rate": 0.0004712496441787646,
      "loss": 7.4033,
      "step": 405
    },
    {
      "epoch": 0.11558718861209964,
      "grad_norm": 0.4855714738368988,
      "learning_rate": 0.00047117847993168233,
      "loss": 7.457,
      "step": 406
    },
    {
      "epoch": 0.11587188612099644,
      "grad_norm": 0.5113205313682556,
      "learning_rate": 0.00047110731568460006,
      "loss": 7.1904,
      "step": 407
    },
    {
      "epoch": 0.11615658362989324,
      "grad_norm": 0.3756895065307617,
      "learning_rate": 0.00047103615143751783,
      "loss": 7.5938,
      "step": 408
    },
    {
      "epoch": 0.11644128113879003,
      "grad_norm": 0.37924525141716003,
      "learning_rate": 0.0004709649871904355,
      "loss": 7.585,
      "step": 409
    },
    {
      "epoch": 0.11672597864768683,
      "grad_norm": 0.4761236608028412,
      "learning_rate": 0.0004708938229433533,
      "loss": 7.3105,
      "step": 410
    },
    {
      "epoch": 0.11701067615658363,
      "grad_norm": 0.4178760051727295,
      "learning_rate": 0.000470822658696271,
      "loss": 7.501,
      "step": 411
    },
    {
      "epoch": 0.11729537366548043,
      "grad_norm": 0.5172440409660339,
      "learning_rate": 0.0004707514944491887,
      "loss": 7.1963,
      "step": 412
    },
    {
      "epoch": 0.11758007117437723,
      "grad_norm": 0.7484517693519592,
      "learning_rate": 0.0004706803302021065,
      "loss": 6.7061,
      "step": 413
    },
    {
      "epoch": 0.11786476868327402,
      "grad_norm": 0.47221601009368896,
      "learning_rate": 0.0004706091659550242,
      "loss": 7.7109,
      "step": 414
    },
    {
      "epoch": 0.11814946619217082,
      "grad_norm": 0.39013606309890747,
      "learning_rate": 0.00047053800170794195,
      "loss": 7.75,
      "step": 415
    },
    {
      "epoch": 0.11843416370106762,
      "grad_norm": 0.549603283405304,
      "learning_rate": 0.00047046683746085967,
      "loss": 6.8496,
      "step": 416
    },
    {
      "epoch": 0.11871886120996442,
      "grad_norm": 0.5139209628105164,
      "learning_rate": 0.0004703956732137774,
      "loss": 7.0254,
      "step": 417
    },
    {
      "epoch": 0.11900355871886122,
      "grad_norm": 0.5115612745285034,
      "learning_rate": 0.00047032450896669517,
      "loss": 7.21,
      "step": 418
    },
    {
      "epoch": 0.119288256227758,
      "grad_norm": 0.4744890034198761,
      "learning_rate": 0.0004702533447196129,
      "loss": 7.0742,
      "step": 419
    },
    {
      "epoch": 0.1195729537366548,
      "grad_norm": 0.43311771750450134,
      "learning_rate": 0.00047018218047253056,
      "loss": 7.0781,
      "step": 420
    },
    {
      "epoch": 0.1198576512455516,
      "grad_norm": 0.4734751582145691,
      "learning_rate": 0.00047011101622544834,
      "loss": 7.2168,
      "step": 421
    },
    {
      "epoch": 0.1201423487544484,
      "grad_norm": 0.5691256523132324,
      "learning_rate": 0.00047003985197836606,
      "loss": 6.623,
      "step": 422
    },
    {
      "epoch": 0.12042704626334519,
      "grad_norm": 0.40093910694122314,
      "learning_rate": 0.00046996868773128384,
      "loss": 7.4346,
      "step": 423
    },
    {
      "epoch": 0.12071174377224199,
      "grad_norm": 0.42212653160095215,
      "learning_rate": 0.00046989752348420156,
      "loss": 7.4355,
      "step": 424
    },
    {
      "epoch": 0.12099644128113879,
      "grad_norm": 0.44689905643463135,
      "learning_rate": 0.0004698263592371193,
      "loss": 7.6377,
      "step": 425
    },
    {
      "epoch": 0.12128113879003559,
      "grad_norm": 0.4593307375907898,
      "learning_rate": 0.000469755194990037,
      "loss": 7.4443,
      "step": 426
    },
    {
      "epoch": 0.12156583629893239,
      "grad_norm": 0.5486469268798828,
      "learning_rate": 0.00046968403074295473,
      "loss": 6.8281,
      "step": 427
    },
    {
      "epoch": 0.12185053380782918,
      "grad_norm": 0.4106900990009308,
      "learning_rate": 0.0004696128664958725,
      "loss": 7.2324,
      "step": 428
    },
    {
      "epoch": 0.12213523131672598,
      "grad_norm": 0.42247626185417175,
      "learning_rate": 0.00046954170224879023,
      "loss": 7.2988,
      "step": 429
    },
    {
      "epoch": 0.12241992882562278,
      "grad_norm": 0.5033213496208191,
      "learning_rate": 0.00046947053800170795,
      "loss": 7.291,
      "step": 430
    },
    {
      "epoch": 0.12270462633451958,
      "grad_norm": 0.42371541261672974,
      "learning_rate": 0.0004693993737546257,
      "loss": 7.458,
      "step": 431
    },
    {
      "epoch": 0.12298932384341638,
      "grad_norm": 0.5228193998336792,
      "learning_rate": 0.0004693282095075434,
      "loss": 7.1699,
      "step": 432
    },
    {
      "epoch": 0.12327402135231316,
      "grad_norm": 0.3674987852573395,
      "learning_rate": 0.0004692570452604611,
      "loss": 7.5742,
      "step": 433
    },
    {
      "epoch": 0.12355871886120996,
      "grad_norm": 0.5292471051216125,
      "learning_rate": 0.0004691858810133789,
      "loss": 6.7803,
      "step": 434
    },
    {
      "epoch": 0.12384341637010676,
      "grad_norm": 0.49759700894355774,
      "learning_rate": 0.0004691147167662966,
      "loss": 7.0801,
      "step": 435
    },
    {
      "epoch": 0.12412811387900356,
      "grad_norm": 0.4338753819465637,
      "learning_rate": 0.0004690435525192144,
      "loss": 7.7227,
      "step": 436
    },
    {
      "epoch": 0.12441281138790036,
      "grad_norm": 0.4573684632778168,
      "learning_rate": 0.00046897238827213207,
      "loss": 7.3672,
      "step": 437
    },
    {
      "epoch": 0.12469750889679715,
      "grad_norm": 0.504006028175354,
      "learning_rate": 0.0004689012240250498,
      "loss": 7.1426,
      "step": 438
    },
    {
      "epoch": 0.12498220640569395,
      "grad_norm": 0.5081356167793274,
      "learning_rate": 0.00046883005977796756,
      "loss": 7.0537,
      "step": 439
    },
    {
      "epoch": 0.12526690391459075,
      "grad_norm": 0.48333534598350525,
      "learning_rate": 0.0004687588955308853,
      "loss": 7.0693,
      "step": 440
    },
    {
      "epoch": 0.12555160142348754,
      "grad_norm": 0.4130278527736664,
      "learning_rate": 0.00046868773128380306,
      "loss": 7.4668,
      "step": 441
    },
    {
      "epoch": 0.12583629893238435,
      "grad_norm": 0.4785782992839813,
      "learning_rate": 0.0004686165670367208,
      "loss": 7.0684,
      "step": 442
    },
    {
      "epoch": 0.12612099644128114,
      "grad_norm": 0.5069476366043091,
      "learning_rate": 0.00046854540278963846,
      "loss": 7.2559,
      "step": 443
    },
    {
      "epoch": 0.12640569395017795,
      "grad_norm": 0.37336277961730957,
      "learning_rate": 0.00046847423854255623,
      "loss": 7.626,
      "step": 444
    },
    {
      "epoch": 0.12669039145907474,
      "grad_norm": 0.401167631149292,
      "learning_rate": 0.00046840307429547396,
      "loss": 7.2949,
      "step": 445
    },
    {
      "epoch": 0.12697508896797152,
      "grad_norm": 0.5884152054786682,
      "learning_rate": 0.00046833191004839173,
      "loss": 7.0771,
      "step": 446
    },
    {
      "epoch": 0.12725978647686834,
      "grad_norm": 0.43068522214889526,
      "learning_rate": 0.00046826074580130946,
      "loss": 7.6777,
      "step": 447
    },
    {
      "epoch": 0.12754448398576512,
      "grad_norm": 0.44112157821655273,
      "learning_rate": 0.0004681895815542271,
      "loss": 7.376,
      "step": 448
    },
    {
      "epoch": 0.1278291814946619,
      "grad_norm": 0.46927693486213684,
      "learning_rate": 0.0004681184173071449,
      "loss": 7.2109,
      "step": 449
    },
    {
      "epoch": 0.12811387900355872,
      "grad_norm": 0.3317018449306488,
      "learning_rate": 0.0004680472530600626,
      "loss": 7.751,
      "step": 450
    },
    {
      "epoch": 0.1283985765124555,
      "grad_norm": 0.4970950782299042,
      "learning_rate": 0.00046797608881298035,
      "loss": 6.835,
      "step": 451
    },
    {
      "epoch": 0.12868327402135232,
      "grad_norm": 0.3675149381160736,
      "learning_rate": 0.0004679049245658981,
      "loss": 7.8193,
      "step": 452
    },
    {
      "epoch": 0.1289679715302491,
      "grad_norm": 0.4956851005554199,
      "learning_rate": 0.00046783376031881585,
      "loss": 6.8252,
      "step": 453
    },
    {
      "epoch": 0.1292526690391459,
      "grad_norm": 0.4222683012485504,
      "learning_rate": 0.00046776259607173357,
      "loss": 7.501,
      "step": 454
    },
    {
      "epoch": 0.1295373665480427,
      "grad_norm": 0.4823405146598816,
      "learning_rate": 0.0004676914318246513,
      "loss": 7.1631,
      "step": 455
    },
    {
      "epoch": 0.1298220640569395,
      "grad_norm": 0.525143563747406,
      "learning_rate": 0.000467620267577569,
      "loss": 7.2627,
      "step": 456
    },
    {
      "epoch": 0.1301067615658363,
      "grad_norm": 0.4748491048812866,
      "learning_rate": 0.0004675491033304868,
      "loss": 7.4766,
      "step": 457
    },
    {
      "epoch": 0.1303914590747331,
      "grad_norm": 0.4361896514892578,
      "learning_rate": 0.0004674779390834045,
      "loss": 7.2959,
      "step": 458
    },
    {
      "epoch": 0.13067615658362988,
      "grad_norm": 0.5087428689002991,
      "learning_rate": 0.0004674067748363223,
      "loss": 7.3447,
      "step": 459
    },
    {
      "epoch": 0.1309608540925267,
      "grad_norm": 0.5033571720123291,
      "learning_rate": 0.00046733561058923996,
      "loss": 7.0791,
      "step": 460
    },
    {
      "epoch": 0.13124555160142348,
      "grad_norm": 0.5781779885292053,
      "learning_rate": 0.0004672644463421577,
      "loss": 7.1611,
      "step": 461
    },
    {
      "epoch": 0.1315302491103203,
      "grad_norm": 0.45533907413482666,
      "learning_rate": 0.00046719328209507546,
      "loss": 7.1758,
      "step": 462
    },
    {
      "epoch": 0.13181494661921708,
      "grad_norm": 0.4691354036331177,
      "learning_rate": 0.0004671221178479932,
      "loss": 7.375,
      "step": 463
    },
    {
      "epoch": 0.13209964412811387,
      "grad_norm": 0.4058699905872345,
      "learning_rate": 0.00046705095360091096,
      "loss": 7.7324,
      "step": 464
    },
    {
      "epoch": 0.13238434163701068,
      "grad_norm": 0.559533953666687,
      "learning_rate": 0.00046697978935382863,
      "loss": 6.6924,
      "step": 465
    },
    {
      "epoch": 0.13266903914590747,
      "grad_norm": 0.4726496934890747,
      "learning_rate": 0.00046690862510674635,
      "loss": 6.9219,
      "step": 466
    },
    {
      "epoch": 0.13295373665480428,
      "grad_norm": 0.4936983287334442,
      "learning_rate": 0.00046683746085966413,
      "loss": 7.1094,
      "step": 467
    },
    {
      "epoch": 0.13323843416370107,
      "grad_norm": 0.4566884934902191,
      "learning_rate": 0.00046676629661258185,
      "loss": 7.5312,
      "step": 468
    },
    {
      "epoch": 0.13352313167259786,
      "grad_norm": 0.4185795187950134,
      "learning_rate": 0.00046669513236549957,
      "loss": 7.5166,
      "step": 469
    },
    {
      "epoch": 0.13380782918149467,
      "grad_norm": 0.5276864767074585,
      "learning_rate": 0.00046662396811841735,
      "loss": 7.4355,
      "step": 470
    },
    {
      "epoch": 0.13409252669039146,
      "grad_norm": 0.5576158761978149,
      "learning_rate": 0.000466552803871335,
      "loss": 7.1006,
      "step": 471
    },
    {
      "epoch": 0.13437722419928827,
      "grad_norm": 0.4316464960575104,
      "learning_rate": 0.0004664816396242528,
      "loss": 7.4736,
      "step": 472
    },
    {
      "epoch": 0.13466192170818506,
      "grad_norm": 0.5265529155731201,
      "learning_rate": 0.0004664104753771705,
      "loss": 6.8154,
      "step": 473
    },
    {
      "epoch": 0.13494661921708184,
      "grad_norm": 0.4530062973499298,
      "learning_rate": 0.00046633931113008824,
      "loss": 7.377,
      "step": 474
    },
    {
      "epoch": 0.13523131672597866,
      "grad_norm": 0.47892332077026367,
      "learning_rate": 0.000466268146883006,
      "loss": 6.8652,
      "step": 475
    },
    {
      "epoch": 0.13551601423487544,
      "grad_norm": 0.5138373970985413,
      "learning_rate": 0.0004661969826359237,
      "loss": 7.0654,
      "step": 476
    },
    {
      "epoch": 0.13580071174377223,
      "grad_norm": 0.4899928867816925,
      "learning_rate": 0.00046612581838884146,
      "loss": 7.2871,
      "step": 477
    },
    {
      "epoch": 0.13608540925266904,
      "grad_norm": 0.5416154861450195,
      "learning_rate": 0.0004660546541417592,
      "loss": 7.0,
      "step": 478
    },
    {
      "epoch": 0.13637010676156583,
      "grad_norm": 0.4114988148212433,
      "learning_rate": 0.0004659834898946769,
      "loss": 7.1953,
      "step": 479
    },
    {
      "epoch": 0.13665480427046264,
      "grad_norm": 0.480387419462204,
      "learning_rate": 0.0004659123256475947,
      "loss": 7.2236,
      "step": 480
    },
    {
      "epoch": 0.13693950177935943,
      "grad_norm": 0.4656660854816437,
      "learning_rate": 0.0004658411614005124,
      "loss": 7.5098,
      "step": 481
    },
    {
      "epoch": 0.13722419928825622,
      "grad_norm": 0.49934709072113037,
      "learning_rate": 0.0004657699971534301,
      "loss": 7.1338,
      "step": 482
    },
    {
      "epoch": 0.13750889679715303,
      "grad_norm": 0.4244711995124817,
      "learning_rate": 0.00046569883290634785,
      "loss": 7.3154,
      "step": 483
    },
    {
      "epoch": 0.13779359430604982,
      "grad_norm": 0.5288747549057007,
      "learning_rate": 0.0004656276686592656,
      "loss": 6.918,
      "step": 484
    },
    {
      "epoch": 0.13807829181494663,
      "grad_norm": 0.5109588503837585,
      "learning_rate": 0.00046555650441218335,
      "loss": 7.1914,
      "step": 485
    },
    {
      "epoch": 0.13836298932384342,
      "grad_norm": 0.4920799434185028,
      "learning_rate": 0.0004654853401651011,
      "loss": 7.457,
      "step": 486
    },
    {
      "epoch": 0.1386476868327402,
      "grad_norm": 0.3864028751850128,
      "learning_rate": 0.0004654141759180188,
      "loss": 7.877,
      "step": 487
    },
    {
      "epoch": 0.13893238434163702,
      "grad_norm": 0.41772550344467163,
      "learning_rate": 0.0004653430116709365,
      "loss": 7.5791,
      "step": 488
    },
    {
      "epoch": 0.1392170818505338,
      "grad_norm": 0.527531087398529,
      "learning_rate": 0.00046527184742385424,
      "loss": 6.834,
      "step": 489
    },
    {
      "epoch": 0.13950177935943062,
      "grad_norm": 0.4375729262828827,
      "learning_rate": 0.000465200683176772,
      "loss": 7.208,
      "step": 490
    },
    {
      "epoch": 0.1397864768683274,
      "grad_norm": 0.5000871419906616,
      "learning_rate": 0.00046512951892968974,
      "loss": 7.2734,
      "step": 491
    },
    {
      "epoch": 0.1400711743772242,
      "grad_norm": 0.39341986179351807,
      "learning_rate": 0.00046505835468260747,
      "loss": 7.627,
      "step": 492
    },
    {
      "epoch": 0.140355871886121,
      "grad_norm": 0.5892451405525208,
      "learning_rate": 0.0004649871904355252,
      "loss": 6.7764,
      "step": 493
    },
    {
      "epoch": 0.1406405693950178,
      "grad_norm": 0.4922020137310028,
      "learning_rate": 0.0004649160261884429,
      "loss": 7.3164,
      "step": 494
    },
    {
      "epoch": 0.1409252669039146,
      "grad_norm": 0.4670257568359375,
      "learning_rate": 0.0004648448619413607,
      "loss": 7.1924,
      "step": 495
    },
    {
      "epoch": 0.1412099644128114,
      "grad_norm": 0.5158605575561523,
      "learning_rate": 0.0004647736976942784,
      "loss": 7.0537,
      "step": 496
    },
    {
      "epoch": 0.14149466192170818,
      "grad_norm": 0.48508933186531067,
      "learning_rate": 0.00046470253344719614,
      "loss": 6.9873,
      "step": 497
    },
    {
      "epoch": 0.141779359430605,
      "grad_norm": 0.4340175688266754,
      "learning_rate": 0.0004646313692001139,
      "loss": 7.2236,
      "step": 498
    },
    {
      "epoch": 0.14206405693950178,
      "grad_norm": 0.4614635109901428,
      "learning_rate": 0.0004645602049530316,
      "loss": 7.4971,
      "step": 499
    },
    {
      "epoch": 0.1423487544483986,
      "grad_norm": 0.43651285767555237,
      "learning_rate": 0.0004644890407059493,
      "loss": 7.0176,
      "step": 500
    },
    {
      "epoch": 0.14263345195729538,
      "grad_norm": 0.4170750081539154,
      "learning_rate": 0.0004644178764588671,
      "loss": 7.3975,
      "step": 501
    },
    {
      "epoch": 0.14291814946619216,
      "grad_norm": 0.45592260360717773,
      "learning_rate": 0.0004643467122117848,
      "loss": 7.4209,
      "step": 502
    },
    {
      "epoch": 0.14320284697508898,
      "grad_norm": 0.5318942666053772,
      "learning_rate": 0.0004642755479647026,
      "loss": 6.6689,
      "step": 503
    },
    {
      "epoch": 0.14348754448398576,
      "grad_norm": 0.43632060289382935,
      "learning_rate": 0.0004642043837176203,
      "loss": 7.3867,
      "step": 504
    },
    {
      "epoch": 0.14377224199288255,
      "grad_norm": 0.48805883526802063,
      "learning_rate": 0.00046413321947053797,
      "loss": 7.0771,
      "step": 505
    },
    {
      "epoch": 0.14405693950177936,
      "grad_norm": 0.5213673710823059,
      "learning_rate": 0.00046406205522345575,
      "loss": 7.0059,
      "step": 506
    },
    {
      "epoch": 0.14434163701067615,
      "grad_norm": 0.4732288420200348,
      "learning_rate": 0.00046399089097637347,
      "loss": 7.2939,
      "step": 507
    },
    {
      "epoch": 0.14462633451957296,
      "grad_norm": 0.44369205832481384,
      "learning_rate": 0.00046391972672929125,
      "loss": 7.5195,
      "step": 508
    },
    {
      "epoch": 0.14491103202846975,
      "grad_norm": 0.48915088176727295,
      "learning_rate": 0.00046384856248220897,
      "loss": 7.1377,
      "step": 509
    },
    {
      "epoch": 0.14519572953736654,
      "grad_norm": 0.4987899959087372,
      "learning_rate": 0.00046377739823512664,
      "loss": 7.0449,
      "step": 510
    },
    {
      "epoch": 0.14548042704626335,
      "grad_norm": 0.48522457480430603,
      "learning_rate": 0.0004637062339880444,
      "loss": 6.9531,
      "step": 511
    },
    {
      "epoch": 0.14576512455516014,
      "grad_norm": 0.45173463225364685,
      "learning_rate": 0.00046363506974096214,
      "loss": 7.4492,
      "step": 512
    },
    {
      "epoch": 0.14604982206405695,
      "grad_norm": 0.4935166835784912,
      "learning_rate": 0.0004635639054938799,
      "loss": 7.6309,
      "step": 513
    },
    {
      "epoch": 0.14633451957295374,
      "grad_norm": 0.467246949672699,
      "learning_rate": 0.00046349274124679764,
      "loss": 7.3164,
      "step": 514
    },
    {
      "epoch": 0.14661921708185052,
      "grad_norm": 0.4763619005680084,
      "learning_rate": 0.00046342157699971536,
      "loss": 7.2002,
      "step": 515
    },
    {
      "epoch": 0.14690391459074734,
      "grad_norm": 0.43141141533851624,
      "learning_rate": 0.0004633504127526331,
      "loss": 7.6465,
      "step": 516
    },
    {
      "epoch": 0.14718861209964412,
      "grad_norm": 0.4468206763267517,
      "learning_rate": 0.0004632792485055508,
      "loss": 7.3887,
      "step": 517
    },
    {
      "epoch": 0.14747330960854094,
      "grad_norm": 0.4942794740200043,
      "learning_rate": 0.00046320808425846853,
      "loss": 6.8789,
      "step": 518
    },
    {
      "epoch": 0.14775800711743772,
      "grad_norm": 0.4572586715221405,
      "learning_rate": 0.0004631369200113863,
      "loss": 7.3047,
      "step": 519
    },
    {
      "epoch": 0.1480427046263345,
      "grad_norm": 0.47273606061935425,
      "learning_rate": 0.00046306575576430403,
      "loss": 6.9844,
      "step": 520
    },
    {
      "epoch": 0.14832740213523132,
      "grad_norm": 0.5519757866859436,
      "learning_rate": 0.00046299459151722175,
      "loss": 7.2256,
      "step": 521
    },
    {
      "epoch": 0.1486120996441281,
      "grad_norm": 0.46243593096733093,
      "learning_rate": 0.0004629234272701395,
      "loss": 7.3311,
      "step": 522
    },
    {
      "epoch": 0.14889679715302492,
      "grad_norm": 0.5186846256256104,
      "learning_rate": 0.0004628522630230572,
      "loss": 7.0801,
      "step": 523
    },
    {
      "epoch": 0.1491814946619217,
      "grad_norm": 0.47964605689048767,
      "learning_rate": 0.000462781098775975,
      "loss": 7.2568,
      "step": 524
    },
    {
      "epoch": 0.1494661921708185,
      "grad_norm": 0.48539063334465027,
      "learning_rate": 0.0004627099345288927,
      "loss": 6.9678,
      "step": 525
    },
    {
      "epoch": 0.1497508896797153,
      "grad_norm": 0.45621731877326965,
      "learning_rate": 0.0004626387702818105,
      "loss": 7.4131,
      "step": 526
    },
    {
      "epoch": 0.1500355871886121,
      "grad_norm": 0.4345969557762146,
      "learning_rate": 0.00046256760603472814,
      "loss": 7.4258,
      "step": 527
    },
    {
      "epoch": 0.1503202846975089,
      "grad_norm": 0.4568675756454468,
      "learning_rate": 0.00046249644178764587,
      "loss": 7.2705,
      "step": 528
    },
    {
      "epoch": 0.1506049822064057,
      "grad_norm": 0.6596431732177734,
      "learning_rate": 0.00046242527754056364,
      "loss": 6.4834,
      "step": 529
    },
    {
      "epoch": 0.15088967971530248,
      "grad_norm": 0.5479394197463989,
      "learning_rate": 0.00046235411329348137,
      "loss": 6.5928,
      "step": 530
    },
    {
      "epoch": 0.1511743772241993,
      "grad_norm": 0.47287479043006897,
      "learning_rate": 0.0004622829490463991,
      "loss": 7.1133,
      "step": 531
    },
    {
      "epoch": 0.15145907473309608,
      "grad_norm": 0.42825841903686523,
      "learning_rate": 0.00046221178479931687,
      "loss": 7.6221,
      "step": 532
    },
    {
      "epoch": 0.15174377224199287,
      "grad_norm": 0.6770346164703369,
      "learning_rate": 0.00046214062055223453,
      "loss": 6.1582,
      "step": 533
    },
    {
      "epoch": 0.15202846975088968,
      "grad_norm": 0.46598076820373535,
      "learning_rate": 0.0004620694563051523,
      "loss": 7.334,
      "step": 534
    },
    {
      "epoch": 0.15231316725978647,
      "grad_norm": 0.48289692401885986,
      "learning_rate": 0.00046199829205807003,
      "loss": 7.0273,
      "step": 535
    },
    {
      "epoch": 0.15259786476868328,
      "grad_norm": 0.5073385238647461,
      "learning_rate": 0.00046192712781098776,
      "loss": 6.874,
      "step": 536
    },
    {
      "epoch": 0.15288256227758007,
      "grad_norm": 0.3938785791397095,
      "learning_rate": 0.00046185596356390553,
      "loss": 7.749,
      "step": 537
    },
    {
      "epoch": 0.15316725978647686,
      "grad_norm": 0.4579390287399292,
      "learning_rate": 0.0004617847993168232,
      "loss": 7.1562,
      "step": 538
    },
    {
      "epoch": 0.15345195729537367,
      "grad_norm": 0.4372034966945648,
      "learning_rate": 0.000461713635069741,
      "loss": 7.2236,
      "step": 539
    },
    {
      "epoch": 0.15373665480427046,
      "grad_norm": 0.4431258738040924,
      "learning_rate": 0.0004616424708226587,
      "loss": 7.3242,
      "step": 540
    },
    {
      "epoch": 0.15402135231316727,
      "grad_norm": 0.43316811323165894,
      "learning_rate": 0.0004615713065755764,
      "loss": 7.3438,
      "step": 541
    },
    {
      "epoch": 0.15430604982206406,
      "grad_norm": 0.4466095566749573,
      "learning_rate": 0.0004615001423284942,
      "loss": 7.2197,
      "step": 542
    },
    {
      "epoch": 0.15459074733096084,
      "grad_norm": 0.45385703444480896,
      "learning_rate": 0.0004614289780814119,
      "loss": 7.3252,
      "step": 543
    },
    {
      "epoch": 0.15487544483985766,
      "grad_norm": 0.4653369188308716,
      "learning_rate": 0.00046135781383432965,
      "loss": 7.4531,
      "step": 544
    },
    {
      "epoch": 0.15516014234875444,
      "grad_norm": 0.4816647469997406,
      "learning_rate": 0.00046128664958724737,
      "loss": 7.3721,
      "step": 545
    },
    {
      "epoch": 0.15544483985765126,
      "grad_norm": 0.47521352767944336,
      "learning_rate": 0.0004612154853401651,
      "loss": 7.2432,
      "step": 546
    },
    {
      "epoch": 0.15572953736654804,
      "grad_norm": 0.34924232959747314,
      "learning_rate": 0.00046114432109308287,
      "loss": 7.96,
      "step": 547
    },
    {
      "epoch": 0.15601423487544483,
      "grad_norm": 0.5224582552909851,
      "learning_rate": 0.0004610731568460006,
      "loss": 6.9951,
      "step": 548
    },
    {
      "epoch": 0.15629893238434164,
      "grad_norm": 0.49455711245536804,
      "learning_rate": 0.0004610019925989183,
      "loss": 7.2002,
      "step": 549
    },
    {
      "epoch": 0.15658362989323843,
      "grad_norm": 0.46338167786598206,
      "learning_rate": 0.00046093082835183604,
      "loss": 7.1396,
      "step": 550
    },
    {
      "epoch": 0.15686832740213524,
      "grad_norm": 0.6119192838668823,
      "learning_rate": 0.00046085966410475376,
      "loss": 6.7188,
      "step": 551
    },
    {
      "epoch": 0.15715302491103203,
      "grad_norm": 0.6386239528656006,
      "learning_rate": 0.00046078849985767154,
      "loss": 6.7236,
      "step": 552
    },
    {
      "epoch": 0.15743772241992882,
      "grad_norm": 0.3954797089099884,
      "learning_rate": 0.00046071733561058926,
      "loss": 7.7266,
      "step": 553
    },
    {
      "epoch": 0.15772241992882563,
      "grad_norm": 0.5548195242881775,
      "learning_rate": 0.000460646171363507,
      "loss": 6.9756,
      "step": 554
    },
    {
      "epoch": 0.15800711743772242,
      "grad_norm": 0.6150522828102112,
      "learning_rate": 0.0004605750071164247,
      "loss": 6.5869,
      "step": 555
    },
    {
      "epoch": 0.15829181494661923,
      "grad_norm": 0.5321157574653625,
      "learning_rate": 0.00046050384286934243,
      "loss": 6.8682,
      "step": 556
    },
    {
      "epoch": 0.15857651245551602,
      "grad_norm": 0.4596554934978485,
      "learning_rate": 0.0004604326786222602,
      "loss": 7.1885,
      "step": 557
    },
    {
      "epoch": 0.1588612099644128,
      "grad_norm": 0.44151896238327026,
      "learning_rate": 0.00046036151437517793,
      "loss": 7.6816,
      "step": 558
    },
    {
      "epoch": 0.15914590747330962,
      "grad_norm": 0.5159528255462646,
      "learning_rate": 0.00046029035012809565,
      "loss": 6.8662,
      "step": 559
    },
    {
      "epoch": 0.1594306049822064,
      "grad_norm": 0.4236861765384674,
      "learning_rate": 0.00046021918588101343,
      "loss": 7.5635,
      "step": 560
    },
    {
      "epoch": 0.1597153024911032,
      "grad_norm": 0.5561614632606506,
      "learning_rate": 0.0004601480216339311,
      "loss": 6.8086,
      "step": 561
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.46602436900138855,
      "learning_rate": 0.0004600768573868488,
      "loss": 7.1611,
      "step": 562
    },
    {
      "epoch": 0.1602846975088968,
      "grad_norm": 0.43928325176239014,
      "learning_rate": 0.0004600056931397666,
      "loss": 7.5225,
      "step": 563
    },
    {
      "epoch": 0.1605693950177936,
      "grad_norm": 0.4085899293422699,
      "learning_rate": 0.0004599345288926843,
      "loss": 7.8828,
      "step": 564
    },
    {
      "epoch": 0.1608540925266904,
      "grad_norm": 0.37990400195121765,
      "learning_rate": 0.0004598633646456021,
      "loss": 7.5117,
      "step": 565
    },
    {
      "epoch": 0.16113879003558718,
      "grad_norm": 0.551302433013916,
      "learning_rate": 0.00045979220039851976,
      "loss": 7.1465,
      "step": 566
    },
    {
      "epoch": 0.161423487544484,
      "grad_norm": 0.4028521776199341,
      "learning_rate": 0.0004597210361514375,
      "loss": 7.6123,
      "step": 567
    },
    {
      "epoch": 0.16170818505338078,
      "grad_norm": 0.5788272619247437,
      "learning_rate": 0.00045964987190435526,
      "loss": 7.1279,
      "step": 568
    },
    {
      "epoch": 0.1619928825622776,
      "grad_norm": 0.5337854623794556,
      "learning_rate": 0.000459578707657273,
      "loss": 6.9785,
      "step": 569
    },
    {
      "epoch": 0.16227758007117438,
      "grad_norm": 0.5863319635391235,
      "learning_rate": 0.00045950754341019076,
      "loss": 7.1436,
      "step": 570
    },
    {
      "epoch": 0.16256227758007116,
      "grad_norm": 0.4670248031616211,
      "learning_rate": 0.0004594363791631085,
      "loss": 7.376,
      "step": 571
    },
    {
      "epoch": 0.16284697508896798,
      "grad_norm": 0.4174482524394989,
      "learning_rate": 0.00045936521491602616,
      "loss": 7.3574,
      "step": 572
    },
    {
      "epoch": 0.16313167259786476,
      "grad_norm": 0.5444307327270508,
      "learning_rate": 0.00045929405066894393,
      "loss": 7.0332,
      "step": 573
    },
    {
      "epoch": 0.16341637010676158,
      "grad_norm": 0.4734575152397156,
      "learning_rate": 0.00045922288642186165,
      "loss": 7.3379,
      "step": 574
    },
    {
      "epoch": 0.16370106761565836,
      "grad_norm": 0.43693485856056213,
      "learning_rate": 0.00045915172217477943,
      "loss": 7.3506,
      "step": 575
    },
    {
      "epoch": 0.16398576512455515,
      "grad_norm": 0.39955365657806396,
      "learning_rate": 0.00045908055792769715,
      "loss": 7.7363,
      "step": 576
    },
    {
      "epoch": 0.16427046263345196,
      "grad_norm": 0.42568933963775635,
      "learning_rate": 0.0004590093936806149,
      "loss": 7.7568,
      "step": 577
    },
    {
      "epoch": 0.16455516014234875,
      "grad_norm": 0.4571026563644409,
      "learning_rate": 0.0004589382294335326,
      "loss": 7.3809,
      "step": 578
    },
    {
      "epoch": 0.16483985765124556,
      "grad_norm": 0.45719119906425476,
      "learning_rate": 0.0004588670651864503,
      "loss": 7.4697,
      "step": 579
    },
    {
      "epoch": 0.16512455516014235,
      "grad_norm": 0.4227726459503174,
      "learning_rate": 0.00045879590093936805,
      "loss": 7.7959,
      "step": 580
    },
    {
      "epoch": 0.16540925266903914,
      "grad_norm": 0.4999239444732666,
      "learning_rate": 0.0004587247366922858,
      "loss": 7.0615,
      "step": 581
    },
    {
      "epoch": 0.16569395017793595,
      "grad_norm": 0.4378338158130646,
      "learning_rate": 0.00045865357244520355,
      "loss": 7.4492,
      "step": 582
    },
    {
      "epoch": 0.16597864768683274,
      "grad_norm": 0.4626426100730896,
      "learning_rate": 0.00045858240819812127,
      "loss": 7.6074,
      "step": 583
    },
    {
      "epoch": 0.16626334519572955,
      "grad_norm": 0.49946653842926025,
      "learning_rate": 0.000458511243951039,
      "loss": 7.5752,
      "step": 584
    },
    {
      "epoch": 0.16654804270462634,
      "grad_norm": 0.4690173864364624,
      "learning_rate": 0.0004584400797039567,
      "loss": 7.4453,
      "step": 585
    },
    {
      "epoch": 0.16683274021352312,
      "grad_norm": 0.5380011796951294,
      "learning_rate": 0.0004583689154568745,
      "loss": 7.085,
      "step": 586
    },
    {
      "epoch": 0.16711743772241994,
      "grad_norm": 0.47833171486854553,
      "learning_rate": 0.0004582977512097922,
      "loss": 7.1475,
      "step": 587
    },
    {
      "epoch": 0.16740213523131672,
      "grad_norm": 0.46506747603416443,
      "learning_rate": 0.00045822658696271,
      "loss": 6.9707,
      "step": 588
    },
    {
      "epoch": 0.1676868327402135,
      "grad_norm": 0.6595041155815125,
      "learning_rate": 0.00045815542271562766,
      "loss": 6.2119,
      "step": 589
    },
    {
      "epoch": 0.16797153024911032,
      "grad_norm": 0.47286534309387207,
      "learning_rate": 0.0004580842584685454,
      "loss": 7.2656,
      "step": 590
    },
    {
      "epoch": 0.1682562277580071,
      "grad_norm": 0.3845736086368561,
      "learning_rate": 0.00045801309422146316,
      "loss": 7.7588,
      "step": 591
    },
    {
      "epoch": 0.16854092526690392,
      "grad_norm": 0.4788035750389099,
      "learning_rate": 0.0004579419299743809,
      "loss": 7.4238,
      "step": 592
    },
    {
      "epoch": 0.1688256227758007,
      "grad_norm": 0.5757747292518616,
      "learning_rate": 0.00045787076572729866,
      "loss": 7.0264,
      "step": 593
    },
    {
      "epoch": 0.1691103202846975,
      "grad_norm": 0.4455467462539673,
      "learning_rate": 0.0004577996014802163,
      "loss": 7.6641,
      "step": 594
    },
    {
      "epoch": 0.1693950177935943,
      "grad_norm": 0.36923131346702576,
      "learning_rate": 0.00045772843723313405,
      "loss": 7.3965,
      "step": 595
    },
    {
      "epoch": 0.1696797153024911,
      "grad_norm": 0.5825194120407104,
      "learning_rate": 0.0004576572729860518,
      "loss": 7.0059,
      "step": 596
    },
    {
      "epoch": 0.1699644128113879,
      "grad_norm": 0.409291535615921,
      "learning_rate": 0.00045758610873896955,
      "loss": 7.3086,
      "step": 597
    },
    {
      "epoch": 0.1702491103202847,
      "grad_norm": 0.5457488298416138,
      "learning_rate": 0.00045751494449188727,
      "loss": 7.4639,
      "step": 598
    },
    {
      "epoch": 0.17053380782918148,
      "grad_norm": 0.5561560988426208,
      "learning_rate": 0.00045744378024480505,
      "loss": 7.0244,
      "step": 599
    },
    {
      "epoch": 0.1708185053380783,
      "grad_norm": 0.40452277660369873,
      "learning_rate": 0.0004573726159977227,
      "loss": 7.9131,
      "step": 600
    },
    {
      "epoch": 0.1708185053380783,
      "eval_bleu": 0.08136284175108972,
      "eval_loss": 7.046875,
      "eval_runtime": 175.1925,
      "eval_samples_per_second": 1.621,
      "eval_steps_per_second": 0.103,
      "step": 600
    },
    {
      "epoch": 0.17110320284697508,
      "grad_norm": 0.5157926082611084,
      "learning_rate": 0.0004573014517506405,
      "loss": 7.6035,
      "step": 601
    },
    {
      "epoch": 0.1713879003558719,
      "grad_norm": 0.5090907216072083,
      "learning_rate": 0.0004572302875035582,
      "loss": 7.3867,
      "step": 602
    },
    {
      "epoch": 0.17167259786476868,
      "grad_norm": 0.5271131992340088,
      "learning_rate": 0.00045715912325647594,
      "loss": 7.1807,
      "step": 603
    },
    {
      "epoch": 0.17195729537366547,
      "grad_norm": 0.479402631521225,
      "learning_rate": 0.0004570879590093937,
      "loss": 7.1025,
      "step": 604
    },
    {
      "epoch": 0.17224199288256228,
      "grad_norm": 0.4699629247188568,
      "learning_rate": 0.00045701679476231144,
      "loss": 7.4434,
      "step": 605
    },
    {
      "epoch": 0.17252669039145907,
      "grad_norm": 0.549485981464386,
      "learning_rate": 0.00045694563051522916,
      "loss": 7.0225,
      "step": 606
    },
    {
      "epoch": 0.17281138790035588,
      "grad_norm": 0.39508330821990967,
      "learning_rate": 0.0004568744662681469,
      "loss": 7.8623,
      "step": 607
    },
    {
      "epoch": 0.17309608540925267,
      "grad_norm": 0.5417274236679077,
      "learning_rate": 0.0004568033020210646,
      "loss": 6.9346,
      "step": 608
    },
    {
      "epoch": 0.17338078291814946,
      "grad_norm": 0.4323408901691437,
      "learning_rate": 0.0004567321377739824,
      "loss": 7.8262,
      "step": 609
    },
    {
      "epoch": 0.17366548042704627,
      "grad_norm": 0.5999831557273865,
      "learning_rate": 0.0004566609735269001,
      "loss": 7.0322,
      "step": 610
    },
    {
      "epoch": 0.17395017793594306,
      "grad_norm": 0.3982037901878357,
      "learning_rate": 0.0004565898092798178,
      "loss": 7.5947,
      "step": 611
    },
    {
      "epoch": 0.17423487544483987,
      "grad_norm": 0.41008031368255615,
      "learning_rate": 0.00045651864503273555,
      "loss": 8.0742,
      "step": 612
    },
    {
      "epoch": 0.17451957295373666,
      "grad_norm": 0.496019184589386,
      "learning_rate": 0.0004564474807856533,
      "loss": 7.5781,
      "step": 613
    },
    {
      "epoch": 0.17480427046263344,
      "grad_norm": 0.4990009665489197,
      "learning_rate": 0.00045637631653857105,
      "loss": 7.6621,
      "step": 614
    },
    {
      "epoch": 0.17508896797153026,
      "grad_norm": 0.510688304901123,
      "learning_rate": 0.0004563051522914888,
      "loss": 6.4463,
      "step": 615
    },
    {
      "epoch": 0.17537366548042704,
      "grad_norm": 0.4763341248035431,
      "learning_rate": 0.0004562339880444065,
      "loss": 7.5117,
      "step": 616
    },
    {
      "epoch": 0.17565836298932383,
      "grad_norm": 0.4208298623561859,
      "learning_rate": 0.0004561628237973242,
      "loss": 7.2637,
      "step": 617
    },
    {
      "epoch": 0.17594306049822064,
      "grad_norm": 0.4986557066440582,
      "learning_rate": 0.00045609165955024194,
      "loss": 6.9355,
      "step": 618
    },
    {
      "epoch": 0.17622775800711743,
      "grad_norm": 0.4251626431941986,
      "learning_rate": 0.0004560204953031597,
      "loss": 7.6758,
      "step": 619
    },
    {
      "epoch": 0.17651245551601424,
      "grad_norm": 0.5883548855781555,
      "learning_rate": 0.00045594933105607744,
      "loss": 6.8984,
      "step": 620
    },
    {
      "epoch": 0.17679715302491103,
      "grad_norm": 0.45804181694984436,
      "learning_rate": 0.00045587816680899517,
      "loss": 7.5889,
      "step": 621
    },
    {
      "epoch": 0.17708185053380782,
      "grad_norm": 0.5595649480819702,
      "learning_rate": 0.00045580700256191294,
      "loss": 7.3184,
      "step": 622
    },
    {
      "epoch": 0.17736654804270463,
      "grad_norm": 0.41659483313560486,
      "learning_rate": 0.0004557358383148306,
      "loss": 7.333,
      "step": 623
    },
    {
      "epoch": 0.17765124555160142,
      "grad_norm": 0.4967384934425354,
      "learning_rate": 0.0004556646740677484,
      "loss": 7.2559,
      "step": 624
    },
    {
      "epoch": 0.17793594306049823,
      "grad_norm": 0.43483781814575195,
      "learning_rate": 0.0004555935098206661,
      "loss": 7.7031,
      "step": 625
    },
    {
      "epoch": 0.17822064056939502,
      "grad_norm": 0.53510981798172,
      "learning_rate": 0.00045552234557358383,
      "loss": 7.0801,
      "step": 626
    },
    {
      "epoch": 0.1785053380782918,
      "grad_norm": 0.5476201176643372,
      "learning_rate": 0.0004554511813265016,
      "loss": 6.9561,
      "step": 627
    },
    {
      "epoch": 0.17879003558718862,
      "grad_norm": 0.5914725661277771,
      "learning_rate": 0.0004553800170794193,
      "loss": 7.0059,
      "step": 628
    },
    {
      "epoch": 0.1790747330960854,
      "grad_norm": 0.5816411972045898,
      "learning_rate": 0.000455308852832337,
      "loss": 7.1006,
      "step": 629
    },
    {
      "epoch": 0.17935943060498222,
      "grad_norm": 0.503734290599823,
      "learning_rate": 0.0004552376885852548,
      "loss": 7.2002,
      "step": 630
    },
    {
      "epoch": 0.179644128113879,
      "grad_norm": 0.4251323938369751,
      "learning_rate": 0.0004551665243381725,
      "loss": 7.6299,
      "step": 631
    },
    {
      "epoch": 0.1799288256227758,
      "grad_norm": 0.5005379319190979,
      "learning_rate": 0.0004550953600910903,
      "loss": 6.8135,
      "step": 632
    },
    {
      "epoch": 0.1802135231316726,
      "grad_norm": 0.4894733428955078,
      "learning_rate": 0.000455024195844008,
      "loss": 7.0869,
      "step": 633
    },
    {
      "epoch": 0.1804982206405694,
      "grad_norm": 0.40313103795051575,
      "learning_rate": 0.00045495303159692567,
      "loss": 7.4717,
      "step": 634
    },
    {
      "epoch": 0.1807829181494662,
      "grad_norm": 0.4881073832511902,
      "learning_rate": 0.00045488186734984345,
      "loss": 7.3096,
      "step": 635
    },
    {
      "epoch": 0.181067615658363,
      "grad_norm": 0.5019959807395935,
      "learning_rate": 0.00045481070310276117,
      "loss": 7.1348,
      "step": 636
    },
    {
      "epoch": 0.18135231316725978,
      "grad_norm": 0.5300182700157166,
      "learning_rate": 0.00045473953885567895,
      "loss": 7.3125,
      "step": 637
    },
    {
      "epoch": 0.1816370106761566,
      "grad_norm": 0.5444533824920654,
      "learning_rate": 0.00045466837460859667,
      "loss": 7.0107,
      "step": 638
    },
    {
      "epoch": 0.18192170818505338,
      "grad_norm": 0.5647503733634949,
      "learning_rate": 0.00045459721036151434,
      "loss": 7.1396,
      "step": 639
    },
    {
      "epoch": 0.1822064056939502,
      "grad_norm": 0.6619802117347717,
      "learning_rate": 0.0004545260461144321,
      "loss": 6.9766,
      "step": 640
    },
    {
      "epoch": 0.18249110320284698,
      "grad_norm": 0.4294302761554718,
      "learning_rate": 0.00045445488186734984,
      "loss": 7.6309,
      "step": 641
    },
    {
      "epoch": 0.18277580071174376,
      "grad_norm": 0.46216872334480286,
      "learning_rate": 0.0004543837176202676,
      "loss": 7.1006,
      "step": 642
    },
    {
      "epoch": 0.18306049822064058,
      "grad_norm": 0.48695653676986694,
      "learning_rate": 0.00045431255337318534,
      "loss": 7.3574,
      "step": 643
    },
    {
      "epoch": 0.18334519572953736,
      "grad_norm": 0.5963466763496399,
      "learning_rate": 0.00045424138912610306,
      "loss": 7.293,
      "step": 644
    },
    {
      "epoch": 0.18362989323843418,
      "grad_norm": 0.5873686671257019,
      "learning_rate": 0.0004541702248790208,
      "loss": 6.9521,
      "step": 645
    },
    {
      "epoch": 0.18391459074733096,
      "grad_norm": 0.368348628282547,
      "learning_rate": 0.0004540990606319385,
      "loss": 7.6436,
      "step": 646
    },
    {
      "epoch": 0.18419928825622775,
      "grad_norm": 0.37956151366233826,
      "learning_rate": 0.00045402789638485623,
      "loss": 7.9355,
      "step": 647
    },
    {
      "epoch": 0.18448398576512456,
      "grad_norm": 0.5874934792518616,
      "learning_rate": 0.000453956732137774,
      "loss": 7.1172,
      "step": 648
    },
    {
      "epoch": 0.18476868327402135,
      "grad_norm": 0.44219866394996643,
      "learning_rate": 0.00045388556789069173,
      "loss": 7.7051,
      "step": 649
    },
    {
      "epoch": 0.18505338078291814,
      "grad_norm": 0.49062198400497437,
      "learning_rate": 0.0004538144036436095,
      "loss": 7.5488,
      "step": 650
    },
    {
      "epoch": 0.18533807829181495,
      "grad_norm": 0.48055267333984375,
      "learning_rate": 0.0004537432393965272,
      "loss": 7.4219,
      "step": 651
    },
    {
      "epoch": 0.18562277580071174,
      "grad_norm": 0.477851927280426,
      "learning_rate": 0.0004536720751494449,
      "loss": 7.5186,
      "step": 652
    },
    {
      "epoch": 0.18590747330960855,
      "grad_norm": 0.44223421812057495,
      "learning_rate": 0.0004536009109023627,
      "loss": 7.4512,
      "step": 653
    },
    {
      "epoch": 0.18619217081850534,
      "grad_norm": 0.48089858889579773,
      "learning_rate": 0.0004535297466552804,
      "loss": 7.2803,
      "step": 654
    },
    {
      "epoch": 0.18647686832740212,
      "grad_norm": 0.5489603877067566,
      "learning_rate": 0.0004534585824081982,
      "loss": 6.2256,
      "step": 655
    },
    {
      "epoch": 0.18676156583629894,
      "grad_norm": 0.4798690676689148,
      "learning_rate": 0.00045338741816111584,
      "loss": 7.459,
      "step": 656
    },
    {
      "epoch": 0.18704626334519572,
      "grad_norm": 0.5676328539848328,
      "learning_rate": 0.00045331625391403357,
      "loss": 6.7197,
      "step": 657
    },
    {
      "epoch": 0.18733096085409254,
      "grad_norm": 0.5007146596908569,
      "learning_rate": 0.00045324508966695134,
      "loss": 7.3262,
      "step": 658
    },
    {
      "epoch": 0.18761565836298932,
      "grad_norm": 0.6378830075263977,
      "learning_rate": 0.00045317392541986906,
      "loss": 7.1758,
      "step": 659
    },
    {
      "epoch": 0.1879003558718861,
      "grad_norm": 0.4543790817260742,
      "learning_rate": 0.00045310276117278684,
      "loss": 7.1152,
      "step": 660
    },
    {
      "epoch": 0.18818505338078292,
      "grad_norm": 0.43836989998817444,
      "learning_rate": 0.00045303159692570456,
      "loss": 7.6982,
      "step": 661
    },
    {
      "epoch": 0.1884697508896797,
      "grad_norm": 0.48784226179122925,
      "learning_rate": 0.00045296043267862223,
      "loss": 7.4922,
      "step": 662
    },
    {
      "epoch": 0.18875444839857652,
      "grad_norm": 0.501962423324585,
      "learning_rate": 0.00045288926843154,
      "loss": 6.9375,
      "step": 663
    },
    {
      "epoch": 0.1890391459074733,
      "grad_norm": 0.4261062443256378,
      "learning_rate": 0.00045281810418445773,
      "loss": 7.4248,
      "step": 664
    },
    {
      "epoch": 0.1893238434163701,
      "grad_norm": 0.4303380250930786,
      "learning_rate": 0.00045274693993737546,
      "loss": 7.3965,
      "step": 665
    },
    {
      "epoch": 0.1896085409252669,
      "grad_norm": 0.4281022250652313,
      "learning_rate": 0.00045267577569029323,
      "loss": 7.6621,
      "step": 666
    },
    {
      "epoch": 0.1898932384341637,
      "grad_norm": 0.5295163989067078,
      "learning_rate": 0.00045260461144321096,
      "loss": 7.4043,
      "step": 667
    },
    {
      "epoch": 0.1901779359430605,
      "grad_norm": 0.48053741455078125,
      "learning_rate": 0.0004525334471961287,
      "loss": 7.4639,
      "step": 668
    },
    {
      "epoch": 0.1904626334519573,
      "grad_norm": 0.4163114130496979,
      "learning_rate": 0.0004524622829490464,
      "loss": 7.7852,
      "step": 669
    },
    {
      "epoch": 0.19074733096085408,
      "grad_norm": 0.43880659341812134,
      "learning_rate": 0.0004523911187019641,
      "loss": 7.457,
      "step": 670
    },
    {
      "epoch": 0.1910320284697509,
      "grad_norm": 0.4487917423248291,
      "learning_rate": 0.0004523199544548819,
      "loss": 7.665,
      "step": 671
    },
    {
      "epoch": 0.19131672597864768,
      "grad_norm": 0.45230862498283386,
      "learning_rate": 0.0004522487902077996,
      "loss": 7.8857,
      "step": 672
    },
    {
      "epoch": 0.1916014234875445,
      "grad_norm": 0.496181458234787,
      "learning_rate": 0.00045217762596071735,
      "loss": 7.2002,
      "step": 673
    },
    {
      "epoch": 0.19188612099644128,
      "grad_norm": 0.35912343859672546,
      "learning_rate": 0.00045210646171363507,
      "loss": 7.5195,
      "step": 674
    },
    {
      "epoch": 0.19217081850533807,
      "grad_norm": 0.462393194437027,
      "learning_rate": 0.0004520352974665528,
      "loss": 7.5742,
      "step": 675
    },
    {
      "epoch": 0.19245551601423488,
      "grad_norm": 0.5089987516403198,
      "learning_rate": 0.00045196413321947057,
      "loss": 7.1533,
      "step": 676
    },
    {
      "epoch": 0.19274021352313167,
      "grad_norm": 0.5419299602508545,
      "learning_rate": 0.0004518929689723883,
      "loss": 7.0479,
      "step": 677
    },
    {
      "epoch": 0.19302491103202846,
      "grad_norm": 0.47667187452316284,
      "learning_rate": 0.000451821804725306,
      "loss": 7.5176,
      "step": 678
    },
    {
      "epoch": 0.19330960854092527,
      "grad_norm": 0.49150925874710083,
      "learning_rate": 0.00045175064047822374,
      "loss": 7.5088,
      "step": 679
    },
    {
      "epoch": 0.19359430604982206,
      "grad_norm": 0.4631671905517578,
      "learning_rate": 0.00045167947623114146,
      "loss": 7.2822,
      "step": 680
    },
    {
      "epoch": 0.19387900355871887,
      "grad_norm": 0.6819197535514832,
      "learning_rate": 0.00045160831198405924,
      "loss": 6.5312,
      "step": 681
    },
    {
      "epoch": 0.19416370106761566,
      "grad_norm": 0.5060408115386963,
      "learning_rate": 0.00045153714773697696,
      "loss": 7.2939,
      "step": 682
    },
    {
      "epoch": 0.19444839857651244,
      "grad_norm": 0.4816899597644806,
      "learning_rate": 0.0004514659834898947,
      "loss": 7.5371,
      "step": 683
    },
    {
      "epoch": 0.19473309608540926,
      "grad_norm": 0.4419718086719513,
      "learning_rate": 0.0004513948192428124,
      "loss": 7.2891,
      "step": 684
    },
    {
      "epoch": 0.19501779359430604,
      "grad_norm": 0.5301364660263062,
      "learning_rate": 0.00045132365499573013,
      "loss": 6.8311,
      "step": 685
    },
    {
      "epoch": 0.19530249110320286,
      "grad_norm": 0.5797151327133179,
      "learning_rate": 0.0004512524907486479,
      "loss": 7.2871,
      "step": 686
    },
    {
      "epoch": 0.19558718861209964,
      "grad_norm": 0.604928195476532,
      "learning_rate": 0.00045118132650156563,
      "loss": 7.0869,
      "step": 687
    },
    {
      "epoch": 0.19587188612099643,
      "grad_norm": 0.4925019443035126,
      "learning_rate": 0.00045111016225448335,
      "loss": 7.1387,
      "step": 688
    },
    {
      "epoch": 0.19615658362989324,
      "grad_norm": 0.5087660551071167,
      "learning_rate": 0.0004510389980074011,
      "loss": 6.9668,
      "step": 689
    },
    {
      "epoch": 0.19644128113879003,
      "grad_norm": 0.4889131188392639,
      "learning_rate": 0.0004509678337603188,
      "loss": 6.9883,
      "step": 690
    },
    {
      "epoch": 0.19672597864768684,
      "grad_norm": 0.5806876420974731,
      "learning_rate": 0.00045089666951323657,
      "loss": 6.6211,
      "step": 691
    },
    {
      "epoch": 0.19701067615658363,
      "grad_norm": 0.6690099239349365,
      "learning_rate": 0.0004508255052661543,
      "loss": 6.5527,
      "step": 692
    },
    {
      "epoch": 0.19729537366548042,
      "grad_norm": 0.4910028278827667,
      "learning_rate": 0.000450754341019072,
      "loss": 7.0664,
      "step": 693
    },
    {
      "epoch": 0.19758007117437723,
      "grad_norm": 0.49179354310035706,
      "learning_rate": 0.0004506831767719898,
      "loss": 7.1865,
      "step": 694
    },
    {
      "epoch": 0.19786476868327402,
      "grad_norm": 0.638883113861084,
      "learning_rate": 0.0004506120125249075,
      "loss": 7.1006,
      "step": 695
    },
    {
      "epoch": 0.19814946619217083,
      "grad_norm": 0.400945246219635,
      "learning_rate": 0.0004505408482778252,
      "loss": 7.6387,
      "step": 696
    },
    {
      "epoch": 0.19843416370106762,
      "grad_norm": 0.5283869504928589,
      "learning_rate": 0.00045046968403074296,
      "loss": 7.2627,
      "step": 697
    },
    {
      "epoch": 0.1987188612099644,
      "grad_norm": 0.5028197765350342,
      "learning_rate": 0.0004503985197836607,
      "loss": 7.2373,
      "step": 698
    },
    {
      "epoch": 0.19900355871886122,
      "grad_norm": 0.5313175916671753,
      "learning_rate": 0.00045032735553657846,
      "loss": 7.4814,
      "step": 699
    },
    {
      "epoch": 0.199288256227758,
      "grad_norm": 0.5412628054618835,
      "learning_rate": 0.0004502561912894962,
      "loss": 7.584,
      "step": 700
    },
    {
      "epoch": 0.19957295373665482,
      "grad_norm": 0.5525862574577332,
      "learning_rate": 0.00045018502704241385,
      "loss": 6.9756,
      "step": 701
    },
    {
      "epoch": 0.1998576512455516,
      "grad_norm": 0.470053106546402,
      "learning_rate": 0.00045011386279533163,
      "loss": 7.3096,
      "step": 702
    },
    {
      "epoch": 0.2001423487544484,
      "grad_norm": 0.4619928300380707,
      "learning_rate": 0.00045004269854824935,
      "loss": 7.4365,
      "step": 703
    },
    {
      "epoch": 0.2004270462633452,
      "grad_norm": 0.511256754398346,
      "learning_rate": 0.00044997153430116713,
      "loss": 7.3418,
      "step": 704
    },
    {
      "epoch": 0.200711743772242,
      "grad_norm": 0.44021302461624146,
      "learning_rate": 0.00044990037005408485,
      "loss": 7.5215,
      "step": 705
    },
    {
      "epoch": 0.20099644128113878,
      "grad_norm": 0.45131561160087585,
      "learning_rate": 0.0004498292058070026,
      "loss": 7.0215,
      "step": 706
    },
    {
      "epoch": 0.2012811387900356,
      "grad_norm": 0.4091982841491699,
      "learning_rate": 0.0004497580415599203,
      "loss": 7.6855,
      "step": 707
    },
    {
      "epoch": 0.20156583629893238,
      "grad_norm": 0.42062363028526306,
      "learning_rate": 0.000449686877312838,
      "loss": 7.7256,
      "step": 708
    },
    {
      "epoch": 0.2018505338078292,
      "grad_norm": 0.49599114060401917,
      "learning_rate": 0.00044961571306575574,
      "loss": 7.1162,
      "step": 709
    },
    {
      "epoch": 0.20213523131672598,
      "grad_norm": 0.4859379231929779,
      "learning_rate": 0.0004495445488186735,
      "loss": 6.9316,
      "step": 710
    },
    {
      "epoch": 0.20241992882562276,
      "grad_norm": 0.40452441573143005,
      "learning_rate": 0.00044947338457159124,
      "loss": 7.4365,
      "step": 711
    },
    {
      "epoch": 0.20270462633451958,
      "grad_norm": 0.6663489937782288,
      "learning_rate": 0.000449402220324509,
      "loss": 6.9199,
      "step": 712
    },
    {
      "epoch": 0.20298932384341636,
      "grad_norm": 0.8615325093269348,
      "learning_rate": 0.0004493310560774267,
      "loss": 7.3496,
      "step": 713
    },
    {
      "epoch": 0.20327402135231318,
      "grad_norm": 0.4769529700279236,
      "learning_rate": 0.0004492598918303444,
      "loss": 7.4785,
      "step": 714
    },
    {
      "epoch": 0.20355871886120996,
      "grad_norm": 0.36396512389183044,
      "learning_rate": 0.0004491887275832622,
      "loss": 7.6992,
      "step": 715
    },
    {
      "epoch": 0.20384341637010675,
      "grad_norm": 0.43238091468811035,
      "learning_rate": 0.0004491175633361799,
      "loss": 7.6221,
      "step": 716
    },
    {
      "epoch": 0.20412811387900356,
      "grad_norm": 0.5147523283958435,
      "learning_rate": 0.0004490463990890977,
      "loss": 7.3389,
      "step": 717
    },
    {
      "epoch": 0.20441281138790035,
      "grad_norm": 0.47087812423706055,
      "learning_rate": 0.00044897523484201536,
      "loss": 7.5195,
      "step": 718
    },
    {
      "epoch": 0.20469750889679716,
      "grad_norm": 0.5744754672050476,
      "learning_rate": 0.0004489040705949331,
      "loss": 6.8584,
      "step": 719
    },
    {
      "epoch": 0.20498220640569395,
      "grad_norm": 0.7531524300575256,
      "learning_rate": 0.00044883290634785086,
      "loss": 7.6885,
      "step": 720
    },
    {
      "epoch": 0.20526690391459074,
      "grad_norm": 0.6688421964645386,
      "learning_rate": 0.0004487617421007686,
      "loss": 7.0498,
      "step": 721
    },
    {
      "epoch": 0.20555160142348755,
      "grad_norm": 0.41622069478034973,
      "learning_rate": 0.00044869057785368636,
      "loss": 7.5127,
      "step": 722
    },
    {
      "epoch": 0.20583629893238434,
      "grad_norm": 0.6788975596427917,
      "learning_rate": 0.0004486194136066041,
      "loss": 6.959,
      "step": 723
    },
    {
      "epoch": 0.20612099644128115,
      "grad_norm": 0.6270914077758789,
      "learning_rate": 0.00044854824935952175,
      "loss": 6.5908,
      "step": 724
    },
    {
      "epoch": 0.20640569395017794,
      "grad_norm": 0.42413002252578735,
      "learning_rate": 0.0004484770851124395,
      "loss": 7.3301,
      "step": 725
    },
    {
      "epoch": 0.20669039145907472,
      "grad_norm": 0.4253315031528473,
      "learning_rate": 0.00044840592086535725,
      "loss": 7.8818,
      "step": 726
    },
    {
      "epoch": 0.20697508896797154,
      "grad_norm": 0.45278969407081604,
      "learning_rate": 0.00044833475661827497,
      "loss": 7.4268,
      "step": 727
    },
    {
      "epoch": 0.20725978647686832,
      "grad_norm": 0.4045217037200928,
      "learning_rate": 0.00044826359237119275,
      "loss": 7.6025,
      "step": 728
    },
    {
      "epoch": 0.20754448398576514,
      "grad_norm": 0.5309168696403503,
      "learning_rate": 0.0004481924281241104,
      "loss": 7.1768,
      "step": 729
    },
    {
      "epoch": 0.20782918149466192,
      "grad_norm": 0.4788176715373993,
      "learning_rate": 0.0004481212638770282,
      "loss": 7.4551,
      "step": 730
    },
    {
      "epoch": 0.2081138790035587,
      "grad_norm": 0.535114586353302,
      "learning_rate": 0.0004480500996299459,
      "loss": 7.1504,
      "step": 731
    },
    {
      "epoch": 0.20839857651245552,
      "grad_norm": 0.5233950614929199,
      "learning_rate": 0.00044797893538286364,
      "loss": 7.1992,
      "step": 732
    },
    {
      "epoch": 0.2086832740213523,
      "grad_norm": 0.5039567351341248,
      "learning_rate": 0.0004479077711357814,
      "loss": 7.2734,
      "step": 733
    },
    {
      "epoch": 0.2089679715302491,
      "grad_norm": 0.5497581958770752,
      "learning_rate": 0.00044783660688869914,
      "loss": 7.126,
      "step": 734
    },
    {
      "epoch": 0.2092526690391459,
      "grad_norm": 0.5098934173583984,
      "learning_rate": 0.00044776544264161686,
      "loss": 7.2588,
      "step": 735
    },
    {
      "epoch": 0.2095373665480427,
      "grad_norm": 0.45094266533851624,
      "learning_rate": 0.0004476942783945346,
      "loss": 7.3164,
      "step": 736
    },
    {
      "epoch": 0.2098220640569395,
      "grad_norm": 0.46048614382743835,
      "learning_rate": 0.0004476231141474523,
      "loss": 7.7139,
      "step": 737
    },
    {
      "epoch": 0.2101067615658363,
      "grad_norm": 0.4291335642337799,
      "learning_rate": 0.0004475519499003701,
      "loss": 7.6035,
      "step": 738
    },
    {
      "epoch": 0.21039145907473308,
      "grad_norm": 0.3905137777328491,
      "learning_rate": 0.0004474807856532878,
      "loss": 7.9336,
      "step": 739
    },
    {
      "epoch": 0.2106761565836299,
      "grad_norm": 0.4790925085544586,
      "learning_rate": 0.0004474096214062056,
      "loss": 7.4365,
      "step": 740
    },
    {
      "epoch": 0.21096085409252668,
      "grad_norm": 0.6108134984970093,
      "learning_rate": 0.00044733845715912325,
      "loss": 6.9443,
      "step": 741
    },
    {
      "epoch": 0.2112455516014235,
      "grad_norm": 0.5870121121406555,
      "learning_rate": 0.000447267292912041,
      "loss": 7.0771,
      "step": 742
    },
    {
      "epoch": 0.21153024911032028,
      "grad_norm": 0.4234772324562073,
      "learning_rate": 0.00044719612866495875,
      "loss": 7.6436,
      "step": 743
    },
    {
      "epoch": 0.21181494661921707,
      "grad_norm": 0.4352046549320221,
      "learning_rate": 0.0004471249644178765,
      "loss": 7.7432,
      "step": 744
    },
    {
      "epoch": 0.21209964412811388,
      "grad_norm": 0.4731186032295227,
      "learning_rate": 0.0004470538001707942,
      "loss": 7.2842,
      "step": 745
    },
    {
      "epoch": 0.21238434163701067,
      "grad_norm": 0.5395245552062988,
      "learning_rate": 0.0004469826359237119,
      "loss": 6.9424,
      "step": 746
    },
    {
      "epoch": 0.21266903914590748,
      "grad_norm": 0.4573425352573395,
      "learning_rate": 0.00044691147167662964,
      "loss": 7.2559,
      "step": 747
    },
    {
      "epoch": 0.21295373665480427,
      "grad_norm": 0.4909103512763977,
      "learning_rate": 0.0004468403074295474,
      "loss": 7.2422,
      "step": 748
    },
    {
      "epoch": 0.21323843416370106,
      "grad_norm": 0.4994555115699768,
      "learning_rate": 0.00044676914318246514,
      "loss": 7.5713,
      "step": 749
    },
    {
      "epoch": 0.21352313167259787,
      "grad_norm": 0.5618712306022644,
      "learning_rate": 0.00044669797893538287,
      "loss": 6.9062,
      "step": 750
    },
    {
      "epoch": 0.21380782918149466,
      "grad_norm": 0.5334068536758423,
      "learning_rate": 0.00044662681468830064,
      "loss": 6.875,
      "step": 751
    },
    {
      "epoch": 0.21409252669039147,
      "grad_norm": 0.518782913684845,
      "learning_rate": 0.0004465556504412183,
      "loss": 7.1416,
      "step": 752
    },
    {
      "epoch": 0.21437722419928826,
      "grad_norm": 0.48912790417671204,
      "learning_rate": 0.0004464844861941361,
      "loss": 7.5664,
      "step": 753
    },
    {
      "epoch": 0.21466192170818504,
      "grad_norm": 0.38679495453834534,
      "learning_rate": 0.0004464133219470538,
      "loss": 7.5771,
      "step": 754
    },
    {
      "epoch": 0.21494661921708186,
      "grad_norm": 0.4669499099254608,
      "learning_rate": 0.00044634215769997153,
      "loss": 7.0508,
      "step": 755
    },
    {
      "epoch": 0.21523131672597864,
      "grad_norm": 0.5104237794876099,
      "learning_rate": 0.0004462709934528893,
      "loss": 6.9834,
      "step": 756
    },
    {
      "epoch": 0.21551601423487546,
      "grad_norm": 0.5343145728111267,
      "learning_rate": 0.000446199829205807,
      "loss": 7.3018,
      "step": 757
    },
    {
      "epoch": 0.21580071174377224,
      "grad_norm": 2.3170905113220215,
      "learning_rate": 0.0004461286649587247,
      "loss": 6.9307,
      "step": 758
    },
    {
      "epoch": 0.21608540925266903,
      "grad_norm": 0.45728206634521484,
      "learning_rate": 0.0004460575007116425,
      "loss": 7.5264,
      "step": 759
    },
    {
      "epoch": 0.21637010676156584,
      "grad_norm": 0.6728972792625427,
      "learning_rate": 0.0004459863364645602,
      "loss": 6.4639,
      "step": 760
    },
    {
      "epoch": 0.21665480427046263,
      "grad_norm": 0.5580065846443176,
      "learning_rate": 0.000445915172217478,
      "loss": 7.2715,
      "step": 761
    },
    {
      "epoch": 0.21693950177935942,
      "grad_norm": 0.5601818561553955,
      "learning_rate": 0.0004458440079703957,
      "loss": 7.0195,
      "step": 762
    },
    {
      "epoch": 0.21722419928825623,
      "grad_norm": 0.53824383020401,
      "learning_rate": 0.00044577284372331337,
      "loss": 7.1748,
      "step": 763
    },
    {
      "epoch": 0.21750889679715302,
      "grad_norm": 0.5673179030418396,
      "learning_rate": 0.00044570167947623115,
      "loss": 7.3262,
      "step": 764
    },
    {
      "epoch": 0.21779359430604983,
      "grad_norm": 0.5143839120864868,
      "learning_rate": 0.00044563051522914887,
      "loss": 7.2246,
      "step": 765
    },
    {
      "epoch": 0.21807829181494662,
      "grad_norm": 0.479536771774292,
      "learning_rate": 0.00044555935098206665,
      "loss": 7.5,
      "step": 766
    },
    {
      "epoch": 0.2183629893238434,
      "grad_norm": 0.5423471927642822,
      "learning_rate": 0.00044548818673498437,
      "loss": 7.626,
      "step": 767
    },
    {
      "epoch": 0.21864768683274022,
      "grad_norm": 0.43286991119384766,
      "learning_rate": 0.0004454170224879021,
      "loss": 7.3652,
      "step": 768
    },
    {
      "epoch": 0.218932384341637,
      "grad_norm": 0.5857230424880981,
      "learning_rate": 0.0004453458582408198,
      "loss": 7.0664,
      "step": 769
    },
    {
      "epoch": 0.21921708185053382,
      "grad_norm": 0.5000907778739929,
      "learning_rate": 0.00044527469399373754,
      "loss": 7.4561,
      "step": 770
    },
    {
      "epoch": 0.2195017793594306,
      "grad_norm": 0.43097177147865295,
      "learning_rate": 0.0004452035297466553,
      "loss": 7.3828,
      "step": 771
    },
    {
      "epoch": 0.2197864768683274,
      "grad_norm": 0.42178913950920105,
      "learning_rate": 0.00044513236549957304,
      "loss": 7.5723,
      "step": 772
    },
    {
      "epoch": 0.2200711743772242,
      "grad_norm": 0.6063342690467834,
      "learning_rate": 0.00044506120125249076,
      "loss": 7.0381,
      "step": 773
    },
    {
      "epoch": 0.220355871886121,
      "grad_norm": 0.5009235143661499,
      "learning_rate": 0.0004449900370054085,
      "loss": 7.2695,
      "step": 774
    },
    {
      "epoch": 0.2206405693950178,
      "grad_norm": 0.5648031830787659,
      "learning_rate": 0.0004449188727583262,
      "loss": 6.8516,
      "step": 775
    },
    {
      "epoch": 0.2209252669039146,
      "grad_norm": 0.5607783794403076,
      "learning_rate": 0.00044484770851124393,
      "loss": 6.6475,
      "step": 776
    },
    {
      "epoch": 0.22120996441281138,
      "grad_norm": 0.5817128419876099,
      "learning_rate": 0.0004447765442641617,
      "loss": 6.7178,
      "step": 777
    },
    {
      "epoch": 0.2214946619217082,
      "grad_norm": 0.5132507085800171,
      "learning_rate": 0.00044470538001707943,
      "loss": 7.5273,
      "step": 778
    },
    {
      "epoch": 0.22177935943060498,
      "grad_norm": 0.491405725479126,
      "learning_rate": 0.0004446342157699972,
      "loss": 7.3223,
      "step": 779
    },
    {
      "epoch": 0.2220640569395018,
      "grad_norm": 0.49990856647491455,
      "learning_rate": 0.0004445630515229149,
      "loss": 7.0146,
      "step": 780
    },
    {
      "epoch": 0.22234875444839858,
      "grad_norm": 0.598653256893158,
      "learning_rate": 0.0004444918872758326,
      "loss": 6.3633,
      "step": 781
    },
    {
      "epoch": 0.22263345195729536,
      "grad_norm": 0.5837319493293762,
      "learning_rate": 0.0004444207230287504,
      "loss": 7.0635,
      "step": 782
    },
    {
      "epoch": 0.22291814946619218,
      "grad_norm": 0.3937888443470001,
      "learning_rate": 0.0004443495587816681,
      "loss": 7.7715,
      "step": 783
    },
    {
      "epoch": 0.22320284697508896,
      "grad_norm": 0.44455066323280334,
      "learning_rate": 0.0004442783945345859,
      "loss": 7.417,
      "step": 784
    },
    {
      "epoch": 0.22348754448398578,
      "grad_norm": 0.4986377954483032,
      "learning_rate": 0.0004442072302875036,
      "loss": 7.3779,
      "step": 785
    },
    {
      "epoch": 0.22377224199288256,
      "grad_norm": 0.5064935088157654,
      "learning_rate": 0.00044413606604042126,
      "loss": 7.123,
      "step": 786
    },
    {
      "epoch": 0.22405693950177935,
      "grad_norm": 0.44009876251220703,
      "learning_rate": 0.00044406490179333904,
      "loss": 7.709,
      "step": 787
    },
    {
      "epoch": 0.22434163701067616,
      "grad_norm": 0.483054518699646,
      "learning_rate": 0.00044399373754625676,
      "loss": 7.5605,
      "step": 788
    },
    {
      "epoch": 0.22462633451957295,
      "grad_norm": 0.5088704228401184,
      "learning_rate": 0.00044392257329917454,
      "loss": 7.2334,
      "step": 789
    },
    {
      "epoch": 0.22491103202846974,
      "grad_norm": 0.41016024351119995,
      "learning_rate": 0.00044385140905209226,
      "loss": 7.791,
      "step": 790
    },
    {
      "epoch": 0.22519572953736655,
      "grad_norm": 0.5502153038978577,
      "learning_rate": 0.00044378024480500993,
      "loss": 7.3965,
      "step": 791
    },
    {
      "epoch": 0.22548042704626334,
      "grad_norm": 0.543286144733429,
      "learning_rate": 0.0004437090805579277,
      "loss": 7.2197,
      "step": 792
    },
    {
      "epoch": 0.22576512455516015,
      "grad_norm": 0.4524809718132019,
      "learning_rate": 0.00044363791631084543,
      "loss": 7.7568,
      "step": 793
    },
    {
      "epoch": 0.22604982206405694,
      "grad_norm": 0.46553486585617065,
      "learning_rate": 0.00044356675206376315,
      "loss": 7.4297,
      "step": 794
    },
    {
      "epoch": 0.22633451957295372,
      "grad_norm": 0.4544578492641449,
      "learning_rate": 0.00044349558781668093,
      "loss": 7.6367,
      "step": 795
    },
    {
      "epoch": 0.22661921708185054,
      "grad_norm": 0.4988901615142822,
      "learning_rate": 0.00044342442356959865,
      "loss": 7.5498,
      "step": 796
    },
    {
      "epoch": 0.22690391459074732,
      "grad_norm": 0.5461361408233643,
      "learning_rate": 0.0004433532593225164,
      "loss": 7.2773,
      "step": 797
    },
    {
      "epoch": 0.22718861209964414,
      "grad_norm": 0.5461740493774414,
      "learning_rate": 0.0004432820950754341,
      "loss": 7.0732,
      "step": 798
    },
    {
      "epoch": 0.22747330960854092,
      "grad_norm": 0.48787885904312134,
      "learning_rate": 0.0004432109308283518,
      "loss": 7.1338,
      "step": 799
    },
    {
      "epoch": 0.2277580071174377,
      "grad_norm": 0.4876340329647064,
      "learning_rate": 0.0004431397665812696,
      "loss": 7.0635,
      "step": 800
    },
    {
      "epoch": 0.2277580071174377,
      "eval_bleu": 0.10199226196567952,
      "eval_loss": 7.03515625,
      "eval_runtime": 179.5646,
      "eval_samples_per_second": 1.582,
      "eval_steps_per_second": 0.1,
      "step": 800
    },
    {
      "epoch": 0.22804270462633452,
      "grad_norm": 0.4812236428260803,
      "learning_rate": 0.0004430686023341873,
      "loss": 7.4922,
      "step": 801
    },
    {
      "epoch": 0.2283274021352313,
      "grad_norm": 0.5275574922561646,
      "learning_rate": 0.00044299743808710505,
      "loss": 7.2969,
      "step": 802
    },
    {
      "epoch": 0.22861209964412813,
      "grad_norm": 0.6468873620033264,
      "learning_rate": 0.00044292627384002277,
      "loss": 7.0674,
      "step": 803
    },
    {
      "epoch": 0.2288967971530249,
      "grad_norm": 0.5731150507926941,
      "learning_rate": 0.0004428551095929405,
      "loss": 6.8281,
      "step": 804
    },
    {
      "epoch": 0.2291814946619217,
      "grad_norm": 0.3813442587852478,
      "learning_rate": 0.00044278394534585827,
      "loss": 7.7334,
      "step": 805
    },
    {
      "epoch": 0.2294661921708185,
      "grad_norm": 0.4659816026687622,
      "learning_rate": 0.000442712781098776,
      "loss": 7.6846,
      "step": 806
    },
    {
      "epoch": 0.2297508896797153,
      "grad_norm": 0.4053337574005127,
      "learning_rate": 0.0004426416168516937,
      "loss": 7.9707,
      "step": 807
    },
    {
      "epoch": 0.2300355871886121,
      "grad_norm": 0.642076849937439,
      "learning_rate": 0.00044257045260461144,
      "loss": 7.5889,
      "step": 808
    },
    {
      "epoch": 0.2303202846975089,
      "grad_norm": 0.5518573522567749,
      "learning_rate": 0.00044249928835752916,
      "loss": 7.4805,
      "step": 809
    },
    {
      "epoch": 0.23060498220640568,
      "grad_norm": 0.5629279017448425,
      "learning_rate": 0.00044242812411044694,
      "loss": 7.0342,
      "step": 810
    },
    {
      "epoch": 0.2308896797153025,
      "grad_norm": 0.5347673892974854,
      "learning_rate": 0.00044235695986336466,
      "loss": 7.0684,
      "step": 811
    },
    {
      "epoch": 0.23117437722419928,
      "grad_norm": 0.5217247605323792,
      "learning_rate": 0.0004422857956162824,
      "loss": 7.2383,
      "step": 812
    },
    {
      "epoch": 0.2314590747330961,
      "grad_norm": 0.6054062843322754,
      "learning_rate": 0.00044221463136920016,
      "loss": 7.4854,
      "step": 813
    },
    {
      "epoch": 0.23174377224199288,
      "grad_norm": 0.6359828114509583,
      "learning_rate": 0.0004421434671221178,
      "loss": 6.7822,
      "step": 814
    },
    {
      "epoch": 0.23202846975088967,
      "grad_norm": 0.508417010307312,
      "learning_rate": 0.0004420723028750356,
      "loss": 7.4092,
      "step": 815
    },
    {
      "epoch": 0.23231316725978648,
      "grad_norm": 0.47506824135780334,
      "learning_rate": 0.0004420011386279533,
      "loss": 7.3555,
      "step": 816
    },
    {
      "epoch": 0.23259786476868327,
      "grad_norm": 0.5379090905189514,
      "learning_rate": 0.00044192997438087105,
      "loss": 6.8398,
      "step": 817
    },
    {
      "epoch": 0.23288256227758006,
      "grad_norm": 0.49462756514549255,
      "learning_rate": 0.0004418588101337888,
      "loss": 7.1006,
      "step": 818
    },
    {
      "epoch": 0.23316725978647687,
      "grad_norm": 0.4736199676990509,
      "learning_rate": 0.0004417876458867065,
      "loss": 7.082,
      "step": 819
    },
    {
      "epoch": 0.23345195729537366,
      "grad_norm": 0.46984589099884033,
      "learning_rate": 0.00044171648163962427,
      "loss": 7.0547,
      "step": 820
    },
    {
      "epoch": 0.23373665480427047,
      "grad_norm": 0.42229676246643066,
      "learning_rate": 0.000441645317392542,
      "loss": 7.5547,
      "step": 821
    },
    {
      "epoch": 0.23402135231316726,
      "grad_norm": 0.5318613052368164,
      "learning_rate": 0.0004415741531454597,
      "loss": 7.2295,
      "step": 822
    },
    {
      "epoch": 0.23430604982206404,
      "grad_norm": 0.5012297034263611,
      "learning_rate": 0.0004415029888983775,
      "loss": 7.3096,
      "step": 823
    },
    {
      "epoch": 0.23459074733096086,
      "grad_norm": 0.4093872606754303,
      "learning_rate": 0.0004414318246512952,
      "loss": 7.4219,
      "step": 824
    },
    {
      "epoch": 0.23487544483985764,
      "grad_norm": 0.542306661605835,
      "learning_rate": 0.0004413606604042129,
      "loss": 7.1299,
      "step": 825
    },
    {
      "epoch": 0.23516014234875446,
      "grad_norm": 0.5459290742874146,
      "learning_rate": 0.00044128949615713066,
      "loss": 7.2705,
      "step": 826
    },
    {
      "epoch": 0.23544483985765124,
      "grad_norm": 0.4683960974216461,
      "learning_rate": 0.0004412183319100484,
      "loss": 7.1016,
      "step": 827
    },
    {
      "epoch": 0.23572953736654803,
      "grad_norm": 0.4583689570426941,
      "learning_rate": 0.00044114716766296616,
      "loss": 7.5137,
      "step": 828
    },
    {
      "epoch": 0.23601423487544484,
      "grad_norm": 0.48082277178764343,
      "learning_rate": 0.0004410760034158839,
      "loss": 7.4746,
      "step": 829
    },
    {
      "epoch": 0.23629893238434163,
      "grad_norm": 0.4595732092857361,
      "learning_rate": 0.0004410048391688016,
      "loss": 7.3242,
      "step": 830
    },
    {
      "epoch": 0.23658362989323845,
      "grad_norm": 0.8203401565551758,
      "learning_rate": 0.00044093367492171933,
      "loss": 7.3457,
      "step": 831
    },
    {
      "epoch": 0.23686832740213523,
      "grad_norm": 0.6309863924980164,
      "learning_rate": 0.00044086251067463705,
      "loss": 6.5508,
      "step": 832
    },
    {
      "epoch": 0.23715302491103202,
      "grad_norm": 0.5663462281227112,
      "learning_rate": 0.00044079134642755483,
      "loss": 6.9541,
      "step": 833
    },
    {
      "epoch": 0.23743772241992883,
      "grad_norm": 0.5051040649414062,
      "learning_rate": 0.00044072018218047255,
      "loss": 6.9756,
      "step": 834
    },
    {
      "epoch": 0.23772241992882562,
      "grad_norm": 0.47865554690361023,
      "learning_rate": 0.0004406490179333903,
      "loss": 7.3057,
      "step": 835
    },
    {
      "epoch": 0.23800711743772243,
      "grad_norm": 0.43697893619537354,
      "learning_rate": 0.000440577853686308,
      "loss": 7.6318,
      "step": 836
    },
    {
      "epoch": 0.23829181494661922,
      "grad_norm": 0.534851610660553,
      "learning_rate": 0.0004405066894392257,
      "loss": 7.0742,
      "step": 837
    },
    {
      "epoch": 0.238576512455516,
      "grad_norm": 0.5143926739692688,
      "learning_rate": 0.0004404355251921435,
      "loss": 6.8213,
      "step": 838
    },
    {
      "epoch": 0.23886120996441282,
      "grad_norm": 0.5173239707946777,
      "learning_rate": 0.0004403643609450612,
      "loss": 7.1689,
      "step": 839
    },
    {
      "epoch": 0.2391459074733096,
      "grad_norm": 0.44212883710861206,
      "learning_rate": 0.00044029319669797894,
      "loss": 7.6211,
      "step": 840
    },
    {
      "epoch": 0.23943060498220642,
      "grad_norm": 0.4778367877006531,
      "learning_rate": 0.0004402220324508967,
      "loss": 7.2168,
      "step": 841
    },
    {
      "epoch": 0.2397153024911032,
      "grad_norm": 0.6036151051521301,
      "learning_rate": 0.0004401508682038144,
      "loss": 6.9395,
      "step": 842
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.5227299928665161,
      "learning_rate": 0.0004400797039567321,
      "loss": 7.4209,
      "step": 843
    },
    {
      "epoch": 0.2402846975088968,
      "grad_norm": 0.4932726323604584,
      "learning_rate": 0.0004400085397096499,
      "loss": 7.5088,
      "step": 844
    },
    {
      "epoch": 0.2405693950177936,
      "grad_norm": 0.41230401396751404,
      "learning_rate": 0.0004399373754625676,
      "loss": 7.6035,
      "step": 845
    },
    {
      "epoch": 0.24085409252669038,
      "grad_norm": 0.6228237748146057,
      "learning_rate": 0.0004398662112154854,
      "loss": 6.9785,
      "step": 846
    },
    {
      "epoch": 0.2411387900355872,
      "grad_norm": 0.5555920004844666,
      "learning_rate": 0.00043979504696840306,
      "loss": 7.0986,
      "step": 847
    },
    {
      "epoch": 0.24142348754448398,
      "grad_norm": 0.5252820253372192,
      "learning_rate": 0.0004397238827213208,
      "loss": 7.1113,
      "step": 848
    },
    {
      "epoch": 0.2417081850533808,
      "grad_norm": 0.5262016654014587,
      "learning_rate": 0.00043965271847423856,
      "loss": 7.3691,
      "step": 849
    },
    {
      "epoch": 0.24199288256227758,
      "grad_norm": 0.5109695196151733,
      "learning_rate": 0.0004395815542271563,
      "loss": 7.3809,
      "step": 850
    },
    {
      "epoch": 0.24227758007117436,
      "grad_norm": 0.4192237854003906,
      "learning_rate": 0.00043951038998007406,
      "loss": 7.791,
      "step": 851
    },
    {
      "epoch": 0.24256227758007118,
      "grad_norm": 0.5321003794670105,
      "learning_rate": 0.0004394392257329918,
      "loss": 7.1904,
      "step": 852
    },
    {
      "epoch": 0.24284697508896796,
      "grad_norm": 0.5241415500640869,
      "learning_rate": 0.00043936806148590945,
      "loss": 7.2236,
      "step": 853
    },
    {
      "epoch": 0.24313167259786478,
      "grad_norm": 0.4347819685935974,
      "learning_rate": 0.0004392968972388272,
      "loss": 7.6279,
      "step": 854
    },
    {
      "epoch": 0.24341637010676156,
      "grad_norm": 0.5250548720359802,
      "learning_rate": 0.00043922573299174495,
      "loss": 7.4512,
      "step": 855
    },
    {
      "epoch": 0.24370106761565835,
      "grad_norm": 0.5096853971481323,
      "learning_rate": 0.00043915456874466267,
      "loss": 7.3613,
      "step": 856
    },
    {
      "epoch": 0.24398576512455517,
      "grad_norm": 0.5170667767524719,
      "learning_rate": 0.00043908340449758045,
      "loss": 7.2148,
      "step": 857
    },
    {
      "epoch": 0.24427046263345195,
      "grad_norm": 0.4182034730911255,
      "learning_rate": 0.00043901224025049817,
      "loss": 7.5996,
      "step": 858
    },
    {
      "epoch": 0.24455516014234877,
      "grad_norm": 0.47495993971824646,
      "learning_rate": 0.0004389410760034159,
      "loss": 7.1172,
      "step": 859
    },
    {
      "epoch": 0.24483985765124555,
      "grad_norm": 0.425849050283432,
      "learning_rate": 0.0004388699117563336,
      "loss": 7.5752,
      "step": 860
    },
    {
      "epoch": 0.24512455516014234,
      "grad_norm": 0.5022419095039368,
      "learning_rate": 0.00043879874750925134,
      "loss": 7.2393,
      "step": 861
    },
    {
      "epoch": 0.24540925266903915,
      "grad_norm": 0.5683399438858032,
      "learning_rate": 0.0004387275832621691,
      "loss": 7.1514,
      "step": 862
    },
    {
      "epoch": 0.24569395017793594,
      "grad_norm": 0.3528391122817993,
      "learning_rate": 0.00043865641901508684,
      "loss": 7.8232,
      "step": 863
    },
    {
      "epoch": 0.24597864768683275,
      "grad_norm": 0.4852282404899597,
      "learning_rate": 0.00043858525476800456,
      "loss": 7.5166,
      "step": 864
    },
    {
      "epoch": 0.24626334519572954,
      "grad_norm": 0.6018944382667542,
      "learning_rate": 0.0004385140905209223,
      "loss": 7.3467,
      "step": 865
    },
    {
      "epoch": 0.24654804270462632,
      "grad_norm": 0.6870088577270508,
      "learning_rate": 0.00043844292627384,
      "loss": 6.6152,
      "step": 866
    },
    {
      "epoch": 0.24683274021352314,
      "grad_norm": 0.4139854907989502,
      "learning_rate": 0.0004383717620267578,
      "loss": 7.709,
      "step": 867
    },
    {
      "epoch": 0.24711743772241992,
      "grad_norm": 0.576387882232666,
      "learning_rate": 0.0004383005977796755,
      "loss": 6.8809,
      "step": 868
    },
    {
      "epoch": 0.24740213523131674,
      "grad_norm": 0.4755402207374573,
      "learning_rate": 0.0004382294335325933,
      "loss": 7.293,
      "step": 869
    },
    {
      "epoch": 0.24768683274021353,
      "grad_norm": 0.5647282600402832,
      "learning_rate": 0.00043815826928551095,
      "loss": 6.9727,
      "step": 870
    },
    {
      "epoch": 0.2479715302491103,
      "grad_norm": 0.6776756644248962,
      "learning_rate": 0.0004380871050384287,
      "loss": 5.9951,
      "step": 871
    },
    {
      "epoch": 0.24825622775800713,
      "grad_norm": 0.45100125670433044,
      "learning_rate": 0.00043801594079134645,
      "loss": 7.3037,
      "step": 872
    },
    {
      "epoch": 0.2485409252669039,
      "grad_norm": 0.4857039749622345,
      "learning_rate": 0.0004379447765442642,
      "loss": 7.4775,
      "step": 873
    },
    {
      "epoch": 0.24882562277580073,
      "grad_norm": 0.5256150960922241,
      "learning_rate": 0.0004378736122971819,
      "loss": 7.2344,
      "step": 874
    },
    {
      "epoch": 0.2491103202846975,
      "grad_norm": 0.5394523739814758,
      "learning_rate": 0.0004378024480500997,
      "loss": 7.0469,
      "step": 875
    },
    {
      "epoch": 0.2493950177935943,
      "grad_norm": 0.4427834153175354,
      "learning_rate": 0.00043773128380301734,
      "loss": 7.4883,
      "step": 876
    },
    {
      "epoch": 0.2496797153024911,
      "grad_norm": 0.5576911568641663,
      "learning_rate": 0.0004376601195559351,
      "loss": 6.9834,
      "step": 877
    },
    {
      "epoch": 0.2499644128113879,
      "grad_norm": 0.5288885235786438,
      "learning_rate": 0.00043758895530885284,
      "loss": 7.5918,
      "step": 878
    },
    {
      "epoch": 0.2502491103202847,
      "grad_norm": 0.8065580129623413,
      "learning_rate": 0.00043751779106177056,
      "loss": 6.6963,
      "step": 879
    },
    {
      "epoch": 0.2505338078291815,
      "grad_norm": 0.6014062762260437,
      "learning_rate": 0.00043744662681468834,
      "loss": 6.7549,
      "step": 880
    },
    {
      "epoch": 0.2508185053380783,
      "grad_norm": 0.4984685480594635,
      "learning_rate": 0.000437375462567606,
      "loss": 7.5518,
      "step": 881
    },
    {
      "epoch": 0.25110320284697507,
      "grad_norm": 0.47743654251098633,
      "learning_rate": 0.0004373042983205238,
      "loss": 7.4434,
      "step": 882
    },
    {
      "epoch": 0.2513879003558719,
      "grad_norm": 0.5659381151199341,
      "learning_rate": 0.0004372331340734415,
      "loss": 7.1982,
      "step": 883
    },
    {
      "epoch": 0.2516725978647687,
      "grad_norm": 0.521162748336792,
      "learning_rate": 0.00043716196982635923,
      "loss": 7.4102,
      "step": 884
    },
    {
      "epoch": 0.25195729537366546,
      "grad_norm": 0.5566650629043579,
      "learning_rate": 0.000437090805579277,
      "loss": 6.7998,
      "step": 885
    },
    {
      "epoch": 0.25224199288256227,
      "grad_norm": 0.5467177629470825,
      "learning_rate": 0.00043701964133219473,
      "loss": 7.2295,
      "step": 886
    },
    {
      "epoch": 0.2525266903914591,
      "grad_norm": 0.5133892297744751,
      "learning_rate": 0.0004369484770851124,
      "loss": 6.835,
      "step": 887
    },
    {
      "epoch": 0.2528113879003559,
      "grad_norm": 0.4159110486507416,
      "learning_rate": 0.0004368773128380302,
      "loss": 7.7285,
      "step": 888
    },
    {
      "epoch": 0.25309608540925266,
      "grad_norm": 0.443999320268631,
      "learning_rate": 0.0004368061485909479,
      "loss": 7.4121,
      "step": 889
    },
    {
      "epoch": 0.25338078291814947,
      "grad_norm": 0.5295789837837219,
      "learning_rate": 0.0004367349843438657,
      "loss": 7.1299,
      "step": 890
    },
    {
      "epoch": 0.2536654804270463,
      "grad_norm": 0.42882344126701355,
      "learning_rate": 0.0004366638200967834,
      "loss": 7.7754,
      "step": 891
    },
    {
      "epoch": 0.25395017793594304,
      "grad_norm": 0.5075104236602783,
      "learning_rate": 0.00043659265584970107,
      "loss": 6.7295,
      "step": 892
    },
    {
      "epoch": 0.25423487544483986,
      "grad_norm": 0.5219612121582031,
      "learning_rate": 0.00043652149160261885,
      "loss": 7.6309,
      "step": 893
    },
    {
      "epoch": 0.2545195729537367,
      "grad_norm": 0.5781932473182678,
      "learning_rate": 0.00043645032735553657,
      "loss": 6.8701,
      "step": 894
    },
    {
      "epoch": 0.25480427046263343,
      "grad_norm": 0.4656875431537628,
      "learning_rate": 0.00043637916310845435,
      "loss": 7.5107,
      "step": 895
    },
    {
      "epoch": 0.25508896797153024,
      "grad_norm": 0.512679934501648,
      "learning_rate": 0.00043630799886137207,
      "loss": 7.2812,
      "step": 896
    },
    {
      "epoch": 0.25537366548042706,
      "grad_norm": 0.530357301235199,
      "learning_rate": 0.0004362368346142898,
      "loss": 6.9248,
      "step": 897
    },
    {
      "epoch": 0.2556583629893238,
      "grad_norm": 0.44879037141799927,
      "learning_rate": 0.0004361656703672075,
      "loss": 7.5576,
      "step": 898
    },
    {
      "epoch": 0.25594306049822063,
      "grad_norm": 0.4848640263080597,
      "learning_rate": 0.00043609450612012524,
      "loss": 7.3281,
      "step": 899
    },
    {
      "epoch": 0.25622775800711745,
      "grad_norm": 0.43455377221107483,
      "learning_rate": 0.000436023341873043,
      "loss": 7.5635,
      "step": 900
    },
    {
      "epoch": 0.25651245551601426,
      "grad_norm": 0.41088178753852844,
      "learning_rate": 0.00043595217762596074,
      "loss": 7.5537,
      "step": 901
    },
    {
      "epoch": 0.256797153024911,
      "grad_norm": 0.5732548832893372,
      "learning_rate": 0.00043588101337887846,
      "loss": 6.7061,
      "step": 902
    },
    {
      "epoch": 0.25708185053380783,
      "grad_norm": 0.5296939015388489,
      "learning_rate": 0.00043580984913179624,
      "loss": 7.0996,
      "step": 903
    },
    {
      "epoch": 0.25736654804270465,
      "grad_norm": 0.44330182671546936,
      "learning_rate": 0.0004357386848847139,
      "loss": 7.5117,
      "step": 904
    },
    {
      "epoch": 0.2576512455516014,
      "grad_norm": 0.625610888004303,
      "learning_rate": 0.00043566752063763163,
      "loss": 7.1572,
      "step": 905
    },
    {
      "epoch": 0.2579359430604982,
      "grad_norm": 0.5074996948242188,
      "learning_rate": 0.0004355963563905494,
      "loss": 7.4404,
      "step": 906
    },
    {
      "epoch": 0.25822064056939503,
      "grad_norm": 0.4755004942417145,
      "learning_rate": 0.00043552519214346713,
      "loss": 7.374,
      "step": 907
    },
    {
      "epoch": 0.2585053380782918,
      "grad_norm": 0.6408856511116028,
      "learning_rate": 0.0004354540278963849,
      "loss": 7.1172,
      "step": 908
    },
    {
      "epoch": 0.2587900355871886,
      "grad_norm": 0.5064488053321838,
      "learning_rate": 0.00043538286364930257,
      "loss": 7.2129,
      "step": 909
    },
    {
      "epoch": 0.2590747330960854,
      "grad_norm": 0.6024318933486938,
      "learning_rate": 0.0004353116994022203,
      "loss": 7.0586,
      "step": 910
    },
    {
      "epoch": 0.25935943060498223,
      "grad_norm": 0.5472372174263,
      "learning_rate": 0.00043524053515513807,
      "loss": 6.8779,
      "step": 911
    },
    {
      "epoch": 0.259644128113879,
      "grad_norm": 0.4426235258579254,
      "learning_rate": 0.0004351693709080558,
      "loss": 7.7656,
      "step": 912
    },
    {
      "epoch": 0.2599288256227758,
      "grad_norm": 0.47683799266815186,
      "learning_rate": 0.00043509820666097357,
      "loss": 7.9033,
      "step": 913
    },
    {
      "epoch": 0.2602135231316726,
      "grad_norm": 0.4714563488960266,
      "learning_rate": 0.0004350270424138913,
      "loss": 7.6914,
      "step": 914
    },
    {
      "epoch": 0.2604982206405694,
      "grad_norm": 0.5874764323234558,
      "learning_rate": 0.00043495587816680896,
      "loss": 6.9482,
      "step": 915
    },
    {
      "epoch": 0.2607829181494662,
      "grad_norm": 0.46627673506736755,
      "learning_rate": 0.00043488471391972674,
      "loss": 7.5713,
      "step": 916
    },
    {
      "epoch": 0.261067615658363,
      "grad_norm": 0.5269532203674316,
      "learning_rate": 0.00043481354967264446,
      "loss": 7.1367,
      "step": 917
    },
    {
      "epoch": 0.26135231316725976,
      "grad_norm": 0.49821069836616516,
      "learning_rate": 0.00043474238542556224,
      "loss": 7.6846,
      "step": 918
    },
    {
      "epoch": 0.2616370106761566,
      "grad_norm": 0.47218114137649536,
      "learning_rate": 0.00043467122117847996,
      "loss": 7.5557,
      "step": 919
    },
    {
      "epoch": 0.2619217081850534,
      "grad_norm": 0.5291839241981506,
      "learning_rate": 0.0004346000569313977,
      "loss": 7.4102,
      "step": 920
    },
    {
      "epoch": 0.26220640569395015,
      "grad_norm": 0.5340070724487305,
      "learning_rate": 0.0004345288926843154,
      "loss": 6.9697,
      "step": 921
    },
    {
      "epoch": 0.26249110320284696,
      "grad_norm": 0.5339959859848022,
      "learning_rate": 0.00043445772843723313,
      "loss": 7.0928,
      "step": 922
    },
    {
      "epoch": 0.2627758007117438,
      "grad_norm": 0.580223798751831,
      "learning_rate": 0.00043438656419015085,
      "loss": 6.8213,
      "step": 923
    },
    {
      "epoch": 0.2630604982206406,
      "grad_norm": 0.5580481290817261,
      "learning_rate": 0.00043431539994306863,
      "loss": 6.8799,
      "step": 924
    },
    {
      "epoch": 0.26334519572953735,
      "grad_norm": 0.4867836534976959,
      "learning_rate": 0.00043424423569598635,
      "loss": 7.8086,
      "step": 925
    },
    {
      "epoch": 0.26362989323843417,
      "grad_norm": 0.4634561538696289,
      "learning_rate": 0.0004341730714489041,
      "loss": 7.4473,
      "step": 926
    },
    {
      "epoch": 0.263914590747331,
      "grad_norm": 0.5524905920028687,
      "learning_rate": 0.0004341019072018218,
      "loss": 6.9121,
      "step": 927
    },
    {
      "epoch": 0.26419928825622774,
      "grad_norm": 0.45816829800605774,
      "learning_rate": 0.0004340307429547395,
      "loss": 7.3428,
      "step": 928
    },
    {
      "epoch": 0.26448398576512455,
      "grad_norm": 0.6225414276123047,
      "learning_rate": 0.0004339595787076573,
      "loss": 6.8535,
      "step": 929
    },
    {
      "epoch": 0.26476868327402137,
      "grad_norm": 0.4236343801021576,
      "learning_rate": 0.000433888414460575,
      "loss": 8.0156,
      "step": 930
    },
    {
      "epoch": 0.2650533807829181,
      "grad_norm": 0.4699235260486603,
      "learning_rate": 0.0004338172502134928,
      "loss": 7.5518,
      "step": 931
    },
    {
      "epoch": 0.26533807829181494,
      "grad_norm": 0.4837797284126282,
      "learning_rate": 0.00043374608596641047,
      "loss": 7.3555,
      "step": 932
    },
    {
      "epoch": 0.26562277580071175,
      "grad_norm": 0.5280996561050415,
      "learning_rate": 0.0004336749217193282,
      "loss": 6.7998,
      "step": 933
    },
    {
      "epoch": 0.26590747330960857,
      "grad_norm": 0.5825293064117432,
      "learning_rate": 0.00043360375747224597,
      "loss": 7.0225,
      "step": 934
    },
    {
      "epoch": 0.2661921708185053,
      "grad_norm": 0.38843557238578796,
      "learning_rate": 0.0004335325932251637,
      "loss": 7.8369,
      "step": 935
    },
    {
      "epoch": 0.26647686832740214,
      "grad_norm": 0.41565942764282227,
      "learning_rate": 0.00043346142897808147,
      "loss": 7.6914,
      "step": 936
    },
    {
      "epoch": 0.26676156583629895,
      "grad_norm": 0.5718416571617126,
      "learning_rate": 0.00043339026473099914,
      "loss": 6.6875,
      "step": 937
    },
    {
      "epoch": 0.2670462633451957,
      "grad_norm": 0.5091272592544556,
      "learning_rate": 0.00043331910048391686,
      "loss": 7.2822,
      "step": 938
    },
    {
      "epoch": 0.2673309608540925,
      "grad_norm": 0.47761550545692444,
      "learning_rate": 0.00043324793623683463,
      "loss": 7.2646,
      "step": 939
    },
    {
      "epoch": 0.26761565836298934,
      "grad_norm": 0.4958757758140564,
      "learning_rate": 0.00043317677198975236,
      "loss": 7.3691,
      "step": 940
    },
    {
      "epoch": 0.2679003558718861,
      "grad_norm": 0.5092728137969971,
      "learning_rate": 0.0004331056077426701,
      "loss": 7.3887,
      "step": 941
    },
    {
      "epoch": 0.2681850533807829,
      "grad_norm": 0.5320535898208618,
      "learning_rate": 0.00043303444349558786,
      "loss": 7.2588,
      "step": 942
    },
    {
      "epoch": 0.2684697508896797,
      "grad_norm": 0.4988536834716797,
      "learning_rate": 0.0004329632792485055,
      "loss": 7.1562,
      "step": 943
    },
    {
      "epoch": 0.26875444839857654,
      "grad_norm": 0.5935326814651489,
      "learning_rate": 0.0004328921150014233,
      "loss": 6.0732,
      "step": 944
    },
    {
      "epoch": 0.2690391459074733,
      "grad_norm": 0.4439448416233063,
      "learning_rate": 0.000432820950754341,
      "loss": 7.1514,
      "step": 945
    },
    {
      "epoch": 0.2693238434163701,
      "grad_norm": 0.41855907440185547,
      "learning_rate": 0.00043274978650725875,
      "loss": 7.793,
      "step": 946
    },
    {
      "epoch": 0.2696085409252669,
      "grad_norm": 0.4971495568752289,
      "learning_rate": 0.0004326786222601765,
      "loss": 7.2061,
      "step": 947
    },
    {
      "epoch": 0.2698932384341637,
      "grad_norm": 0.5047661662101746,
      "learning_rate": 0.00043260745801309425,
      "loss": 6.791,
      "step": 948
    },
    {
      "epoch": 0.2701779359430605,
      "grad_norm": 0.4491565525531769,
      "learning_rate": 0.00043253629376601197,
      "loss": 7.3984,
      "step": 949
    },
    {
      "epoch": 0.2704626334519573,
      "grad_norm": 0.5155214071273804,
      "learning_rate": 0.0004324651295189297,
      "loss": 7.3164,
      "step": 950
    },
    {
      "epoch": 0.27074733096085407,
      "grad_norm": 0.46427953243255615,
      "learning_rate": 0.0004323939652718474,
      "loss": 7.3857,
      "step": 951
    },
    {
      "epoch": 0.2710320284697509,
      "grad_norm": 0.5414927005767822,
      "learning_rate": 0.0004323228010247652,
      "loss": 7.0195,
      "step": 952
    },
    {
      "epoch": 0.2713167259786477,
      "grad_norm": 0.5723718404769897,
      "learning_rate": 0.0004322516367776829,
      "loss": 7.0186,
      "step": 953
    },
    {
      "epoch": 0.27160142348754446,
      "grad_norm": 0.5131714940071106,
      "learning_rate": 0.0004321804725306006,
      "loss": 7.1494,
      "step": 954
    },
    {
      "epoch": 0.27188612099644127,
      "grad_norm": 0.6219976544380188,
      "learning_rate": 0.00043210930828351836,
      "loss": 6.8535,
      "step": 955
    },
    {
      "epoch": 0.2721708185053381,
      "grad_norm": 0.4529915452003479,
      "learning_rate": 0.0004320381440364361,
      "loss": 7.4473,
      "step": 956
    },
    {
      "epoch": 0.2724555160142349,
      "grad_norm": 0.5033783912658691,
      "learning_rate": 0.00043196697978935386,
      "loss": 7.4102,
      "step": 957
    },
    {
      "epoch": 0.27274021352313166,
      "grad_norm": 0.44644036889076233,
      "learning_rate": 0.0004318958155422716,
      "loss": 7.7686,
      "step": 958
    },
    {
      "epoch": 0.27302491103202847,
      "grad_norm": 0.5473158359527588,
      "learning_rate": 0.0004318246512951893,
      "loss": 7.25,
      "step": 959
    },
    {
      "epoch": 0.2733096085409253,
      "grad_norm": 0.4492653012275696,
      "learning_rate": 0.00043175348704810703,
      "loss": 7.8701,
      "step": 960
    },
    {
      "epoch": 0.27359430604982204,
      "grad_norm": 0.47338056564331055,
      "learning_rate": 0.00043168232280102475,
      "loss": 7.2969,
      "step": 961
    },
    {
      "epoch": 0.27387900355871886,
      "grad_norm": 0.5589231848716736,
      "learning_rate": 0.00043161115855394253,
      "loss": 7.4736,
      "step": 962
    },
    {
      "epoch": 0.2741637010676157,
      "grad_norm": 0.4764990210533142,
      "learning_rate": 0.00043153999430686025,
      "loss": 7.3125,
      "step": 963
    },
    {
      "epoch": 0.27444839857651243,
      "grad_norm": 0.5841454863548279,
      "learning_rate": 0.000431468830059778,
      "loss": 7.0547,
      "step": 964
    },
    {
      "epoch": 0.27473309608540925,
      "grad_norm": 0.52353835105896,
      "learning_rate": 0.0004313976658126957,
      "loss": 7.459,
      "step": 965
    },
    {
      "epoch": 0.27501779359430606,
      "grad_norm": 0.523868978023529,
      "learning_rate": 0.0004313265015656134,
      "loss": 7.3135,
      "step": 966
    },
    {
      "epoch": 0.2753024911032029,
      "grad_norm": 0.4624759554862976,
      "learning_rate": 0.0004312553373185312,
      "loss": 7.6846,
      "step": 967
    },
    {
      "epoch": 0.27558718861209963,
      "grad_norm": 0.5388631224632263,
      "learning_rate": 0.0004311841730714489,
      "loss": 7.3652,
      "step": 968
    },
    {
      "epoch": 0.27587188612099645,
      "grad_norm": 0.5112046599388123,
      "learning_rate": 0.00043111300882436664,
      "loss": 7.6865,
      "step": 969
    },
    {
      "epoch": 0.27615658362989326,
      "grad_norm": 0.40607786178588867,
      "learning_rate": 0.0004310418445772844,
      "loss": 7.4023,
      "step": 970
    },
    {
      "epoch": 0.27644128113879,
      "grad_norm": 0.44282764196395874,
      "learning_rate": 0.0004309706803302021,
      "loss": 7.7432,
      "step": 971
    },
    {
      "epoch": 0.27672597864768683,
      "grad_norm": 0.4683508574962616,
      "learning_rate": 0.0004308995160831198,
      "loss": 7.6504,
      "step": 972
    },
    {
      "epoch": 0.27701067615658365,
      "grad_norm": 0.5277723073959351,
      "learning_rate": 0.0004308283518360376,
      "loss": 7.3008,
      "step": 973
    },
    {
      "epoch": 0.2772953736654804,
      "grad_norm": 0.48368245363235474,
      "learning_rate": 0.0004307571875889553,
      "loss": 7.2158,
      "step": 974
    },
    {
      "epoch": 0.2775800711743772,
      "grad_norm": 0.5163562893867493,
      "learning_rate": 0.0004306860233418731,
      "loss": 7.3232,
      "step": 975
    },
    {
      "epoch": 0.27786476868327403,
      "grad_norm": 0.5331327319145203,
      "learning_rate": 0.0004306148590947908,
      "loss": 6.9707,
      "step": 976
    },
    {
      "epoch": 0.2781494661921708,
      "grad_norm": 0.5096694231033325,
      "learning_rate": 0.0004305436948477085,
      "loss": 7.1387,
      "step": 977
    },
    {
      "epoch": 0.2784341637010676,
      "grad_norm": 0.6267108917236328,
      "learning_rate": 0.00043047253060062626,
      "loss": 6.917,
      "step": 978
    },
    {
      "epoch": 0.2787188612099644,
      "grad_norm": 0.4161131680011749,
      "learning_rate": 0.000430401366353544,
      "loss": 7.3604,
      "step": 979
    },
    {
      "epoch": 0.27900355871886123,
      "grad_norm": 0.41714102029800415,
      "learning_rate": 0.00043033020210646176,
      "loss": 7.6514,
      "step": 980
    },
    {
      "epoch": 0.279288256227758,
      "grad_norm": 0.5368282794952393,
      "learning_rate": 0.0004302590378593795,
      "loss": 7.1318,
      "step": 981
    },
    {
      "epoch": 0.2795729537366548,
      "grad_norm": 0.42313826084136963,
      "learning_rate": 0.00043018787361229715,
      "loss": 7.5117,
      "step": 982
    },
    {
      "epoch": 0.2798576512455516,
      "grad_norm": 0.4829246997833252,
      "learning_rate": 0.0004301167093652149,
      "loss": 7.4912,
      "step": 983
    },
    {
      "epoch": 0.2801423487544484,
      "grad_norm": 0.6406773924827576,
      "learning_rate": 0.00043004554511813265,
      "loss": 6.4014,
      "step": 984
    },
    {
      "epoch": 0.2804270462633452,
      "grad_norm": 0.3736668527126312,
      "learning_rate": 0.0004299743808710504,
      "loss": 7.9062,
      "step": 985
    },
    {
      "epoch": 0.280711743772242,
      "grad_norm": 0.522930383682251,
      "learning_rate": 0.00042990321662396815,
      "loss": 7.0312,
      "step": 986
    },
    {
      "epoch": 0.28099644128113876,
      "grad_norm": 0.4965778887271881,
      "learning_rate": 0.00042983205237688587,
      "loss": 7.1846,
      "step": 987
    },
    {
      "epoch": 0.2812811387900356,
      "grad_norm": 0.45272135734558105,
      "learning_rate": 0.0004297608881298036,
      "loss": 7.2119,
      "step": 988
    },
    {
      "epoch": 0.2815658362989324,
      "grad_norm": 0.5090800523757935,
      "learning_rate": 0.0004296897238827213,
      "loss": 7.0088,
      "step": 989
    },
    {
      "epoch": 0.2818505338078292,
      "grad_norm": 0.6086441874504089,
      "learning_rate": 0.00042961855963563904,
      "loss": 7.2715,
      "step": 990
    },
    {
      "epoch": 0.28213523131672597,
      "grad_norm": 0.7149702906608582,
      "learning_rate": 0.0004295473953885568,
      "loss": 6.3223,
      "step": 991
    },
    {
      "epoch": 0.2824199288256228,
      "grad_norm": 0.574139416217804,
      "learning_rate": 0.00042947623114147454,
      "loss": 7.1348,
      "step": 992
    },
    {
      "epoch": 0.2827046263345196,
      "grad_norm": 0.4520958960056305,
      "learning_rate": 0.0004294050668943923,
      "loss": 7.4697,
      "step": 993
    },
    {
      "epoch": 0.28298932384341635,
      "grad_norm": 0.4637325406074524,
      "learning_rate": 0.00042933390264731,
      "loss": 7.2773,
      "step": 994
    },
    {
      "epoch": 0.28327402135231317,
      "grad_norm": 0.4676058888435364,
      "learning_rate": 0.0004292627384002277,
      "loss": 7.3975,
      "step": 995
    },
    {
      "epoch": 0.28355871886121,
      "grad_norm": 0.42332184314727783,
      "learning_rate": 0.0004291915741531455,
      "loss": 7.6719,
      "step": 996
    },
    {
      "epoch": 0.28384341637010674,
      "grad_norm": 0.47916945815086365,
      "learning_rate": 0.0004291204099060632,
      "loss": 7.3799,
      "step": 997
    },
    {
      "epoch": 0.28412811387900355,
      "grad_norm": 0.5687378644943237,
      "learning_rate": 0.000429049245658981,
      "loss": 7.1699,
      "step": 998
    },
    {
      "epoch": 0.28441281138790037,
      "grad_norm": 0.38771921396255493,
      "learning_rate": 0.00042897808141189865,
      "loss": 7.5986,
      "step": 999
    },
    {
      "epoch": 0.2846975088967972,
      "grad_norm": 0.43714073300361633,
      "learning_rate": 0.0004289069171648164,
      "loss": 7.5859,
      "step": 1000
    },
    {
      "epoch": 0.2846975088967972,
      "eval_bleu": 0.10196535812756952,
      "eval_loss": 7.04296875,
      "eval_runtime": 121.1357,
      "eval_samples_per_second": 2.344,
      "eval_steps_per_second": 0.149,
      "step": 1000
    },
    {
      "epoch": 0.28498220640569394,
      "grad_norm": 0.4797312617301941,
      "learning_rate": 0.00042883575291773415,
      "loss": 7.6777,
      "step": 1001
    },
    {
      "epoch": 0.28526690391459075,
      "grad_norm": 0.47663864493370056,
      "learning_rate": 0.0004287645886706519,
      "loss": 7.6465,
      "step": 1002
    },
    {
      "epoch": 0.28555160142348757,
      "grad_norm": 0.4880309998989105,
      "learning_rate": 0.0004286934244235696,
      "loss": 7.5508,
      "step": 1003
    },
    {
      "epoch": 0.2858362989323843,
      "grad_norm": 0.4995867908000946,
      "learning_rate": 0.0004286222601764874,
      "loss": 7.2236,
      "step": 1004
    },
    {
      "epoch": 0.28612099644128114,
      "grad_norm": 0.5235399603843689,
      "learning_rate": 0.00042855109592940504,
      "loss": 6.8809,
      "step": 1005
    },
    {
      "epoch": 0.28640569395017795,
      "grad_norm": 0.4574708938598633,
      "learning_rate": 0.0004284799316823228,
      "loss": 7.4971,
      "step": 1006
    },
    {
      "epoch": 0.2866903914590747,
      "grad_norm": 0.6360961198806763,
      "learning_rate": 0.00042840876743524054,
      "loss": 7.5049,
      "step": 1007
    },
    {
      "epoch": 0.2869750889679715,
      "grad_norm": 0.5685173869132996,
      "learning_rate": 0.00042833760318815826,
      "loss": 6.958,
      "step": 1008
    },
    {
      "epoch": 0.28725978647686834,
      "grad_norm": 0.4308505654335022,
      "learning_rate": 0.00042826643894107604,
      "loss": 7.6084,
      "step": 1009
    },
    {
      "epoch": 0.2875444839857651,
      "grad_norm": 0.4639454483985901,
      "learning_rate": 0.0004281952746939937,
      "loss": 7.4463,
      "step": 1010
    },
    {
      "epoch": 0.2878291814946619,
      "grad_norm": 0.4709155857563019,
      "learning_rate": 0.0004281241104469115,
      "loss": 7.5322,
      "step": 1011
    },
    {
      "epoch": 0.2881138790035587,
      "grad_norm": 0.43574628233909607,
      "learning_rate": 0.0004280529461998292,
      "loss": 7.5186,
      "step": 1012
    },
    {
      "epoch": 0.28839857651245554,
      "grad_norm": 0.4731388986110687,
      "learning_rate": 0.00042798178195274693,
      "loss": 7.2949,
      "step": 1013
    },
    {
      "epoch": 0.2886832740213523,
      "grad_norm": 0.4204134941101074,
      "learning_rate": 0.0004279106177056647,
      "loss": 7.5859,
      "step": 1014
    },
    {
      "epoch": 0.2889679715302491,
      "grad_norm": 0.5122082829475403,
      "learning_rate": 0.00042783945345858243,
      "loss": 7.2627,
      "step": 1015
    },
    {
      "epoch": 0.2892526690391459,
      "grad_norm": 0.3954521119594574,
      "learning_rate": 0.00042776828921150015,
      "loss": 7.543,
      "step": 1016
    },
    {
      "epoch": 0.2895373665480427,
      "grad_norm": 0.4996376037597656,
      "learning_rate": 0.0004276971249644179,
      "loss": 7.4268,
      "step": 1017
    },
    {
      "epoch": 0.2898220640569395,
      "grad_norm": 0.48199692368507385,
      "learning_rate": 0.0004276259607173356,
      "loss": 7.4111,
      "step": 1018
    },
    {
      "epoch": 0.2901067615658363,
      "grad_norm": 0.4896969497203827,
      "learning_rate": 0.0004275547964702534,
      "loss": 7.5732,
      "step": 1019
    },
    {
      "epoch": 0.29039145907473307,
      "grad_norm": 0.5372595191001892,
      "learning_rate": 0.0004274836322231711,
      "loss": 6.9336,
      "step": 1020
    },
    {
      "epoch": 0.2906761565836299,
      "grad_norm": 0.46799662709236145,
      "learning_rate": 0.0004274124679760888,
      "loss": 7.6201,
      "step": 1021
    },
    {
      "epoch": 0.2909608540925267,
      "grad_norm": 0.47574394941329956,
      "learning_rate": 0.00042734130372900655,
      "loss": 7.2734,
      "step": 1022
    },
    {
      "epoch": 0.2912455516014235,
      "grad_norm": 0.4846518337726593,
      "learning_rate": 0.00042727013948192427,
      "loss": 7.4453,
      "step": 1023
    },
    {
      "epoch": 0.29153024911032027,
      "grad_norm": 0.5557963848114014,
      "learning_rate": 0.00042719897523484205,
      "loss": 7.0713,
      "step": 1024
    },
    {
      "epoch": 0.2918149466192171,
      "grad_norm": 0.4947628080844879,
      "learning_rate": 0.00042712781098775977,
      "loss": 7.5947,
      "step": 1025
    },
    {
      "epoch": 0.2920996441281139,
      "grad_norm": 0.492767333984375,
      "learning_rate": 0.0004270566467406775,
      "loss": 7.252,
      "step": 1026
    },
    {
      "epoch": 0.29238434163701066,
      "grad_norm": 0.6265051960945129,
      "learning_rate": 0.0004269854824935952,
      "loss": 6.3564,
      "step": 1027
    },
    {
      "epoch": 0.2926690391459075,
      "grad_norm": 0.5161956548690796,
      "learning_rate": 0.00042691431824651294,
      "loss": 7.5752,
      "step": 1028
    },
    {
      "epoch": 0.2929537366548043,
      "grad_norm": 0.5737975239753723,
      "learning_rate": 0.0004268431539994307,
      "loss": 7.0615,
      "step": 1029
    },
    {
      "epoch": 0.29323843416370104,
      "grad_norm": 0.4803426265716553,
      "learning_rate": 0.00042677198975234844,
      "loss": 7.5,
      "step": 1030
    },
    {
      "epoch": 0.29352313167259786,
      "grad_norm": 0.4499126076698303,
      "learning_rate": 0.00042670082550526616,
      "loss": 7.2568,
      "step": 1031
    },
    {
      "epoch": 0.2938078291814947,
      "grad_norm": 0.4502011239528656,
      "learning_rate": 0.00042662966125818394,
      "loss": 7.7002,
      "step": 1032
    },
    {
      "epoch": 0.29409252669039143,
      "grad_norm": 0.43873316049575806,
      "learning_rate": 0.0004265584970111016,
      "loss": 7.4287,
      "step": 1033
    },
    {
      "epoch": 0.29437722419928825,
      "grad_norm": 0.4562997817993164,
      "learning_rate": 0.0004264873327640193,
      "loss": 7.5312,
      "step": 1034
    },
    {
      "epoch": 0.29466192170818506,
      "grad_norm": 0.5061038136482239,
      "learning_rate": 0.0004264161685169371,
      "loss": 7.4629,
      "step": 1035
    },
    {
      "epoch": 0.2949466192170819,
      "grad_norm": 0.4653073847293854,
      "learning_rate": 0.0004263450042698548,
      "loss": 7.5996,
      "step": 1036
    },
    {
      "epoch": 0.29523131672597863,
      "grad_norm": 0.6139973402023315,
      "learning_rate": 0.0004262738400227726,
      "loss": 6.3779,
      "step": 1037
    },
    {
      "epoch": 0.29551601423487545,
      "grad_norm": 0.5320224761962891,
      "learning_rate": 0.0004262026757756903,
      "loss": 7.127,
      "step": 1038
    },
    {
      "epoch": 0.29580071174377226,
      "grad_norm": 0.5026519298553467,
      "learning_rate": 0.000426131511528608,
      "loss": 7.3281,
      "step": 1039
    },
    {
      "epoch": 0.296085409252669,
      "grad_norm": 0.5016393065452576,
      "learning_rate": 0.00042606034728152577,
      "loss": 7.0791,
      "step": 1040
    },
    {
      "epoch": 0.29637010676156583,
      "grad_norm": 0.516268253326416,
      "learning_rate": 0.0004259891830344435,
      "loss": 7.041,
      "step": 1041
    },
    {
      "epoch": 0.29665480427046265,
      "grad_norm": 0.48828786611557007,
      "learning_rate": 0.00042591801878736127,
      "loss": 7.4434,
      "step": 1042
    },
    {
      "epoch": 0.2969395017793594,
      "grad_norm": 0.4357978105545044,
      "learning_rate": 0.000425846854540279,
      "loss": 7.1211,
      "step": 1043
    },
    {
      "epoch": 0.2972241992882562,
      "grad_norm": 0.5390711426734924,
      "learning_rate": 0.00042577569029319666,
      "loss": 6.9951,
      "step": 1044
    },
    {
      "epoch": 0.29750889679715303,
      "grad_norm": 0.43850216269493103,
      "learning_rate": 0.00042570452604611444,
      "loss": 7.4346,
      "step": 1045
    },
    {
      "epoch": 0.29779359430604985,
      "grad_norm": 0.5152047276496887,
      "learning_rate": 0.00042563336179903216,
      "loss": 7.2969,
      "step": 1046
    },
    {
      "epoch": 0.2980782918149466,
      "grad_norm": 0.4733029305934906,
      "learning_rate": 0.00042556219755194994,
      "loss": 7.626,
      "step": 1047
    },
    {
      "epoch": 0.2983629893238434,
      "grad_norm": 0.5408203601837158,
      "learning_rate": 0.00042549103330486766,
      "loss": 7.0078,
      "step": 1048
    },
    {
      "epoch": 0.29864768683274023,
      "grad_norm": 0.39213523268699646,
      "learning_rate": 0.0004254198690577854,
      "loss": 7.5322,
      "step": 1049
    },
    {
      "epoch": 0.298932384341637,
      "grad_norm": 0.4816019833087921,
      "learning_rate": 0.0004253487048107031,
      "loss": 7.7979,
      "step": 1050
    },
    {
      "epoch": 0.2992170818505338,
      "grad_norm": 0.47788047790527344,
      "learning_rate": 0.00042527754056362083,
      "loss": 7.3506,
      "step": 1051
    },
    {
      "epoch": 0.2995017793594306,
      "grad_norm": 0.5580766201019287,
      "learning_rate": 0.00042520637631653855,
      "loss": 6.7529,
      "step": 1052
    },
    {
      "epoch": 0.2997864768683274,
      "grad_norm": 0.4852105379104614,
      "learning_rate": 0.00042513521206945633,
      "loss": 7.3311,
      "step": 1053
    },
    {
      "epoch": 0.3000711743772242,
      "grad_norm": 0.5395891070365906,
      "learning_rate": 0.00042506404782237405,
      "loss": 6.998,
      "step": 1054
    },
    {
      "epoch": 0.300355871886121,
      "grad_norm": 0.5457566976547241,
      "learning_rate": 0.0004249928835752918,
      "loss": 7.085,
      "step": 1055
    },
    {
      "epoch": 0.3006405693950178,
      "grad_norm": 0.5181650519371033,
      "learning_rate": 0.0004249217193282095,
      "loss": 7.3096,
      "step": 1056
    },
    {
      "epoch": 0.3009252669039146,
      "grad_norm": 0.5144093036651611,
      "learning_rate": 0.0004248505550811272,
      "loss": 7.0566,
      "step": 1057
    },
    {
      "epoch": 0.3012099644128114,
      "grad_norm": 0.6092381477355957,
      "learning_rate": 0.000424779390834045,
      "loss": 7.3281,
      "step": 1058
    },
    {
      "epoch": 0.3014946619217082,
      "grad_norm": 0.5332916975021362,
      "learning_rate": 0.0004247082265869627,
      "loss": 7.002,
      "step": 1059
    },
    {
      "epoch": 0.30177935943060497,
      "grad_norm": 0.5226554274559021,
      "learning_rate": 0.0004246370623398805,
      "loss": 7.1387,
      "step": 1060
    },
    {
      "epoch": 0.3020640569395018,
      "grad_norm": 0.5610384941101074,
      "learning_rate": 0.00042456589809279817,
      "loss": 7.3506,
      "step": 1061
    },
    {
      "epoch": 0.3023487544483986,
      "grad_norm": 0.42654889822006226,
      "learning_rate": 0.0004244947338457159,
      "loss": 7.5635,
      "step": 1062
    },
    {
      "epoch": 0.30263345195729535,
      "grad_norm": 0.5280149579048157,
      "learning_rate": 0.00042442356959863367,
      "loss": 7.29,
      "step": 1063
    },
    {
      "epoch": 0.30291814946619217,
      "grad_norm": 0.4202621579170227,
      "learning_rate": 0.0004243524053515514,
      "loss": 7.5176,
      "step": 1064
    },
    {
      "epoch": 0.303202846975089,
      "grad_norm": 0.5048372745513916,
      "learning_rate": 0.00042428124110446917,
      "loss": 6.957,
      "step": 1065
    },
    {
      "epoch": 0.30348754448398574,
      "grad_norm": 0.6739668846130371,
      "learning_rate": 0.0004242100768573869,
      "loss": 6.7461,
      "step": 1066
    },
    {
      "epoch": 0.30377224199288255,
      "grad_norm": 0.74714595079422,
      "learning_rate": 0.00042413891261030456,
      "loss": 7.7529,
      "step": 1067
    },
    {
      "epoch": 0.30405693950177937,
      "grad_norm": 0.6046257019042969,
      "learning_rate": 0.00042406774836322233,
      "loss": 7.0068,
      "step": 1068
    },
    {
      "epoch": 0.3043416370106762,
      "grad_norm": 0.5528740882873535,
      "learning_rate": 0.00042399658411614006,
      "loss": 7.2773,
      "step": 1069
    },
    {
      "epoch": 0.30462633451957294,
      "grad_norm": 0.47163739800453186,
      "learning_rate": 0.0004239254198690578,
      "loss": 7.4395,
      "step": 1070
    },
    {
      "epoch": 0.30491103202846975,
      "grad_norm": 0.4603082239627838,
      "learning_rate": 0.00042385425562197556,
      "loss": 7.54,
      "step": 1071
    },
    {
      "epoch": 0.30519572953736657,
      "grad_norm": 0.5182943344116211,
      "learning_rate": 0.0004237830913748932,
      "loss": 7.0479,
      "step": 1072
    },
    {
      "epoch": 0.3054804270462633,
      "grad_norm": 0.4548114240169525,
      "learning_rate": 0.000423711927127811,
      "loss": 7.7168,
      "step": 1073
    },
    {
      "epoch": 0.30576512455516014,
      "grad_norm": 0.5856614708900452,
      "learning_rate": 0.0004236407628807287,
      "loss": 7.1514,
      "step": 1074
    },
    {
      "epoch": 0.30604982206405695,
      "grad_norm": 0.4826880395412445,
      "learning_rate": 0.00042356959863364645,
      "loss": 7.3438,
      "step": 1075
    },
    {
      "epoch": 0.3063345195729537,
      "grad_norm": 0.5586170554161072,
      "learning_rate": 0.0004234984343865642,
      "loss": 7.1416,
      "step": 1076
    },
    {
      "epoch": 0.3066192170818505,
      "grad_norm": 0.4764145612716675,
      "learning_rate": 0.00042342727013948195,
      "loss": 7.0732,
      "step": 1077
    },
    {
      "epoch": 0.30690391459074734,
      "grad_norm": 0.6197493076324463,
      "learning_rate": 0.00042335610589239967,
      "loss": 7.1006,
      "step": 1078
    },
    {
      "epoch": 0.30718861209964415,
      "grad_norm": 0.47980010509490967,
      "learning_rate": 0.0004232849416453174,
      "loss": 7.5361,
      "step": 1079
    },
    {
      "epoch": 0.3074733096085409,
      "grad_norm": 0.5147501826286316,
      "learning_rate": 0.0004232137773982351,
      "loss": 7.2217,
      "step": 1080
    },
    {
      "epoch": 0.3077580071174377,
      "grad_norm": 0.6229455471038818,
      "learning_rate": 0.0004231426131511529,
      "loss": 6.8916,
      "step": 1081
    },
    {
      "epoch": 0.30804270462633454,
      "grad_norm": 0.519205629825592,
      "learning_rate": 0.0004230714489040706,
      "loss": 7.1543,
      "step": 1082
    },
    {
      "epoch": 0.3083274021352313,
      "grad_norm": 0.5824966430664062,
      "learning_rate": 0.0004230002846569884,
      "loss": 6.1338,
      "step": 1083
    },
    {
      "epoch": 0.3086120996441281,
      "grad_norm": 0.5626207590103149,
      "learning_rate": 0.00042292912040990606,
      "loss": 6.9707,
      "step": 1084
    },
    {
      "epoch": 0.3088967971530249,
      "grad_norm": 0.5577024817466736,
      "learning_rate": 0.0004228579561628238,
      "loss": 6.8984,
      "step": 1085
    },
    {
      "epoch": 0.3091814946619217,
      "grad_norm": 0.48237547278404236,
      "learning_rate": 0.00042278679191574156,
      "loss": 7.4697,
      "step": 1086
    },
    {
      "epoch": 0.3094661921708185,
      "grad_norm": 0.4760837256908417,
      "learning_rate": 0.0004227156276686593,
      "loss": 7.3721,
      "step": 1087
    },
    {
      "epoch": 0.3097508896797153,
      "grad_norm": 0.466458797454834,
      "learning_rate": 0.000422644463421577,
      "loss": 7.5654,
      "step": 1088
    },
    {
      "epoch": 0.3100355871886121,
      "grad_norm": 0.5326457023620605,
      "learning_rate": 0.00042257329917449473,
      "loss": 7.2549,
      "step": 1089
    },
    {
      "epoch": 0.3103202846975089,
      "grad_norm": 0.6053488850593567,
      "learning_rate": 0.00042250213492741245,
      "loss": 6.918,
      "step": 1090
    },
    {
      "epoch": 0.3106049822064057,
      "grad_norm": 0.4849885404109955,
      "learning_rate": 0.00042243097068033023,
      "loss": 6.9521,
      "step": 1091
    },
    {
      "epoch": 0.3108896797153025,
      "grad_norm": 0.46493789553642273,
      "learning_rate": 0.00042235980643324795,
      "loss": 7.5,
      "step": 1092
    },
    {
      "epoch": 0.3111743772241993,
      "grad_norm": 0.5597470998764038,
      "learning_rate": 0.0004222886421861657,
      "loss": 7.1045,
      "step": 1093
    },
    {
      "epoch": 0.3114590747330961,
      "grad_norm": 0.46983328461647034,
      "learning_rate": 0.00042221747793908345,
      "loss": 7.376,
      "step": 1094
    },
    {
      "epoch": 0.3117437722419929,
      "grad_norm": 0.7661401033401489,
      "learning_rate": 0.0004221463136920011,
      "loss": 7.9004,
      "step": 1095
    },
    {
      "epoch": 0.31202846975088966,
      "grad_norm": 0.5901150703430176,
      "learning_rate": 0.0004220751494449189,
      "loss": 7.0488,
      "step": 1096
    },
    {
      "epoch": 0.3123131672597865,
      "grad_norm": 0.49586740136146545,
      "learning_rate": 0.0004220039851978366,
      "loss": 7.3682,
      "step": 1097
    },
    {
      "epoch": 0.3125978647686833,
      "grad_norm": 0.42957544326782227,
      "learning_rate": 0.00042193282095075434,
      "loss": 7.3447,
      "step": 1098
    },
    {
      "epoch": 0.31288256227758005,
      "grad_norm": 0.6224203705787659,
      "learning_rate": 0.0004218616567036721,
      "loss": 6.999,
      "step": 1099
    },
    {
      "epoch": 0.31316725978647686,
      "grad_norm": 0.5391501188278198,
      "learning_rate": 0.0004217904924565898,
      "loss": 7.1846,
      "step": 1100
    },
    {
      "epoch": 0.3134519572953737,
      "grad_norm": 0.4944549798965454,
      "learning_rate": 0.0004217193282095075,
      "loss": 7.5273,
      "step": 1101
    },
    {
      "epoch": 0.3137366548042705,
      "grad_norm": 0.5197928547859192,
      "learning_rate": 0.0004216481639624253,
      "loss": 7.1572,
      "step": 1102
    },
    {
      "epoch": 0.31402135231316725,
      "grad_norm": 0.45653462409973145,
      "learning_rate": 0.000421576999715343,
      "loss": 7.1807,
      "step": 1103
    },
    {
      "epoch": 0.31430604982206406,
      "grad_norm": 0.5307984352111816,
      "learning_rate": 0.0004215058354682608,
      "loss": 7.4922,
      "step": 1104
    },
    {
      "epoch": 0.3145907473309609,
      "grad_norm": 0.4564886689186096,
      "learning_rate": 0.0004214346712211785,
      "loss": 7.3174,
      "step": 1105
    },
    {
      "epoch": 0.31487544483985763,
      "grad_norm": 0.5012796521186829,
      "learning_rate": 0.0004213635069740962,
      "loss": 7.165,
      "step": 1106
    },
    {
      "epoch": 0.31516014234875445,
      "grad_norm": 3.3040928840637207,
      "learning_rate": 0.00042129234272701396,
      "loss": 7.4414,
      "step": 1107
    },
    {
      "epoch": 0.31544483985765126,
      "grad_norm": 0.5748804211616516,
      "learning_rate": 0.0004212211784799317,
      "loss": 7.124,
      "step": 1108
    },
    {
      "epoch": 0.315729537366548,
      "grad_norm": 0.565574586391449,
      "learning_rate": 0.00042115001423284946,
      "loss": 6.9238,
      "step": 1109
    },
    {
      "epoch": 0.31601423487544483,
      "grad_norm": 0.5565568208694458,
      "learning_rate": 0.0004210788499857672,
      "loss": 7.1934,
      "step": 1110
    },
    {
      "epoch": 0.31629893238434165,
      "grad_norm": 0.4593667685985565,
      "learning_rate": 0.0004210076857386849,
      "loss": 7.4541,
      "step": 1111
    },
    {
      "epoch": 0.31658362989323846,
      "grad_norm": 0.4457915127277374,
      "learning_rate": 0.0004209365214916026,
      "loss": 7.6582,
      "step": 1112
    },
    {
      "epoch": 0.3168683274021352,
      "grad_norm": 0.40129971504211426,
      "learning_rate": 0.00042086535724452035,
      "loss": 7.7979,
      "step": 1113
    },
    {
      "epoch": 0.31715302491103203,
      "grad_norm": 0.5046826004981995,
      "learning_rate": 0.0004207941929974381,
      "loss": 7.2188,
      "step": 1114
    },
    {
      "epoch": 0.31743772241992885,
      "grad_norm": 0.4885587990283966,
      "learning_rate": 0.00042072302875035585,
      "loss": 7.6162,
      "step": 1115
    },
    {
      "epoch": 0.3177224199288256,
      "grad_norm": 0.4405319392681122,
      "learning_rate": 0.00042065186450327357,
      "loss": 7.4834,
      "step": 1116
    },
    {
      "epoch": 0.3180071174377224,
      "grad_norm": 0.5957425236701965,
      "learning_rate": 0.0004205807002561913,
      "loss": 6.667,
      "step": 1117
    },
    {
      "epoch": 0.31829181494661923,
      "grad_norm": 0.5472882986068726,
      "learning_rate": 0.000420509536009109,
      "loss": 7.4531,
      "step": 1118
    },
    {
      "epoch": 0.318576512455516,
      "grad_norm": 0.4806768596172333,
      "learning_rate": 0.00042043837176202674,
      "loss": 7.4805,
      "step": 1119
    },
    {
      "epoch": 0.3188612099644128,
      "grad_norm": 0.4944848120212555,
      "learning_rate": 0.0004203672075149445,
      "loss": 7.5342,
      "step": 1120
    },
    {
      "epoch": 0.3191459074733096,
      "grad_norm": 0.5867344737052917,
      "learning_rate": 0.00042029604326786224,
      "loss": 6.6504,
      "step": 1121
    },
    {
      "epoch": 0.3194306049822064,
      "grad_norm": 0.45524725317955017,
      "learning_rate": 0.00042022487902078,
      "loss": 7.334,
      "step": 1122
    },
    {
      "epoch": 0.3197153024911032,
      "grad_norm": 0.47168710827827454,
      "learning_rate": 0.0004201537147736977,
      "loss": 7.3643,
      "step": 1123
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.46018290519714355,
      "learning_rate": 0.0004200825505266154,
      "loss": 7.2725,
      "step": 1124
    },
    {
      "epoch": 0.3202846975088968,
      "grad_norm": 0.49334147572517395,
      "learning_rate": 0.0004200113862795332,
      "loss": 7.3291,
      "step": 1125
    },
    {
      "epoch": 0.3205693950177936,
      "grad_norm": 0.6619085073471069,
      "learning_rate": 0.0004199402220324509,
      "loss": 6.3311,
      "step": 1126
    },
    {
      "epoch": 0.3208540925266904,
      "grad_norm": 0.4803929626941681,
      "learning_rate": 0.0004198690577853687,
      "loss": 7.1162,
      "step": 1127
    },
    {
      "epoch": 0.3211387900355872,
      "grad_norm": 0.4872837960720062,
      "learning_rate": 0.00041979789353828635,
      "loss": 7.2471,
      "step": 1128
    },
    {
      "epoch": 0.32142348754448397,
      "grad_norm": 0.5039362907409668,
      "learning_rate": 0.00041972672929120407,
      "loss": 7.9453,
      "step": 1129
    },
    {
      "epoch": 0.3217081850533808,
      "grad_norm": 0.4333154559135437,
      "learning_rate": 0.00041965556504412185,
      "loss": 7.4307,
      "step": 1130
    },
    {
      "epoch": 0.3219928825622776,
      "grad_norm": 0.6478785276412964,
      "learning_rate": 0.00041958440079703957,
      "loss": 7.7002,
      "step": 1131
    },
    {
      "epoch": 0.32227758007117435,
      "grad_norm": 0.5212644934654236,
      "learning_rate": 0.0004195132365499573,
      "loss": 7.1113,
      "step": 1132
    },
    {
      "epoch": 0.32256227758007117,
      "grad_norm": 0.4614885449409485,
      "learning_rate": 0.00041944207230287507,
      "loss": 7.6172,
      "step": 1133
    },
    {
      "epoch": 0.322846975088968,
      "grad_norm": 0.5155771374702454,
      "learning_rate": 0.00041937090805579274,
      "loss": 7.1846,
      "step": 1134
    },
    {
      "epoch": 0.3231316725978648,
      "grad_norm": 0.5528173446655273,
      "learning_rate": 0.0004192997438087105,
      "loss": 7.2109,
      "step": 1135
    },
    {
      "epoch": 0.32341637010676155,
      "grad_norm": 0.505190372467041,
      "learning_rate": 0.00041922857956162824,
      "loss": 6.9854,
      "step": 1136
    },
    {
      "epoch": 0.32370106761565837,
      "grad_norm": 0.4246407747268677,
      "learning_rate": 0.00041915741531454596,
      "loss": 7.6191,
      "step": 1137
    },
    {
      "epoch": 0.3239857651245552,
      "grad_norm": 0.5137282609939575,
      "learning_rate": 0.00041908625106746374,
      "loss": 6.8467,
      "step": 1138
    },
    {
      "epoch": 0.32427046263345194,
      "grad_norm": 0.5498955845832825,
      "learning_rate": 0.00041901508682038146,
      "loss": 7.3867,
      "step": 1139
    },
    {
      "epoch": 0.32455516014234875,
      "grad_norm": 0.46873560547828674,
      "learning_rate": 0.0004189439225732992,
      "loss": 7.5898,
      "step": 1140
    },
    {
      "epoch": 0.32483985765124557,
      "grad_norm": 0.4682499170303345,
      "learning_rate": 0.0004188727583262169,
      "loss": 7.0762,
      "step": 1141
    },
    {
      "epoch": 0.3251245551601423,
      "grad_norm": 0.6929412484169006,
      "learning_rate": 0.00041880159407913463,
      "loss": 6.9707,
      "step": 1142
    },
    {
      "epoch": 0.32540925266903914,
      "grad_norm": 0.5219119787216187,
      "learning_rate": 0.0004187304298320524,
      "loss": 6.9746,
      "step": 1143
    },
    {
      "epoch": 0.32569395017793595,
      "grad_norm": 0.6330713629722595,
      "learning_rate": 0.00041865926558497013,
      "loss": 6.1309,
      "step": 1144
    },
    {
      "epoch": 0.32597864768683277,
      "grad_norm": 0.6545981168746948,
      "learning_rate": 0.00041858810133788785,
      "loss": 6.5879,
      "step": 1145
    },
    {
      "epoch": 0.3262633451957295,
      "grad_norm": 0.5922051668167114,
      "learning_rate": 0.0004185169370908056,
      "loss": 7.0771,
      "step": 1146
    },
    {
      "epoch": 0.32654804270462634,
      "grad_norm": 0.6108818054199219,
      "learning_rate": 0.0004184457728437233,
      "loss": 7.0137,
      "step": 1147
    },
    {
      "epoch": 0.32683274021352315,
      "grad_norm": 0.5289431810379028,
      "learning_rate": 0.0004183746085966411,
      "loss": 7.001,
      "step": 1148
    },
    {
      "epoch": 0.3271174377224199,
      "grad_norm": 0.4609707295894623,
      "learning_rate": 0.0004183034443495588,
      "loss": 7.4082,
      "step": 1149
    },
    {
      "epoch": 0.3274021352313167,
      "grad_norm": 0.4477441608905792,
      "learning_rate": 0.0004182322801024765,
      "loss": 7.6816,
      "step": 1150
    },
    {
      "epoch": 0.32768683274021354,
      "grad_norm": 0.4503907859325409,
      "learning_rate": 0.00041816111585539424,
      "loss": 7.873,
      "step": 1151
    },
    {
      "epoch": 0.3279715302491103,
      "grad_norm": 0.972274661064148,
      "learning_rate": 0.00041808995160831197,
      "loss": 7.2227,
      "step": 1152
    },
    {
      "epoch": 0.3282562277580071,
      "grad_norm": 0.5258875489234924,
      "learning_rate": 0.00041801878736122974,
      "loss": 7.418,
      "step": 1153
    },
    {
      "epoch": 0.3285409252669039,
      "grad_norm": 0.47450560331344604,
      "learning_rate": 0.00041794762311414747,
      "loss": 7.2832,
      "step": 1154
    },
    {
      "epoch": 0.3288256227758007,
      "grad_norm": 0.4881042540073395,
      "learning_rate": 0.0004178764588670652,
      "loss": 7.5781,
      "step": 1155
    },
    {
      "epoch": 0.3291103202846975,
      "grad_norm": 0.5091273784637451,
      "learning_rate": 0.00041780529461998297,
      "loss": 7.2471,
      "step": 1156
    },
    {
      "epoch": 0.3293950177935943,
      "grad_norm": 0.5281254053115845,
      "learning_rate": 0.00041773413037290064,
      "loss": 7.1455,
      "step": 1157
    },
    {
      "epoch": 0.3296797153024911,
      "grad_norm": 0.44808077812194824,
      "learning_rate": 0.0004176629661258184,
      "loss": 7.7734,
      "step": 1158
    },
    {
      "epoch": 0.3299644128113879,
      "grad_norm": 0.49891433119773865,
      "learning_rate": 0.00041759180187873613,
      "loss": 7.4795,
      "step": 1159
    },
    {
      "epoch": 0.3302491103202847,
      "grad_norm": 0.5285683274269104,
      "learning_rate": 0.00041752063763165386,
      "loss": 7.4639,
      "step": 1160
    },
    {
      "epoch": 0.3305338078291815,
      "grad_norm": 0.4684009253978729,
      "learning_rate": 0.00041744947338457163,
      "loss": 7.6631,
      "step": 1161
    },
    {
      "epoch": 0.3308185053380783,
      "grad_norm": 0.711145281791687,
      "learning_rate": 0.0004173783091374893,
      "loss": 7.2764,
      "step": 1162
    },
    {
      "epoch": 0.3311032028469751,
      "grad_norm": 0.49265673756599426,
      "learning_rate": 0.0004173071448904071,
      "loss": 7.21,
      "step": 1163
    },
    {
      "epoch": 0.3313879003558719,
      "grad_norm": 0.6499678492546082,
      "learning_rate": 0.0004172359806433248,
      "loss": 6.4902,
      "step": 1164
    },
    {
      "epoch": 0.33167259786476866,
      "grad_norm": 0.48859643936157227,
      "learning_rate": 0.0004171648163962425,
      "loss": 7.4043,
      "step": 1165
    },
    {
      "epoch": 0.3319572953736655,
      "grad_norm": 0.43188872933387756,
      "learning_rate": 0.0004170936521491603,
      "loss": 7.8252,
      "step": 1166
    },
    {
      "epoch": 0.3322419928825623,
      "grad_norm": 0.4691556990146637,
      "learning_rate": 0.000417022487902078,
      "loss": 7.4482,
      "step": 1167
    },
    {
      "epoch": 0.3325266903914591,
      "grad_norm": 0.582195520401001,
      "learning_rate": 0.0004169513236549957,
      "loss": 7.5371,
      "step": 1168
    },
    {
      "epoch": 0.33281138790035586,
      "grad_norm": 0.4443216919898987,
      "learning_rate": 0.00041688015940791347,
      "loss": 7.7871,
      "step": 1169
    },
    {
      "epoch": 0.3330960854092527,
      "grad_norm": 0.6017194390296936,
      "learning_rate": 0.0004168089951608312,
      "loss": 6.9932,
      "step": 1170
    },
    {
      "epoch": 0.3333807829181495,
      "grad_norm": 0.5376923680305481,
      "learning_rate": 0.00041673783091374897,
      "loss": 7.125,
      "step": 1171
    },
    {
      "epoch": 0.33366548042704625,
      "grad_norm": 0.4595070779323578,
      "learning_rate": 0.0004166666666666667,
      "loss": 7.8262,
      "step": 1172
    },
    {
      "epoch": 0.33395017793594306,
      "grad_norm": 0.5157293081283569,
      "learning_rate": 0.00041659550241958436,
      "loss": 7.165,
      "step": 1173
    },
    {
      "epoch": 0.3342348754448399,
      "grad_norm": 0.6389365196228027,
      "learning_rate": 0.00041652433817250214,
      "loss": 6.8311,
      "step": 1174
    },
    {
      "epoch": 0.33451957295373663,
      "grad_norm": 0.4487728476524353,
      "learning_rate": 0.00041645317392541986,
      "loss": 7.7686,
      "step": 1175
    },
    {
      "epoch": 0.33480427046263345,
      "grad_norm": 0.4286770224571228,
      "learning_rate": 0.00041638200967833764,
      "loss": 7.6963,
      "step": 1176
    },
    {
      "epoch": 0.33508896797153026,
      "grad_norm": 0.4808689057826996,
      "learning_rate": 0.00041631084543125536,
      "loss": 7.1982,
      "step": 1177
    },
    {
      "epoch": 0.335373665480427,
      "grad_norm": 0.5197522044181824,
      "learning_rate": 0.0004162396811841731,
      "loss": 7.3545,
      "step": 1178
    },
    {
      "epoch": 0.33565836298932383,
      "grad_norm": 0.4670189917087555,
      "learning_rate": 0.0004161685169370908,
      "loss": 7.1934,
      "step": 1179
    },
    {
      "epoch": 0.33594306049822065,
      "grad_norm": 0.5172544121742249,
      "learning_rate": 0.00041609735269000853,
      "loss": 7.1943,
      "step": 1180
    },
    {
      "epoch": 0.33622775800711746,
      "grad_norm": 0.4959327280521393,
      "learning_rate": 0.00041602618844292625,
      "loss": 7.3457,
      "step": 1181
    },
    {
      "epoch": 0.3365124555160142,
      "grad_norm": 0.5002378225326538,
      "learning_rate": 0.00041595502419584403,
      "loss": 7.502,
      "step": 1182
    },
    {
      "epoch": 0.33679715302491103,
      "grad_norm": 0.4305514097213745,
      "learning_rate": 0.00041588385994876175,
      "loss": 7.751,
      "step": 1183
    },
    {
      "epoch": 0.33708185053380785,
      "grad_norm": 0.46878859400749207,
      "learning_rate": 0.00041581269570167953,
      "loss": 7.2695,
      "step": 1184
    },
    {
      "epoch": 0.3373665480427046,
      "grad_norm": 0.4970056712627411,
      "learning_rate": 0.0004157415314545972,
      "loss": 7.2119,
      "step": 1185
    },
    {
      "epoch": 0.3376512455516014,
      "grad_norm": 0.5278494358062744,
      "learning_rate": 0.0004156703672075149,
      "loss": 7.2178,
      "step": 1186
    },
    {
      "epoch": 0.33793594306049823,
      "grad_norm": 0.6189301013946533,
      "learning_rate": 0.0004155992029604327,
      "loss": 7.0967,
      "step": 1187
    },
    {
      "epoch": 0.338220640569395,
      "grad_norm": 0.5286704301834106,
      "learning_rate": 0.0004155280387133504,
      "loss": 7.4258,
      "step": 1188
    },
    {
      "epoch": 0.3385053380782918,
      "grad_norm": 0.5079885721206665,
      "learning_rate": 0.0004154568744662682,
      "loss": 7.3896,
      "step": 1189
    },
    {
      "epoch": 0.3387900355871886,
      "grad_norm": 0.6349208950996399,
      "learning_rate": 0.00041538571021918587,
      "loss": 7.4512,
      "step": 1190
    },
    {
      "epoch": 0.33907473309608543,
      "grad_norm": 0.6474052667617798,
      "learning_rate": 0.0004153145459721036,
      "loss": 6.4209,
      "step": 1191
    },
    {
      "epoch": 0.3393594306049822,
      "grad_norm": 0.3994763195514679,
      "learning_rate": 0.00041524338172502137,
      "loss": 7.8379,
      "step": 1192
    },
    {
      "epoch": 0.339644128113879,
      "grad_norm": 0.46896281838417053,
      "learning_rate": 0.0004151722174779391,
      "loss": 7.4766,
      "step": 1193
    },
    {
      "epoch": 0.3399288256227758,
      "grad_norm": 4.865256309509277,
      "learning_rate": 0.00041510105323085687,
      "loss": 7.6611,
      "step": 1194
    },
    {
      "epoch": 0.3402135231316726,
      "grad_norm": 0.5058532953262329,
      "learning_rate": 0.0004150298889837746,
      "loss": 7.6475,
      "step": 1195
    },
    {
      "epoch": 0.3404982206405694,
      "grad_norm": 0.4236146807670593,
      "learning_rate": 0.00041495872473669226,
      "loss": 7.4639,
      "step": 1196
    },
    {
      "epoch": 0.3407829181494662,
      "grad_norm": 0.5200099945068359,
      "learning_rate": 0.00041488756048961003,
      "loss": 6.7969,
      "step": 1197
    },
    {
      "epoch": 0.34106761565836297,
      "grad_norm": 0.5131163597106934,
      "learning_rate": 0.00041481639624252776,
      "loss": 7.0908,
      "step": 1198
    },
    {
      "epoch": 0.3413523131672598,
      "grad_norm": 0.42460203170776367,
      "learning_rate": 0.0004147452319954455,
      "loss": 7.6074,
      "step": 1199
    },
    {
      "epoch": 0.3416370106761566,
      "grad_norm": 0.4523850977420807,
      "learning_rate": 0.00041467406774836326,
      "loss": 7.1504,
      "step": 1200
    },
    {
      "epoch": 0.3416370106761566,
      "eval_bleu": 0.10037787970140617,
      "eval_loss": 7.05078125,
      "eval_runtime": 151.2375,
      "eval_samples_per_second": 1.878,
      "eval_steps_per_second": 0.119,
      "step": 1200
    }
  ],
  "logging_steps": 1,
  "max_steps": 7026,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 200,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 2613571485696000.0,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
