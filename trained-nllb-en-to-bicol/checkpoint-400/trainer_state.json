{
  "best_global_step": 400,
  "best_metric": 7.00390625,
  "best_model_checkpoint": "trained-nllb-en-to-bicol\\checkpoint-400",
  "epoch": 0.11387900355871886,
  "eval_steps": 200,
  "global_step": 400,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.00028469750889679714,
      "grad_norm": 0.13050267100334167,
      "learning_rate": 0.0005,
      "loss": 11.8242,
      "step": 1
    },
    {
      "epoch": 0.0005693950177935943,
      "grad_norm": 0.16353872418403625,
      "learning_rate": 0.0004999288357529178,
      "loss": 12.7012,
      "step": 2
    },
    {
      "epoch": 0.0008540925266903915,
      "grad_norm": 0.1825779378414154,
      "learning_rate": 0.0004998576715058355,
      "loss": 11.4883,
      "step": 3
    },
    {
      "epoch": 0.0011387900355871886,
      "grad_norm": 0.2419777810573578,
      "learning_rate": 0.0004997865072587532,
      "loss": 11.9531,
      "step": 4
    },
    {
      "epoch": 0.0014234875444839859,
      "grad_norm": 0.31029146909713745,
      "learning_rate": 0.0004997153430116709,
      "loss": 11.7852,
      "step": 5
    },
    {
      "epoch": 0.001708185053380783,
      "grad_norm": 0.3981788754463196,
      "learning_rate": 0.0004996441787645887,
      "loss": 11.8242,
      "step": 6
    },
    {
      "epoch": 0.00199288256227758,
      "grad_norm": 0.44912195205688477,
      "learning_rate": 0.0004995730145175065,
      "loss": 11.1836,
      "step": 7
    },
    {
      "epoch": 0.002277580071174377,
      "grad_norm": 0.5424309372901917,
      "learning_rate": 0.0004995018502704241,
      "loss": 11.6504,
      "step": 8
    },
    {
      "epoch": 0.002562277580071174,
      "grad_norm": 0.6373918056488037,
      "learning_rate": 0.0004994306860233419,
      "loss": 11.3926,
      "step": 9
    },
    {
      "epoch": 0.0028469750889679717,
      "grad_norm": 0.7767484188079834,
      "learning_rate": 0.0004993595217762596,
      "loss": 11.5098,
      "step": 10
    },
    {
      "epoch": 0.003131672597864769,
      "grad_norm": 0.7795925736427307,
      "learning_rate": 0.0004992883575291773,
      "loss": 12.1816,
      "step": 11
    },
    {
      "epoch": 0.003416370106761566,
      "grad_norm": 0.5827488303184509,
      "learning_rate": 0.0004992171932820951,
      "loss": 11.4355,
      "step": 12
    },
    {
      "epoch": 0.003701067615658363,
      "grad_norm": 0.5459573864936829,
      "learning_rate": 0.0004991460290350128,
      "loss": 11.0156,
      "step": 13
    },
    {
      "epoch": 0.00398576512455516,
      "grad_norm": 0.513734757900238,
      "learning_rate": 0.0004990748647879306,
      "loss": 11.6016,
      "step": 14
    },
    {
      "epoch": 0.004270462633451958,
      "grad_norm": 0.5204881429672241,
      "learning_rate": 0.0004990037005408483,
      "loss": 11.1738,
      "step": 15
    },
    {
      "epoch": 0.004555160142348754,
      "grad_norm": 0.5095188021659851,
      "learning_rate": 0.000498932536293766,
      "loss": 11.6738,
      "step": 16
    },
    {
      "epoch": 0.004839857651245552,
      "grad_norm": 0.5002894997596741,
      "learning_rate": 0.0004988613720466838,
      "loss": 11.1172,
      "step": 17
    },
    {
      "epoch": 0.005124555160142348,
      "grad_norm": 0.5175882577896118,
      "learning_rate": 0.0004987902077996015,
      "loss": 11.709,
      "step": 18
    },
    {
      "epoch": 0.005409252669039146,
      "grad_norm": 0.481122225522995,
      "learning_rate": 0.0004987190435525192,
      "loss": 11.2383,
      "step": 19
    },
    {
      "epoch": 0.0056939501779359435,
      "grad_norm": 0.4916481077671051,
      "learning_rate": 0.000498647879305437,
      "loss": 11.1875,
      "step": 20
    },
    {
      "epoch": 0.00597864768683274,
      "grad_norm": 0.4639431834220886,
      "learning_rate": 0.0004985767150583547,
      "loss": 10.959,
      "step": 21
    },
    {
      "epoch": 0.006263345195729538,
      "grad_norm": 0.471029669046402,
      "learning_rate": 0.0004985055508112725,
      "loss": 10.5039,
      "step": 22
    },
    {
      "epoch": 0.006548042704626334,
      "grad_norm": 0.5494607090950012,
      "learning_rate": 0.0004984343865641901,
      "loss": 10.7871,
      "step": 23
    },
    {
      "epoch": 0.006832740213523132,
      "grad_norm": 0.542377769947052,
      "learning_rate": 0.0004983632223171079,
      "loss": 10.7402,
      "step": 24
    },
    {
      "epoch": 0.0071174377224199285,
      "grad_norm": 0.6290743947029114,
      "learning_rate": 0.0004982920580700257,
      "loss": 11.1191,
      "step": 25
    },
    {
      "epoch": 0.007402135231316726,
      "grad_norm": 0.5208401083946228,
      "learning_rate": 0.0004982208938229434,
      "loss": 10.3535,
      "step": 26
    },
    {
      "epoch": 0.0076868327402135235,
      "grad_norm": 0.5653201937675476,
      "learning_rate": 0.0004981497295758611,
      "loss": 10.2676,
      "step": 27
    },
    {
      "epoch": 0.00797153024911032,
      "grad_norm": 0.5279284119606018,
      "learning_rate": 0.0004980785653287788,
      "loss": 10.6504,
      "step": 28
    },
    {
      "epoch": 0.008256227758007117,
      "grad_norm": 0.5287833213806152,
      "learning_rate": 0.0004980074010816966,
      "loss": 9.9648,
      "step": 29
    },
    {
      "epoch": 0.008540925266903915,
      "grad_norm": 0.4779149293899536,
      "learning_rate": 0.0004979362368346143,
      "loss": 11.1738,
      "step": 30
    },
    {
      "epoch": 0.008825622775800712,
      "grad_norm": 0.4311399459838867,
      "learning_rate": 0.000497865072587532,
      "loss": 10.7617,
      "step": 31
    },
    {
      "epoch": 0.009110320284697508,
      "grad_norm": 0.4880855083465576,
      "learning_rate": 0.0004977939083404498,
      "loss": 10.6113,
      "step": 32
    },
    {
      "epoch": 0.009395017793594307,
      "grad_norm": 0.4771440029144287,
      "learning_rate": 0.0004977227440933675,
      "loss": 10.2031,
      "step": 33
    },
    {
      "epoch": 0.009679715302491104,
      "grad_norm": 0.4335767924785614,
      "learning_rate": 0.0004976515798462852,
      "loss": 9.9766,
      "step": 34
    },
    {
      "epoch": 0.0099644128113879,
      "grad_norm": 0.47250261902809143,
      "learning_rate": 0.000497580415599203,
      "loss": 10.582,
      "step": 35
    },
    {
      "epoch": 0.010249110320284697,
      "grad_norm": 0.5027048587799072,
      "learning_rate": 0.0004975092513521207,
      "loss": 9.252,
      "step": 36
    },
    {
      "epoch": 0.010533807829181495,
      "grad_norm": 0.4936491847038269,
      "learning_rate": 0.0004974380871050385,
      "loss": 9.9648,
      "step": 37
    },
    {
      "epoch": 0.010818505338078292,
      "grad_norm": 0.4429067373275757,
      "learning_rate": 0.0004973669228579561,
      "loss": 10.1699,
      "step": 38
    },
    {
      "epoch": 0.011103202846975089,
      "grad_norm": 0.43701812624931335,
      "learning_rate": 0.0004972957586108739,
      "loss": 10.1875,
      "step": 39
    },
    {
      "epoch": 0.011387900355871887,
      "grad_norm": 0.44829320907592773,
      "learning_rate": 0.0004972245943637917,
      "loss": 9.7109,
      "step": 40
    },
    {
      "epoch": 0.011672597864768684,
      "grad_norm": 0.44973376393318176,
      "learning_rate": 0.0004971534301167094,
      "loss": 9.9023,
      "step": 41
    },
    {
      "epoch": 0.01195729537366548,
      "grad_norm": 0.4918513894081116,
      "learning_rate": 0.0004970822658696271,
      "loss": 10.1934,
      "step": 42
    },
    {
      "epoch": 0.012241992882562277,
      "grad_norm": 0.4607585668563843,
      "learning_rate": 0.0004970111016225449,
      "loss": 9.9941,
      "step": 43
    },
    {
      "epoch": 0.012526690391459075,
      "grad_norm": 0.45157185196876526,
      "learning_rate": 0.0004969399373754626,
      "loss": 9.6797,
      "step": 44
    },
    {
      "epoch": 0.012811387900355872,
      "grad_norm": 0.45155540108680725,
      "learning_rate": 0.0004968687731283804,
      "loss": 9.8535,
      "step": 45
    },
    {
      "epoch": 0.013096085409252669,
      "grad_norm": 0.45471465587615967,
      "learning_rate": 0.000496797608881298,
      "loss": 9.6699,
      "step": 46
    },
    {
      "epoch": 0.013380782918149467,
      "grad_norm": 0.45609229803085327,
      "learning_rate": 0.0004967264446342158,
      "loss": 9.834,
      "step": 47
    },
    {
      "epoch": 0.013665480427046264,
      "grad_norm": 0.46065598726272583,
      "learning_rate": 0.0004966552803871336,
      "loss": 9.1973,
      "step": 48
    },
    {
      "epoch": 0.01395017793594306,
      "grad_norm": 0.4395595192909241,
      "learning_rate": 0.0004965841161400512,
      "loss": 9.3379,
      "step": 49
    },
    {
      "epoch": 0.014234875444839857,
      "grad_norm": 0.42804303765296936,
      "learning_rate": 0.0004965129518929689,
      "loss": 8.1885,
      "step": 50
    },
    {
      "epoch": 0.014519572953736655,
      "grad_norm": 0.4336685538291931,
      "learning_rate": 0.0004964417876458867,
      "loss": 9.4141,
      "step": 51
    },
    {
      "epoch": 0.014804270462633452,
      "grad_norm": 0.4083915054798126,
      "learning_rate": 0.0004963706233988045,
      "loss": 9.2363,
      "step": 52
    },
    {
      "epoch": 0.015088967971530249,
      "grad_norm": 0.39727815985679626,
      "learning_rate": 0.0004962994591517222,
      "loss": 8.5439,
      "step": 53
    },
    {
      "epoch": 0.015373665480427047,
      "grad_norm": 0.4038327634334564,
      "learning_rate": 0.0004962282949046399,
      "loss": 9.0723,
      "step": 54
    },
    {
      "epoch": 0.015658362989323844,
      "grad_norm": 0.4088612198829651,
      "learning_rate": 0.0004961571306575576,
      "loss": 8.5332,
      "step": 55
    },
    {
      "epoch": 0.01594306049822064,
      "grad_norm": 0.39859700202941895,
      "learning_rate": 0.0004960859664104754,
      "loss": 9.1191,
      "step": 56
    },
    {
      "epoch": 0.016227758007117437,
      "grad_norm": 0.39439693093299866,
      "learning_rate": 0.0004960148021633931,
      "loss": 8.0508,
      "step": 57
    },
    {
      "epoch": 0.016512455516014234,
      "grad_norm": 0.41306084394454956,
      "learning_rate": 0.0004959436379163109,
      "loss": 8.2998,
      "step": 58
    },
    {
      "epoch": 0.016797153024911034,
      "grad_norm": 0.4390506148338318,
      "learning_rate": 0.0004958724736692286,
      "loss": 8.1816,
      "step": 59
    },
    {
      "epoch": 0.01708185053380783,
      "grad_norm": 0.40014925599098206,
      "learning_rate": 0.0004958013094221464,
      "loss": 8.248,
      "step": 60
    },
    {
      "epoch": 0.017366548042704627,
      "grad_norm": 0.3710031807422638,
      "learning_rate": 0.000495730145175064,
      "loss": 7.7432,
      "step": 61
    },
    {
      "epoch": 0.017651245551601424,
      "grad_norm": 0.3743155002593994,
      "learning_rate": 0.0004956589809279818,
      "loss": 8.9805,
      "step": 62
    },
    {
      "epoch": 0.01793594306049822,
      "grad_norm": 0.4065592288970947,
      "learning_rate": 0.0004955878166808996,
      "loss": 8.9219,
      "step": 63
    },
    {
      "epoch": 0.018220640569395017,
      "grad_norm": 0.3826305866241455,
      "learning_rate": 0.0004955166524338172,
      "loss": 8.377,
      "step": 64
    },
    {
      "epoch": 0.018505338078291814,
      "grad_norm": 0.3792859613895416,
      "learning_rate": 0.000495445488186735,
      "loss": 7.8789,
      "step": 65
    },
    {
      "epoch": 0.018790035587188614,
      "grad_norm": 0.3933613896369934,
      "learning_rate": 0.0004953743239396527,
      "loss": 8.4092,
      "step": 66
    },
    {
      "epoch": 0.01907473309608541,
      "grad_norm": 0.39127129316329956,
      "learning_rate": 0.0004953031596925705,
      "loss": 7.9512,
      "step": 67
    },
    {
      "epoch": 0.019359430604982207,
      "grad_norm": 0.34493568539619446,
      "learning_rate": 0.0004952319954454881,
      "loss": 8.3105,
      "step": 68
    },
    {
      "epoch": 0.019644128113879004,
      "grad_norm": 0.3507670760154724,
      "learning_rate": 0.0004951608311984059,
      "loss": 7.9043,
      "step": 69
    },
    {
      "epoch": 0.0199288256227758,
      "grad_norm": 0.3754495084285736,
      "learning_rate": 0.0004950896669513237,
      "loss": 7.9131,
      "step": 70
    },
    {
      "epoch": 0.020213523131672597,
      "grad_norm": 0.3600054383277893,
      "learning_rate": 0.0004950185027042415,
      "loss": 8.1426,
      "step": 71
    },
    {
      "epoch": 0.020498220640569394,
      "grad_norm": 0.3194250166416168,
      "learning_rate": 0.0004949473384571591,
      "loss": 7.8213,
      "step": 72
    },
    {
      "epoch": 0.020782918149466194,
      "grad_norm": 0.3438234329223633,
      "learning_rate": 0.0004948761742100768,
      "loss": 8.1318,
      "step": 73
    },
    {
      "epoch": 0.02106761565836299,
      "grad_norm": 0.34680306911468506,
      "learning_rate": 0.0004948050099629946,
      "loss": 8.1611,
      "step": 74
    },
    {
      "epoch": 0.021352313167259787,
      "grad_norm": 0.3298954665660858,
      "learning_rate": 0.0004947338457159124,
      "loss": 7.6816,
      "step": 75
    },
    {
      "epoch": 0.021637010676156584,
      "grad_norm": 0.35546237230300903,
      "learning_rate": 0.0004946626814688301,
      "loss": 7.9482,
      "step": 76
    },
    {
      "epoch": 0.02192170818505338,
      "grad_norm": 0.31350335478782654,
      "learning_rate": 0.0004945915172217478,
      "loss": 7.916,
      "step": 77
    },
    {
      "epoch": 0.022206405693950177,
      "grad_norm": 0.2875521183013916,
      "learning_rate": 0.0004945203529746655,
      "loss": 8.1787,
      "step": 78
    },
    {
      "epoch": 0.022491103202846974,
      "grad_norm": 0.2984481453895569,
      "learning_rate": 0.0004944491887275833,
      "loss": 8.0068,
      "step": 79
    },
    {
      "epoch": 0.022775800711743774,
      "grad_norm": 0.32493481040000916,
      "learning_rate": 0.000494378024480501,
      "loss": 7.7959,
      "step": 80
    },
    {
      "epoch": 0.02306049822064057,
      "grad_norm": 0.3053205907344818,
      "learning_rate": 0.0004943068602334188,
      "loss": 7.7959,
      "step": 81
    },
    {
      "epoch": 0.023345195729537367,
      "grad_norm": 0.29084035754203796,
      "learning_rate": 0.0004942356959863365,
      "loss": 7.9629,
      "step": 82
    },
    {
      "epoch": 0.023629893238434164,
      "grad_norm": 0.3073493242263794,
      "learning_rate": 0.0004941645317392541,
      "loss": 7.7549,
      "step": 83
    },
    {
      "epoch": 0.02391459074733096,
      "grad_norm": 0.30308839678764343,
      "learning_rate": 0.0004940933674921719,
      "loss": 8.0791,
      "step": 84
    },
    {
      "epoch": 0.024199288256227757,
      "grad_norm": 0.38702166080474854,
      "learning_rate": 0.0004940222032450897,
      "loss": 7.2637,
      "step": 85
    },
    {
      "epoch": 0.024483985765124554,
      "grad_norm": 0.33084896206855774,
      "learning_rate": 0.0004939510389980074,
      "loss": 7.833,
      "step": 86
    },
    {
      "epoch": 0.024768683274021354,
      "grad_norm": 0.2839316129684448,
      "learning_rate": 0.0004938798747509251,
      "loss": 7.9053,
      "step": 87
    },
    {
      "epoch": 0.02505338078291815,
      "grad_norm": 0.35194167494773865,
      "learning_rate": 0.0004938087105038429,
      "loss": 7.4053,
      "step": 88
    },
    {
      "epoch": 0.025338078291814947,
      "grad_norm": 0.34102603793144226,
      "learning_rate": 0.0004937375462567606,
      "loss": 7.6406,
      "step": 89
    },
    {
      "epoch": 0.025622775800711744,
      "grad_norm": 0.34196290373802185,
      "learning_rate": 0.0004936663820096784,
      "loss": 7.7129,
      "step": 90
    },
    {
      "epoch": 0.02590747330960854,
      "grad_norm": 0.33342525362968445,
      "learning_rate": 0.000493595217762596,
      "loss": 7.1934,
      "step": 91
    },
    {
      "epoch": 0.026192170818505337,
      "grad_norm": 0.2916184365749359,
      "learning_rate": 0.0004935240535155138,
      "loss": 7.7559,
      "step": 92
    },
    {
      "epoch": 0.026476868327402134,
      "grad_norm": 0.3170425295829773,
      "learning_rate": 0.0004934528892684316,
      "loss": 7.7617,
      "step": 93
    },
    {
      "epoch": 0.026761565836298934,
      "grad_norm": 0.3077421188354492,
      "learning_rate": 0.0004933817250213493,
      "loss": 7.7852,
      "step": 94
    },
    {
      "epoch": 0.02704626334519573,
      "grad_norm": 0.34794363379478455,
      "learning_rate": 0.000493310560774267,
      "loss": 7.5391,
      "step": 95
    },
    {
      "epoch": 0.027330960854092527,
      "grad_norm": 0.27851757407188416,
      "learning_rate": 0.0004932393965271847,
      "loss": 8.0039,
      "step": 96
    },
    {
      "epoch": 0.027615658362989324,
      "grad_norm": 0.3033468723297119,
      "learning_rate": 0.0004931682322801025,
      "loss": 7.8076,
      "step": 97
    },
    {
      "epoch": 0.02790035587188612,
      "grad_norm": 0.29290416836738586,
      "learning_rate": 0.0004930970680330203,
      "loss": 7.8281,
      "step": 98
    },
    {
      "epoch": 0.028185053380782917,
      "grad_norm": 0.27407997846603394,
      "learning_rate": 0.000493025903785938,
      "loss": 7.8564,
      "step": 99
    },
    {
      "epoch": 0.028469750889679714,
      "grad_norm": 0.32197993993759155,
      "learning_rate": 0.0004929547395388557,
      "loss": 7.3594,
      "step": 100
    },
    {
      "epoch": 0.028754448398576514,
      "grad_norm": 0.27607426047325134,
      "learning_rate": 0.0004928835752917734,
      "loss": 7.9219,
      "step": 101
    },
    {
      "epoch": 0.02903914590747331,
      "grad_norm": 0.25733911991119385,
      "learning_rate": 0.0004928124110446911,
      "loss": 7.6963,
      "step": 102
    },
    {
      "epoch": 0.029323843416370107,
      "grad_norm": 0.33134591579437256,
      "learning_rate": 0.0004927412467976089,
      "loss": 7.5918,
      "step": 103
    },
    {
      "epoch": 0.029608540925266904,
      "grad_norm": 0.3447519838809967,
      "learning_rate": 0.0004926700825505266,
      "loss": 7.3262,
      "step": 104
    },
    {
      "epoch": 0.0298932384341637,
      "grad_norm": 0.30481886863708496,
      "learning_rate": 0.0004925989183034444,
      "loss": 7.5986,
      "step": 105
    },
    {
      "epoch": 0.030177935943060497,
      "grad_norm": 0.3810344338417053,
      "learning_rate": 0.000492527754056362,
      "loss": 7.2734,
      "step": 106
    },
    {
      "epoch": 0.030462633451957294,
      "grad_norm": 0.26661789417266846,
      "learning_rate": 0.0004924565898092798,
      "loss": 7.8818,
      "step": 107
    },
    {
      "epoch": 0.030747330960854094,
      "grad_norm": 0.3144116997718811,
      "learning_rate": 0.0004923854255621976,
      "loss": 7.4697,
      "step": 108
    },
    {
      "epoch": 0.03103202846975089,
      "grad_norm": 0.3234042525291443,
      "learning_rate": 0.0004923142613151153,
      "loss": 7.6611,
      "step": 109
    },
    {
      "epoch": 0.03131672597864769,
      "grad_norm": 0.2908277213573456,
      "learning_rate": 0.000492243097068033,
      "loss": 7.6924,
      "step": 110
    },
    {
      "epoch": 0.03160142348754449,
      "grad_norm": 0.282621830701828,
      "learning_rate": 0.0004921719328209507,
      "loss": 7.6748,
      "step": 111
    },
    {
      "epoch": 0.03188612099644128,
      "grad_norm": 0.25899139046669006,
      "learning_rate": 0.0004921007685738685,
      "loss": 7.7725,
      "step": 112
    },
    {
      "epoch": 0.03217081850533808,
      "grad_norm": 0.30150163173675537,
      "learning_rate": 0.0004920296043267863,
      "loss": 7.6016,
      "step": 113
    },
    {
      "epoch": 0.032455516014234874,
      "grad_norm": 0.3718966245651245,
      "learning_rate": 0.0004919584400797039,
      "loss": 7.0254,
      "step": 114
    },
    {
      "epoch": 0.032740213523131674,
      "grad_norm": 0.30984169244766235,
      "learning_rate": 0.0004918872758326217,
      "loss": 7.4707,
      "step": 115
    },
    {
      "epoch": 0.03302491103202847,
      "grad_norm": 0.25567907094955444,
      "learning_rate": 0.0004918161115855395,
      "loss": 7.668,
      "step": 116
    },
    {
      "epoch": 0.03330960854092527,
      "grad_norm": 0.38791143894195557,
      "learning_rate": 0.0004917449473384572,
      "loss": 7.3613,
      "step": 117
    },
    {
      "epoch": 0.03359430604982207,
      "grad_norm": 0.28487956523895264,
      "learning_rate": 0.0004916737830913749,
      "loss": 7.5039,
      "step": 118
    },
    {
      "epoch": 0.03387900355871886,
      "grad_norm": 0.4281052350997925,
      "learning_rate": 0.0004916026188442926,
      "loss": 7.0996,
      "step": 119
    },
    {
      "epoch": 0.03416370106761566,
      "grad_norm": 0.30284109711647034,
      "learning_rate": 0.0004915314545972104,
      "loss": 7.2402,
      "step": 120
    },
    {
      "epoch": 0.034448398576512454,
      "grad_norm": 0.3210189938545227,
      "learning_rate": 0.0004914602903501282,
      "loss": 7.3271,
      "step": 121
    },
    {
      "epoch": 0.034733096085409254,
      "grad_norm": 0.373404324054718,
      "learning_rate": 0.0004913891261030458,
      "loss": 6.876,
      "step": 122
    },
    {
      "epoch": 0.03501779359430605,
      "grad_norm": 0.34432628750801086,
      "learning_rate": 0.0004913179618559636,
      "loss": 7.5469,
      "step": 123
    },
    {
      "epoch": 0.03530249110320285,
      "grad_norm": 0.3198752999305725,
      "learning_rate": 0.0004912467976088813,
      "loss": 7.6924,
      "step": 124
    },
    {
      "epoch": 0.03558718861209965,
      "grad_norm": 0.38547608256340027,
      "learning_rate": 0.000491175633361799,
      "loss": 7.0049,
      "step": 125
    },
    {
      "epoch": 0.03587188612099644,
      "grad_norm": 0.34361645579338074,
      "learning_rate": 0.0004911044691147168,
      "loss": 7.3477,
      "step": 126
    },
    {
      "epoch": 0.03615658362989324,
      "grad_norm": 0.32878419756889343,
      "learning_rate": 0.0004910333048676345,
      "loss": 7.167,
      "step": 127
    },
    {
      "epoch": 0.036441281138790034,
      "grad_norm": 0.2640332579612732,
      "learning_rate": 0.0004909621406205523,
      "loss": 7.5791,
      "step": 128
    },
    {
      "epoch": 0.036725978647686834,
      "grad_norm": 0.34086892008781433,
      "learning_rate": 0.0004908909763734699,
      "loss": 7.5127,
      "step": 129
    },
    {
      "epoch": 0.03701067615658363,
      "grad_norm": 0.3057088851928711,
      "learning_rate": 0.0004908198121263877,
      "loss": 7.6523,
      "step": 130
    },
    {
      "epoch": 0.03729537366548043,
      "grad_norm": 0.325603187084198,
      "learning_rate": 0.0004907486478793055,
      "loss": 7.2979,
      "step": 131
    },
    {
      "epoch": 0.03758007117437723,
      "grad_norm": 0.3464660942554474,
      "learning_rate": 0.0004906774836322232,
      "loss": 7.4541,
      "step": 132
    },
    {
      "epoch": 0.03786476868327402,
      "grad_norm": 0.3628098964691162,
      "learning_rate": 0.0004906063193851409,
      "loss": 7.2969,
      "step": 133
    },
    {
      "epoch": 0.03814946619217082,
      "grad_norm": 0.33785781264305115,
      "learning_rate": 0.0004905351551380586,
      "loss": 7.4209,
      "step": 134
    },
    {
      "epoch": 0.038434163701067614,
      "grad_norm": 0.3329750895500183,
      "learning_rate": 0.0004904639908909764,
      "loss": 7.5674,
      "step": 135
    },
    {
      "epoch": 0.038718861209964414,
      "grad_norm": 0.3496299088001251,
      "learning_rate": 0.0004903928266438942,
      "loss": 7.4629,
      "step": 136
    },
    {
      "epoch": 0.03900355871886121,
      "grad_norm": 0.3395816385746002,
      "learning_rate": 0.0004903216623968118,
      "loss": 7.5713,
      "step": 137
    },
    {
      "epoch": 0.03928825622775801,
      "grad_norm": 0.3240024447441101,
      "learning_rate": 0.0004902504981497296,
      "loss": 7.6055,
      "step": 138
    },
    {
      "epoch": 0.03957295373665481,
      "grad_norm": 0.2986396253108978,
      "learning_rate": 0.0004901793339026473,
      "loss": 7.4951,
      "step": 139
    },
    {
      "epoch": 0.0398576512455516,
      "grad_norm": 0.30460289120674133,
      "learning_rate": 0.000490108169655565,
      "loss": 7.7354,
      "step": 140
    },
    {
      "epoch": 0.0401423487544484,
      "grad_norm": 0.2872254252433777,
      "learning_rate": 0.0004900370054084828,
      "loss": 7.7559,
      "step": 141
    },
    {
      "epoch": 0.040427046263345194,
      "grad_norm": 0.3015432059764862,
      "learning_rate": 0.0004899658411614005,
      "loss": 7.7129,
      "step": 142
    },
    {
      "epoch": 0.040711743772241994,
      "grad_norm": 0.33976587653160095,
      "learning_rate": 0.0004898946769143183,
      "loss": 7.4961,
      "step": 143
    },
    {
      "epoch": 0.04099644128113879,
      "grad_norm": 0.3869113028049469,
      "learning_rate": 0.000489823512667236,
      "loss": 7.2979,
      "step": 144
    },
    {
      "epoch": 0.04128113879003559,
      "grad_norm": 0.3606325685977936,
      "learning_rate": 0.0004897523484201537,
      "loss": 7.4873,
      "step": 145
    },
    {
      "epoch": 0.04156583629893239,
      "grad_norm": 0.3285134434700012,
      "learning_rate": 0.0004896811841730715,
      "loss": 7.8018,
      "step": 146
    },
    {
      "epoch": 0.04185053380782918,
      "grad_norm": 0.371952086687088,
      "learning_rate": 0.0004896100199259892,
      "loss": 7.2168,
      "step": 147
    },
    {
      "epoch": 0.04213523131672598,
      "grad_norm": 0.3381865620613098,
      "learning_rate": 0.0004895388556789069,
      "loss": 7.3682,
      "step": 148
    },
    {
      "epoch": 0.042419928825622774,
      "grad_norm": 0.30638259649276733,
      "learning_rate": 0.0004894676914318247,
      "loss": 7.459,
      "step": 149
    },
    {
      "epoch": 0.042704626334519574,
      "grad_norm": 0.3618326187133789,
      "learning_rate": 0.0004893965271847424,
      "loss": 7.293,
      "step": 150
    },
    {
      "epoch": 0.04298932384341637,
      "grad_norm": 0.34743648767471313,
      "learning_rate": 0.0004893253629376602,
      "loss": 7.2275,
      "step": 151
    },
    {
      "epoch": 0.04327402135231317,
      "grad_norm": 0.24103522300720215,
      "learning_rate": 0.0004892541986905778,
      "loss": 7.6436,
      "step": 152
    },
    {
      "epoch": 0.04355871886120997,
      "grad_norm": 0.47605815529823303,
      "learning_rate": 0.0004891830344434956,
      "loss": 6.8848,
      "step": 153
    },
    {
      "epoch": 0.04384341637010676,
      "grad_norm": 0.2737746834754944,
      "learning_rate": 0.0004891118701964134,
      "loss": 7.5234,
      "step": 154
    },
    {
      "epoch": 0.04412811387900356,
      "grad_norm": 0.34337320923805237,
      "learning_rate": 0.000489040705949331,
      "loss": 7.3691,
      "step": 155
    },
    {
      "epoch": 0.044412811387900354,
      "grad_norm": 0.29171985387802124,
      "learning_rate": 0.0004889695417022488,
      "loss": 7.7021,
      "step": 156
    },
    {
      "epoch": 0.044697508896797154,
      "grad_norm": 0.2947385907173157,
      "learning_rate": 0.0004888983774551665,
      "loss": 7.7725,
      "step": 157
    },
    {
      "epoch": 0.04498220640569395,
      "grad_norm": 0.33179420232772827,
      "learning_rate": 0.0004888272132080843,
      "loss": 7.4795,
      "step": 158
    },
    {
      "epoch": 0.04526690391459075,
      "grad_norm": 0.30874258279800415,
      "learning_rate": 0.000488756048961002,
      "loss": 7.667,
      "step": 159
    },
    {
      "epoch": 0.04555160142348755,
      "grad_norm": 0.37757524847984314,
      "learning_rate": 0.0004886848847139197,
      "loss": 7.1436,
      "step": 160
    },
    {
      "epoch": 0.04583629893238434,
      "grad_norm": 0.35947147011756897,
      "learning_rate": 0.0004886137204668375,
      "loss": 7.2451,
      "step": 161
    },
    {
      "epoch": 0.04612099644128114,
      "grad_norm": 0.38363611698150635,
      "learning_rate": 0.0004885425562197552,
      "loss": 7.2061,
      "step": 162
    },
    {
      "epoch": 0.046405693950177934,
      "grad_norm": 0.3002108335494995,
      "learning_rate": 0.0004884713919726729,
      "loss": 7.6328,
      "step": 163
    },
    {
      "epoch": 0.046690391459074734,
      "grad_norm": 0.36899691820144653,
      "learning_rate": 0.0004884002277255907,
      "loss": 7.2451,
      "step": 164
    },
    {
      "epoch": 0.04697508896797153,
      "grad_norm": 0.3589462637901306,
      "learning_rate": 0.0004883290634785084,
      "loss": 7.4961,
      "step": 165
    },
    {
      "epoch": 0.04725978647686833,
      "grad_norm": 0.3247394263744354,
      "learning_rate": 0.00048825789923142616,
      "loss": 7.4971,
      "step": 166
    },
    {
      "epoch": 0.04754448398576513,
      "grad_norm": 0.4397582411766052,
      "learning_rate": 0.00048818673498434383,
      "loss": 7.2158,
      "step": 167
    },
    {
      "epoch": 0.04782918149466192,
      "grad_norm": 0.34369221329689026,
      "learning_rate": 0.0004881155707372616,
      "loss": 7.3965,
      "step": 168
    },
    {
      "epoch": 0.04811387900355872,
      "grad_norm": 0.315603107213974,
      "learning_rate": 0.00048804440649017933,
      "loss": 7.4365,
      "step": 169
    },
    {
      "epoch": 0.048398576512455514,
      "grad_norm": 0.5103448033332825,
      "learning_rate": 0.0004879732422430971,
      "loss": 6.8682,
      "step": 170
    },
    {
      "epoch": 0.048683274021352314,
      "grad_norm": 0.43841269612312317,
      "learning_rate": 0.00048790207799601483,
      "loss": 7.3838,
      "step": 171
    },
    {
      "epoch": 0.04896797153024911,
      "grad_norm": 0.43405452370643616,
      "learning_rate": 0.00048783091374893255,
      "loss": 6.9912,
      "step": 172
    },
    {
      "epoch": 0.04925266903914591,
      "grad_norm": 0.4368981122970581,
      "learning_rate": 0.0004877597495018503,
      "loss": 7.1592,
      "step": 173
    },
    {
      "epoch": 0.04953736654804271,
      "grad_norm": 0.3200138211250305,
      "learning_rate": 0.000487688585254768,
      "loss": 7.5615,
      "step": 174
    },
    {
      "epoch": 0.0498220640569395,
      "grad_norm": 0.3974745571613312,
      "learning_rate": 0.0004876174210076857,
      "loss": 7.0674,
      "step": 175
    },
    {
      "epoch": 0.0501067615658363,
      "grad_norm": 0.3726997673511505,
      "learning_rate": 0.0004875462567606035,
      "loss": 7.3057,
      "step": 176
    },
    {
      "epoch": 0.050391459074733094,
      "grad_norm": 0.37156978249549866,
      "learning_rate": 0.0004874750925135212,
      "loss": 7.3672,
      "step": 177
    },
    {
      "epoch": 0.050676156583629894,
      "grad_norm": 0.3446742594242096,
      "learning_rate": 0.000487403928266439,
      "loss": 7.3379,
      "step": 178
    },
    {
      "epoch": 0.05096085409252669,
      "grad_norm": 0.3899495601654053,
      "learning_rate": 0.00048733276401935667,
      "loss": 7.0742,
      "step": 179
    },
    {
      "epoch": 0.05124555160142349,
      "grad_norm": 0.365047425031662,
      "learning_rate": 0.0004872615997722744,
      "loss": 7.2949,
      "step": 180
    },
    {
      "epoch": 0.05153024911032029,
      "grad_norm": 0.3096078336238861,
      "learning_rate": 0.00048719043552519217,
      "loss": 7.585,
      "step": 181
    },
    {
      "epoch": 0.05181494661921708,
      "grad_norm": 0.3642439842224121,
      "learning_rate": 0.0004871192712781099,
      "loss": 7.293,
      "step": 182
    },
    {
      "epoch": 0.05209964412811388,
      "grad_norm": 0.31257525086402893,
      "learning_rate": 0.00048704810703102767,
      "loss": 7.6279,
      "step": 183
    },
    {
      "epoch": 0.052384341637010674,
      "grad_norm": 0.3482113480567932,
      "learning_rate": 0.00048697694278394533,
      "loss": 7.4062,
      "step": 184
    },
    {
      "epoch": 0.052669039145907474,
      "grad_norm": 0.40061259269714355,
      "learning_rate": 0.00048690577853686306,
      "loss": 6.9482,
      "step": 185
    },
    {
      "epoch": 0.05295373665480427,
      "grad_norm": 0.4041823744773865,
      "learning_rate": 0.00048683461428978083,
      "loss": 7.1836,
      "step": 186
    },
    {
      "epoch": 0.05323843416370107,
      "grad_norm": 0.41112077236175537,
      "learning_rate": 0.00048676345004269856,
      "loss": 6.8662,
      "step": 187
    },
    {
      "epoch": 0.05352313167259787,
      "grad_norm": 0.38859066367149353,
      "learning_rate": 0.00048669228579561633,
      "loss": 7.626,
      "step": 188
    },
    {
      "epoch": 0.05380782918149466,
      "grad_norm": 0.3483869135379791,
      "learning_rate": 0.00048662112154853406,
      "loss": 7.2656,
      "step": 189
    },
    {
      "epoch": 0.05409252669039146,
      "grad_norm": 0.36553943157196045,
      "learning_rate": 0.0004865499573014517,
      "loss": 7.0918,
      "step": 190
    },
    {
      "epoch": 0.054377224199288254,
      "grad_norm": 0.45641273260116577,
      "learning_rate": 0.0004864787930543695,
      "loss": 7.1328,
      "step": 191
    },
    {
      "epoch": 0.054661921708185054,
      "grad_norm": 0.5525060296058655,
      "learning_rate": 0.0004864076288072872,
      "loss": 6.6797,
      "step": 192
    },
    {
      "epoch": 0.05494661921708185,
      "grad_norm": 0.35605090856552124,
      "learning_rate": 0.00048633646456020495,
      "loss": 7.3145,
      "step": 193
    },
    {
      "epoch": 0.05523131672597865,
      "grad_norm": 0.33720216155052185,
      "learning_rate": 0.0004862653003131227,
      "loss": 7.583,
      "step": 194
    },
    {
      "epoch": 0.05551601423487545,
      "grad_norm": 0.39028072357177734,
      "learning_rate": 0.0004861941360660404,
      "loss": 7.041,
      "step": 195
    },
    {
      "epoch": 0.05580071174377224,
      "grad_norm": 0.37324032187461853,
      "learning_rate": 0.00048612297181895817,
      "loss": 7.5771,
      "step": 196
    },
    {
      "epoch": 0.05608540925266904,
      "grad_norm": 0.3692830204963684,
      "learning_rate": 0.0004860518075718759,
      "loss": 6.9414,
      "step": 197
    },
    {
      "epoch": 0.056370106761565834,
      "grad_norm": 0.3400883674621582,
      "learning_rate": 0.0004859806433247936,
      "loss": 7.6484,
      "step": 198
    },
    {
      "epoch": 0.056654804270462635,
      "grad_norm": 0.36927610635757446,
      "learning_rate": 0.0004859094790777114,
      "loss": 7.0127,
      "step": 199
    },
    {
      "epoch": 0.05693950177935943,
      "grad_norm": 0.3512347936630249,
      "learning_rate": 0.0004858383148306291,
      "loss": 7.5596,
      "step": 200
    },
    {
      "epoch": 0.05693950177935943,
      "eval_bleu": 0.04428465159832603,
      "eval_loss": 7.046875,
      "eval_runtime": 206.9516,
      "eval_samples_per_second": 1.372,
      "eval_steps_per_second": 0.087,
      "step": 200
    },
    {
      "epoch": 0.05722419928825623,
      "grad_norm": 0.40993812680244446,
      "learning_rate": 0.00048576715058354684,
      "loss": 7.165,
      "step": 201
    },
    {
      "epoch": 0.05750889679715303,
      "grad_norm": 0.37675940990448,
      "learning_rate": 0.00048569598633646456,
      "loss": 7.5469,
      "step": 202
    },
    {
      "epoch": 0.05779359430604982,
      "grad_norm": 0.41827306151390076,
      "learning_rate": 0.0004856248220893823,
      "loss": 7.252,
      "step": 203
    },
    {
      "epoch": 0.05807829181494662,
      "grad_norm": 0.40496769547462463,
      "learning_rate": 0.00048555365784230006,
      "loss": 7.3262,
      "step": 204
    },
    {
      "epoch": 0.058362989323843414,
      "grad_norm": 0.4544559121131897,
      "learning_rate": 0.0004854824935952178,
      "loss": 7.2686,
      "step": 205
    },
    {
      "epoch": 0.058647686832740215,
      "grad_norm": 0.43348002433776855,
      "learning_rate": 0.0004854113293481355,
      "loss": 7.1436,
      "step": 206
    },
    {
      "epoch": 0.05893238434163701,
      "grad_norm": 0.5123205780982971,
      "learning_rate": 0.00048534016510105323,
      "loss": 6.9443,
      "step": 207
    },
    {
      "epoch": 0.05921708185053381,
      "grad_norm": 0.3473433554172516,
      "learning_rate": 0.00048526900085397095,
      "loss": 7.4775,
      "step": 208
    },
    {
      "epoch": 0.05950177935943061,
      "grad_norm": 0.3604001998901367,
      "learning_rate": 0.00048519783660688873,
      "loss": 7.5098,
      "step": 209
    },
    {
      "epoch": 0.0597864768683274,
      "grad_norm": 0.33064237236976624,
      "learning_rate": 0.00048512667235980645,
      "loss": 7.6191,
      "step": 210
    },
    {
      "epoch": 0.0600711743772242,
      "grad_norm": 0.5796976089477539,
      "learning_rate": 0.0004850555081127242,
      "loss": 6.7324,
      "step": 211
    },
    {
      "epoch": 0.060355871886120994,
      "grad_norm": 0.3960364758968353,
      "learning_rate": 0.0004849843438656419,
      "loss": 7.2354,
      "step": 212
    },
    {
      "epoch": 0.060640569395017795,
      "grad_norm": 0.3528068959712982,
      "learning_rate": 0.0004849131796185596,
      "loss": 7.4775,
      "step": 213
    },
    {
      "epoch": 0.06092526690391459,
      "grad_norm": 0.33591997623443604,
      "learning_rate": 0.0004848420153714774,
      "loss": 7.5264,
      "step": 214
    },
    {
      "epoch": 0.06120996441281139,
      "grad_norm": 0.3542058765888214,
      "learning_rate": 0.0004847708511243951,
      "loss": 7.4365,
      "step": 215
    },
    {
      "epoch": 0.06149466192170819,
      "grad_norm": 0.4078296422958374,
      "learning_rate": 0.00048469968687731284,
      "loss": 6.9795,
      "step": 216
    },
    {
      "epoch": 0.06177935943060498,
      "grad_norm": 0.3528203070163727,
      "learning_rate": 0.0004846285226302306,
      "loss": 7.251,
      "step": 217
    },
    {
      "epoch": 0.06206405693950178,
      "grad_norm": 0.454461932182312,
      "learning_rate": 0.0004845573583831483,
      "loss": 6.876,
      "step": 218
    },
    {
      "epoch": 0.062348754448398575,
      "grad_norm": 0.39278143644332886,
      "learning_rate": 0.00048448619413606606,
      "loss": 7.2432,
      "step": 219
    },
    {
      "epoch": 0.06263345195729537,
      "grad_norm": 0.4267103374004364,
      "learning_rate": 0.0004844150298889838,
      "loss": 6.8594,
      "step": 220
    },
    {
      "epoch": 0.06291814946619217,
      "grad_norm": 0.3892408609390259,
      "learning_rate": 0.0004843438656419015,
      "loss": 6.8838,
      "step": 221
    },
    {
      "epoch": 0.06320284697508897,
      "grad_norm": 0.3549274802207947,
      "learning_rate": 0.0004842727013948193,
      "loss": 7.2236,
      "step": 222
    },
    {
      "epoch": 0.06348754448398576,
      "grad_norm": 0.33616870641708374,
      "learning_rate": 0.000484201537147737,
      "loss": 7.7422,
      "step": 223
    },
    {
      "epoch": 0.06377224199288256,
      "grad_norm": 0.33883076906204224,
      "learning_rate": 0.0004841303729006547,
      "loss": 7.5654,
      "step": 224
    },
    {
      "epoch": 0.06405693950177936,
      "grad_norm": 0.3884200155735016,
      "learning_rate": 0.00048405920865357246,
      "loss": 7.3457,
      "step": 225
    },
    {
      "epoch": 0.06434163701067616,
      "grad_norm": 0.3521203398704529,
      "learning_rate": 0.0004839880444064902,
      "loss": 7.2559,
      "step": 226
    },
    {
      "epoch": 0.06462633451957295,
      "grad_norm": 0.5035505294799805,
      "learning_rate": 0.00048391688015940796,
      "loss": 7.1816,
      "step": 227
    },
    {
      "epoch": 0.06491103202846975,
      "grad_norm": 0.37743714451789856,
      "learning_rate": 0.0004838457159123257,
      "loss": 7.5049,
      "step": 228
    },
    {
      "epoch": 0.06519572953736655,
      "grad_norm": 0.3604282736778259,
      "learning_rate": 0.00048377455166524335,
      "loss": 7.7832,
      "step": 229
    },
    {
      "epoch": 0.06548042704626335,
      "grad_norm": 0.6729333400726318,
      "learning_rate": 0.0004837033874181611,
      "loss": 6.8105,
      "step": 230
    },
    {
      "epoch": 0.06576512455516015,
      "grad_norm": 0.41351523995399475,
      "learning_rate": 0.00048363222317107885,
      "loss": 6.7354,
      "step": 231
    },
    {
      "epoch": 0.06604982206405693,
      "grad_norm": 0.3609699308872223,
      "learning_rate": 0.0004835610589239966,
      "loss": 7.3662,
      "step": 232
    },
    {
      "epoch": 0.06633451957295373,
      "grad_norm": 0.34872451424598694,
      "learning_rate": 0.00048348989467691435,
      "loss": 7.4785,
      "step": 233
    },
    {
      "epoch": 0.06661921708185053,
      "grad_norm": 0.3711255192756653,
      "learning_rate": 0.00048341873042983207,
      "loss": 7.2041,
      "step": 234
    },
    {
      "epoch": 0.06690391459074733,
      "grad_norm": 0.4340938925743103,
      "learning_rate": 0.0004833475661827498,
      "loss": 6.8232,
      "step": 235
    },
    {
      "epoch": 0.06718861209964413,
      "grad_norm": 0.3729588985443115,
      "learning_rate": 0.0004832764019356675,
      "loss": 7.2197,
      "step": 236
    },
    {
      "epoch": 0.06747330960854092,
      "grad_norm": 0.429115355014801,
      "learning_rate": 0.00048320523768858524,
      "loss": 7.1836,
      "step": 237
    },
    {
      "epoch": 0.06775800711743772,
      "grad_norm": 0.47582298517227173,
      "learning_rate": 0.000483134073441503,
      "loss": 6.8828,
      "step": 238
    },
    {
      "epoch": 0.06804270462633452,
      "grad_norm": 0.44467341899871826,
      "learning_rate": 0.00048306290919442074,
      "loss": 7.1777,
      "step": 239
    },
    {
      "epoch": 0.06832740213523132,
      "grad_norm": 0.438344269990921,
      "learning_rate": 0.00048299174494733846,
      "loss": 6.8701,
      "step": 240
    },
    {
      "epoch": 0.06861209964412811,
      "grad_norm": 0.4138439893722534,
      "learning_rate": 0.0004829205807002562,
      "loss": 7.1123,
      "step": 241
    },
    {
      "epoch": 0.06889679715302491,
      "grad_norm": 0.39080458879470825,
      "learning_rate": 0.0004828494164531739,
      "loss": 7.3242,
      "step": 242
    },
    {
      "epoch": 0.06918149466192171,
      "grad_norm": 0.43729132413864136,
      "learning_rate": 0.0004827782522060917,
      "loss": 6.7295,
      "step": 243
    },
    {
      "epoch": 0.06946619217081851,
      "grad_norm": 0.4565962851047516,
      "learning_rate": 0.0004827070879590094,
      "loss": 7.4229,
      "step": 244
    },
    {
      "epoch": 0.06975088967971531,
      "grad_norm": 0.4301145374774933,
      "learning_rate": 0.0004826359237119272,
      "loss": 7.3975,
      "step": 245
    },
    {
      "epoch": 0.0700355871886121,
      "grad_norm": 0.4343007504940033,
      "learning_rate": 0.00048256475946484485,
      "loss": 7.1396,
      "step": 246
    },
    {
      "epoch": 0.0703202846975089,
      "grad_norm": 0.3824312388896942,
      "learning_rate": 0.0004824935952177626,
      "loss": 7.3086,
      "step": 247
    },
    {
      "epoch": 0.0706049822064057,
      "grad_norm": 0.38155433535575867,
      "learning_rate": 0.00048242243097068035,
      "loss": 7.1982,
      "step": 248
    },
    {
      "epoch": 0.0708896797153025,
      "grad_norm": 0.36901432275772095,
      "learning_rate": 0.00048235126672359807,
      "loss": 7.3613,
      "step": 249
    },
    {
      "epoch": 0.0711743772241993,
      "grad_norm": 0.368551105260849,
      "learning_rate": 0.00048228010247651585,
      "loss": 7.3828,
      "step": 250
    },
    {
      "epoch": 0.07145907473309608,
      "grad_norm": 0.3908926844596863,
      "learning_rate": 0.00048220893822943357,
      "loss": 6.9717,
      "step": 251
    },
    {
      "epoch": 0.07174377224199288,
      "grad_norm": 0.46843716502189636,
      "learning_rate": 0.00048213777398235124,
      "loss": 7.2441,
      "step": 252
    },
    {
      "epoch": 0.07202846975088968,
      "grad_norm": 0.4673958420753479,
      "learning_rate": 0.000482066609735269,
      "loss": 7.4209,
      "step": 253
    },
    {
      "epoch": 0.07231316725978648,
      "grad_norm": 0.45724111795425415,
      "learning_rate": 0.00048199544548818674,
      "loss": 6.9893,
      "step": 254
    },
    {
      "epoch": 0.07259786476868327,
      "grad_norm": 0.48041409254074097,
      "learning_rate": 0.00048192428124110446,
      "loss": 6.8398,
      "step": 255
    },
    {
      "epoch": 0.07288256227758007,
      "grad_norm": 0.3993391692638397,
      "learning_rate": 0.00048185311699402224,
      "loss": 7.3496,
      "step": 256
    },
    {
      "epoch": 0.07316725978647687,
      "grad_norm": 0.38099411129951477,
      "learning_rate": 0.0004817819527469399,
      "loss": 7.5664,
      "step": 257
    },
    {
      "epoch": 0.07345195729537367,
      "grad_norm": 0.36448702216148376,
      "learning_rate": 0.0004817107884998577,
      "loss": 7.5957,
      "step": 258
    },
    {
      "epoch": 0.07373665480427047,
      "grad_norm": 0.3705407977104187,
      "learning_rate": 0.0004816396242527754,
      "loss": 7.4092,
      "step": 259
    },
    {
      "epoch": 0.07402135231316725,
      "grad_norm": 0.4222451150417328,
      "learning_rate": 0.00048156846000569313,
      "loss": 7.2832,
      "step": 260
    },
    {
      "epoch": 0.07430604982206405,
      "grad_norm": 0.43533411622047424,
      "learning_rate": 0.0004814972957586109,
      "loss": 7.0967,
      "step": 261
    },
    {
      "epoch": 0.07459074733096085,
      "grad_norm": 0.45843270421028137,
      "learning_rate": 0.00048142613151152863,
      "loss": 7.1719,
      "step": 262
    },
    {
      "epoch": 0.07487544483985765,
      "grad_norm": 0.38327649235725403,
      "learning_rate": 0.00048135496726444635,
      "loss": 7.2725,
      "step": 263
    },
    {
      "epoch": 0.07516014234875446,
      "grad_norm": 0.4202622175216675,
      "learning_rate": 0.0004812838030173641,
      "loss": 7.4424,
      "step": 264
    },
    {
      "epoch": 0.07544483985765124,
      "grad_norm": 0.5358533263206482,
      "learning_rate": 0.0004812126387702818,
      "loss": 6.6904,
      "step": 265
    },
    {
      "epoch": 0.07572953736654804,
      "grad_norm": 0.4957423806190491,
      "learning_rate": 0.0004811414745231996,
      "loss": 6.8877,
      "step": 266
    },
    {
      "epoch": 0.07601423487544484,
      "grad_norm": 0.40162715315818787,
      "learning_rate": 0.0004810703102761173,
      "loss": 7.21,
      "step": 267
    },
    {
      "epoch": 0.07629893238434164,
      "grad_norm": 0.46528732776641846,
      "learning_rate": 0.000480999146029035,
      "loss": 7.2451,
      "step": 268
    },
    {
      "epoch": 0.07658362989323843,
      "grad_norm": 0.4028266668319702,
      "learning_rate": 0.00048092798178195274,
      "loss": 7.4043,
      "step": 269
    },
    {
      "epoch": 0.07686832740213523,
      "grad_norm": 0.46121296286582947,
      "learning_rate": 0.00048085681753487047,
      "loss": 7.2305,
      "step": 270
    },
    {
      "epoch": 0.07715302491103203,
      "grad_norm": 0.43850409984588623,
      "learning_rate": 0.00048078565328778824,
      "loss": 7.1523,
      "step": 271
    },
    {
      "epoch": 0.07743772241992883,
      "grad_norm": 0.36082831025123596,
      "learning_rate": 0.00048071448904070597,
      "loss": 7.6426,
      "step": 272
    },
    {
      "epoch": 0.07772241992882563,
      "grad_norm": 0.42451992630958557,
      "learning_rate": 0.0004806433247936237,
      "loss": 7.5566,
      "step": 273
    },
    {
      "epoch": 0.07800711743772241,
      "grad_norm": 0.5189881324768066,
      "learning_rate": 0.0004805721605465414,
      "loss": 6.9717,
      "step": 274
    },
    {
      "epoch": 0.07829181494661921,
      "grad_norm": 0.34993991255760193,
      "learning_rate": 0.00048050099629945914,
      "loss": 7.75,
      "step": 275
    },
    {
      "epoch": 0.07857651245551601,
      "grad_norm": 0.38793134689331055,
      "learning_rate": 0.0004804298320523769,
      "loss": 7.0029,
      "step": 276
    },
    {
      "epoch": 0.07886120996441282,
      "grad_norm": 0.4095827639102936,
      "learning_rate": 0.00048035866780529464,
      "loss": 7.1084,
      "step": 277
    },
    {
      "epoch": 0.07914590747330962,
      "grad_norm": 0.400534987449646,
      "learning_rate": 0.00048028750355821236,
      "loss": 7.5254,
      "step": 278
    },
    {
      "epoch": 0.0794306049822064,
      "grad_norm": 0.38447555899620056,
      "learning_rate": 0.00048021633931113013,
      "loss": 7.5615,
      "step": 279
    },
    {
      "epoch": 0.0797153024911032,
      "grad_norm": 0.4068629741668701,
      "learning_rate": 0.0004801451750640478,
      "loss": 7.459,
      "step": 280
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.39288195967674255,
      "learning_rate": 0.0004800740108169656,
      "loss": 7.1357,
      "step": 281
    },
    {
      "epoch": 0.0802846975088968,
      "grad_norm": 0.44044890999794006,
      "learning_rate": 0.0004800028465698833,
      "loss": 6.8838,
      "step": 282
    },
    {
      "epoch": 0.08056939501779359,
      "grad_norm": 0.3920339047908783,
      "learning_rate": 0.000479931682322801,
      "loss": 7.4258,
      "step": 283
    },
    {
      "epoch": 0.08085409252669039,
      "grad_norm": 0.42204195261001587,
      "learning_rate": 0.0004798605180757188,
      "loss": 7.2412,
      "step": 284
    },
    {
      "epoch": 0.08113879003558719,
      "grad_norm": 0.4086628258228302,
      "learning_rate": 0.00047978935382863647,
      "loss": 7.4141,
      "step": 285
    },
    {
      "epoch": 0.08142348754448399,
      "grad_norm": 0.48516544699668884,
      "learning_rate": 0.0004797181895815542,
      "loss": 6.7148,
      "step": 286
    },
    {
      "epoch": 0.08170818505338079,
      "grad_norm": 0.4337480366230011,
      "learning_rate": 0.00047964702533447197,
      "loss": 7.1201,
      "step": 287
    },
    {
      "epoch": 0.08199288256227757,
      "grad_norm": 0.4319247305393219,
      "learning_rate": 0.0004795758610873897,
      "loss": 7.1279,
      "step": 288
    },
    {
      "epoch": 0.08227758007117437,
      "grad_norm": 0.49747705459594727,
      "learning_rate": 0.00047950469684030747,
      "loss": 7.2217,
      "step": 289
    },
    {
      "epoch": 0.08256227758007118,
      "grad_norm": 0.39577603340148926,
      "learning_rate": 0.0004794335325932252,
      "loss": 7.4053,
      "step": 290
    },
    {
      "epoch": 0.08284697508896798,
      "grad_norm": 0.4405271112918854,
      "learning_rate": 0.00047936236834614286,
      "loss": 7.0312,
      "step": 291
    },
    {
      "epoch": 0.08313167259786478,
      "grad_norm": 0.3082965910434723,
      "learning_rate": 0.00047929120409906064,
      "loss": 7.7031,
      "step": 292
    },
    {
      "epoch": 0.08341637010676156,
      "grad_norm": 0.5239378809928894,
      "learning_rate": 0.00047922003985197836,
      "loss": 6.9287,
      "step": 293
    },
    {
      "epoch": 0.08370106761565836,
      "grad_norm": 0.38422203063964844,
      "learning_rate": 0.00047914887560489614,
      "loss": 7.251,
      "step": 294
    },
    {
      "epoch": 0.08398576512455516,
      "grad_norm": 0.44853416085243225,
      "learning_rate": 0.00047907771135781386,
      "loss": 7.0537,
      "step": 295
    },
    {
      "epoch": 0.08427046263345196,
      "grad_norm": 0.3908516466617584,
      "learning_rate": 0.0004790065471107316,
      "loss": 7.5029,
      "step": 296
    },
    {
      "epoch": 0.08455516014234875,
      "grad_norm": 0.5178523659706116,
      "learning_rate": 0.0004789353828636493,
      "loss": 6.6699,
      "step": 297
    },
    {
      "epoch": 0.08483985765124555,
      "grad_norm": 0.39277517795562744,
      "learning_rate": 0.00047886421861656703,
      "loss": 7.6855,
      "step": 298
    },
    {
      "epoch": 0.08512455516014235,
      "grad_norm": 0.4601745307445526,
      "learning_rate": 0.0004787930543694848,
      "loss": 6.6934,
      "step": 299
    },
    {
      "epoch": 0.08540925266903915,
      "grad_norm": 0.4405549168586731,
      "learning_rate": 0.00047872189012240253,
      "loss": 7.2129,
      "step": 300
    },
    {
      "epoch": 0.08569395017793595,
      "grad_norm": 0.4540632665157318,
      "learning_rate": 0.00047865072587532025,
      "loss": 6.9492,
      "step": 301
    },
    {
      "epoch": 0.08597864768683273,
      "grad_norm": 0.4265691936016083,
      "learning_rate": 0.000478579561628238,
      "loss": 7.1064,
      "step": 302
    },
    {
      "epoch": 0.08626334519572953,
      "grad_norm": 0.45279815793037415,
      "learning_rate": 0.0004785083973811557,
      "loss": 7.001,
      "step": 303
    },
    {
      "epoch": 0.08654804270462634,
      "grad_norm": 0.4508437216281891,
      "learning_rate": 0.0004784372331340734,
      "loss": 6.9727,
      "step": 304
    },
    {
      "epoch": 0.08683274021352314,
      "grad_norm": 0.39535853266716003,
      "learning_rate": 0.0004783660688869912,
      "loss": 7.4971,
      "step": 305
    },
    {
      "epoch": 0.08711743772241994,
      "grad_norm": 0.38876810669898987,
      "learning_rate": 0.0004782949046399089,
      "loss": 6.96,
      "step": 306
    },
    {
      "epoch": 0.08740213523131672,
      "grad_norm": 0.3352130055427551,
      "learning_rate": 0.0004782237403928267,
      "loss": 7.5664,
      "step": 307
    },
    {
      "epoch": 0.08768683274021352,
      "grad_norm": 0.43718674778938293,
      "learning_rate": 0.00047815257614574437,
      "loss": 7.3652,
      "step": 308
    },
    {
      "epoch": 0.08797153024911032,
      "grad_norm": 0.41846373677253723,
      "learning_rate": 0.0004780814118986621,
      "loss": 7.4209,
      "step": 309
    },
    {
      "epoch": 0.08825622775800712,
      "grad_norm": 0.35628408193588257,
      "learning_rate": 0.00047801024765157987,
      "loss": 7.665,
      "step": 310
    },
    {
      "epoch": 0.08854092526690391,
      "grad_norm": 0.43138930201530457,
      "learning_rate": 0.0004779390834044976,
      "loss": 7.0596,
      "step": 311
    },
    {
      "epoch": 0.08882562277580071,
      "grad_norm": 0.41381263732910156,
      "learning_rate": 0.00047786791915741537,
      "loss": 7.4902,
      "step": 312
    },
    {
      "epoch": 0.08911032028469751,
      "grad_norm": 0.44806408882141113,
      "learning_rate": 0.00047779675491033303,
      "loss": 7.3066,
      "step": 313
    },
    {
      "epoch": 0.08939501779359431,
      "grad_norm": 0.41572806239128113,
      "learning_rate": 0.00047772559066325076,
      "loss": 7.1211,
      "step": 314
    },
    {
      "epoch": 0.08967971530249111,
      "grad_norm": 0.4351762533187866,
      "learning_rate": 0.00047765442641616853,
      "loss": 7.1748,
      "step": 315
    },
    {
      "epoch": 0.0899644128113879,
      "grad_norm": 0.5767992734909058,
      "learning_rate": 0.00047758326216908626,
      "loss": 6.5059,
      "step": 316
    },
    {
      "epoch": 0.0902491103202847,
      "grad_norm": 0.4354422092437744,
      "learning_rate": 0.00047751209792200403,
      "loss": 6.9727,
      "step": 317
    },
    {
      "epoch": 0.0905338078291815,
      "grad_norm": 0.48386743664741516,
      "learning_rate": 0.00047744093367492176,
      "loss": 7.5859,
      "step": 318
    },
    {
      "epoch": 0.0908185053380783,
      "grad_norm": 0.447219580411911,
      "learning_rate": 0.0004773697694278394,
      "loss": 7.5195,
      "step": 319
    },
    {
      "epoch": 0.0911032028469751,
      "grad_norm": 0.4001949727535248,
      "learning_rate": 0.0004772986051807572,
      "loss": 7.3672,
      "step": 320
    },
    {
      "epoch": 0.09138790035587188,
      "grad_norm": 0.4028257131576538,
      "learning_rate": 0.0004772274409336749,
      "loss": 7.1143,
      "step": 321
    },
    {
      "epoch": 0.09167259786476868,
      "grad_norm": 0.6439388990402222,
      "learning_rate": 0.00047715627668659265,
      "loss": 6.9395,
      "step": 322
    },
    {
      "epoch": 0.09195729537366548,
      "grad_norm": 0.434939980506897,
      "learning_rate": 0.0004770851124395104,
      "loss": 7.0215,
      "step": 323
    },
    {
      "epoch": 0.09224199288256228,
      "grad_norm": 0.4516619145870209,
      "learning_rate": 0.00047701394819242815,
      "loss": 6.9014,
      "step": 324
    },
    {
      "epoch": 0.09252669039145907,
      "grad_norm": 0.4030342102050781,
      "learning_rate": 0.00047694278394534587,
      "loss": 7.5859,
      "step": 325
    },
    {
      "epoch": 0.09281138790035587,
      "grad_norm": 0.3350996673107147,
      "learning_rate": 0.0004768716196982636,
      "loss": 7.5254,
      "step": 326
    },
    {
      "epoch": 0.09309608540925267,
      "grad_norm": 0.5201140642166138,
      "learning_rate": 0.0004768004554511813,
      "loss": 7.3613,
      "step": 327
    },
    {
      "epoch": 0.09338078291814947,
      "grad_norm": 0.3434700667858124,
      "learning_rate": 0.0004767292912040991,
      "loss": 7.5518,
      "step": 328
    },
    {
      "epoch": 0.09366548042704627,
      "grad_norm": 0.39710113406181335,
      "learning_rate": 0.0004766581269570168,
      "loss": 7.3145,
      "step": 329
    },
    {
      "epoch": 0.09395017793594305,
      "grad_norm": 0.3940322995185852,
      "learning_rate": 0.00047658696270993454,
      "loss": 7.4756,
      "step": 330
    },
    {
      "epoch": 0.09423487544483986,
      "grad_norm": 0.35256409645080566,
      "learning_rate": 0.00047651579846285226,
      "loss": 7.3662,
      "step": 331
    },
    {
      "epoch": 0.09451957295373666,
      "grad_norm": 0.490875780582428,
      "learning_rate": 0.00047644463421577,
      "loss": 6.9189,
      "step": 332
    },
    {
      "epoch": 0.09480427046263346,
      "grad_norm": 0.3466689884662628,
      "learning_rate": 0.00047637346996868776,
      "loss": 7.7227,
      "step": 333
    },
    {
      "epoch": 0.09508896797153026,
      "grad_norm": 0.41715121269226074,
      "learning_rate": 0.0004763023057216055,
      "loss": 7.4795,
      "step": 334
    },
    {
      "epoch": 0.09537366548042704,
      "grad_norm": 0.42658162117004395,
      "learning_rate": 0.00047623114147452326,
      "loss": 7.2119,
      "step": 335
    },
    {
      "epoch": 0.09565836298932384,
      "grad_norm": 0.4596523344516754,
      "learning_rate": 0.00047615997722744093,
      "loss": 7.0312,
      "step": 336
    },
    {
      "epoch": 0.09594306049822064,
      "grad_norm": 0.44432130455970764,
      "learning_rate": 0.00047608881298035865,
      "loss": 7.1562,
      "step": 337
    },
    {
      "epoch": 0.09622775800711744,
      "grad_norm": 0.41700947284698486,
      "learning_rate": 0.00047601764873327643,
      "loss": 7.0879,
      "step": 338
    },
    {
      "epoch": 0.09651245551601423,
      "grad_norm": 0.41166216135025024,
      "learning_rate": 0.00047594648448619415,
      "loss": 7.2783,
      "step": 339
    },
    {
      "epoch": 0.09679715302491103,
      "grad_norm": 0.3937351107597351,
      "learning_rate": 0.0004758753202391119,
      "loss": 7.4902,
      "step": 340
    },
    {
      "epoch": 0.09708185053380783,
      "grad_norm": 0.49600276350975037,
      "learning_rate": 0.00047580415599202965,
      "loss": 7.0215,
      "step": 341
    },
    {
      "epoch": 0.09736654804270463,
      "grad_norm": 0.39852508902549744,
      "learning_rate": 0.0004757329917449473,
      "loss": 7.5752,
      "step": 342
    },
    {
      "epoch": 0.09765124555160143,
      "grad_norm": 0.43215256929397583,
      "learning_rate": 0.0004756618274978651,
      "loss": 7.3994,
      "step": 343
    },
    {
      "epoch": 0.09793594306049822,
      "grad_norm": 0.395315945148468,
      "learning_rate": 0.0004755906632507828,
      "loss": 7.4033,
      "step": 344
    },
    {
      "epoch": 0.09822064056939502,
      "grad_norm": 0.4142919182777405,
      "learning_rate": 0.00047551949900370054,
      "loss": 7.3242,
      "step": 345
    },
    {
      "epoch": 0.09850533807829182,
      "grad_norm": 0.5080282092094421,
      "learning_rate": 0.0004754483347566183,
      "loss": 6.625,
      "step": 346
    },
    {
      "epoch": 0.09879003558718862,
      "grad_norm": 0.44504308700561523,
      "learning_rate": 0.000475377170509536,
      "loss": 7.668,
      "step": 347
    },
    {
      "epoch": 0.09907473309608542,
      "grad_norm": 0.5334663391113281,
      "learning_rate": 0.00047530600626245376,
      "loss": 7.2246,
      "step": 348
    },
    {
      "epoch": 0.0993594306049822,
      "grad_norm": 0.4300445020198822,
      "learning_rate": 0.0004752348420153715,
      "loss": 7.2812,
      "step": 349
    },
    {
      "epoch": 0.099644128113879,
      "grad_norm": 0.43815192580223083,
      "learning_rate": 0.0004751636777682892,
      "loss": 6.8447,
      "step": 350
    },
    {
      "epoch": 0.0999288256227758,
      "grad_norm": 0.45553478598594666,
      "learning_rate": 0.000475092513521207,
      "loss": 7.3037,
      "step": 351
    },
    {
      "epoch": 0.1002135231316726,
      "grad_norm": 0.4697282612323761,
      "learning_rate": 0.0004750213492741247,
      "loss": 7.0762,
      "step": 352
    },
    {
      "epoch": 0.10049822064056939,
      "grad_norm": 0.5708103179931641,
      "learning_rate": 0.0004749501850270424,
      "loss": 7.0889,
      "step": 353
    },
    {
      "epoch": 0.10078291814946619,
      "grad_norm": 0.46478471159935,
      "learning_rate": 0.00047487902077996015,
      "loss": 7.2305,
      "step": 354
    },
    {
      "epoch": 0.10106761565836299,
      "grad_norm": 0.4347301721572876,
      "learning_rate": 0.0004748078565328779,
      "loss": 7.417,
      "step": 355
    },
    {
      "epoch": 0.10135231316725979,
      "grad_norm": 0.3864136040210724,
      "learning_rate": 0.00047473669228579565,
      "loss": 7.7119,
      "step": 356
    },
    {
      "epoch": 0.10163701067615659,
      "grad_norm": 0.3752633333206177,
      "learning_rate": 0.0004746655280387134,
      "loss": 7.6006,
      "step": 357
    },
    {
      "epoch": 0.10192170818505338,
      "grad_norm": 0.5481764674186707,
      "learning_rate": 0.00047459436379163105,
      "loss": 7.0039,
      "step": 358
    },
    {
      "epoch": 0.10220640569395018,
      "grad_norm": 0.40612098574638367,
      "learning_rate": 0.0004745231995445488,
      "loss": 7.4434,
      "step": 359
    },
    {
      "epoch": 0.10249110320284698,
      "grad_norm": 0.4205334484577179,
      "learning_rate": 0.00047445203529746655,
      "loss": 7.2832,
      "step": 360
    },
    {
      "epoch": 0.10277580071174378,
      "grad_norm": 0.3059813678264618,
      "learning_rate": 0.0004743808710503843,
      "loss": 7.6846,
      "step": 361
    },
    {
      "epoch": 0.10306049822064058,
      "grad_norm": 0.48755931854248047,
      "learning_rate": 0.00047430970680330205,
      "loss": 6.9141,
      "step": 362
    },
    {
      "epoch": 0.10334519572953736,
      "grad_norm": 0.42173999547958374,
      "learning_rate": 0.00047423854255621977,
      "loss": 7.5166,
      "step": 363
    },
    {
      "epoch": 0.10362989323843416,
      "grad_norm": 0.5560771226882935,
      "learning_rate": 0.0004741673783091375,
      "loss": 7.418,
      "step": 364
    },
    {
      "epoch": 0.10391459074733096,
      "grad_norm": 0.6154012680053711,
      "learning_rate": 0.0004740962140620552,
      "loss": 7.2734,
      "step": 365
    },
    {
      "epoch": 0.10419928825622776,
      "grad_norm": 0.4501911997795105,
      "learning_rate": 0.000474025049814973,
      "loss": 6.9023,
      "step": 366
    },
    {
      "epoch": 0.10448398576512455,
      "grad_norm": 0.4486687183380127,
      "learning_rate": 0.0004739538855678907,
      "loss": 7.3291,
      "step": 367
    },
    {
      "epoch": 0.10476868327402135,
      "grad_norm": 0.406693696975708,
      "learning_rate": 0.00047388272132080844,
      "loss": 7.501,
      "step": 368
    },
    {
      "epoch": 0.10505338078291815,
      "grad_norm": 0.4880397915840149,
      "learning_rate": 0.0004738115570737262,
      "loss": 6.6982,
      "step": 369
    },
    {
      "epoch": 0.10533807829181495,
      "grad_norm": 0.4266977608203888,
      "learning_rate": 0.0004737403928266439,
      "loss": 7.2031,
      "step": 370
    },
    {
      "epoch": 0.10562277580071175,
      "grad_norm": 0.4237028658390045,
      "learning_rate": 0.0004736692285795616,
      "loss": 6.9111,
      "step": 371
    },
    {
      "epoch": 0.10590747330960854,
      "grad_norm": 0.5166893601417542,
      "learning_rate": 0.0004735980643324794,
      "loss": 7.3945,
      "step": 372
    },
    {
      "epoch": 0.10619217081850534,
      "grad_norm": 0.4069686233997345,
      "learning_rate": 0.0004735269000853971,
      "loss": 7.54,
      "step": 373
    },
    {
      "epoch": 0.10647686832740214,
      "grad_norm": 0.43857699632644653,
      "learning_rate": 0.0004734557358383149,
      "loss": 7.3135,
      "step": 374
    },
    {
      "epoch": 0.10676156583629894,
      "grad_norm": 0.39594122767448425,
      "learning_rate": 0.00047338457159123255,
      "loss": 7.8369,
      "step": 375
    },
    {
      "epoch": 0.10704626334519574,
      "grad_norm": 0.4282735586166382,
      "learning_rate": 0.00047331340734415027,
      "loss": 7.4375,
      "step": 376
    },
    {
      "epoch": 0.10733096085409252,
      "grad_norm": 0.45673078298568726,
      "learning_rate": 0.00047324224309706805,
      "loss": 7.3848,
      "step": 377
    },
    {
      "epoch": 0.10761565836298932,
      "grad_norm": 0.4529609978199005,
      "learning_rate": 0.00047317107884998577,
      "loss": 7.1904,
      "step": 378
    },
    {
      "epoch": 0.10790035587188612,
      "grad_norm": 0.508343517780304,
      "learning_rate": 0.00047309991460290355,
      "loss": 7.1104,
      "step": 379
    },
    {
      "epoch": 0.10818505338078292,
      "grad_norm": 0.5695661306381226,
      "learning_rate": 0.00047302875035582127,
      "loss": 6.9775,
      "step": 380
    },
    {
      "epoch": 0.10846975088967971,
      "grad_norm": 0.4289855659008026,
      "learning_rate": 0.00047295758610873894,
      "loss": 7.3799,
      "step": 381
    },
    {
      "epoch": 0.10875444839857651,
      "grad_norm": 0.45409122109413147,
      "learning_rate": 0.0004728864218616567,
      "loss": 6.877,
      "step": 382
    },
    {
      "epoch": 0.10903914590747331,
      "grad_norm": 0.43355792760849,
      "learning_rate": 0.00047281525761457444,
      "loss": 7.1543,
      "step": 383
    },
    {
      "epoch": 0.10932384341637011,
      "grad_norm": 0.3788268268108368,
      "learning_rate": 0.00047274409336749216,
      "loss": 7.582,
      "step": 384
    },
    {
      "epoch": 0.10960854092526691,
      "grad_norm": 0.4172390103340149,
      "learning_rate": 0.00047267292912040994,
      "loss": 7.4766,
      "step": 385
    },
    {
      "epoch": 0.1098932384341637,
      "grad_norm": 0.5651355385780334,
      "learning_rate": 0.00047260176487332766,
      "loss": 6.8984,
      "step": 386
    },
    {
      "epoch": 0.1101779359430605,
      "grad_norm": 0.4061424136161804,
      "learning_rate": 0.0004725306006262454,
      "loss": 7.6387,
      "step": 387
    },
    {
      "epoch": 0.1104626334519573,
      "grad_norm": 0.41827505826950073,
      "learning_rate": 0.0004724594363791631,
      "loss": 7.2285,
      "step": 388
    },
    {
      "epoch": 0.1107473309608541,
      "grad_norm": 0.37790295481681824,
      "learning_rate": 0.00047238827213208083,
      "loss": 7.583,
      "step": 389
    },
    {
      "epoch": 0.1110320284697509,
      "grad_norm": 0.41281452775001526,
      "learning_rate": 0.0004723171078849986,
      "loss": 7.4941,
      "step": 390
    },
    {
      "epoch": 0.11131672597864768,
      "grad_norm": 0.41881147027015686,
      "learning_rate": 0.00047224594363791633,
      "loss": 7.3848,
      "step": 391
    },
    {
      "epoch": 0.11160142348754448,
      "grad_norm": 0.41661399602890015,
      "learning_rate": 0.00047217477939083405,
      "loss": 7.4121,
      "step": 392
    },
    {
      "epoch": 0.11188612099644128,
      "grad_norm": 0.4192899763584137,
      "learning_rate": 0.0004721036151437518,
      "loss": 7.0596,
      "step": 393
    },
    {
      "epoch": 0.11217081850533808,
      "grad_norm": 0.45563167333602905,
      "learning_rate": 0.0004720324508966695,
      "loss": 7.4004,
      "step": 394
    },
    {
      "epoch": 0.11245551601423487,
      "grad_norm": 0.4283228814601898,
      "learning_rate": 0.0004719612866495873,
      "loss": 7.6162,
      "step": 395
    },
    {
      "epoch": 0.11274021352313167,
      "grad_norm": 0.4331873655319214,
      "learning_rate": 0.000471890122402505,
      "loss": 7.001,
      "step": 396
    },
    {
      "epoch": 0.11302491103202847,
      "grad_norm": 0.4580132067203522,
      "learning_rate": 0.0004718189581554228,
      "loss": 7.0654,
      "step": 397
    },
    {
      "epoch": 0.11330960854092527,
      "grad_norm": 0.46618202328681946,
      "learning_rate": 0.00047174779390834044,
      "loss": 7.0742,
      "step": 398
    },
    {
      "epoch": 0.11359430604982207,
      "grad_norm": 0.4677276909351349,
      "learning_rate": 0.00047167662966125817,
      "loss": 7.1973,
      "step": 399
    },
    {
      "epoch": 0.11387900355871886,
      "grad_norm": 0.4366690516471863,
      "learning_rate": 0.00047160546541417594,
      "loss": 7.4443,
      "step": 400
    },
    {
      "epoch": 0.11387900355871886,
      "eval_bleu": 0.06055701380020064,
      "eval_loss": 7.00390625,
      "eval_runtime": 195.2568,
      "eval_samples_per_second": 1.454,
      "eval_steps_per_second": 0.092,
      "step": 400
    }
  ],
  "logging_steps": 1,
  "max_steps": 7026,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 200,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 871190495232000.0,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
