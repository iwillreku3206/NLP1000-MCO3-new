{
  "best_global_step": 200,
  "best_metric": 7.046875,
  "best_model_checkpoint": "trained-nllb-en-to-bicol\\checkpoint-200",
  "epoch": 0.05693950177935943,
  "eval_steps": 200,
  "global_step": 200,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.00028469750889679714,
      "grad_norm": 0.13050267100334167,
      "learning_rate": 0.0005,
      "loss": 11.8242,
      "step": 1
    },
    {
      "epoch": 0.0005693950177935943,
      "grad_norm": 0.16353872418403625,
      "learning_rate": 0.0004999288357529178,
      "loss": 12.7012,
      "step": 2
    },
    {
      "epoch": 0.0008540925266903915,
      "grad_norm": 0.1825779378414154,
      "learning_rate": 0.0004998576715058355,
      "loss": 11.4883,
      "step": 3
    },
    {
      "epoch": 0.0011387900355871886,
      "grad_norm": 0.2419777810573578,
      "learning_rate": 0.0004997865072587532,
      "loss": 11.9531,
      "step": 4
    },
    {
      "epoch": 0.0014234875444839859,
      "grad_norm": 0.31029146909713745,
      "learning_rate": 0.0004997153430116709,
      "loss": 11.7852,
      "step": 5
    },
    {
      "epoch": 0.001708185053380783,
      "grad_norm": 0.3981788754463196,
      "learning_rate": 0.0004996441787645887,
      "loss": 11.8242,
      "step": 6
    },
    {
      "epoch": 0.00199288256227758,
      "grad_norm": 0.44912195205688477,
      "learning_rate": 0.0004995730145175065,
      "loss": 11.1836,
      "step": 7
    },
    {
      "epoch": 0.002277580071174377,
      "grad_norm": 0.5424309372901917,
      "learning_rate": 0.0004995018502704241,
      "loss": 11.6504,
      "step": 8
    },
    {
      "epoch": 0.002562277580071174,
      "grad_norm": 0.6373918056488037,
      "learning_rate": 0.0004994306860233419,
      "loss": 11.3926,
      "step": 9
    },
    {
      "epoch": 0.0028469750889679717,
      "grad_norm": 0.7767484188079834,
      "learning_rate": 0.0004993595217762596,
      "loss": 11.5098,
      "step": 10
    },
    {
      "epoch": 0.003131672597864769,
      "grad_norm": 0.7795925736427307,
      "learning_rate": 0.0004992883575291773,
      "loss": 12.1816,
      "step": 11
    },
    {
      "epoch": 0.003416370106761566,
      "grad_norm": 0.5827488303184509,
      "learning_rate": 0.0004992171932820951,
      "loss": 11.4355,
      "step": 12
    },
    {
      "epoch": 0.003701067615658363,
      "grad_norm": 0.5459573864936829,
      "learning_rate": 0.0004991460290350128,
      "loss": 11.0156,
      "step": 13
    },
    {
      "epoch": 0.00398576512455516,
      "grad_norm": 0.513734757900238,
      "learning_rate": 0.0004990748647879306,
      "loss": 11.6016,
      "step": 14
    },
    {
      "epoch": 0.004270462633451958,
      "grad_norm": 0.5204881429672241,
      "learning_rate": 0.0004990037005408483,
      "loss": 11.1738,
      "step": 15
    },
    {
      "epoch": 0.004555160142348754,
      "grad_norm": 0.5095188021659851,
      "learning_rate": 0.000498932536293766,
      "loss": 11.6738,
      "step": 16
    },
    {
      "epoch": 0.004839857651245552,
      "grad_norm": 0.5002894997596741,
      "learning_rate": 0.0004988613720466838,
      "loss": 11.1172,
      "step": 17
    },
    {
      "epoch": 0.005124555160142348,
      "grad_norm": 0.5175882577896118,
      "learning_rate": 0.0004987902077996015,
      "loss": 11.709,
      "step": 18
    },
    {
      "epoch": 0.005409252669039146,
      "grad_norm": 0.481122225522995,
      "learning_rate": 0.0004987190435525192,
      "loss": 11.2383,
      "step": 19
    },
    {
      "epoch": 0.0056939501779359435,
      "grad_norm": 0.4916481077671051,
      "learning_rate": 0.000498647879305437,
      "loss": 11.1875,
      "step": 20
    },
    {
      "epoch": 0.00597864768683274,
      "grad_norm": 0.4639431834220886,
      "learning_rate": 0.0004985767150583547,
      "loss": 10.959,
      "step": 21
    },
    {
      "epoch": 0.006263345195729538,
      "grad_norm": 0.471029669046402,
      "learning_rate": 0.0004985055508112725,
      "loss": 10.5039,
      "step": 22
    },
    {
      "epoch": 0.006548042704626334,
      "grad_norm": 0.5494607090950012,
      "learning_rate": 0.0004984343865641901,
      "loss": 10.7871,
      "step": 23
    },
    {
      "epoch": 0.006832740213523132,
      "grad_norm": 0.542377769947052,
      "learning_rate": 0.0004983632223171079,
      "loss": 10.7402,
      "step": 24
    },
    {
      "epoch": 0.0071174377224199285,
      "grad_norm": 0.6290743947029114,
      "learning_rate": 0.0004982920580700257,
      "loss": 11.1191,
      "step": 25
    },
    {
      "epoch": 0.007402135231316726,
      "grad_norm": 0.5208401083946228,
      "learning_rate": 0.0004982208938229434,
      "loss": 10.3535,
      "step": 26
    },
    {
      "epoch": 0.0076868327402135235,
      "grad_norm": 0.5653201937675476,
      "learning_rate": 0.0004981497295758611,
      "loss": 10.2676,
      "step": 27
    },
    {
      "epoch": 0.00797153024911032,
      "grad_norm": 0.5279284119606018,
      "learning_rate": 0.0004980785653287788,
      "loss": 10.6504,
      "step": 28
    },
    {
      "epoch": 0.008256227758007117,
      "grad_norm": 0.5287833213806152,
      "learning_rate": 0.0004980074010816966,
      "loss": 9.9648,
      "step": 29
    },
    {
      "epoch": 0.008540925266903915,
      "grad_norm": 0.4779149293899536,
      "learning_rate": 0.0004979362368346143,
      "loss": 11.1738,
      "step": 30
    },
    {
      "epoch": 0.008825622775800712,
      "grad_norm": 0.4311399459838867,
      "learning_rate": 0.000497865072587532,
      "loss": 10.7617,
      "step": 31
    },
    {
      "epoch": 0.009110320284697508,
      "grad_norm": 0.4880855083465576,
      "learning_rate": 0.0004977939083404498,
      "loss": 10.6113,
      "step": 32
    },
    {
      "epoch": 0.009395017793594307,
      "grad_norm": 0.4771440029144287,
      "learning_rate": 0.0004977227440933675,
      "loss": 10.2031,
      "step": 33
    },
    {
      "epoch": 0.009679715302491104,
      "grad_norm": 0.4335767924785614,
      "learning_rate": 0.0004976515798462852,
      "loss": 9.9766,
      "step": 34
    },
    {
      "epoch": 0.0099644128113879,
      "grad_norm": 0.47250261902809143,
      "learning_rate": 0.000497580415599203,
      "loss": 10.582,
      "step": 35
    },
    {
      "epoch": 0.010249110320284697,
      "grad_norm": 0.5027048587799072,
      "learning_rate": 0.0004975092513521207,
      "loss": 9.252,
      "step": 36
    },
    {
      "epoch": 0.010533807829181495,
      "grad_norm": 0.4936491847038269,
      "learning_rate": 0.0004974380871050385,
      "loss": 9.9648,
      "step": 37
    },
    {
      "epoch": 0.010818505338078292,
      "grad_norm": 0.4429067373275757,
      "learning_rate": 0.0004973669228579561,
      "loss": 10.1699,
      "step": 38
    },
    {
      "epoch": 0.011103202846975089,
      "grad_norm": 0.43701812624931335,
      "learning_rate": 0.0004972957586108739,
      "loss": 10.1875,
      "step": 39
    },
    {
      "epoch": 0.011387900355871887,
      "grad_norm": 0.44829320907592773,
      "learning_rate": 0.0004972245943637917,
      "loss": 9.7109,
      "step": 40
    },
    {
      "epoch": 0.011672597864768684,
      "grad_norm": 0.44973376393318176,
      "learning_rate": 0.0004971534301167094,
      "loss": 9.9023,
      "step": 41
    },
    {
      "epoch": 0.01195729537366548,
      "grad_norm": 0.4918513894081116,
      "learning_rate": 0.0004970822658696271,
      "loss": 10.1934,
      "step": 42
    },
    {
      "epoch": 0.012241992882562277,
      "grad_norm": 0.4607585668563843,
      "learning_rate": 0.0004970111016225449,
      "loss": 9.9941,
      "step": 43
    },
    {
      "epoch": 0.012526690391459075,
      "grad_norm": 0.45157185196876526,
      "learning_rate": 0.0004969399373754626,
      "loss": 9.6797,
      "step": 44
    },
    {
      "epoch": 0.012811387900355872,
      "grad_norm": 0.45155540108680725,
      "learning_rate": 0.0004968687731283804,
      "loss": 9.8535,
      "step": 45
    },
    {
      "epoch": 0.013096085409252669,
      "grad_norm": 0.45471465587615967,
      "learning_rate": 0.000496797608881298,
      "loss": 9.6699,
      "step": 46
    },
    {
      "epoch": 0.013380782918149467,
      "grad_norm": 0.45609229803085327,
      "learning_rate": 0.0004967264446342158,
      "loss": 9.834,
      "step": 47
    },
    {
      "epoch": 0.013665480427046264,
      "grad_norm": 0.46065598726272583,
      "learning_rate": 0.0004966552803871336,
      "loss": 9.1973,
      "step": 48
    },
    {
      "epoch": 0.01395017793594306,
      "grad_norm": 0.4395595192909241,
      "learning_rate": 0.0004965841161400512,
      "loss": 9.3379,
      "step": 49
    },
    {
      "epoch": 0.014234875444839857,
      "grad_norm": 0.42804303765296936,
      "learning_rate": 0.0004965129518929689,
      "loss": 8.1885,
      "step": 50
    },
    {
      "epoch": 0.014519572953736655,
      "grad_norm": 0.4336685538291931,
      "learning_rate": 0.0004964417876458867,
      "loss": 9.4141,
      "step": 51
    },
    {
      "epoch": 0.014804270462633452,
      "grad_norm": 0.4083915054798126,
      "learning_rate": 0.0004963706233988045,
      "loss": 9.2363,
      "step": 52
    },
    {
      "epoch": 0.015088967971530249,
      "grad_norm": 0.39727815985679626,
      "learning_rate": 0.0004962994591517222,
      "loss": 8.5439,
      "step": 53
    },
    {
      "epoch": 0.015373665480427047,
      "grad_norm": 0.4038327634334564,
      "learning_rate": 0.0004962282949046399,
      "loss": 9.0723,
      "step": 54
    },
    {
      "epoch": 0.015658362989323844,
      "grad_norm": 0.4088612198829651,
      "learning_rate": 0.0004961571306575576,
      "loss": 8.5332,
      "step": 55
    },
    {
      "epoch": 0.01594306049822064,
      "grad_norm": 0.39859700202941895,
      "learning_rate": 0.0004960859664104754,
      "loss": 9.1191,
      "step": 56
    },
    {
      "epoch": 0.016227758007117437,
      "grad_norm": 0.39439693093299866,
      "learning_rate": 0.0004960148021633931,
      "loss": 8.0508,
      "step": 57
    },
    {
      "epoch": 0.016512455516014234,
      "grad_norm": 0.41306084394454956,
      "learning_rate": 0.0004959436379163109,
      "loss": 8.2998,
      "step": 58
    },
    {
      "epoch": 0.016797153024911034,
      "grad_norm": 0.4390506148338318,
      "learning_rate": 0.0004958724736692286,
      "loss": 8.1816,
      "step": 59
    },
    {
      "epoch": 0.01708185053380783,
      "grad_norm": 0.40014925599098206,
      "learning_rate": 0.0004958013094221464,
      "loss": 8.248,
      "step": 60
    },
    {
      "epoch": 0.017366548042704627,
      "grad_norm": 0.3710031807422638,
      "learning_rate": 0.000495730145175064,
      "loss": 7.7432,
      "step": 61
    },
    {
      "epoch": 0.017651245551601424,
      "grad_norm": 0.3743155002593994,
      "learning_rate": 0.0004956589809279818,
      "loss": 8.9805,
      "step": 62
    },
    {
      "epoch": 0.01793594306049822,
      "grad_norm": 0.4065592288970947,
      "learning_rate": 0.0004955878166808996,
      "loss": 8.9219,
      "step": 63
    },
    {
      "epoch": 0.018220640569395017,
      "grad_norm": 0.3826305866241455,
      "learning_rate": 0.0004955166524338172,
      "loss": 8.377,
      "step": 64
    },
    {
      "epoch": 0.018505338078291814,
      "grad_norm": 0.3792859613895416,
      "learning_rate": 0.000495445488186735,
      "loss": 7.8789,
      "step": 65
    },
    {
      "epoch": 0.018790035587188614,
      "grad_norm": 0.3933613896369934,
      "learning_rate": 0.0004953743239396527,
      "loss": 8.4092,
      "step": 66
    },
    {
      "epoch": 0.01907473309608541,
      "grad_norm": 0.39127129316329956,
      "learning_rate": 0.0004953031596925705,
      "loss": 7.9512,
      "step": 67
    },
    {
      "epoch": 0.019359430604982207,
      "grad_norm": 0.34493568539619446,
      "learning_rate": 0.0004952319954454881,
      "loss": 8.3105,
      "step": 68
    },
    {
      "epoch": 0.019644128113879004,
      "grad_norm": 0.3507670760154724,
      "learning_rate": 0.0004951608311984059,
      "loss": 7.9043,
      "step": 69
    },
    {
      "epoch": 0.0199288256227758,
      "grad_norm": 0.3754495084285736,
      "learning_rate": 0.0004950896669513237,
      "loss": 7.9131,
      "step": 70
    },
    {
      "epoch": 0.020213523131672597,
      "grad_norm": 0.3600054383277893,
      "learning_rate": 0.0004950185027042415,
      "loss": 8.1426,
      "step": 71
    },
    {
      "epoch": 0.020498220640569394,
      "grad_norm": 0.3194250166416168,
      "learning_rate": 0.0004949473384571591,
      "loss": 7.8213,
      "step": 72
    },
    {
      "epoch": 0.020782918149466194,
      "grad_norm": 0.3438234329223633,
      "learning_rate": 0.0004948761742100768,
      "loss": 8.1318,
      "step": 73
    },
    {
      "epoch": 0.02106761565836299,
      "grad_norm": 0.34680306911468506,
      "learning_rate": 0.0004948050099629946,
      "loss": 8.1611,
      "step": 74
    },
    {
      "epoch": 0.021352313167259787,
      "grad_norm": 0.3298954665660858,
      "learning_rate": 0.0004947338457159124,
      "loss": 7.6816,
      "step": 75
    },
    {
      "epoch": 0.021637010676156584,
      "grad_norm": 0.35546237230300903,
      "learning_rate": 0.0004946626814688301,
      "loss": 7.9482,
      "step": 76
    },
    {
      "epoch": 0.02192170818505338,
      "grad_norm": 0.31350335478782654,
      "learning_rate": 0.0004945915172217478,
      "loss": 7.916,
      "step": 77
    },
    {
      "epoch": 0.022206405693950177,
      "grad_norm": 0.2875521183013916,
      "learning_rate": 0.0004945203529746655,
      "loss": 8.1787,
      "step": 78
    },
    {
      "epoch": 0.022491103202846974,
      "grad_norm": 0.2984481453895569,
      "learning_rate": 0.0004944491887275833,
      "loss": 8.0068,
      "step": 79
    },
    {
      "epoch": 0.022775800711743774,
      "grad_norm": 0.32493481040000916,
      "learning_rate": 0.000494378024480501,
      "loss": 7.7959,
      "step": 80
    },
    {
      "epoch": 0.02306049822064057,
      "grad_norm": 0.3053205907344818,
      "learning_rate": 0.0004943068602334188,
      "loss": 7.7959,
      "step": 81
    },
    {
      "epoch": 0.023345195729537367,
      "grad_norm": 0.29084035754203796,
      "learning_rate": 0.0004942356959863365,
      "loss": 7.9629,
      "step": 82
    },
    {
      "epoch": 0.023629893238434164,
      "grad_norm": 0.3073493242263794,
      "learning_rate": 0.0004941645317392541,
      "loss": 7.7549,
      "step": 83
    },
    {
      "epoch": 0.02391459074733096,
      "grad_norm": 0.30308839678764343,
      "learning_rate": 0.0004940933674921719,
      "loss": 8.0791,
      "step": 84
    },
    {
      "epoch": 0.024199288256227757,
      "grad_norm": 0.38702166080474854,
      "learning_rate": 0.0004940222032450897,
      "loss": 7.2637,
      "step": 85
    },
    {
      "epoch": 0.024483985765124554,
      "grad_norm": 0.33084896206855774,
      "learning_rate": 0.0004939510389980074,
      "loss": 7.833,
      "step": 86
    },
    {
      "epoch": 0.024768683274021354,
      "grad_norm": 0.2839316129684448,
      "learning_rate": 0.0004938798747509251,
      "loss": 7.9053,
      "step": 87
    },
    {
      "epoch": 0.02505338078291815,
      "grad_norm": 0.35194167494773865,
      "learning_rate": 0.0004938087105038429,
      "loss": 7.4053,
      "step": 88
    },
    {
      "epoch": 0.025338078291814947,
      "grad_norm": 0.34102603793144226,
      "learning_rate": 0.0004937375462567606,
      "loss": 7.6406,
      "step": 89
    },
    {
      "epoch": 0.025622775800711744,
      "grad_norm": 0.34196290373802185,
      "learning_rate": 0.0004936663820096784,
      "loss": 7.7129,
      "step": 90
    },
    {
      "epoch": 0.02590747330960854,
      "grad_norm": 0.33342525362968445,
      "learning_rate": 0.000493595217762596,
      "loss": 7.1934,
      "step": 91
    },
    {
      "epoch": 0.026192170818505337,
      "grad_norm": 0.2916184365749359,
      "learning_rate": 0.0004935240535155138,
      "loss": 7.7559,
      "step": 92
    },
    {
      "epoch": 0.026476868327402134,
      "grad_norm": 0.3170425295829773,
      "learning_rate": 0.0004934528892684316,
      "loss": 7.7617,
      "step": 93
    },
    {
      "epoch": 0.026761565836298934,
      "grad_norm": 0.3077421188354492,
      "learning_rate": 0.0004933817250213493,
      "loss": 7.7852,
      "step": 94
    },
    {
      "epoch": 0.02704626334519573,
      "grad_norm": 0.34794363379478455,
      "learning_rate": 0.000493310560774267,
      "loss": 7.5391,
      "step": 95
    },
    {
      "epoch": 0.027330960854092527,
      "grad_norm": 0.27851757407188416,
      "learning_rate": 0.0004932393965271847,
      "loss": 8.0039,
      "step": 96
    },
    {
      "epoch": 0.027615658362989324,
      "grad_norm": 0.3033468723297119,
      "learning_rate": 0.0004931682322801025,
      "loss": 7.8076,
      "step": 97
    },
    {
      "epoch": 0.02790035587188612,
      "grad_norm": 0.29290416836738586,
      "learning_rate": 0.0004930970680330203,
      "loss": 7.8281,
      "step": 98
    },
    {
      "epoch": 0.028185053380782917,
      "grad_norm": 0.27407997846603394,
      "learning_rate": 0.000493025903785938,
      "loss": 7.8564,
      "step": 99
    },
    {
      "epoch": 0.028469750889679714,
      "grad_norm": 0.32197993993759155,
      "learning_rate": 0.0004929547395388557,
      "loss": 7.3594,
      "step": 100
    },
    {
      "epoch": 0.028754448398576514,
      "grad_norm": 0.27607426047325134,
      "learning_rate": 0.0004928835752917734,
      "loss": 7.9219,
      "step": 101
    },
    {
      "epoch": 0.02903914590747331,
      "grad_norm": 0.25733911991119385,
      "learning_rate": 0.0004928124110446911,
      "loss": 7.6963,
      "step": 102
    },
    {
      "epoch": 0.029323843416370107,
      "grad_norm": 0.33134591579437256,
      "learning_rate": 0.0004927412467976089,
      "loss": 7.5918,
      "step": 103
    },
    {
      "epoch": 0.029608540925266904,
      "grad_norm": 0.3447519838809967,
      "learning_rate": 0.0004926700825505266,
      "loss": 7.3262,
      "step": 104
    },
    {
      "epoch": 0.0298932384341637,
      "grad_norm": 0.30481886863708496,
      "learning_rate": 0.0004925989183034444,
      "loss": 7.5986,
      "step": 105
    },
    {
      "epoch": 0.030177935943060497,
      "grad_norm": 0.3810344338417053,
      "learning_rate": 0.000492527754056362,
      "loss": 7.2734,
      "step": 106
    },
    {
      "epoch": 0.030462633451957294,
      "grad_norm": 0.26661789417266846,
      "learning_rate": 0.0004924565898092798,
      "loss": 7.8818,
      "step": 107
    },
    {
      "epoch": 0.030747330960854094,
      "grad_norm": 0.3144116997718811,
      "learning_rate": 0.0004923854255621976,
      "loss": 7.4697,
      "step": 108
    },
    {
      "epoch": 0.03103202846975089,
      "grad_norm": 0.3234042525291443,
      "learning_rate": 0.0004923142613151153,
      "loss": 7.6611,
      "step": 109
    },
    {
      "epoch": 0.03131672597864769,
      "grad_norm": 0.2908277213573456,
      "learning_rate": 0.000492243097068033,
      "loss": 7.6924,
      "step": 110
    },
    {
      "epoch": 0.03160142348754449,
      "grad_norm": 0.282621830701828,
      "learning_rate": 0.0004921719328209507,
      "loss": 7.6748,
      "step": 111
    },
    {
      "epoch": 0.03188612099644128,
      "grad_norm": 0.25899139046669006,
      "learning_rate": 0.0004921007685738685,
      "loss": 7.7725,
      "step": 112
    },
    {
      "epoch": 0.03217081850533808,
      "grad_norm": 0.30150163173675537,
      "learning_rate": 0.0004920296043267863,
      "loss": 7.6016,
      "step": 113
    },
    {
      "epoch": 0.032455516014234874,
      "grad_norm": 0.3718966245651245,
      "learning_rate": 0.0004919584400797039,
      "loss": 7.0254,
      "step": 114
    },
    {
      "epoch": 0.032740213523131674,
      "grad_norm": 0.30984169244766235,
      "learning_rate": 0.0004918872758326217,
      "loss": 7.4707,
      "step": 115
    },
    {
      "epoch": 0.03302491103202847,
      "grad_norm": 0.25567907094955444,
      "learning_rate": 0.0004918161115855395,
      "loss": 7.668,
      "step": 116
    },
    {
      "epoch": 0.03330960854092527,
      "grad_norm": 0.38791143894195557,
      "learning_rate": 0.0004917449473384572,
      "loss": 7.3613,
      "step": 117
    },
    {
      "epoch": 0.03359430604982207,
      "grad_norm": 0.28487956523895264,
      "learning_rate": 0.0004916737830913749,
      "loss": 7.5039,
      "step": 118
    },
    {
      "epoch": 0.03387900355871886,
      "grad_norm": 0.4281052350997925,
      "learning_rate": 0.0004916026188442926,
      "loss": 7.0996,
      "step": 119
    },
    {
      "epoch": 0.03416370106761566,
      "grad_norm": 0.30284109711647034,
      "learning_rate": 0.0004915314545972104,
      "loss": 7.2402,
      "step": 120
    },
    {
      "epoch": 0.034448398576512454,
      "grad_norm": 0.3210189938545227,
      "learning_rate": 0.0004914602903501282,
      "loss": 7.3271,
      "step": 121
    },
    {
      "epoch": 0.034733096085409254,
      "grad_norm": 0.373404324054718,
      "learning_rate": 0.0004913891261030458,
      "loss": 6.876,
      "step": 122
    },
    {
      "epoch": 0.03501779359430605,
      "grad_norm": 0.34432628750801086,
      "learning_rate": 0.0004913179618559636,
      "loss": 7.5469,
      "step": 123
    },
    {
      "epoch": 0.03530249110320285,
      "grad_norm": 0.3198752999305725,
      "learning_rate": 0.0004912467976088813,
      "loss": 7.6924,
      "step": 124
    },
    {
      "epoch": 0.03558718861209965,
      "grad_norm": 0.38547608256340027,
      "learning_rate": 0.000491175633361799,
      "loss": 7.0049,
      "step": 125
    },
    {
      "epoch": 0.03587188612099644,
      "grad_norm": 0.34361645579338074,
      "learning_rate": 0.0004911044691147168,
      "loss": 7.3477,
      "step": 126
    },
    {
      "epoch": 0.03615658362989324,
      "grad_norm": 0.32878419756889343,
      "learning_rate": 0.0004910333048676345,
      "loss": 7.167,
      "step": 127
    },
    {
      "epoch": 0.036441281138790034,
      "grad_norm": 0.2640332579612732,
      "learning_rate": 0.0004909621406205523,
      "loss": 7.5791,
      "step": 128
    },
    {
      "epoch": 0.036725978647686834,
      "grad_norm": 0.34086892008781433,
      "learning_rate": 0.0004908909763734699,
      "loss": 7.5127,
      "step": 129
    },
    {
      "epoch": 0.03701067615658363,
      "grad_norm": 0.3057088851928711,
      "learning_rate": 0.0004908198121263877,
      "loss": 7.6523,
      "step": 130
    },
    {
      "epoch": 0.03729537366548043,
      "grad_norm": 0.325603187084198,
      "learning_rate": 0.0004907486478793055,
      "loss": 7.2979,
      "step": 131
    },
    {
      "epoch": 0.03758007117437723,
      "grad_norm": 0.3464660942554474,
      "learning_rate": 0.0004906774836322232,
      "loss": 7.4541,
      "step": 132
    },
    {
      "epoch": 0.03786476868327402,
      "grad_norm": 0.3628098964691162,
      "learning_rate": 0.0004906063193851409,
      "loss": 7.2969,
      "step": 133
    },
    {
      "epoch": 0.03814946619217082,
      "grad_norm": 0.33785781264305115,
      "learning_rate": 0.0004905351551380586,
      "loss": 7.4209,
      "step": 134
    },
    {
      "epoch": 0.038434163701067614,
      "grad_norm": 0.3329750895500183,
      "learning_rate": 0.0004904639908909764,
      "loss": 7.5674,
      "step": 135
    },
    {
      "epoch": 0.038718861209964414,
      "grad_norm": 0.3496299088001251,
      "learning_rate": 0.0004903928266438942,
      "loss": 7.4629,
      "step": 136
    },
    {
      "epoch": 0.03900355871886121,
      "grad_norm": 0.3395816385746002,
      "learning_rate": 0.0004903216623968118,
      "loss": 7.5713,
      "step": 137
    },
    {
      "epoch": 0.03928825622775801,
      "grad_norm": 0.3240024447441101,
      "learning_rate": 0.0004902504981497296,
      "loss": 7.6055,
      "step": 138
    },
    {
      "epoch": 0.03957295373665481,
      "grad_norm": 0.2986396253108978,
      "learning_rate": 0.0004901793339026473,
      "loss": 7.4951,
      "step": 139
    },
    {
      "epoch": 0.0398576512455516,
      "grad_norm": 0.30460289120674133,
      "learning_rate": 0.000490108169655565,
      "loss": 7.7354,
      "step": 140
    },
    {
      "epoch": 0.0401423487544484,
      "grad_norm": 0.2872254252433777,
      "learning_rate": 0.0004900370054084828,
      "loss": 7.7559,
      "step": 141
    },
    {
      "epoch": 0.040427046263345194,
      "grad_norm": 0.3015432059764862,
      "learning_rate": 0.0004899658411614005,
      "loss": 7.7129,
      "step": 142
    },
    {
      "epoch": 0.040711743772241994,
      "grad_norm": 0.33976587653160095,
      "learning_rate": 0.0004898946769143183,
      "loss": 7.4961,
      "step": 143
    },
    {
      "epoch": 0.04099644128113879,
      "grad_norm": 0.3869113028049469,
      "learning_rate": 0.000489823512667236,
      "loss": 7.2979,
      "step": 144
    },
    {
      "epoch": 0.04128113879003559,
      "grad_norm": 0.3606325685977936,
      "learning_rate": 0.0004897523484201537,
      "loss": 7.4873,
      "step": 145
    },
    {
      "epoch": 0.04156583629893239,
      "grad_norm": 0.3285134434700012,
      "learning_rate": 0.0004896811841730715,
      "loss": 7.8018,
      "step": 146
    },
    {
      "epoch": 0.04185053380782918,
      "grad_norm": 0.371952086687088,
      "learning_rate": 0.0004896100199259892,
      "loss": 7.2168,
      "step": 147
    },
    {
      "epoch": 0.04213523131672598,
      "grad_norm": 0.3381865620613098,
      "learning_rate": 0.0004895388556789069,
      "loss": 7.3682,
      "step": 148
    },
    {
      "epoch": 0.042419928825622774,
      "grad_norm": 0.30638259649276733,
      "learning_rate": 0.0004894676914318247,
      "loss": 7.459,
      "step": 149
    },
    {
      "epoch": 0.042704626334519574,
      "grad_norm": 0.3618326187133789,
      "learning_rate": 0.0004893965271847424,
      "loss": 7.293,
      "step": 150
    },
    {
      "epoch": 0.04298932384341637,
      "grad_norm": 0.34743648767471313,
      "learning_rate": 0.0004893253629376602,
      "loss": 7.2275,
      "step": 151
    },
    {
      "epoch": 0.04327402135231317,
      "grad_norm": 0.24103522300720215,
      "learning_rate": 0.0004892541986905778,
      "loss": 7.6436,
      "step": 152
    },
    {
      "epoch": 0.04355871886120997,
      "grad_norm": 0.47605815529823303,
      "learning_rate": 0.0004891830344434956,
      "loss": 6.8848,
      "step": 153
    },
    {
      "epoch": 0.04384341637010676,
      "grad_norm": 0.2737746834754944,
      "learning_rate": 0.0004891118701964134,
      "loss": 7.5234,
      "step": 154
    },
    {
      "epoch": 0.04412811387900356,
      "grad_norm": 0.34337320923805237,
      "learning_rate": 0.000489040705949331,
      "loss": 7.3691,
      "step": 155
    },
    {
      "epoch": 0.044412811387900354,
      "grad_norm": 0.29171985387802124,
      "learning_rate": 0.0004889695417022488,
      "loss": 7.7021,
      "step": 156
    },
    {
      "epoch": 0.044697508896797154,
      "grad_norm": 0.2947385907173157,
      "learning_rate": 0.0004888983774551665,
      "loss": 7.7725,
      "step": 157
    },
    {
      "epoch": 0.04498220640569395,
      "grad_norm": 0.33179420232772827,
      "learning_rate": 0.0004888272132080843,
      "loss": 7.4795,
      "step": 158
    },
    {
      "epoch": 0.04526690391459075,
      "grad_norm": 0.30874258279800415,
      "learning_rate": 0.000488756048961002,
      "loss": 7.667,
      "step": 159
    },
    {
      "epoch": 0.04555160142348755,
      "grad_norm": 0.37757524847984314,
      "learning_rate": 0.0004886848847139197,
      "loss": 7.1436,
      "step": 160
    },
    {
      "epoch": 0.04583629893238434,
      "grad_norm": 0.35947147011756897,
      "learning_rate": 0.0004886137204668375,
      "loss": 7.2451,
      "step": 161
    },
    {
      "epoch": 0.04612099644128114,
      "grad_norm": 0.38363611698150635,
      "learning_rate": 0.0004885425562197552,
      "loss": 7.2061,
      "step": 162
    },
    {
      "epoch": 0.046405693950177934,
      "grad_norm": 0.3002108335494995,
      "learning_rate": 0.0004884713919726729,
      "loss": 7.6328,
      "step": 163
    },
    {
      "epoch": 0.046690391459074734,
      "grad_norm": 0.36899691820144653,
      "learning_rate": 0.0004884002277255907,
      "loss": 7.2451,
      "step": 164
    },
    {
      "epoch": 0.04697508896797153,
      "grad_norm": 0.3589462637901306,
      "learning_rate": 0.0004883290634785084,
      "loss": 7.4961,
      "step": 165
    },
    {
      "epoch": 0.04725978647686833,
      "grad_norm": 0.3247394263744354,
      "learning_rate": 0.00048825789923142616,
      "loss": 7.4971,
      "step": 166
    },
    {
      "epoch": 0.04754448398576513,
      "grad_norm": 0.4397582411766052,
      "learning_rate": 0.00048818673498434383,
      "loss": 7.2158,
      "step": 167
    },
    {
      "epoch": 0.04782918149466192,
      "grad_norm": 0.34369221329689026,
      "learning_rate": 0.0004881155707372616,
      "loss": 7.3965,
      "step": 168
    },
    {
      "epoch": 0.04811387900355872,
      "grad_norm": 0.315603107213974,
      "learning_rate": 0.00048804440649017933,
      "loss": 7.4365,
      "step": 169
    },
    {
      "epoch": 0.048398576512455514,
      "grad_norm": 0.5103448033332825,
      "learning_rate": 0.0004879732422430971,
      "loss": 6.8682,
      "step": 170
    },
    {
      "epoch": 0.048683274021352314,
      "grad_norm": 0.43841269612312317,
      "learning_rate": 0.00048790207799601483,
      "loss": 7.3838,
      "step": 171
    },
    {
      "epoch": 0.04896797153024911,
      "grad_norm": 0.43405452370643616,
      "learning_rate": 0.00048783091374893255,
      "loss": 6.9912,
      "step": 172
    },
    {
      "epoch": 0.04925266903914591,
      "grad_norm": 0.4368981122970581,
      "learning_rate": 0.0004877597495018503,
      "loss": 7.1592,
      "step": 173
    },
    {
      "epoch": 0.04953736654804271,
      "grad_norm": 0.3200138211250305,
      "learning_rate": 0.000487688585254768,
      "loss": 7.5615,
      "step": 174
    },
    {
      "epoch": 0.0498220640569395,
      "grad_norm": 0.3974745571613312,
      "learning_rate": 0.0004876174210076857,
      "loss": 7.0674,
      "step": 175
    },
    {
      "epoch": 0.0501067615658363,
      "grad_norm": 0.3726997673511505,
      "learning_rate": 0.0004875462567606035,
      "loss": 7.3057,
      "step": 176
    },
    {
      "epoch": 0.050391459074733094,
      "grad_norm": 0.37156978249549866,
      "learning_rate": 0.0004874750925135212,
      "loss": 7.3672,
      "step": 177
    },
    {
      "epoch": 0.050676156583629894,
      "grad_norm": 0.3446742594242096,
      "learning_rate": 0.000487403928266439,
      "loss": 7.3379,
      "step": 178
    },
    {
      "epoch": 0.05096085409252669,
      "grad_norm": 0.3899495601654053,
      "learning_rate": 0.00048733276401935667,
      "loss": 7.0742,
      "step": 179
    },
    {
      "epoch": 0.05124555160142349,
      "grad_norm": 0.365047425031662,
      "learning_rate": 0.0004872615997722744,
      "loss": 7.2949,
      "step": 180
    },
    {
      "epoch": 0.05153024911032029,
      "grad_norm": 0.3096078336238861,
      "learning_rate": 0.00048719043552519217,
      "loss": 7.585,
      "step": 181
    },
    {
      "epoch": 0.05181494661921708,
      "grad_norm": 0.3642439842224121,
      "learning_rate": 0.0004871192712781099,
      "loss": 7.293,
      "step": 182
    },
    {
      "epoch": 0.05209964412811388,
      "grad_norm": 0.31257525086402893,
      "learning_rate": 0.00048704810703102767,
      "loss": 7.6279,
      "step": 183
    },
    {
      "epoch": 0.052384341637010674,
      "grad_norm": 0.3482113480567932,
      "learning_rate": 0.00048697694278394533,
      "loss": 7.4062,
      "step": 184
    },
    {
      "epoch": 0.052669039145907474,
      "grad_norm": 0.40061259269714355,
      "learning_rate": 0.00048690577853686306,
      "loss": 6.9482,
      "step": 185
    },
    {
      "epoch": 0.05295373665480427,
      "grad_norm": 0.4041823744773865,
      "learning_rate": 0.00048683461428978083,
      "loss": 7.1836,
      "step": 186
    },
    {
      "epoch": 0.05323843416370107,
      "grad_norm": 0.41112077236175537,
      "learning_rate": 0.00048676345004269856,
      "loss": 6.8662,
      "step": 187
    },
    {
      "epoch": 0.05352313167259787,
      "grad_norm": 0.38859066367149353,
      "learning_rate": 0.00048669228579561633,
      "loss": 7.626,
      "step": 188
    },
    {
      "epoch": 0.05380782918149466,
      "grad_norm": 0.3483869135379791,
      "learning_rate": 0.00048662112154853406,
      "loss": 7.2656,
      "step": 189
    },
    {
      "epoch": 0.05409252669039146,
      "grad_norm": 0.36553943157196045,
      "learning_rate": 0.0004865499573014517,
      "loss": 7.0918,
      "step": 190
    },
    {
      "epoch": 0.054377224199288254,
      "grad_norm": 0.45641273260116577,
      "learning_rate": 0.0004864787930543695,
      "loss": 7.1328,
      "step": 191
    },
    {
      "epoch": 0.054661921708185054,
      "grad_norm": 0.5525060296058655,
      "learning_rate": 0.0004864076288072872,
      "loss": 6.6797,
      "step": 192
    },
    {
      "epoch": 0.05494661921708185,
      "grad_norm": 0.35605090856552124,
      "learning_rate": 0.00048633646456020495,
      "loss": 7.3145,
      "step": 193
    },
    {
      "epoch": 0.05523131672597865,
      "grad_norm": 0.33720216155052185,
      "learning_rate": 0.0004862653003131227,
      "loss": 7.583,
      "step": 194
    },
    {
      "epoch": 0.05551601423487545,
      "grad_norm": 0.39028072357177734,
      "learning_rate": 0.0004861941360660404,
      "loss": 7.041,
      "step": 195
    },
    {
      "epoch": 0.05580071174377224,
      "grad_norm": 0.37324032187461853,
      "learning_rate": 0.00048612297181895817,
      "loss": 7.5771,
      "step": 196
    },
    {
      "epoch": 0.05608540925266904,
      "grad_norm": 0.3692830204963684,
      "learning_rate": 0.0004860518075718759,
      "loss": 6.9414,
      "step": 197
    },
    {
      "epoch": 0.056370106761565834,
      "grad_norm": 0.3400883674621582,
      "learning_rate": 0.0004859806433247936,
      "loss": 7.6484,
      "step": 198
    },
    {
      "epoch": 0.056654804270462635,
      "grad_norm": 0.36927610635757446,
      "learning_rate": 0.0004859094790777114,
      "loss": 7.0127,
      "step": 199
    },
    {
      "epoch": 0.05693950177935943,
      "grad_norm": 0.3512347936630249,
      "learning_rate": 0.0004858383148306291,
      "loss": 7.5596,
      "step": 200
    },
    {
      "epoch": 0.05693950177935943,
      "eval_bleu": 0.04428465159832603,
      "eval_loss": 7.046875,
      "eval_runtime": 206.9516,
      "eval_samples_per_second": 1.372,
      "eval_steps_per_second": 0.087,
      "step": 200
    }
  ],
  "logging_steps": 1,
  "max_steps": 7026,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 200,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 435595247616000.0,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
