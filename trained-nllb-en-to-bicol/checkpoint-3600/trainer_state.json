{
  "best_global_step": 400,
  "best_metric": 7.00390625,
  "best_model_checkpoint": "trained-nllb-en-to-bicol\\checkpoint-400",
  "epoch": 1.025053380782918,
  "eval_steps": 200,
  "global_step": 3600,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.00028469750889679714,
      "grad_norm": 0.13050267100334167,
      "learning_rate": 0.0005,
      "loss": 11.8242,
      "step": 1
    },
    {
      "epoch": 0.0005693950177935943,
      "grad_norm": 0.16353872418403625,
      "learning_rate": 0.0004999288357529178,
      "loss": 12.7012,
      "step": 2
    },
    {
      "epoch": 0.0008540925266903915,
      "grad_norm": 0.1825779378414154,
      "learning_rate": 0.0004998576715058355,
      "loss": 11.4883,
      "step": 3
    },
    {
      "epoch": 0.0011387900355871886,
      "grad_norm": 0.2419777810573578,
      "learning_rate": 0.0004997865072587532,
      "loss": 11.9531,
      "step": 4
    },
    {
      "epoch": 0.0014234875444839859,
      "grad_norm": 0.31029146909713745,
      "learning_rate": 0.0004997153430116709,
      "loss": 11.7852,
      "step": 5
    },
    {
      "epoch": 0.001708185053380783,
      "grad_norm": 0.3981788754463196,
      "learning_rate": 0.0004996441787645887,
      "loss": 11.8242,
      "step": 6
    },
    {
      "epoch": 0.00199288256227758,
      "grad_norm": 0.44912195205688477,
      "learning_rate": 0.0004995730145175065,
      "loss": 11.1836,
      "step": 7
    },
    {
      "epoch": 0.002277580071174377,
      "grad_norm": 0.5424309372901917,
      "learning_rate": 0.0004995018502704241,
      "loss": 11.6504,
      "step": 8
    },
    {
      "epoch": 0.002562277580071174,
      "grad_norm": 0.6373918056488037,
      "learning_rate": 0.0004994306860233419,
      "loss": 11.3926,
      "step": 9
    },
    {
      "epoch": 0.0028469750889679717,
      "grad_norm": 0.7767484188079834,
      "learning_rate": 0.0004993595217762596,
      "loss": 11.5098,
      "step": 10
    },
    {
      "epoch": 0.003131672597864769,
      "grad_norm": 0.7795925736427307,
      "learning_rate": 0.0004992883575291773,
      "loss": 12.1816,
      "step": 11
    },
    {
      "epoch": 0.003416370106761566,
      "grad_norm": 0.5827488303184509,
      "learning_rate": 0.0004992171932820951,
      "loss": 11.4355,
      "step": 12
    },
    {
      "epoch": 0.003701067615658363,
      "grad_norm": 0.5459573864936829,
      "learning_rate": 0.0004991460290350128,
      "loss": 11.0156,
      "step": 13
    },
    {
      "epoch": 0.00398576512455516,
      "grad_norm": 0.513734757900238,
      "learning_rate": 0.0004990748647879306,
      "loss": 11.6016,
      "step": 14
    },
    {
      "epoch": 0.004270462633451958,
      "grad_norm": 0.5204881429672241,
      "learning_rate": 0.0004990037005408483,
      "loss": 11.1738,
      "step": 15
    },
    {
      "epoch": 0.004555160142348754,
      "grad_norm": 0.5095188021659851,
      "learning_rate": 0.000498932536293766,
      "loss": 11.6738,
      "step": 16
    },
    {
      "epoch": 0.004839857651245552,
      "grad_norm": 0.5002894997596741,
      "learning_rate": 0.0004988613720466838,
      "loss": 11.1172,
      "step": 17
    },
    {
      "epoch": 0.005124555160142348,
      "grad_norm": 0.5175882577896118,
      "learning_rate": 0.0004987902077996015,
      "loss": 11.709,
      "step": 18
    },
    {
      "epoch": 0.005409252669039146,
      "grad_norm": 0.481122225522995,
      "learning_rate": 0.0004987190435525192,
      "loss": 11.2383,
      "step": 19
    },
    {
      "epoch": 0.0056939501779359435,
      "grad_norm": 0.4916481077671051,
      "learning_rate": 0.000498647879305437,
      "loss": 11.1875,
      "step": 20
    },
    {
      "epoch": 0.00597864768683274,
      "grad_norm": 0.4639431834220886,
      "learning_rate": 0.0004985767150583547,
      "loss": 10.959,
      "step": 21
    },
    {
      "epoch": 0.006263345195729538,
      "grad_norm": 0.471029669046402,
      "learning_rate": 0.0004985055508112725,
      "loss": 10.5039,
      "step": 22
    },
    {
      "epoch": 0.006548042704626334,
      "grad_norm": 0.5494607090950012,
      "learning_rate": 0.0004984343865641901,
      "loss": 10.7871,
      "step": 23
    },
    {
      "epoch": 0.006832740213523132,
      "grad_norm": 0.542377769947052,
      "learning_rate": 0.0004983632223171079,
      "loss": 10.7402,
      "step": 24
    },
    {
      "epoch": 0.0071174377224199285,
      "grad_norm": 0.6290743947029114,
      "learning_rate": 0.0004982920580700257,
      "loss": 11.1191,
      "step": 25
    },
    {
      "epoch": 0.007402135231316726,
      "grad_norm": 0.5208401083946228,
      "learning_rate": 0.0004982208938229434,
      "loss": 10.3535,
      "step": 26
    },
    {
      "epoch": 0.0076868327402135235,
      "grad_norm": 0.5653201937675476,
      "learning_rate": 0.0004981497295758611,
      "loss": 10.2676,
      "step": 27
    },
    {
      "epoch": 0.00797153024911032,
      "grad_norm": 0.5279284119606018,
      "learning_rate": 0.0004980785653287788,
      "loss": 10.6504,
      "step": 28
    },
    {
      "epoch": 0.008256227758007117,
      "grad_norm": 0.5287833213806152,
      "learning_rate": 0.0004980074010816966,
      "loss": 9.9648,
      "step": 29
    },
    {
      "epoch": 0.008540925266903915,
      "grad_norm": 0.4779149293899536,
      "learning_rate": 0.0004979362368346143,
      "loss": 11.1738,
      "step": 30
    },
    {
      "epoch": 0.008825622775800712,
      "grad_norm": 0.4311399459838867,
      "learning_rate": 0.000497865072587532,
      "loss": 10.7617,
      "step": 31
    },
    {
      "epoch": 0.009110320284697508,
      "grad_norm": 0.4880855083465576,
      "learning_rate": 0.0004977939083404498,
      "loss": 10.6113,
      "step": 32
    },
    {
      "epoch": 0.009395017793594307,
      "grad_norm": 0.4771440029144287,
      "learning_rate": 0.0004977227440933675,
      "loss": 10.2031,
      "step": 33
    },
    {
      "epoch": 0.009679715302491104,
      "grad_norm": 0.4335767924785614,
      "learning_rate": 0.0004976515798462852,
      "loss": 9.9766,
      "step": 34
    },
    {
      "epoch": 0.0099644128113879,
      "grad_norm": 0.47250261902809143,
      "learning_rate": 0.000497580415599203,
      "loss": 10.582,
      "step": 35
    },
    {
      "epoch": 0.010249110320284697,
      "grad_norm": 0.5027048587799072,
      "learning_rate": 0.0004975092513521207,
      "loss": 9.252,
      "step": 36
    },
    {
      "epoch": 0.010533807829181495,
      "grad_norm": 0.4936491847038269,
      "learning_rate": 0.0004974380871050385,
      "loss": 9.9648,
      "step": 37
    },
    {
      "epoch": 0.010818505338078292,
      "grad_norm": 0.4429067373275757,
      "learning_rate": 0.0004973669228579561,
      "loss": 10.1699,
      "step": 38
    },
    {
      "epoch": 0.011103202846975089,
      "grad_norm": 0.43701812624931335,
      "learning_rate": 0.0004972957586108739,
      "loss": 10.1875,
      "step": 39
    },
    {
      "epoch": 0.011387900355871887,
      "grad_norm": 0.44829320907592773,
      "learning_rate": 0.0004972245943637917,
      "loss": 9.7109,
      "step": 40
    },
    {
      "epoch": 0.011672597864768684,
      "grad_norm": 0.44973376393318176,
      "learning_rate": 0.0004971534301167094,
      "loss": 9.9023,
      "step": 41
    },
    {
      "epoch": 0.01195729537366548,
      "grad_norm": 0.4918513894081116,
      "learning_rate": 0.0004970822658696271,
      "loss": 10.1934,
      "step": 42
    },
    {
      "epoch": 0.012241992882562277,
      "grad_norm": 0.4607585668563843,
      "learning_rate": 0.0004970111016225449,
      "loss": 9.9941,
      "step": 43
    },
    {
      "epoch": 0.012526690391459075,
      "grad_norm": 0.45157185196876526,
      "learning_rate": 0.0004969399373754626,
      "loss": 9.6797,
      "step": 44
    },
    {
      "epoch": 0.012811387900355872,
      "grad_norm": 0.45155540108680725,
      "learning_rate": 0.0004968687731283804,
      "loss": 9.8535,
      "step": 45
    },
    {
      "epoch": 0.013096085409252669,
      "grad_norm": 0.45471465587615967,
      "learning_rate": 0.000496797608881298,
      "loss": 9.6699,
      "step": 46
    },
    {
      "epoch": 0.013380782918149467,
      "grad_norm": 0.45609229803085327,
      "learning_rate": 0.0004967264446342158,
      "loss": 9.834,
      "step": 47
    },
    {
      "epoch": 0.013665480427046264,
      "grad_norm": 0.46065598726272583,
      "learning_rate": 0.0004966552803871336,
      "loss": 9.1973,
      "step": 48
    },
    {
      "epoch": 0.01395017793594306,
      "grad_norm": 0.4395595192909241,
      "learning_rate": 0.0004965841161400512,
      "loss": 9.3379,
      "step": 49
    },
    {
      "epoch": 0.014234875444839857,
      "grad_norm": 0.42804303765296936,
      "learning_rate": 0.0004965129518929689,
      "loss": 8.1885,
      "step": 50
    },
    {
      "epoch": 0.014519572953736655,
      "grad_norm": 0.4336685538291931,
      "learning_rate": 0.0004964417876458867,
      "loss": 9.4141,
      "step": 51
    },
    {
      "epoch": 0.014804270462633452,
      "grad_norm": 0.4083915054798126,
      "learning_rate": 0.0004963706233988045,
      "loss": 9.2363,
      "step": 52
    },
    {
      "epoch": 0.015088967971530249,
      "grad_norm": 0.39727815985679626,
      "learning_rate": 0.0004962994591517222,
      "loss": 8.5439,
      "step": 53
    },
    {
      "epoch": 0.015373665480427047,
      "grad_norm": 0.4038327634334564,
      "learning_rate": 0.0004962282949046399,
      "loss": 9.0723,
      "step": 54
    },
    {
      "epoch": 0.015658362989323844,
      "grad_norm": 0.4088612198829651,
      "learning_rate": 0.0004961571306575576,
      "loss": 8.5332,
      "step": 55
    },
    {
      "epoch": 0.01594306049822064,
      "grad_norm": 0.39859700202941895,
      "learning_rate": 0.0004960859664104754,
      "loss": 9.1191,
      "step": 56
    },
    {
      "epoch": 0.016227758007117437,
      "grad_norm": 0.39439693093299866,
      "learning_rate": 0.0004960148021633931,
      "loss": 8.0508,
      "step": 57
    },
    {
      "epoch": 0.016512455516014234,
      "grad_norm": 0.41306084394454956,
      "learning_rate": 0.0004959436379163109,
      "loss": 8.2998,
      "step": 58
    },
    {
      "epoch": 0.016797153024911034,
      "grad_norm": 0.4390506148338318,
      "learning_rate": 0.0004958724736692286,
      "loss": 8.1816,
      "step": 59
    },
    {
      "epoch": 0.01708185053380783,
      "grad_norm": 0.40014925599098206,
      "learning_rate": 0.0004958013094221464,
      "loss": 8.248,
      "step": 60
    },
    {
      "epoch": 0.017366548042704627,
      "grad_norm": 0.3710031807422638,
      "learning_rate": 0.000495730145175064,
      "loss": 7.7432,
      "step": 61
    },
    {
      "epoch": 0.017651245551601424,
      "grad_norm": 0.3743155002593994,
      "learning_rate": 0.0004956589809279818,
      "loss": 8.9805,
      "step": 62
    },
    {
      "epoch": 0.01793594306049822,
      "grad_norm": 0.4065592288970947,
      "learning_rate": 0.0004955878166808996,
      "loss": 8.9219,
      "step": 63
    },
    {
      "epoch": 0.018220640569395017,
      "grad_norm": 0.3826305866241455,
      "learning_rate": 0.0004955166524338172,
      "loss": 8.377,
      "step": 64
    },
    {
      "epoch": 0.018505338078291814,
      "grad_norm": 0.3792859613895416,
      "learning_rate": 0.000495445488186735,
      "loss": 7.8789,
      "step": 65
    },
    {
      "epoch": 0.018790035587188614,
      "grad_norm": 0.3933613896369934,
      "learning_rate": 0.0004953743239396527,
      "loss": 8.4092,
      "step": 66
    },
    {
      "epoch": 0.01907473309608541,
      "grad_norm": 0.39127129316329956,
      "learning_rate": 0.0004953031596925705,
      "loss": 7.9512,
      "step": 67
    },
    {
      "epoch": 0.019359430604982207,
      "grad_norm": 0.34493568539619446,
      "learning_rate": 0.0004952319954454881,
      "loss": 8.3105,
      "step": 68
    },
    {
      "epoch": 0.019644128113879004,
      "grad_norm": 0.3507670760154724,
      "learning_rate": 0.0004951608311984059,
      "loss": 7.9043,
      "step": 69
    },
    {
      "epoch": 0.0199288256227758,
      "grad_norm": 0.3754495084285736,
      "learning_rate": 0.0004950896669513237,
      "loss": 7.9131,
      "step": 70
    },
    {
      "epoch": 0.020213523131672597,
      "grad_norm": 0.3600054383277893,
      "learning_rate": 0.0004950185027042415,
      "loss": 8.1426,
      "step": 71
    },
    {
      "epoch": 0.020498220640569394,
      "grad_norm": 0.3194250166416168,
      "learning_rate": 0.0004949473384571591,
      "loss": 7.8213,
      "step": 72
    },
    {
      "epoch": 0.020782918149466194,
      "grad_norm": 0.3438234329223633,
      "learning_rate": 0.0004948761742100768,
      "loss": 8.1318,
      "step": 73
    },
    {
      "epoch": 0.02106761565836299,
      "grad_norm": 0.34680306911468506,
      "learning_rate": 0.0004948050099629946,
      "loss": 8.1611,
      "step": 74
    },
    {
      "epoch": 0.021352313167259787,
      "grad_norm": 0.3298954665660858,
      "learning_rate": 0.0004947338457159124,
      "loss": 7.6816,
      "step": 75
    },
    {
      "epoch": 0.021637010676156584,
      "grad_norm": 0.35546237230300903,
      "learning_rate": 0.0004946626814688301,
      "loss": 7.9482,
      "step": 76
    },
    {
      "epoch": 0.02192170818505338,
      "grad_norm": 0.31350335478782654,
      "learning_rate": 0.0004945915172217478,
      "loss": 7.916,
      "step": 77
    },
    {
      "epoch": 0.022206405693950177,
      "grad_norm": 0.2875521183013916,
      "learning_rate": 0.0004945203529746655,
      "loss": 8.1787,
      "step": 78
    },
    {
      "epoch": 0.022491103202846974,
      "grad_norm": 0.2984481453895569,
      "learning_rate": 0.0004944491887275833,
      "loss": 8.0068,
      "step": 79
    },
    {
      "epoch": 0.022775800711743774,
      "grad_norm": 0.32493481040000916,
      "learning_rate": 0.000494378024480501,
      "loss": 7.7959,
      "step": 80
    },
    {
      "epoch": 0.02306049822064057,
      "grad_norm": 0.3053205907344818,
      "learning_rate": 0.0004943068602334188,
      "loss": 7.7959,
      "step": 81
    },
    {
      "epoch": 0.023345195729537367,
      "grad_norm": 0.29084035754203796,
      "learning_rate": 0.0004942356959863365,
      "loss": 7.9629,
      "step": 82
    },
    {
      "epoch": 0.023629893238434164,
      "grad_norm": 0.3073493242263794,
      "learning_rate": 0.0004941645317392541,
      "loss": 7.7549,
      "step": 83
    },
    {
      "epoch": 0.02391459074733096,
      "grad_norm": 0.30308839678764343,
      "learning_rate": 0.0004940933674921719,
      "loss": 8.0791,
      "step": 84
    },
    {
      "epoch": 0.024199288256227757,
      "grad_norm": 0.38702166080474854,
      "learning_rate": 0.0004940222032450897,
      "loss": 7.2637,
      "step": 85
    },
    {
      "epoch": 0.024483985765124554,
      "grad_norm": 0.33084896206855774,
      "learning_rate": 0.0004939510389980074,
      "loss": 7.833,
      "step": 86
    },
    {
      "epoch": 0.024768683274021354,
      "grad_norm": 0.2839316129684448,
      "learning_rate": 0.0004938798747509251,
      "loss": 7.9053,
      "step": 87
    },
    {
      "epoch": 0.02505338078291815,
      "grad_norm": 0.35194167494773865,
      "learning_rate": 0.0004938087105038429,
      "loss": 7.4053,
      "step": 88
    },
    {
      "epoch": 0.025338078291814947,
      "grad_norm": 0.34102603793144226,
      "learning_rate": 0.0004937375462567606,
      "loss": 7.6406,
      "step": 89
    },
    {
      "epoch": 0.025622775800711744,
      "grad_norm": 0.34196290373802185,
      "learning_rate": 0.0004936663820096784,
      "loss": 7.7129,
      "step": 90
    },
    {
      "epoch": 0.02590747330960854,
      "grad_norm": 0.33342525362968445,
      "learning_rate": 0.000493595217762596,
      "loss": 7.1934,
      "step": 91
    },
    {
      "epoch": 0.026192170818505337,
      "grad_norm": 0.2916184365749359,
      "learning_rate": 0.0004935240535155138,
      "loss": 7.7559,
      "step": 92
    },
    {
      "epoch": 0.026476868327402134,
      "grad_norm": 0.3170425295829773,
      "learning_rate": 0.0004934528892684316,
      "loss": 7.7617,
      "step": 93
    },
    {
      "epoch": 0.026761565836298934,
      "grad_norm": 0.3077421188354492,
      "learning_rate": 0.0004933817250213493,
      "loss": 7.7852,
      "step": 94
    },
    {
      "epoch": 0.02704626334519573,
      "grad_norm": 0.34794363379478455,
      "learning_rate": 0.000493310560774267,
      "loss": 7.5391,
      "step": 95
    },
    {
      "epoch": 0.027330960854092527,
      "grad_norm": 0.27851757407188416,
      "learning_rate": 0.0004932393965271847,
      "loss": 8.0039,
      "step": 96
    },
    {
      "epoch": 0.027615658362989324,
      "grad_norm": 0.3033468723297119,
      "learning_rate": 0.0004931682322801025,
      "loss": 7.8076,
      "step": 97
    },
    {
      "epoch": 0.02790035587188612,
      "grad_norm": 0.29290416836738586,
      "learning_rate": 0.0004930970680330203,
      "loss": 7.8281,
      "step": 98
    },
    {
      "epoch": 0.028185053380782917,
      "grad_norm": 0.27407997846603394,
      "learning_rate": 0.000493025903785938,
      "loss": 7.8564,
      "step": 99
    },
    {
      "epoch": 0.028469750889679714,
      "grad_norm": 0.32197993993759155,
      "learning_rate": 0.0004929547395388557,
      "loss": 7.3594,
      "step": 100
    },
    {
      "epoch": 0.028754448398576514,
      "grad_norm": 0.27607426047325134,
      "learning_rate": 0.0004928835752917734,
      "loss": 7.9219,
      "step": 101
    },
    {
      "epoch": 0.02903914590747331,
      "grad_norm": 0.25733911991119385,
      "learning_rate": 0.0004928124110446911,
      "loss": 7.6963,
      "step": 102
    },
    {
      "epoch": 0.029323843416370107,
      "grad_norm": 0.33134591579437256,
      "learning_rate": 0.0004927412467976089,
      "loss": 7.5918,
      "step": 103
    },
    {
      "epoch": 0.029608540925266904,
      "grad_norm": 0.3447519838809967,
      "learning_rate": 0.0004926700825505266,
      "loss": 7.3262,
      "step": 104
    },
    {
      "epoch": 0.0298932384341637,
      "grad_norm": 0.30481886863708496,
      "learning_rate": 0.0004925989183034444,
      "loss": 7.5986,
      "step": 105
    },
    {
      "epoch": 0.030177935943060497,
      "grad_norm": 0.3810344338417053,
      "learning_rate": 0.000492527754056362,
      "loss": 7.2734,
      "step": 106
    },
    {
      "epoch": 0.030462633451957294,
      "grad_norm": 0.26661789417266846,
      "learning_rate": 0.0004924565898092798,
      "loss": 7.8818,
      "step": 107
    },
    {
      "epoch": 0.030747330960854094,
      "grad_norm": 0.3144116997718811,
      "learning_rate": 0.0004923854255621976,
      "loss": 7.4697,
      "step": 108
    },
    {
      "epoch": 0.03103202846975089,
      "grad_norm": 0.3234042525291443,
      "learning_rate": 0.0004923142613151153,
      "loss": 7.6611,
      "step": 109
    },
    {
      "epoch": 0.03131672597864769,
      "grad_norm": 0.2908277213573456,
      "learning_rate": 0.000492243097068033,
      "loss": 7.6924,
      "step": 110
    },
    {
      "epoch": 0.03160142348754449,
      "grad_norm": 0.282621830701828,
      "learning_rate": 0.0004921719328209507,
      "loss": 7.6748,
      "step": 111
    },
    {
      "epoch": 0.03188612099644128,
      "grad_norm": 0.25899139046669006,
      "learning_rate": 0.0004921007685738685,
      "loss": 7.7725,
      "step": 112
    },
    {
      "epoch": 0.03217081850533808,
      "grad_norm": 0.30150163173675537,
      "learning_rate": 0.0004920296043267863,
      "loss": 7.6016,
      "step": 113
    },
    {
      "epoch": 0.032455516014234874,
      "grad_norm": 0.3718966245651245,
      "learning_rate": 0.0004919584400797039,
      "loss": 7.0254,
      "step": 114
    },
    {
      "epoch": 0.032740213523131674,
      "grad_norm": 0.30984169244766235,
      "learning_rate": 0.0004918872758326217,
      "loss": 7.4707,
      "step": 115
    },
    {
      "epoch": 0.03302491103202847,
      "grad_norm": 0.25567907094955444,
      "learning_rate": 0.0004918161115855395,
      "loss": 7.668,
      "step": 116
    },
    {
      "epoch": 0.03330960854092527,
      "grad_norm": 0.38791143894195557,
      "learning_rate": 0.0004917449473384572,
      "loss": 7.3613,
      "step": 117
    },
    {
      "epoch": 0.03359430604982207,
      "grad_norm": 0.28487956523895264,
      "learning_rate": 0.0004916737830913749,
      "loss": 7.5039,
      "step": 118
    },
    {
      "epoch": 0.03387900355871886,
      "grad_norm": 0.4281052350997925,
      "learning_rate": 0.0004916026188442926,
      "loss": 7.0996,
      "step": 119
    },
    {
      "epoch": 0.03416370106761566,
      "grad_norm": 0.30284109711647034,
      "learning_rate": 0.0004915314545972104,
      "loss": 7.2402,
      "step": 120
    },
    {
      "epoch": 0.034448398576512454,
      "grad_norm": 0.3210189938545227,
      "learning_rate": 0.0004914602903501282,
      "loss": 7.3271,
      "step": 121
    },
    {
      "epoch": 0.034733096085409254,
      "grad_norm": 0.373404324054718,
      "learning_rate": 0.0004913891261030458,
      "loss": 6.876,
      "step": 122
    },
    {
      "epoch": 0.03501779359430605,
      "grad_norm": 0.34432628750801086,
      "learning_rate": 0.0004913179618559636,
      "loss": 7.5469,
      "step": 123
    },
    {
      "epoch": 0.03530249110320285,
      "grad_norm": 0.3198752999305725,
      "learning_rate": 0.0004912467976088813,
      "loss": 7.6924,
      "step": 124
    },
    {
      "epoch": 0.03558718861209965,
      "grad_norm": 0.38547608256340027,
      "learning_rate": 0.000491175633361799,
      "loss": 7.0049,
      "step": 125
    },
    {
      "epoch": 0.03587188612099644,
      "grad_norm": 0.34361645579338074,
      "learning_rate": 0.0004911044691147168,
      "loss": 7.3477,
      "step": 126
    },
    {
      "epoch": 0.03615658362989324,
      "grad_norm": 0.32878419756889343,
      "learning_rate": 0.0004910333048676345,
      "loss": 7.167,
      "step": 127
    },
    {
      "epoch": 0.036441281138790034,
      "grad_norm": 0.2640332579612732,
      "learning_rate": 0.0004909621406205523,
      "loss": 7.5791,
      "step": 128
    },
    {
      "epoch": 0.036725978647686834,
      "grad_norm": 0.34086892008781433,
      "learning_rate": 0.0004908909763734699,
      "loss": 7.5127,
      "step": 129
    },
    {
      "epoch": 0.03701067615658363,
      "grad_norm": 0.3057088851928711,
      "learning_rate": 0.0004908198121263877,
      "loss": 7.6523,
      "step": 130
    },
    {
      "epoch": 0.03729537366548043,
      "grad_norm": 0.325603187084198,
      "learning_rate": 0.0004907486478793055,
      "loss": 7.2979,
      "step": 131
    },
    {
      "epoch": 0.03758007117437723,
      "grad_norm": 0.3464660942554474,
      "learning_rate": 0.0004906774836322232,
      "loss": 7.4541,
      "step": 132
    },
    {
      "epoch": 0.03786476868327402,
      "grad_norm": 0.3628098964691162,
      "learning_rate": 0.0004906063193851409,
      "loss": 7.2969,
      "step": 133
    },
    {
      "epoch": 0.03814946619217082,
      "grad_norm": 0.33785781264305115,
      "learning_rate": 0.0004905351551380586,
      "loss": 7.4209,
      "step": 134
    },
    {
      "epoch": 0.038434163701067614,
      "grad_norm": 0.3329750895500183,
      "learning_rate": 0.0004904639908909764,
      "loss": 7.5674,
      "step": 135
    },
    {
      "epoch": 0.038718861209964414,
      "grad_norm": 0.3496299088001251,
      "learning_rate": 0.0004903928266438942,
      "loss": 7.4629,
      "step": 136
    },
    {
      "epoch": 0.03900355871886121,
      "grad_norm": 0.3395816385746002,
      "learning_rate": 0.0004903216623968118,
      "loss": 7.5713,
      "step": 137
    },
    {
      "epoch": 0.03928825622775801,
      "grad_norm": 0.3240024447441101,
      "learning_rate": 0.0004902504981497296,
      "loss": 7.6055,
      "step": 138
    },
    {
      "epoch": 0.03957295373665481,
      "grad_norm": 0.2986396253108978,
      "learning_rate": 0.0004901793339026473,
      "loss": 7.4951,
      "step": 139
    },
    {
      "epoch": 0.0398576512455516,
      "grad_norm": 0.30460289120674133,
      "learning_rate": 0.000490108169655565,
      "loss": 7.7354,
      "step": 140
    },
    {
      "epoch": 0.0401423487544484,
      "grad_norm": 0.2872254252433777,
      "learning_rate": 0.0004900370054084828,
      "loss": 7.7559,
      "step": 141
    },
    {
      "epoch": 0.040427046263345194,
      "grad_norm": 0.3015432059764862,
      "learning_rate": 0.0004899658411614005,
      "loss": 7.7129,
      "step": 142
    },
    {
      "epoch": 0.040711743772241994,
      "grad_norm": 0.33976587653160095,
      "learning_rate": 0.0004898946769143183,
      "loss": 7.4961,
      "step": 143
    },
    {
      "epoch": 0.04099644128113879,
      "grad_norm": 0.3869113028049469,
      "learning_rate": 0.000489823512667236,
      "loss": 7.2979,
      "step": 144
    },
    {
      "epoch": 0.04128113879003559,
      "grad_norm": 0.3606325685977936,
      "learning_rate": 0.0004897523484201537,
      "loss": 7.4873,
      "step": 145
    },
    {
      "epoch": 0.04156583629893239,
      "grad_norm": 0.3285134434700012,
      "learning_rate": 0.0004896811841730715,
      "loss": 7.8018,
      "step": 146
    },
    {
      "epoch": 0.04185053380782918,
      "grad_norm": 0.371952086687088,
      "learning_rate": 0.0004896100199259892,
      "loss": 7.2168,
      "step": 147
    },
    {
      "epoch": 0.04213523131672598,
      "grad_norm": 0.3381865620613098,
      "learning_rate": 0.0004895388556789069,
      "loss": 7.3682,
      "step": 148
    },
    {
      "epoch": 0.042419928825622774,
      "grad_norm": 0.30638259649276733,
      "learning_rate": 0.0004894676914318247,
      "loss": 7.459,
      "step": 149
    },
    {
      "epoch": 0.042704626334519574,
      "grad_norm": 0.3618326187133789,
      "learning_rate": 0.0004893965271847424,
      "loss": 7.293,
      "step": 150
    },
    {
      "epoch": 0.04298932384341637,
      "grad_norm": 0.34743648767471313,
      "learning_rate": 0.0004893253629376602,
      "loss": 7.2275,
      "step": 151
    },
    {
      "epoch": 0.04327402135231317,
      "grad_norm": 0.24103522300720215,
      "learning_rate": 0.0004892541986905778,
      "loss": 7.6436,
      "step": 152
    },
    {
      "epoch": 0.04355871886120997,
      "grad_norm": 0.47605815529823303,
      "learning_rate": 0.0004891830344434956,
      "loss": 6.8848,
      "step": 153
    },
    {
      "epoch": 0.04384341637010676,
      "grad_norm": 0.2737746834754944,
      "learning_rate": 0.0004891118701964134,
      "loss": 7.5234,
      "step": 154
    },
    {
      "epoch": 0.04412811387900356,
      "grad_norm": 0.34337320923805237,
      "learning_rate": 0.000489040705949331,
      "loss": 7.3691,
      "step": 155
    },
    {
      "epoch": 0.044412811387900354,
      "grad_norm": 0.29171985387802124,
      "learning_rate": 0.0004889695417022488,
      "loss": 7.7021,
      "step": 156
    },
    {
      "epoch": 0.044697508896797154,
      "grad_norm": 0.2947385907173157,
      "learning_rate": 0.0004888983774551665,
      "loss": 7.7725,
      "step": 157
    },
    {
      "epoch": 0.04498220640569395,
      "grad_norm": 0.33179420232772827,
      "learning_rate": 0.0004888272132080843,
      "loss": 7.4795,
      "step": 158
    },
    {
      "epoch": 0.04526690391459075,
      "grad_norm": 0.30874258279800415,
      "learning_rate": 0.000488756048961002,
      "loss": 7.667,
      "step": 159
    },
    {
      "epoch": 0.04555160142348755,
      "grad_norm": 0.37757524847984314,
      "learning_rate": 0.0004886848847139197,
      "loss": 7.1436,
      "step": 160
    },
    {
      "epoch": 0.04583629893238434,
      "grad_norm": 0.35947147011756897,
      "learning_rate": 0.0004886137204668375,
      "loss": 7.2451,
      "step": 161
    },
    {
      "epoch": 0.04612099644128114,
      "grad_norm": 0.38363611698150635,
      "learning_rate": 0.0004885425562197552,
      "loss": 7.2061,
      "step": 162
    },
    {
      "epoch": 0.046405693950177934,
      "grad_norm": 0.3002108335494995,
      "learning_rate": 0.0004884713919726729,
      "loss": 7.6328,
      "step": 163
    },
    {
      "epoch": 0.046690391459074734,
      "grad_norm": 0.36899691820144653,
      "learning_rate": 0.0004884002277255907,
      "loss": 7.2451,
      "step": 164
    },
    {
      "epoch": 0.04697508896797153,
      "grad_norm": 0.3589462637901306,
      "learning_rate": 0.0004883290634785084,
      "loss": 7.4961,
      "step": 165
    },
    {
      "epoch": 0.04725978647686833,
      "grad_norm": 0.3247394263744354,
      "learning_rate": 0.00048825789923142616,
      "loss": 7.4971,
      "step": 166
    },
    {
      "epoch": 0.04754448398576513,
      "grad_norm": 0.4397582411766052,
      "learning_rate": 0.00048818673498434383,
      "loss": 7.2158,
      "step": 167
    },
    {
      "epoch": 0.04782918149466192,
      "grad_norm": 0.34369221329689026,
      "learning_rate": 0.0004881155707372616,
      "loss": 7.3965,
      "step": 168
    },
    {
      "epoch": 0.04811387900355872,
      "grad_norm": 0.315603107213974,
      "learning_rate": 0.00048804440649017933,
      "loss": 7.4365,
      "step": 169
    },
    {
      "epoch": 0.048398576512455514,
      "grad_norm": 0.5103448033332825,
      "learning_rate": 0.0004879732422430971,
      "loss": 6.8682,
      "step": 170
    },
    {
      "epoch": 0.048683274021352314,
      "grad_norm": 0.43841269612312317,
      "learning_rate": 0.00048790207799601483,
      "loss": 7.3838,
      "step": 171
    },
    {
      "epoch": 0.04896797153024911,
      "grad_norm": 0.43405452370643616,
      "learning_rate": 0.00048783091374893255,
      "loss": 6.9912,
      "step": 172
    },
    {
      "epoch": 0.04925266903914591,
      "grad_norm": 0.4368981122970581,
      "learning_rate": 0.0004877597495018503,
      "loss": 7.1592,
      "step": 173
    },
    {
      "epoch": 0.04953736654804271,
      "grad_norm": 0.3200138211250305,
      "learning_rate": 0.000487688585254768,
      "loss": 7.5615,
      "step": 174
    },
    {
      "epoch": 0.0498220640569395,
      "grad_norm": 0.3974745571613312,
      "learning_rate": 0.0004876174210076857,
      "loss": 7.0674,
      "step": 175
    },
    {
      "epoch": 0.0501067615658363,
      "grad_norm": 0.3726997673511505,
      "learning_rate": 0.0004875462567606035,
      "loss": 7.3057,
      "step": 176
    },
    {
      "epoch": 0.050391459074733094,
      "grad_norm": 0.37156978249549866,
      "learning_rate": 0.0004874750925135212,
      "loss": 7.3672,
      "step": 177
    },
    {
      "epoch": 0.050676156583629894,
      "grad_norm": 0.3446742594242096,
      "learning_rate": 0.000487403928266439,
      "loss": 7.3379,
      "step": 178
    },
    {
      "epoch": 0.05096085409252669,
      "grad_norm": 0.3899495601654053,
      "learning_rate": 0.00048733276401935667,
      "loss": 7.0742,
      "step": 179
    },
    {
      "epoch": 0.05124555160142349,
      "grad_norm": 0.365047425031662,
      "learning_rate": 0.0004872615997722744,
      "loss": 7.2949,
      "step": 180
    },
    {
      "epoch": 0.05153024911032029,
      "grad_norm": 0.3096078336238861,
      "learning_rate": 0.00048719043552519217,
      "loss": 7.585,
      "step": 181
    },
    {
      "epoch": 0.05181494661921708,
      "grad_norm": 0.3642439842224121,
      "learning_rate": 0.0004871192712781099,
      "loss": 7.293,
      "step": 182
    },
    {
      "epoch": 0.05209964412811388,
      "grad_norm": 0.31257525086402893,
      "learning_rate": 0.00048704810703102767,
      "loss": 7.6279,
      "step": 183
    },
    {
      "epoch": 0.052384341637010674,
      "grad_norm": 0.3482113480567932,
      "learning_rate": 0.00048697694278394533,
      "loss": 7.4062,
      "step": 184
    },
    {
      "epoch": 0.052669039145907474,
      "grad_norm": 0.40061259269714355,
      "learning_rate": 0.00048690577853686306,
      "loss": 6.9482,
      "step": 185
    },
    {
      "epoch": 0.05295373665480427,
      "grad_norm": 0.4041823744773865,
      "learning_rate": 0.00048683461428978083,
      "loss": 7.1836,
      "step": 186
    },
    {
      "epoch": 0.05323843416370107,
      "grad_norm": 0.41112077236175537,
      "learning_rate": 0.00048676345004269856,
      "loss": 6.8662,
      "step": 187
    },
    {
      "epoch": 0.05352313167259787,
      "grad_norm": 0.38859066367149353,
      "learning_rate": 0.00048669228579561633,
      "loss": 7.626,
      "step": 188
    },
    {
      "epoch": 0.05380782918149466,
      "grad_norm": 0.3483869135379791,
      "learning_rate": 0.00048662112154853406,
      "loss": 7.2656,
      "step": 189
    },
    {
      "epoch": 0.05409252669039146,
      "grad_norm": 0.36553943157196045,
      "learning_rate": 0.0004865499573014517,
      "loss": 7.0918,
      "step": 190
    },
    {
      "epoch": 0.054377224199288254,
      "grad_norm": 0.45641273260116577,
      "learning_rate": 0.0004864787930543695,
      "loss": 7.1328,
      "step": 191
    },
    {
      "epoch": 0.054661921708185054,
      "grad_norm": 0.5525060296058655,
      "learning_rate": 0.0004864076288072872,
      "loss": 6.6797,
      "step": 192
    },
    {
      "epoch": 0.05494661921708185,
      "grad_norm": 0.35605090856552124,
      "learning_rate": 0.00048633646456020495,
      "loss": 7.3145,
      "step": 193
    },
    {
      "epoch": 0.05523131672597865,
      "grad_norm": 0.33720216155052185,
      "learning_rate": 0.0004862653003131227,
      "loss": 7.583,
      "step": 194
    },
    {
      "epoch": 0.05551601423487545,
      "grad_norm": 0.39028072357177734,
      "learning_rate": 0.0004861941360660404,
      "loss": 7.041,
      "step": 195
    },
    {
      "epoch": 0.05580071174377224,
      "grad_norm": 0.37324032187461853,
      "learning_rate": 0.00048612297181895817,
      "loss": 7.5771,
      "step": 196
    },
    {
      "epoch": 0.05608540925266904,
      "grad_norm": 0.3692830204963684,
      "learning_rate": 0.0004860518075718759,
      "loss": 6.9414,
      "step": 197
    },
    {
      "epoch": 0.056370106761565834,
      "grad_norm": 0.3400883674621582,
      "learning_rate": 0.0004859806433247936,
      "loss": 7.6484,
      "step": 198
    },
    {
      "epoch": 0.056654804270462635,
      "grad_norm": 0.36927610635757446,
      "learning_rate": 0.0004859094790777114,
      "loss": 7.0127,
      "step": 199
    },
    {
      "epoch": 0.05693950177935943,
      "grad_norm": 0.3512347936630249,
      "learning_rate": 0.0004858383148306291,
      "loss": 7.5596,
      "step": 200
    },
    {
      "epoch": 0.05693950177935943,
      "eval_bleu": 0.04428465159832603,
      "eval_loss": 7.046875,
      "eval_runtime": 206.9516,
      "eval_samples_per_second": 1.372,
      "eval_steps_per_second": 0.087,
      "step": 200
    },
    {
      "epoch": 0.05722419928825623,
      "grad_norm": 0.40993812680244446,
      "learning_rate": 0.00048576715058354684,
      "loss": 7.165,
      "step": 201
    },
    {
      "epoch": 0.05750889679715303,
      "grad_norm": 0.37675940990448,
      "learning_rate": 0.00048569598633646456,
      "loss": 7.5469,
      "step": 202
    },
    {
      "epoch": 0.05779359430604982,
      "grad_norm": 0.41827306151390076,
      "learning_rate": 0.0004856248220893823,
      "loss": 7.252,
      "step": 203
    },
    {
      "epoch": 0.05807829181494662,
      "grad_norm": 0.40496769547462463,
      "learning_rate": 0.00048555365784230006,
      "loss": 7.3262,
      "step": 204
    },
    {
      "epoch": 0.058362989323843414,
      "grad_norm": 0.4544559121131897,
      "learning_rate": 0.0004854824935952178,
      "loss": 7.2686,
      "step": 205
    },
    {
      "epoch": 0.058647686832740215,
      "grad_norm": 0.43348002433776855,
      "learning_rate": 0.0004854113293481355,
      "loss": 7.1436,
      "step": 206
    },
    {
      "epoch": 0.05893238434163701,
      "grad_norm": 0.5123205780982971,
      "learning_rate": 0.00048534016510105323,
      "loss": 6.9443,
      "step": 207
    },
    {
      "epoch": 0.05921708185053381,
      "grad_norm": 0.3473433554172516,
      "learning_rate": 0.00048526900085397095,
      "loss": 7.4775,
      "step": 208
    },
    {
      "epoch": 0.05950177935943061,
      "grad_norm": 0.3604001998901367,
      "learning_rate": 0.00048519783660688873,
      "loss": 7.5098,
      "step": 209
    },
    {
      "epoch": 0.0597864768683274,
      "grad_norm": 0.33064237236976624,
      "learning_rate": 0.00048512667235980645,
      "loss": 7.6191,
      "step": 210
    },
    {
      "epoch": 0.0600711743772242,
      "grad_norm": 0.5796976089477539,
      "learning_rate": 0.0004850555081127242,
      "loss": 6.7324,
      "step": 211
    },
    {
      "epoch": 0.060355871886120994,
      "grad_norm": 0.3960364758968353,
      "learning_rate": 0.0004849843438656419,
      "loss": 7.2354,
      "step": 212
    },
    {
      "epoch": 0.060640569395017795,
      "grad_norm": 0.3528068959712982,
      "learning_rate": 0.0004849131796185596,
      "loss": 7.4775,
      "step": 213
    },
    {
      "epoch": 0.06092526690391459,
      "grad_norm": 0.33591997623443604,
      "learning_rate": 0.0004848420153714774,
      "loss": 7.5264,
      "step": 214
    },
    {
      "epoch": 0.06120996441281139,
      "grad_norm": 0.3542058765888214,
      "learning_rate": 0.0004847708511243951,
      "loss": 7.4365,
      "step": 215
    },
    {
      "epoch": 0.06149466192170819,
      "grad_norm": 0.4078296422958374,
      "learning_rate": 0.00048469968687731284,
      "loss": 6.9795,
      "step": 216
    },
    {
      "epoch": 0.06177935943060498,
      "grad_norm": 0.3528203070163727,
      "learning_rate": 0.0004846285226302306,
      "loss": 7.251,
      "step": 217
    },
    {
      "epoch": 0.06206405693950178,
      "grad_norm": 0.454461932182312,
      "learning_rate": 0.0004845573583831483,
      "loss": 6.876,
      "step": 218
    },
    {
      "epoch": 0.062348754448398575,
      "grad_norm": 0.39278143644332886,
      "learning_rate": 0.00048448619413606606,
      "loss": 7.2432,
      "step": 219
    },
    {
      "epoch": 0.06263345195729537,
      "grad_norm": 0.4267103374004364,
      "learning_rate": 0.0004844150298889838,
      "loss": 6.8594,
      "step": 220
    },
    {
      "epoch": 0.06291814946619217,
      "grad_norm": 0.3892408609390259,
      "learning_rate": 0.0004843438656419015,
      "loss": 6.8838,
      "step": 221
    },
    {
      "epoch": 0.06320284697508897,
      "grad_norm": 0.3549274802207947,
      "learning_rate": 0.0004842727013948193,
      "loss": 7.2236,
      "step": 222
    },
    {
      "epoch": 0.06348754448398576,
      "grad_norm": 0.33616870641708374,
      "learning_rate": 0.000484201537147737,
      "loss": 7.7422,
      "step": 223
    },
    {
      "epoch": 0.06377224199288256,
      "grad_norm": 0.33883076906204224,
      "learning_rate": 0.0004841303729006547,
      "loss": 7.5654,
      "step": 224
    },
    {
      "epoch": 0.06405693950177936,
      "grad_norm": 0.3884200155735016,
      "learning_rate": 0.00048405920865357246,
      "loss": 7.3457,
      "step": 225
    },
    {
      "epoch": 0.06434163701067616,
      "grad_norm": 0.3521203398704529,
      "learning_rate": 0.0004839880444064902,
      "loss": 7.2559,
      "step": 226
    },
    {
      "epoch": 0.06462633451957295,
      "grad_norm": 0.5035505294799805,
      "learning_rate": 0.00048391688015940796,
      "loss": 7.1816,
      "step": 227
    },
    {
      "epoch": 0.06491103202846975,
      "grad_norm": 0.37743714451789856,
      "learning_rate": 0.0004838457159123257,
      "loss": 7.5049,
      "step": 228
    },
    {
      "epoch": 0.06519572953736655,
      "grad_norm": 0.3604282736778259,
      "learning_rate": 0.00048377455166524335,
      "loss": 7.7832,
      "step": 229
    },
    {
      "epoch": 0.06548042704626335,
      "grad_norm": 0.6729333400726318,
      "learning_rate": 0.0004837033874181611,
      "loss": 6.8105,
      "step": 230
    },
    {
      "epoch": 0.06576512455516015,
      "grad_norm": 0.41351523995399475,
      "learning_rate": 0.00048363222317107885,
      "loss": 6.7354,
      "step": 231
    },
    {
      "epoch": 0.06604982206405693,
      "grad_norm": 0.3609699308872223,
      "learning_rate": 0.0004835610589239966,
      "loss": 7.3662,
      "step": 232
    },
    {
      "epoch": 0.06633451957295373,
      "grad_norm": 0.34872451424598694,
      "learning_rate": 0.00048348989467691435,
      "loss": 7.4785,
      "step": 233
    },
    {
      "epoch": 0.06661921708185053,
      "grad_norm": 0.3711255192756653,
      "learning_rate": 0.00048341873042983207,
      "loss": 7.2041,
      "step": 234
    },
    {
      "epoch": 0.06690391459074733,
      "grad_norm": 0.4340938925743103,
      "learning_rate": 0.0004833475661827498,
      "loss": 6.8232,
      "step": 235
    },
    {
      "epoch": 0.06718861209964413,
      "grad_norm": 0.3729588985443115,
      "learning_rate": 0.0004832764019356675,
      "loss": 7.2197,
      "step": 236
    },
    {
      "epoch": 0.06747330960854092,
      "grad_norm": 0.429115355014801,
      "learning_rate": 0.00048320523768858524,
      "loss": 7.1836,
      "step": 237
    },
    {
      "epoch": 0.06775800711743772,
      "grad_norm": 0.47582298517227173,
      "learning_rate": 0.000483134073441503,
      "loss": 6.8828,
      "step": 238
    },
    {
      "epoch": 0.06804270462633452,
      "grad_norm": 0.44467341899871826,
      "learning_rate": 0.00048306290919442074,
      "loss": 7.1777,
      "step": 239
    },
    {
      "epoch": 0.06832740213523132,
      "grad_norm": 0.438344269990921,
      "learning_rate": 0.00048299174494733846,
      "loss": 6.8701,
      "step": 240
    },
    {
      "epoch": 0.06861209964412811,
      "grad_norm": 0.4138439893722534,
      "learning_rate": 0.0004829205807002562,
      "loss": 7.1123,
      "step": 241
    },
    {
      "epoch": 0.06889679715302491,
      "grad_norm": 0.39080458879470825,
      "learning_rate": 0.0004828494164531739,
      "loss": 7.3242,
      "step": 242
    },
    {
      "epoch": 0.06918149466192171,
      "grad_norm": 0.43729132413864136,
      "learning_rate": 0.0004827782522060917,
      "loss": 6.7295,
      "step": 243
    },
    {
      "epoch": 0.06946619217081851,
      "grad_norm": 0.4565962851047516,
      "learning_rate": 0.0004827070879590094,
      "loss": 7.4229,
      "step": 244
    },
    {
      "epoch": 0.06975088967971531,
      "grad_norm": 0.4301145374774933,
      "learning_rate": 0.0004826359237119272,
      "loss": 7.3975,
      "step": 245
    },
    {
      "epoch": 0.0700355871886121,
      "grad_norm": 0.4343007504940033,
      "learning_rate": 0.00048256475946484485,
      "loss": 7.1396,
      "step": 246
    },
    {
      "epoch": 0.0703202846975089,
      "grad_norm": 0.3824312388896942,
      "learning_rate": 0.0004824935952177626,
      "loss": 7.3086,
      "step": 247
    },
    {
      "epoch": 0.0706049822064057,
      "grad_norm": 0.38155433535575867,
      "learning_rate": 0.00048242243097068035,
      "loss": 7.1982,
      "step": 248
    },
    {
      "epoch": 0.0708896797153025,
      "grad_norm": 0.36901432275772095,
      "learning_rate": 0.00048235126672359807,
      "loss": 7.3613,
      "step": 249
    },
    {
      "epoch": 0.0711743772241993,
      "grad_norm": 0.368551105260849,
      "learning_rate": 0.00048228010247651585,
      "loss": 7.3828,
      "step": 250
    },
    {
      "epoch": 0.07145907473309608,
      "grad_norm": 0.3908926844596863,
      "learning_rate": 0.00048220893822943357,
      "loss": 6.9717,
      "step": 251
    },
    {
      "epoch": 0.07174377224199288,
      "grad_norm": 0.46843716502189636,
      "learning_rate": 0.00048213777398235124,
      "loss": 7.2441,
      "step": 252
    },
    {
      "epoch": 0.07202846975088968,
      "grad_norm": 0.4673958420753479,
      "learning_rate": 0.000482066609735269,
      "loss": 7.4209,
      "step": 253
    },
    {
      "epoch": 0.07231316725978648,
      "grad_norm": 0.45724111795425415,
      "learning_rate": 0.00048199544548818674,
      "loss": 6.9893,
      "step": 254
    },
    {
      "epoch": 0.07259786476868327,
      "grad_norm": 0.48041409254074097,
      "learning_rate": 0.00048192428124110446,
      "loss": 6.8398,
      "step": 255
    },
    {
      "epoch": 0.07288256227758007,
      "grad_norm": 0.3993391692638397,
      "learning_rate": 0.00048185311699402224,
      "loss": 7.3496,
      "step": 256
    },
    {
      "epoch": 0.07316725978647687,
      "grad_norm": 0.38099411129951477,
      "learning_rate": 0.0004817819527469399,
      "loss": 7.5664,
      "step": 257
    },
    {
      "epoch": 0.07345195729537367,
      "grad_norm": 0.36448702216148376,
      "learning_rate": 0.0004817107884998577,
      "loss": 7.5957,
      "step": 258
    },
    {
      "epoch": 0.07373665480427047,
      "grad_norm": 0.3705407977104187,
      "learning_rate": 0.0004816396242527754,
      "loss": 7.4092,
      "step": 259
    },
    {
      "epoch": 0.07402135231316725,
      "grad_norm": 0.4222451150417328,
      "learning_rate": 0.00048156846000569313,
      "loss": 7.2832,
      "step": 260
    },
    {
      "epoch": 0.07430604982206405,
      "grad_norm": 0.43533411622047424,
      "learning_rate": 0.0004814972957586109,
      "loss": 7.0967,
      "step": 261
    },
    {
      "epoch": 0.07459074733096085,
      "grad_norm": 0.45843270421028137,
      "learning_rate": 0.00048142613151152863,
      "loss": 7.1719,
      "step": 262
    },
    {
      "epoch": 0.07487544483985765,
      "grad_norm": 0.38327649235725403,
      "learning_rate": 0.00048135496726444635,
      "loss": 7.2725,
      "step": 263
    },
    {
      "epoch": 0.07516014234875446,
      "grad_norm": 0.4202622175216675,
      "learning_rate": 0.0004812838030173641,
      "loss": 7.4424,
      "step": 264
    },
    {
      "epoch": 0.07544483985765124,
      "grad_norm": 0.5358533263206482,
      "learning_rate": 0.0004812126387702818,
      "loss": 6.6904,
      "step": 265
    },
    {
      "epoch": 0.07572953736654804,
      "grad_norm": 0.4957423806190491,
      "learning_rate": 0.0004811414745231996,
      "loss": 6.8877,
      "step": 266
    },
    {
      "epoch": 0.07601423487544484,
      "grad_norm": 0.40162715315818787,
      "learning_rate": 0.0004810703102761173,
      "loss": 7.21,
      "step": 267
    },
    {
      "epoch": 0.07629893238434164,
      "grad_norm": 0.46528732776641846,
      "learning_rate": 0.000480999146029035,
      "loss": 7.2451,
      "step": 268
    },
    {
      "epoch": 0.07658362989323843,
      "grad_norm": 0.4028266668319702,
      "learning_rate": 0.00048092798178195274,
      "loss": 7.4043,
      "step": 269
    },
    {
      "epoch": 0.07686832740213523,
      "grad_norm": 0.46121296286582947,
      "learning_rate": 0.00048085681753487047,
      "loss": 7.2305,
      "step": 270
    },
    {
      "epoch": 0.07715302491103203,
      "grad_norm": 0.43850409984588623,
      "learning_rate": 0.00048078565328778824,
      "loss": 7.1523,
      "step": 271
    },
    {
      "epoch": 0.07743772241992883,
      "grad_norm": 0.36082831025123596,
      "learning_rate": 0.00048071448904070597,
      "loss": 7.6426,
      "step": 272
    },
    {
      "epoch": 0.07772241992882563,
      "grad_norm": 0.42451992630958557,
      "learning_rate": 0.0004806433247936237,
      "loss": 7.5566,
      "step": 273
    },
    {
      "epoch": 0.07800711743772241,
      "grad_norm": 0.5189881324768066,
      "learning_rate": 0.0004805721605465414,
      "loss": 6.9717,
      "step": 274
    },
    {
      "epoch": 0.07829181494661921,
      "grad_norm": 0.34993991255760193,
      "learning_rate": 0.00048050099629945914,
      "loss": 7.75,
      "step": 275
    },
    {
      "epoch": 0.07857651245551601,
      "grad_norm": 0.38793134689331055,
      "learning_rate": 0.0004804298320523769,
      "loss": 7.0029,
      "step": 276
    },
    {
      "epoch": 0.07886120996441282,
      "grad_norm": 0.4095827639102936,
      "learning_rate": 0.00048035866780529464,
      "loss": 7.1084,
      "step": 277
    },
    {
      "epoch": 0.07914590747330962,
      "grad_norm": 0.400534987449646,
      "learning_rate": 0.00048028750355821236,
      "loss": 7.5254,
      "step": 278
    },
    {
      "epoch": 0.0794306049822064,
      "grad_norm": 0.38447555899620056,
      "learning_rate": 0.00048021633931113013,
      "loss": 7.5615,
      "step": 279
    },
    {
      "epoch": 0.0797153024911032,
      "grad_norm": 0.4068629741668701,
      "learning_rate": 0.0004801451750640478,
      "loss": 7.459,
      "step": 280
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.39288195967674255,
      "learning_rate": 0.0004800740108169656,
      "loss": 7.1357,
      "step": 281
    },
    {
      "epoch": 0.0802846975088968,
      "grad_norm": 0.44044890999794006,
      "learning_rate": 0.0004800028465698833,
      "loss": 6.8838,
      "step": 282
    },
    {
      "epoch": 0.08056939501779359,
      "grad_norm": 0.3920339047908783,
      "learning_rate": 0.000479931682322801,
      "loss": 7.4258,
      "step": 283
    },
    {
      "epoch": 0.08085409252669039,
      "grad_norm": 0.42204195261001587,
      "learning_rate": 0.0004798605180757188,
      "loss": 7.2412,
      "step": 284
    },
    {
      "epoch": 0.08113879003558719,
      "grad_norm": 0.4086628258228302,
      "learning_rate": 0.00047978935382863647,
      "loss": 7.4141,
      "step": 285
    },
    {
      "epoch": 0.08142348754448399,
      "grad_norm": 0.48516544699668884,
      "learning_rate": 0.0004797181895815542,
      "loss": 6.7148,
      "step": 286
    },
    {
      "epoch": 0.08170818505338079,
      "grad_norm": 0.4337480366230011,
      "learning_rate": 0.00047964702533447197,
      "loss": 7.1201,
      "step": 287
    },
    {
      "epoch": 0.08199288256227757,
      "grad_norm": 0.4319247305393219,
      "learning_rate": 0.0004795758610873897,
      "loss": 7.1279,
      "step": 288
    },
    {
      "epoch": 0.08227758007117437,
      "grad_norm": 0.49747705459594727,
      "learning_rate": 0.00047950469684030747,
      "loss": 7.2217,
      "step": 289
    },
    {
      "epoch": 0.08256227758007118,
      "grad_norm": 0.39577603340148926,
      "learning_rate": 0.0004794335325932252,
      "loss": 7.4053,
      "step": 290
    },
    {
      "epoch": 0.08284697508896798,
      "grad_norm": 0.4405271112918854,
      "learning_rate": 0.00047936236834614286,
      "loss": 7.0312,
      "step": 291
    },
    {
      "epoch": 0.08313167259786478,
      "grad_norm": 0.3082965910434723,
      "learning_rate": 0.00047929120409906064,
      "loss": 7.7031,
      "step": 292
    },
    {
      "epoch": 0.08341637010676156,
      "grad_norm": 0.5239378809928894,
      "learning_rate": 0.00047922003985197836,
      "loss": 6.9287,
      "step": 293
    },
    {
      "epoch": 0.08370106761565836,
      "grad_norm": 0.38422203063964844,
      "learning_rate": 0.00047914887560489614,
      "loss": 7.251,
      "step": 294
    },
    {
      "epoch": 0.08398576512455516,
      "grad_norm": 0.44853416085243225,
      "learning_rate": 0.00047907771135781386,
      "loss": 7.0537,
      "step": 295
    },
    {
      "epoch": 0.08427046263345196,
      "grad_norm": 0.3908516466617584,
      "learning_rate": 0.0004790065471107316,
      "loss": 7.5029,
      "step": 296
    },
    {
      "epoch": 0.08455516014234875,
      "grad_norm": 0.5178523659706116,
      "learning_rate": 0.0004789353828636493,
      "loss": 6.6699,
      "step": 297
    },
    {
      "epoch": 0.08483985765124555,
      "grad_norm": 0.39277517795562744,
      "learning_rate": 0.00047886421861656703,
      "loss": 7.6855,
      "step": 298
    },
    {
      "epoch": 0.08512455516014235,
      "grad_norm": 0.4601745307445526,
      "learning_rate": 0.0004787930543694848,
      "loss": 6.6934,
      "step": 299
    },
    {
      "epoch": 0.08540925266903915,
      "grad_norm": 0.4405549168586731,
      "learning_rate": 0.00047872189012240253,
      "loss": 7.2129,
      "step": 300
    },
    {
      "epoch": 0.08569395017793595,
      "grad_norm": 0.4540632665157318,
      "learning_rate": 0.00047865072587532025,
      "loss": 6.9492,
      "step": 301
    },
    {
      "epoch": 0.08597864768683273,
      "grad_norm": 0.4265691936016083,
      "learning_rate": 0.000478579561628238,
      "loss": 7.1064,
      "step": 302
    },
    {
      "epoch": 0.08626334519572953,
      "grad_norm": 0.45279815793037415,
      "learning_rate": 0.0004785083973811557,
      "loss": 7.001,
      "step": 303
    },
    {
      "epoch": 0.08654804270462634,
      "grad_norm": 0.4508437216281891,
      "learning_rate": 0.0004784372331340734,
      "loss": 6.9727,
      "step": 304
    },
    {
      "epoch": 0.08683274021352314,
      "grad_norm": 0.39535853266716003,
      "learning_rate": 0.0004783660688869912,
      "loss": 7.4971,
      "step": 305
    },
    {
      "epoch": 0.08711743772241994,
      "grad_norm": 0.38876810669898987,
      "learning_rate": 0.0004782949046399089,
      "loss": 6.96,
      "step": 306
    },
    {
      "epoch": 0.08740213523131672,
      "grad_norm": 0.3352130055427551,
      "learning_rate": 0.0004782237403928267,
      "loss": 7.5664,
      "step": 307
    },
    {
      "epoch": 0.08768683274021352,
      "grad_norm": 0.43718674778938293,
      "learning_rate": 0.00047815257614574437,
      "loss": 7.3652,
      "step": 308
    },
    {
      "epoch": 0.08797153024911032,
      "grad_norm": 0.41846373677253723,
      "learning_rate": 0.0004780814118986621,
      "loss": 7.4209,
      "step": 309
    },
    {
      "epoch": 0.08825622775800712,
      "grad_norm": 0.35628408193588257,
      "learning_rate": 0.00047801024765157987,
      "loss": 7.665,
      "step": 310
    },
    {
      "epoch": 0.08854092526690391,
      "grad_norm": 0.43138930201530457,
      "learning_rate": 0.0004779390834044976,
      "loss": 7.0596,
      "step": 311
    },
    {
      "epoch": 0.08882562277580071,
      "grad_norm": 0.41381263732910156,
      "learning_rate": 0.00047786791915741537,
      "loss": 7.4902,
      "step": 312
    },
    {
      "epoch": 0.08911032028469751,
      "grad_norm": 0.44806408882141113,
      "learning_rate": 0.00047779675491033303,
      "loss": 7.3066,
      "step": 313
    },
    {
      "epoch": 0.08939501779359431,
      "grad_norm": 0.41572806239128113,
      "learning_rate": 0.00047772559066325076,
      "loss": 7.1211,
      "step": 314
    },
    {
      "epoch": 0.08967971530249111,
      "grad_norm": 0.4351762533187866,
      "learning_rate": 0.00047765442641616853,
      "loss": 7.1748,
      "step": 315
    },
    {
      "epoch": 0.0899644128113879,
      "grad_norm": 0.5767992734909058,
      "learning_rate": 0.00047758326216908626,
      "loss": 6.5059,
      "step": 316
    },
    {
      "epoch": 0.0902491103202847,
      "grad_norm": 0.4354422092437744,
      "learning_rate": 0.00047751209792200403,
      "loss": 6.9727,
      "step": 317
    },
    {
      "epoch": 0.0905338078291815,
      "grad_norm": 0.48386743664741516,
      "learning_rate": 0.00047744093367492176,
      "loss": 7.5859,
      "step": 318
    },
    {
      "epoch": 0.0908185053380783,
      "grad_norm": 0.447219580411911,
      "learning_rate": 0.0004773697694278394,
      "loss": 7.5195,
      "step": 319
    },
    {
      "epoch": 0.0911032028469751,
      "grad_norm": 0.4001949727535248,
      "learning_rate": 0.0004772986051807572,
      "loss": 7.3672,
      "step": 320
    },
    {
      "epoch": 0.09138790035587188,
      "grad_norm": 0.4028257131576538,
      "learning_rate": 0.0004772274409336749,
      "loss": 7.1143,
      "step": 321
    },
    {
      "epoch": 0.09167259786476868,
      "grad_norm": 0.6439388990402222,
      "learning_rate": 0.00047715627668659265,
      "loss": 6.9395,
      "step": 322
    },
    {
      "epoch": 0.09195729537366548,
      "grad_norm": 0.434939980506897,
      "learning_rate": 0.0004770851124395104,
      "loss": 7.0215,
      "step": 323
    },
    {
      "epoch": 0.09224199288256228,
      "grad_norm": 0.4516619145870209,
      "learning_rate": 0.00047701394819242815,
      "loss": 6.9014,
      "step": 324
    },
    {
      "epoch": 0.09252669039145907,
      "grad_norm": 0.4030342102050781,
      "learning_rate": 0.00047694278394534587,
      "loss": 7.5859,
      "step": 325
    },
    {
      "epoch": 0.09281138790035587,
      "grad_norm": 0.3350996673107147,
      "learning_rate": 0.0004768716196982636,
      "loss": 7.5254,
      "step": 326
    },
    {
      "epoch": 0.09309608540925267,
      "grad_norm": 0.5201140642166138,
      "learning_rate": 0.0004768004554511813,
      "loss": 7.3613,
      "step": 327
    },
    {
      "epoch": 0.09338078291814947,
      "grad_norm": 0.3434700667858124,
      "learning_rate": 0.0004767292912040991,
      "loss": 7.5518,
      "step": 328
    },
    {
      "epoch": 0.09366548042704627,
      "grad_norm": 0.39710113406181335,
      "learning_rate": 0.0004766581269570168,
      "loss": 7.3145,
      "step": 329
    },
    {
      "epoch": 0.09395017793594305,
      "grad_norm": 0.3940322995185852,
      "learning_rate": 0.00047658696270993454,
      "loss": 7.4756,
      "step": 330
    },
    {
      "epoch": 0.09423487544483986,
      "grad_norm": 0.35256409645080566,
      "learning_rate": 0.00047651579846285226,
      "loss": 7.3662,
      "step": 331
    },
    {
      "epoch": 0.09451957295373666,
      "grad_norm": 0.490875780582428,
      "learning_rate": 0.00047644463421577,
      "loss": 6.9189,
      "step": 332
    },
    {
      "epoch": 0.09480427046263346,
      "grad_norm": 0.3466689884662628,
      "learning_rate": 0.00047637346996868776,
      "loss": 7.7227,
      "step": 333
    },
    {
      "epoch": 0.09508896797153026,
      "grad_norm": 0.41715121269226074,
      "learning_rate": 0.0004763023057216055,
      "loss": 7.4795,
      "step": 334
    },
    {
      "epoch": 0.09537366548042704,
      "grad_norm": 0.42658162117004395,
      "learning_rate": 0.00047623114147452326,
      "loss": 7.2119,
      "step": 335
    },
    {
      "epoch": 0.09565836298932384,
      "grad_norm": 0.4596523344516754,
      "learning_rate": 0.00047615997722744093,
      "loss": 7.0312,
      "step": 336
    },
    {
      "epoch": 0.09594306049822064,
      "grad_norm": 0.44432130455970764,
      "learning_rate": 0.00047608881298035865,
      "loss": 7.1562,
      "step": 337
    },
    {
      "epoch": 0.09622775800711744,
      "grad_norm": 0.41700947284698486,
      "learning_rate": 0.00047601764873327643,
      "loss": 7.0879,
      "step": 338
    },
    {
      "epoch": 0.09651245551601423,
      "grad_norm": 0.41166216135025024,
      "learning_rate": 0.00047594648448619415,
      "loss": 7.2783,
      "step": 339
    },
    {
      "epoch": 0.09679715302491103,
      "grad_norm": 0.3937351107597351,
      "learning_rate": 0.0004758753202391119,
      "loss": 7.4902,
      "step": 340
    },
    {
      "epoch": 0.09708185053380783,
      "grad_norm": 0.49600276350975037,
      "learning_rate": 0.00047580415599202965,
      "loss": 7.0215,
      "step": 341
    },
    {
      "epoch": 0.09736654804270463,
      "grad_norm": 0.39852508902549744,
      "learning_rate": 0.0004757329917449473,
      "loss": 7.5752,
      "step": 342
    },
    {
      "epoch": 0.09765124555160143,
      "grad_norm": 0.43215256929397583,
      "learning_rate": 0.0004756618274978651,
      "loss": 7.3994,
      "step": 343
    },
    {
      "epoch": 0.09793594306049822,
      "grad_norm": 0.395315945148468,
      "learning_rate": 0.0004755906632507828,
      "loss": 7.4033,
      "step": 344
    },
    {
      "epoch": 0.09822064056939502,
      "grad_norm": 0.4142919182777405,
      "learning_rate": 0.00047551949900370054,
      "loss": 7.3242,
      "step": 345
    },
    {
      "epoch": 0.09850533807829182,
      "grad_norm": 0.5080282092094421,
      "learning_rate": 0.0004754483347566183,
      "loss": 6.625,
      "step": 346
    },
    {
      "epoch": 0.09879003558718862,
      "grad_norm": 0.44504308700561523,
      "learning_rate": 0.000475377170509536,
      "loss": 7.668,
      "step": 347
    },
    {
      "epoch": 0.09907473309608542,
      "grad_norm": 0.5334663391113281,
      "learning_rate": 0.00047530600626245376,
      "loss": 7.2246,
      "step": 348
    },
    {
      "epoch": 0.0993594306049822,
      "grad_norm": 0.4300445020198822,
      "learning_rate": 0.0004752348420153715,
      "loss": 7.2812,
      "step": 349
    },
    {
      "epoch": 0.099644128113879,
      "grad_norm": 0.43815192580223083,
      "learning_rate": 0.0004751636777682892,
      "loss": 6.8447,
      "step": 350
    },
    {
      "epoch": 0.0999288256227758,
      "grad_norm": 0.45553478598594666,
      "learning_rate": 0.000475092513521207,
      "loss": 7.3037,
      "step": 351
    },
    {
      "epoch": 0.1002135231316726,
      "grad_norm": 0.4697282612323761,
      "learning_rate": 0.0004750213492741247,
      "loss": 7.0762,
      "step": 352
    },
    {
      "epoch": 0.10049822064056939,
      "grad_norm": 0.5708103179931641,
      "learning_rate": 0.0004749501850270424,
      "loss": 7.0889,
      "step": 353
    },
    {
      "epoch": 0.10078291814946619,
      "grad_norm": 0.46478471159935,
      "learning_rate": 0.00047487902077996015,
      "loss": 7.2305,
      "step": 354
    },
    {
      "epoch": 0.10106761565836299,
      "grad_norm": 0.4347301721572876,
      "learning_rate": 0.0004748078565328779,
      "loss": 7.417,
      "step": 355
    },
    {
      "epoch": 0.10135231316725979,
      "grad_norm": 0.3864136040210724,
      "learning_rate": 0.00047473669228579565,
      "loss": 7.7119,
      "step": 356
    },
    {
      "epoch": 0.10163701067615659,
      "grad_norm": 0.3752633333206177,
      "learning_rate": 0.0004746655280387134,
      "loss": 7.6006,
      "step": 357
    },
    {
      "epoch": 0.10192170818505338,
      "grad_norm": 0.5481764674186707,
      "learning_rate": 0.00047459436379163105,
      "loss": 7.0039,
      "step": 358
    },
    {
      "epoch": 0.10220640569395018,
      "grad_norm": 0.40612098574638367,
      "learning_rate": 0.0004745231995445488,
      "loss": 7.4434,
      "step": 359
    },
    {
      "epoch": 0.10249110320284698,
      "grad_norm": 0.4205334484577179,
      "learning_rate": 0.00047445203529746655,
      "loss": 7.2832,
      "step": 360
    },
    {
      "epoch": 0.10277580071174378,
      "grad_norm": 0.3059813678264618,
      "learning_rate": 0.0004743808710503843,
      "loss": 7.6846,
      "step": 361
    },
    {
      "epoch": 0.10306049822064058,
      "grad_norm": 0.48755931854248047,
      "learning_rate": 0.00047430970680330205,
      "loss": 6.9141,
      "step": 362
    },
    {
      "epoch": 0.10334519572953736,
      "grad_norm": 0.42173999547958374,
      "learning_rate": 0.00047423854255621977,
      "loss": 7.5166,
      "step": 363
    },
    {
      "epoch": 0.10362989323843416,
      "grad_norm": 0.5560771226882935,
      "learning_rate": 0.0004741673783091375,
      "loss": 7.418,
      "step": 364
    },
    {
      "epoch": 0.10391459074733096,
      "grad_norm": 0.6154012680053711,
      "learning_rate": 0.0004740962140620552,
      "loss": 7.2734,
      "step": 365
    },
    {
      "epoch": 0.10419928825622776,
      "grad_norm": 0.4501911997795105,
      "learning_rate": 0.000474025049814973,
      "loss": 6.9023,
      "step": 366
    },
    {
      "epoch": 0.10448398576512455,
      "grad_norm": 0.4486687183380127,
      "learning_rate": 0.0004739538855678907,
      "loss": 7.3291,
      "step": 367
    },
    {
      "epoch": 0.10476868327402135,
      "grad_norm": 0.406693696975708,
      "learning_rate": 0.00047388272132080844,
      "loss": 7.501,
      "step": 368
    },
    {
      "epoch": 0.10505338078291815,
      "grad_norm": 0.4880397915840149,
      "learning_rate": 0.0004738115570737262,
      "loss": 6.6982,
      "step": 369
    },
    {
      "epoch": 0.10533807829181495,
      "grad_norm": 0.4266977608203888,
      "learning_rate": 0.0004737403928266439,
      "loss": 7.2031,
      "step": 370
    },
    {
      "epoch": 0.10562277580071175,
      "grad_norm": 0.4237028658390045,
      "learning_rate": 0.0004736692285795616,
      "loss": 6.9111,
      "step": 371
    },
    {
      "epoch": 0.10590747330960854,
      "grad_norm": 0.5166893601417542,
      "learning_rate": 0.0004735980643324794,
      "loss": 7.3945,
      "step": 372
    },
    {
      "epoch": 0.10619217081850534,
      "grad_norm": 0.4069686233997345,
      "learning_rate": 0.0004735269000853971,
      "loss": 7.54,
      "step": 373
    },
    {
      "epoch": 0.10647686832740214,
      "grad_norm": 0.43857699632644653,
      "learning_rate": 0.0004734557358383149,
      "loss": 7.3135,
      "step": 374
    },
    {
      "epoch": 0.10676156583629894,
      "grad_norm": 0.39594122767448425,
      "learning_rate": 0.00047338457159123255,
      "loss": 7.8369,
      "step": 375
    },
    {
      "epoch": 0.10704626334519574,
      "grad_norm": 0.4282735586166382,
      "learning_rate": 0.00047331340734415027,
      "loss": 7.4375,
      "step": 376
    },
    {
      "epoch": 0.10733096085409252,
      "grad_norm": 0.45673078298568726,
      "learning_rate": 0.00047324224309706805,
      "loss": 7.3848,
      "step": 377
    },
    {
      "epoch": 0.10761565836298932,
      "grad_norm": 0.4529609978199005,
      "learning_rate": 0.00047317107884998577,
      "loss": 7.1904,
      "step": 378
    },
    {
      "epoch": 0.10790035587188612,
      "grad_norm": 0.508343517780304,
      "learning_rate": 0.00047309991460290355,
      "loss": 7.1104,
      "step": 379
    },
    {
      "epoch": 0.10818505338078292,
      "grad_norm": 0.5695661306381226,
      "learning_rate": 0.00047302875035582127,
      "loss": 6.9775,
      "step": 380
    },
    {
      "epoch": 0.10846975088967971,
      "grad_norm": 0.4289855659008026,
      "learning_rate": 0.00047295758610873894,
      "loss": 7.3799,
      "step": 381
    },
    {
      "epoch": 0.10875444839857651,
      "grad_norm": 0.45409122109413147,
      "learning_rate": 0.0004728864218616567,
      "loss": 6.877,
      "step": 382
    },
    {
      "epoch": 0.10903914590747331,
      "grad_norm": 0.43355792760849,
      "learning_rate": 0.00047281525761457444,
      "loss": 7.1543,
      "step": 383
    },
    {
      "epoch": 0.10932384341637011,
      "grad_norm": 0.3788268268108368,
      "learning_rate": 0.00047274409336749216,
      "loss": 7.582,
      "step": 384
    },
    {
      "epoch": 0.10960854092526691,
      "grad_norm": 0.4172390103340149,
      "learning_rate": 0.00047267292912040994,
      "loss": 7.4766,
      "step": 385
    },
    {
      "epoch": 0.1098932384341637,
      "grad_norm": 0.5651355385780334,
      "learning_rate": 0.00047260176487332766,
      "loss": 6.8984,
      "step": 386
    },
    {
      "epoch": 0.1101779359430605,
      "grad_norm": 0.4061424136161804,
      "learning_rate": 0.0004725306006262454,
      "loss": 7.6387,
      "step": 387
    },
    {
      "epoch": 0.1104626334519573,
      "grad_norm": 0.41827505826950073,
      "learning_rate": 0.0004724594363791631,
      "loss": 7.2285,
      "step": 388
    },
    {
      "epoch": 0.1107473309608541,
      "grad_norm": 0.37790295481681824,
      "learning_rate": 0.00047238827213208083,
      "loss": 7.583,
      "step": 389
    },
    {
      "epoch": 0.1110320284697509,
      "grad_norm": 0.41281452775001526,
      "learning_rate": 0.0004723171078849986,
      "loss": 7.4941,
      "step": 390
    },
    {
      "epoch": 0.11131672597864768,
      "grad_norm": 0.41881147027015686,
      "learning_rate": 0.00047224594363791633,
      "loss": 7.3848,
      "step": 391
    },
    {
      "epoch": 0.11160142348754448,
      "grad_norm": 0.41661399602890015,
      "learning_rate": 0.00047217477939083405,
      "loss": 7.4121,
      "step": 392
    },
    {
      "epoch": 0.11188612099644128,
      "grad_norm": 0.4192899763584137,
      "learning_rate": 0.0004721036151437518,
      "loss": 7.0596,
      "step": 393
    },
    {
      "epoch": 0.11217081850533808,
      "grad_norm": 0.45563167333602905,
      "learning_rate": 0.0004720324508966695,
      "loss": 7.4004,
      "step": 394
    },
    {
      "epoch": 0.11245551601423487,
      "grad_norm": 0.4283228814601898,
      "learning_rate": 0.0004719612866495873,
      "loss": 7.6162,
      "step": 395
    },
    {
      "epoch": 0.11274021352313167,
      "grad_norm": 0.4331873655319214,
      "learning_rate": 0.000471890122402505,
      "loss": 7.001,
      "step": 396
    },
    {
      "epoch": 0.11302491103202847,
      "grad_norm": 0.4580132067203522,
      "learning_rate": 0.0004718189581554228,
      "loss": 7.0654,
      "step": 397
    },
    {
      "epoch": 0.11330960854092527,
      "grad_norm": 0.46618202328681946,
      "learning_rate": 0.00047174779390834044,
      "loss": 7.0742,
      "step": 398
    },
    {
      "epoch": 0.11359430604982207,
      "grad_norm": 0.4677276909351349,
      "learning_rate": 0.00047167662966125817,
      "loss": 7.1973,
      "step": 399
    },
    {
      "epoch": 0.11387900355871886,
      "grad_norm": 0.4366690516471863,
      "learning_rate": 0.00047160546541417594,
      "loss": 7.4443,
      "step": 400
    },
    {
      "epoch": 0.11387900355871886,
      "eval_bleu": 0.06055701380020064,
      "eval_loss": 7.00390625,
      "eval_runtime": 195.2568,
      "eval_samples_per_second": 1.454,
      "eval_steps_per_second": 0.092,
      "step": 400
    },
    {
      "epoch": 0.11416370106761566,
      "grad_norm": 0.3341705799102783,
      "learning_rate": 0.00047153430116709367,
      "loss": 7.7119,
      "step": 401
    },
    {
      "epoch": 0.11444839857651246,
      "grad_norm": 0.4076211154460907,
      "learning_rate": 0.0004714631369200114,
      "loss": 7.4521,
      "step": 402
    },
    {
      "epoch": 0.11473309608540926,
      "grad_norm": 0.3936387598514557,
      "learning_rate": 0.0004713919726729291,
      "loss": 7.2227,
      "step": 403
    },
    {
      "epoch": 0.11501779359430606,
      "grad_norm": 0.39969155192375183,
      "learning_rate": 0.00047132080842584683,
      "loss": 7.5381,
      "step": 404
    },
    {
      "epoch": 0.11530249110320284,
      "grad_norm": 0.46734946966171265,
      "learning_rate": 0.0004712496441787646,
      "loss": 7.4033,
      "step": 405
    },
    {
      "epoch": 0.11558718861209964,
      "grad_norm": 0.4855714738368988,
      "learning_rate": 0.00047117847993168233,
      "loss": 7.457,
      "step": 406
    },
    {
      "epoch": 0.11587188612099644,
      "grad_norm": 0.5113205313682556,
      "learning_rate": 0.00047110731568460006,
      "loss": 7.1904,
      "step": 407
    },
    {
      "epoch": 0.11615658362989324,
      "grad_norm": 0.3756895065307617,
      "learning_rate": 0.00047103615143751783,
      "loss": 7.5938,
      "step": 408
    },
    {
      "epoch": 0.11644128113879003,
      "grad_norm": 0.37924525141716003,
      "learning_rate": 0.0004709649871904355,
      "loss": 7.585,
      "step": 409
    },
    {
      "epoch": 0.11672597864768683,
      "grad_norm": 0.4761236608028412,
      "learning_rate": 0.0004708938229433533,
      "loss": 7.3105,
      "step": 410
    },
    {
      "epoch": 0.11701067615658363,
      "grad_norm": 0.4178760051727295,
      "learning_rate": 0.000470822658696271,
      "loss": 7.501,
      "step": 411
    },
    {
      "epoch": 0.11729537366548043,
      "grad_norm": 0.5172440409660339,
      "learning_rate": 0.0004707514944491887,
      "loss": 7.1963,
      "step": 412
    },
    {
      "epoch": 0.11758007117437723,
      "grad_norm": 0.7484517693519592,
      "learning_rate": 0.0004706803302021065,
      "loss": 6.7061,
      "step": 413
    },
    {
      "epoch": 0.11786476868327402,
      "grad_norm": 0.47221601009368896,
      "learning_rate": 0.0004706091659550242,
      "loss": 7.7109,
      "step": 414
    },
    {
      "epoch": 0.11814946619217082,
      "grad_norm": 0.39013606309890747,
      "learning_rate": 0.00047053800170794195,
      "loss": 7.75,
      "step": 415
    },
    {
      "epoch": 0.11843416370106762,
      "grad_norm": 0.549603283405304,
      "learning_rate": 0.00047046683746085967,
      "loss": 6.8496,
      "step": 416
    },
    {
      "epoch": 0.11871886120996442,
      "grad_norm": 0.5139209628105164,
      "learning_rate": 0.0004703956732137774,
      "loss": 7.0254,
      "step": 417
    },
    {
      "epoch": 0.11900355871886122,
      "grad_norm": 0.5115612745285034,
      "learning_rate": 0.00047032450896669517,
      "loss": 7.21,
      "step": 418
    },
    {
      "epoch": 0.119288256227758,
      "grad_norm": 0.4744890034198761,
      "learning_rate": 0.0004702533447196129,
      "loss": 7.0742,
      "step": 419
    },
    {
      "epoch": 0.1195729537366548,
      "grad_norm": 0.43311771750450134,
      "learning_rate": 0.00047018218047253056,
      "loss": 7.0781,
      "step": 420
    },
    {
      "epoch": 0.1198576512455516,
      "grad_norm": 0.4734751582145691,
      "learning_rate": 0.00047011101622544834,
      "loss": 7.2168,
      "step": 421
    },
    {
      "epoch": 0.1201423487544484,
      "grad_norm": 0.5691256523132324,
      "learning_rate": 0.00047003985197836606,
      "loss": 6.623,
      "step": 422
    },
    {
      "epoch": 0.12042704626334519,
      "grad_norm": 0.40093910694122314,
      "learning_rate": 0.00046996868773128384,
      "loss": 7.4346,
      "step": 423
    },
    {
      "epoch": 0.12071174377224199,
      "grad_norm": 0.42212653160095215,
      "learning_rate": 0.00046989752348420156,
      "loss": 7.4355,
      "step": 424
    },
    {
      "epoch": 0.12099644128113879,
      "grad_norm": 0.44689905643463135,
      "learning_rate": 0.0004698263592371193,
      "loss": 7.6377,
      "step": 425
    },
    {
      "epoch": 0.12128113879003559,
      "grad_norm": 0.4593307375907898,
      "learning_rate": 0.000469755194990037,
      "loss": 7.4443,
      "step": 426
    },
    {
      "epoch": 0.12156583629893239,
      "grad_norm": 0.5486469268798828,
      "learning_rate": 0.00046968403074295473,
      "loss": 6.8281,
      "step": 427
    },
    {
      "epoch": 0.12185053380782918,
      "grad_norm": 0.4106900990009308,
      "learning_rate": 0.0004696128664958725,
      "loss": 7.2324,
      "step": 428
    },
    {
      "epoch": 0.12213523131672598,
      "grad_norm": 0.42247626185417175,
      "learning_rate": 0.00046954170224879023,
      "loss": 7.2988,
      "step": 429
    },
    {
      "epoch": 0.12241992882562278,
      "grad_norm": 0.5033213496208191,
      "learning_rate": 0.00046947053800170795,
      "loss": 7.291,
      "step": 430
    },
    {
      "epoch": 0.12270462633451958,
      "grad_norm": 0.42371541261672974,
      "learning_rate": 0.0004693993737546257,
      "loss": 7.458,
      "step": 431
    },
    {
      "epoch": 0.12298932384341638,
      "grad_norm": 0.5228193998336792,
      "learning_rate": 0.0004693282095075434,
      "loss": 7.1699,
      "step": 432
    },
    {
      "epoch": 0.12327402135231316,
      "grad_norm": 0.3674987852573395,
      "learning_rate": 0.0004692570452604611,
      "loss": 7.5742,
      "step": 433
    },
    {
      "epoch": 0.12355871886120996,
      "grad_norm": 0.5292471051216125,
      "learning_rate": 0.0004691858810133789,
      "loss": 6.7803,
      "step": 434
    },
    {
      "epoch": 0.12384341637010676,
      "grad_norm": 0.49759700894355774,
      "learning_rate": 0.0004691147167662966,
      "loss": 7.0801,
      "step": 435
    },
    {
      "epoch": 0.12412811387900356,
      "grad_norm": 0.4338753819465637,
      "learning_rate": 0.0004690435525192144,
      "loss": 7.7227,
      "step": 436
    },
    {
      "epoch": 0.12441281138790036,
      "grad_norm": 0.4573684632778168,
      "learning_rate": 0.00046897238827213207,
      "loss": 7.3672,
      "step": 437
    },
    {
      "epoch": 0.12469750889679715,
      "grad_norm": 0.504006028175354,
      "learning_rate": 0.0004689012240250498,
      "loss": 7.1426,
      "step": 438
    },
    {
      "epoch": 0.12498220640569395,
      "grad_norm": 0.5081356167793274,
      "learning_rate": 0.00046883005977796756,
      "loss": 7.0537,
      "step": 439
    },
    {
      "epoch": 0.12526690391459075,
      "grad_norm": 0.48333534598350525,
      "learning_rate": 0.0004687588955308853,
      "loss": 7.0693,
      "step": 440
    },
    {
      "epoch": 0.12555160142348754,
      "grad_norm": 0.4130278527736664,
      "learning_rate": 0.00046868773128380306,
      "loss": 7.4668,
      "step": 441
    },
    {
      "epoch": 0.12583629893238435,
      "grad_norm": 0.4785782992839813,
      "learning_rate": 0.0004686165670367208,
      "loss": 7.0684,
      "step": 442
    },
    {
      "epoch": 0.12612099644128114,
      "grad_norm": 0.5069476366043091,
      "learning_rate": 0.00046854540278963846,
      "loss": 7.2559,
      "step": 443
    },
    {
      "epoch": 0.12640569395017795,
      "grad_norm": 0.37336277961730957,
      "learning_rate": 0.00046847423854255623,
      "loss": 7.626,
      "step": 444
    },
    {
      "epoch": 0.12669039145907474,
      "grad_norm": 0.401167631149292,
      "learning_rate": 0.00046840307429547396,
      "loss": 7.2949,
      "step": 445
    },
    {
      "epoch": 0.12697508896797152,
      "grad_norm": 0.5884152054786682,
      "learning_rate": 0.00046833191004839173,
      "loss": 7.0771,
      "step": 446
    },
    {
      "epoch": 0.12725978647686834,
      "grad_norm": 0.43068522214889526,
      "learning_rate": 0.00046826074580130946,
      "loss": 7.6777,
      "step": 447
    },
    {
      "epoch": 0.12754448398576512,
      "grad_norm": 0.44112157821655273,
      "learning_rate": 0.0004681895815542271,
      "loss": 7.376,
      "step": 448
    },
    {
      "epoch": 0.1278291814946619,
      "grad_norm": 0.46927693486213684,
      "learning_rate": 0.0004681184173071449,
      "loss": 7.2109,
      "step": 449
    },
    {
      "epoch": 0.12811387900355872,
      "grad_norm": 0.3317018449306488,
      "learning_rate": 0.0004680472530600626,
      "loss": 7.751,
      "step": 450
    },
    {
      "epoch": 0.1283985765124555,
      "grad_norm": 0.4970950782299042,
      "learning_rate": 0.00046797608881298035,
      "loss": 6.835,
      "step": 451
    },
    {
      "epoch": 0.12868327402135232,
      "grad_norm": 0.3675149381160736,
      "learning_rate": 0.0004679049245658981,
      "loss": 7.8193,
      "step": 452
    },
    {
      "epoch": 0.1289679715302491,
      "grad_norm": 0.4956851005554199,
      "learning_rate": 0.00046783376031881585,
      "loss": 6.8252,
      "step": 453
    },
    {
      "epoch": 0.1292526690391459,
      "grad_norm": 0.4222683012485504,
      "learning_rate": 0.00046776259607173357,
      "loss": 7.501,
      "step": 454
    },
    {
      "epoch": 0.1295373665480427,
      "grad_norm": 0.4823405146598816,
      "learning_rate": 0.0004676914318246513,
      "loss": 7.1631,
      "step": 455
    },
    {
      "epoch": 0.1298220640569395,
      "grad_norm": 0.525143563747406,
      "learning_rate": 0.000467620267577569,
      "loss": 7.2627,
      "step": 456
    },
    {
      "epoch": 0.1301067615658363,
      "grad_norm": 0.4748491048812866,
      "learning_rate": 0.0004675491033304868,
      "loss": 7.4766,
      "step": 457
    },
    {
      "epoch": 0.1303914590747331,
      "grad_norm": 0.4361896514892578,
      "learning_rate": 0.0004674779390834045,
      "loss": 7.2959,
      "step": 458
    },
    {
      "epoch": 0.13067615658362988,
      "grad_norm": 0.5087428689002991,
      "learning_rate": 0.0004674067748363223,
      "loss": 7.3447,
      "step": 459
    },
    {
      "epoch": 0.1309608540925267,
      "grad_norm": 0.5033571720123291,
      "learning_rate": 0.00046733561058923996,
      "loss": 7.0791,
      "step": 460
    },
    {
      "epoch": 0.13124555160142348,
      "grad_norm": 0.5781779885292053,
      "learning_rate": 0.0004672644463421577,
      "loss": 7.1611,
      "step": 461
    },
    {
      "epoch": 0.1315302491103203,
      "grad_norm": 0.45533907413482666,
      "learning_rate": 0.00046719328209507546,
      "loss": 7.1758,
      "step": 462
    },
    {
      "epoch": 0.13181494661921708,
      "grad_norm": 0.4691354036331177,
      "learning_rate": 0.0004671221178479932,
      "loss": 7.375,
      "step": 463
    },
    {
      "epoch": 0.13209964412811387,
      "grad_norm": 0.4058699905872345,
      "learning_rate": 0.00046705095360091096,
      "loss": 7.7324,
      "step": 464
    },
    {
      "epoch": 0.13238434163701068,
      "grad_norm": 0.559533953666687,
      "learning_rate": 0.00046697978935382863,
      "loss": 6.6924,
      "step": 465
    },
    {
      "epoch": 0.13266903914590747,
      "grad_norm": 0.4726496934890747,
      "learning_rate": 0.00046690862510674635,
      "loss": 6.9219,
      "step": 466
    },
    {
      "epoch": 0.13295373665480428,
      "grad_norm": 0.4936983287334442,
      "learning_rate": 0.00046683746085966413,
      "loss": 7.1094,
      "step": 467
    },
    {
      "epoch": 0.13323843416370107,
      "grad_norm": 0.4566884934902191,
      "learning_rate": 0.00046676629661258185,
      "loss": 7.5312,
      "step": 468
    },
    {
      "epoch": 0.13352313167259786,
      "grad_norm": 0.4185795187950134,
      "learning_rate": 0.00046669513236549957,
      "loss": 7.5166,
      "step": 469
    },
    {
      "epoch": 0.13380782918149467,
      "grad_norm": 0.5276864767074585,
      "learning_rate": 0.00046662396811841735,
      "loss": 7.4355,
      "step": 470
    },
    {
      "epoch": 0.13409252669039146,
      "grad_norm": 0.5576158761978149,
      "learning_rate": 0.000466552803871335,
      "loss": 7.1006,
      "step": 471
    },
    {
      "epoch": 0.13437722419928827,
      "grad_norm": 0.4316464960575104,
      "learning_rate": 0.0004664816396242528,
      "loss": 7.4736,
      "step": 472
    },
    {
      "epoch": 0.13466192170818506,
      "grad_norm": 0.5265529155731201,
      "learning_rate": 0.0004664104753771705,
      "loss": 6.8154,
      "step": 473
    },
    {
      "epoch": 0.13494661921708184,
      "grad_norm": 0.4530062973499298,
      "learning_rate": 0.00046633931113008824,
      "loss": 7.377,
      "step": 474
    },
    {
      "epoch": 0.13523131672597866,
      "grad_norm": 0.47892332077026367,
      "learning_rate": 0.000466268146883006,
      "loss": 6.8652,
      "step": 475
    },
    {
      "epoch": 0.13551601423487544,
      "grad_norm": 0.5138373970985413,
      "learning_rate": 0.0004661969826359237,
      "loss": 7.0654,
      "step": 476
    },
    {
      "epoch": 0.13580071174377223,
      "grad_norm": 0.4899928867816925,
      "learning_rate": 0.00046612581838884146,
      "loss": 7.2871,
      "step": 477
    },
    {
      "epoch": 0.13608540925266904,
      "grad_norm": 0.5416154861450195,
      "learning_rate": 0.0004660546541417592,
      "loss": 7.0,
      "step": 478
    },
    {
      "epoch": 0.13637010676156583,
      "grad_norm": 0.4114988148212433,
      "learning_rate": 0.0004659834898946769,
      "loss": 7.1953,
      "step": 479
    },
    {
      "epoch": 0.13665480427046264,
      "grad_norm": 0.480387419462204,
      "learning_rate": 0.0004659123256475947,
      "loss": 7.2236,
      "step": 480
    },
    {
      "epoch": 0.13693950177935943,
      "grad_norm": 0.4656660854816437,
      "learning_rate": 0.0004658411614005124,
      "loss": 7.5098,
      "step": 481
    },
    {
      "epoch": 0.13722419928825622,
      "grad_norm": 0.49934709072113037,
      "learning_rate": 0.0004657699971534301,
      "loss": 7.1338,
      "step": 482
    },
    {
      "epoch": 0.13750889679715303,
      "grad_norm": 0.4244711995124817,
      "learning_rate": 0.00046569883290634785,
      "loss": 7.3154,
      "step": 483
    },
    {
      "epoch": 0.13779359430604982,
      "grad_norm": 0.5288747549057007,
      "learning_rate": 0.0004656276686592656,
      "loss": 6.918,
      "step": 484
    },
    {
      "epoch": 0.13807829181494663,
      "grad_norm": 0.5109588503837585,
      "learning_rate": 0.00046555650441218335,
      "loss": 7.1914,
      "step": 485
    },
    {
      "epoch": 0.13836298932384342,
      "grad_norm": 0.4920799434185028,
      "learning_rate": 0.0004654853401651011,
      "loss": 7.457,
      "step": 486
    },
    {
      "epoch": 0.1386476868327402,
      "grad_norm": 0.3864028751850128,
      "learning_rate": 0.0004654141759180188,
      "loss": 7.877,
      "step": 487
    },
    {
      "epoch": 0.13893238434163702,
      "grad_norm": 0.41772550344467163,
      "learning_rate": 0.0004653430116709365,
      "loss": 7.5791,
      "step": 488
    },
    {
      "epoch": 0.1392170818505338,
      "grad_norm": 0.527531087398529,
      "learning_rate": 0.00046527184742385424,
      "loss": 6.834,
      "step": 489
    },
    {
      "epoch": 0.13950177935943062,
      "grad_norm": 0.4375729262828827,
      "learning_rate": 0.000465200683176772,
      "loss": 7.208,
      "step": 490
    },
    {
      "epoch": 0.1397864768683274,
      "grad_norm": 0.5000871419906616,
      "learning_rate": 0.00046512951892968974,
      "loss": 7.2734,
      "step": 491
    },
    {
      "epoch": 0.1400711743772242,
      "grad_norm": 0.39341986179351807,
      "learning_rate": 0.00046505835468260747,
      "loss": 7.627,
      "step": 492
    },
    {
      "epoch": 0.140355871886121,
      "grad_norm": 0.5892451405525208,
      "learning_rate": 0.0004649871904355252,
      "loss": 6.7764,
      "step": 493
    },
    {
      "epoch": 0.1406405693950178,
      "grad_norm": 0.4922020137310028,
      "learning_rate": 0.0004649160261884429,
      "loss": 7.3164,
      "step": 494
    },
    {
      "epoch": 0.1409252669039146,
      "grad_norm": 0.4670257568359375,
      "learning_rate": 0.0004648448619413607,
      "loss": 7.1924,
      "step": 495
    },
    {
      "epoch": 0.1412099644128114,
      "grad_norm": 0.5158605575561523,
      "learning_rate": 0.0004647736976942784,
      "loss": 7.0537,
      "step": 496
    },
    {
      "epoch": 0.14149466192170818,
      "grad_norm": 0.48508933186531067,
      "learning_rate": 0.00046470253344719614,
      "loss": 6.9873,
      "step": 497
    },
    {
      "epoch": 0.141779359430605,
      "grad_norm": 0.4340175688266754,
      "learning_rate": 0.0004646313692001139,
      "loss": 7.2236,
      "step": 498
    },
    {
      "epoch": 0.14206405693950178,
      "grad_norm": 0.4614635109901428,
      "learning_rate": 0.0004645602049530316,
      "loss": 7.4971,
      "step": 499
    },
    {
      "epoch": 0.1423487544483986,
      "grad_norm": 0.43651285767555237,
      "learning_rate": 0.0004644890407059493,
      "loss": 7.0176,
      "step": 500
    },
    {
      "epoch": 0.14263345195729538,
      "grad_norm": 0.4170750081539154,
      "learning_rate": 0.0004644178764588671,
      "loss": 7.3975,
      "step": 501
    },
    {
      "epoch": 0.14291814946619216,
      "grad_norm": 0.45592260360717773,
      "learning_rate": 0.0004643467122117848,
      "loss": 7.4209,
      "step": 502
    },
    {
      "epoch": 0.14320284697508898,
      "grad_norm": 0.5318942666053772,
      "learning_rate": 0.0004642755479647026,
      "loss": 6.6689,
      "step": 503
    },
    {
      "epoch": 0.14348754448398576,
      "grad_norm": 0.43632060289382935,
      "learning_rate": 0.0004642043837176203,
      "loss": 7.3867,
      "step": 504
    },
    {
      "epoch": 0.14377224199288255,
      "grad_norm": 0.48805883526802063,
      "learning_rate": 0.00046413321947053797,
      "loss": 7.0771,
      "step": 505
    },
    {
      "epoch": 0.14405693950177936,
      "grad_norm": 0.5213673710823059,
      "learning_rate": 0.00046406205522345575,
      "loss": 7.0059,
      "step": 506
    },
    {
      "epoch": 0.14434163701067615,
      "grad_norm": 0.4732288420200348,
      "learning_rate": 0.00046399089097637347,
      "loss": 7.2939,
      "step": 507
    },
    {
      "epoch": 0.14462633451957296,
      "grad_norm": 0.44369205832481384,
      "learning_rate": 0.00046391972672929125,
      "loss": 7.5195,
      "step": 508
    },
    {
      "epoch": 0.14491103202846975,
      "grad_norm": 0.48915088176727295,
      "learning_rate": 0.00046384856248220897,
      "loss": 7.1377,
      "step": 509
    },
    {
      "epoch": 0.14519572953736654,
      "grad_norm": 0.4987899959087372,
      "learning_rate": 0.00046377739823512664,
      "loss": 7.0449,
      "step": 510
    },
    {
      "epoch": 0.14548042704626335,
      "grad_norm": 0.48522457480430603,
      "learning_rate": 0.0004637062339880444,
      "loss": 6.9531,
      "step": 511
    },
    {
      "epoch": 0.14576512455516014,
      "grad_norm": 0.45173463225364685,
      "learning_rate": 0.00046363506974096214,
      "loss": 7.4492,
      "step": 512
    },
    {
      "epoch": 0.14604982206405695,
      "grad_norm": 0.4935166835784912,
      "learning_rate": 0.0004635639054938799,
      "loss": 7.6309,
      "step": 513
    },
    {
      "epoch": 0.14633451957295374,
      "grad_norm": 0.467246949672699,
      "learning_rate": 0.00046349274124679764,
      "loss": 7.3164,
      "step": 514
    },
    {
      "epoch": 0.14661921708185052,
      "grad_norm": 0.4763619005680084,
      "learning_rate": 0.00046342157699971536,
      "loss": 7.2002,
      "step": 515
    },
    {
      "epoch": 0.14690391459074734,
      "grad_norm": 0.43141141533851624,
      "learning_rate": 0.0004633504127526331,
      "loss": 7.6465,
      "step": 516
    },
    {
      "epoch": 0.14718861209964412,
      "grad_norm": 0.4468206763267517,
      "learning_rate": 0.0004632792485055508,
      "loss": 7.3887,
      "step": 517
    },
    {
      "epoch": 0.14747330960854094,
      "grad_norm": 0.4942794740200043,
      "learning_rate": 0.00046320808425846853,
      "loss": 6.8789,
      "step": 518
    },
    {
      "epoch": 0.14775800711743772,
      "grad_norm": 0.4572586715221405,
      "learning_rate": 0.0004631369200113863,
      "loss": 7.3047,
      "step": 519
    },
    {
      "epoch": 0.1480427046263345,
      "grad_norm": 0.47273606061935425,
      "learning_rate": 0.00046306575576430403,
      "loss": 6.9844,
      "step": 520
    },
    {
      "epoch": 0.14832740213523132,
      "grad_norm": 0.5519757866859436,
      "learning_rate": 0.00046299459151722175,
      "loss": 7.2256,
      "step": 521
    },
    {
      "epoch": 0.1486120996441281,
      "grad_norm": 0.46243593096733093,
      "learning_rate": 0.0004629234272701395,
      "loss": 7.3311,
      "step": 522
    },
    {
      "epoch": 0.14889679715302492,
      "grad_norm": 0.5186846256256104,
      "learning_rate": 0.0004628522630230572,
      "loss": 7.0801,
      "step": 523
    },
    {
      "epoch": 0.1491814946619217,
      "grad_norm": 0.47964605689048767,
      "learning_rate": 0.000462781098775975,
      "loss": 7.2568,
      "step": 524
    },
    {
      "epoch": 0.1494661921708185,
      "grad_norm": 0.48539063334465027,
      "learning_rate": 0.0004627099345288927,
      "loss": 6.9678,
      "step": 525
    },
    {
      "epoch": 0.1497508896797153,
      "grad_norm": 0.45621731877326965,
      "learning_rate": 0.0004626387702818105,
      "loss": 7.4131,
      "step": 526
    },
    {
      "epoch": 0.1500355871886121,
      "grad_norm": 0.4345969557762146,
      "learning_rate": 0.00046256760603472814,
      "loss": 7.4258,
      "step": 527
    },
    {
      "epoch": 0.1503202846975089,
      "grad_norm": 0.4568675756454468,
      "learning_rate": 0.00046249644178764587,
      "loss": 7.2705,
      "step": 528
    },
    {
      "epoch": 0.1506049822064057,
      "grad_norm": 0.6596431732177734,
      "learning_rate": 0.00046242527754056364,
      "loss": 6.4834,
      "step": 529
    },
    {
      "epoch": 0.15088967971530248,
      "grad_norm": 0.5479394197463989,
      "learning_rate": 0.00046235411329348137,
      "loss": 6.5928,
      "step": 530
    },
    {
      "epoch": 0.1511743772241993,
      "grad_norm": 0.47287479043006897,
      "learning_rate": 0.0004622829490463991,
      "loss": 7.1133,
      "step": 531
    },
    {
      "epoch": 0.15145907473309608,
      "grad_norm": 0.42825841903686523,
      "learning_rate": 0.00046221178479931687,
      "loss": 7.6221,
      "step": 532
    },
    {
      "epoch": 0.15174377224199287,
      "grad_norm": 0.6770346164703369,
      "learning_rate": 0.00046214062055223453,
      "loss": 6.1582,
      "step": 533
    },
    {
      "epoch": 0.15202846975088968,
      "grad_norm": 0.46598076820373535,
      "learning_rate": 0.0004620694563051523,
      "loss": 7.334,
      "step": 534
    },
    {
      "epoch": 0.15231316725978647,
      "grad_norm": 0.48289692401885986,
      "learning_rate": 0.00046199829205807003,
      "loss": 7.0273,
      "step": 535
    },
    {
      "epoch": 0.15259786476868328,
      "grad_norm": 0.5073385238647461,
      "learning_rate": 0.00046192712781098776,
      "loss": 6.874,
      "step": 536
    },
    {
      "epoch": 0.15288256227758007,
      "grad_norm": 0.3938785791397095,
      "learning_rate": 0.00046185596356390553,
      "loss": 7.749,
      "step": 537
    },
    {
      "epoch": 0.15316725978647686,
      "grad_norm": 0.4579390287399292,
      "learning_rate": 0.0004617847993168232,
      "loss": 7.1562,
      "step": 538
    },
    {
      "epoch": 0.15345195729537367,
      "grad_norm": 0.4372034966945648,
      "learning_rate": 0.000461713635069741,
      "loss": 7.2236,
      "step": 539
    },
    {
      "epoch": 0.15373665480427046,
      "grad_norm": 0.4431258738040924,
      "learning_rate": 0.0004616424708226587,
      "loss": 7.3242,
      "step": 540
    },
    {
      "epoch": 0.15402135231316727,
      "grad_norm": 0.43316811323165894,
      "learning_rate": 0.0004615713065755764,
      "loss": 7.3438,
      "step": 541
    },
    {
      "epoch": 0.15430604982206406,
      "grad_norm": 0.4466095566749573,
      "learning_rate": 0.0004615001423284942,
      "loss": 7.2197,
      "step": 542
    },
    {
      "epoch": 0.15459074733096084,
      "grad_norm": 0.45385703444480896,
      "learning_rate": 0.0004614289780814119,
      "loss": 7.3252,
      "step": 543
    },
    {
      "epoch": 0.15487544483985766,
      "grad_norm": 0.4653369188308716,
      "learning_rate": 0.00046135781383432965,
      "loss": 7.4531,
      "step": 544
    },
    {
      "epoch": 0.15516014234875444,
      "grad_norm": 0.4816647469997406,
      "learning_rate": 0.00046128664958724737,
      "loss": 7.3721,
      "step": 545
    },
    {
      "epoch": 0.15544483985765126,
      "grad_norm": 0.47521352767944336,
      "learning_rate": 0.0004612154853401651,
      "loss": 7.2432,
      "step": 546
    },
    {
      "epoch": 0.15572953736654804,
      "grad_norm": 0.34924232959747314,
      "learning_rate": 0.00046114432109308287,
      "loss": 7.96,
      "step": 547
    },
    {
      "epoch": 0.15601423487544483,
      "grad_norm": 0.5224582552909851,
      "learning_rate": 0.0004610731568460006,
      "loss": 6.9951,
      "step": 548
    },
    {
      "epoch": 0.15629893238434164,
      "grad_norm": 0.49455711245536804,
      "learning_rate": 0.0004610019925989183,
      "loss": 7.2002,
      "step": 549
    },
    {
      "epoch": 0.15658362989323843,
      "grad_norm": 0.46338167786598206,
      "learning_rate": 0.00046093082835183604,
      "loss": 7.1396,
      "step": 550
    },
    {
      "epoch": 0.15686832740213524,
      "grad_norm": 0.6119192838668823,
      "learning_rate": 0.00046085966410475376,
      "loss": 6.7188,
      "step": 551
    },
    {
      "epoch": 0.15715302491103203,
      "grad_norm": 0.6386239528656006,
      "learning_rate": 0.00046078849985767154,
      "loss": 6.7236,
      "step": 552
    },
    {
      "epoch": 0.15743772241992882,
      "grad_norm": 0.3954797089099884,
      "learning_rate": 0.00046071733561058926,
      "loss": 7.7266,
      "step": 553
    },
    {
      "epoch": 0.15772241992882563,
      "grad_norm": 0.5548195242881775,
      "learning_rate": 0.000460646171363507,
      "loss": 6.9756,
      "step": 554
    },
    {
      "epoch": 0.15800711743772242,
      "grad_norm": 0.6150522828102112,
      "learning_rate": 0.0004605750071164247,
      "loss": 6.5869,
      "step": 555
    },
    {
      "epoch": 0.15829181494661923,
      "grad_norm": 0.5321157574653625,
      "learning_rate": 0.00046050384286934243,
      "loss": 6.8682,
      "step": 556
    },
    {
      "epoch": 0.15857651245551602,
      "grad_norm": 0.4596554934978485,
      "learning_rate": 0.0004604326786222602,
      "loss": 7.1885,
      "step": 557
    },
    {
      "epoch": 0.1588612099644128,
      "grad_norm": 0.44151896238327026,
      "learning_rate": 0.00046036151437517793,
      "loss": 7.6816,
      "step": 558
    },
    {
      "epoch": 0.15914590747330962,
      "grad_norm": 0.5159528255462646,
      "learning_rate": 0.00046029035012809565,
      "loss": 6.8662,
      "step": 559
    },
    {
      "epoch": 0.1594306049822064,
      "grad_norm": 0.4236861765384674,
      "learning_rate": 0.00046021918588101343,
      "loss": 7.5635,
      "step": 560
    },
    {
      "epoch": 0.1597153024911032,
      "grad_norm": 0.5561614632606506,
      "learning_rate": 0.0004601480216339311,
      "loss": 6.8086,
      "step": 561
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.46602436900138855,
      "learning_rate": 0.0004600768573868488,
      "loss": 7.1611,
      "step": 562
    },
    {
      "epoch": 0.1602846975088968,
      "grad_norm": 0.43928325176239014,
      "learning_rate": 0.0004600056931397666,
      "loss": 7.5225,
      "step": 563
    },
    {
      "epoch": 0.1605693950177936,
      "grad_norm": 0.4085899293422699,
      "learning_rate": 0.0004599345288926843,
      "loss": 7.8828,
      "step": 564
    },
    {
      "epoch": 0.1608540925266904,
      "grad_norm": 0.37990400195121765,
      "learning_rate": 0.0004598633646456021,
      "loss": 7.5117,
      "step": 565
    },
    {
      "epoch": 0.16113879003558718,
      "grad_norm": 0.551302433013916,
      "learning_rate": 0.00045979220039851976,
      "loss": 7.1465,
      "step": 566
    },
    {
      "epoch": 0.161423487544484,
      "grad_norm": 0.4028521776199341,
      "learning_rate": 0.0004597210361514375,
      "loss": 7.6123,
      "step": 567
    },
    {
      "epoch": 0.16170818505338078,
      "grad_norm": 0.5788272619247437,
      "learning_rate": 0.00045964987190435526,
      "loss": 7.1279,
      "step": 568
    },
    {
      "epoch": 0.1619928825622776,
      "grad_norm": 0.5337854623794556,
      "learning_rate": 0.000459578707657273,
      "loss": 6.9785,
      "step": 569
    },
    {
      "epoch": 0.16227758007117438,
      "grad_norm": 0.5863319635391235,
      "learning_rate": 0.00045950754341019076,
      "loss": 7.1436,
      "step": 570
    },
    {
      "epoch": 0.16256227758007116,
      "grad_norm": 0.4670248031616211,
      "learning_rate": 0.0004594363791631085,
      "loss": 7.376,
      "step": 571
    },
    {
      "epoch": 0.16284697508896798,
      "grad_norm": 0.4174482524394989,
      "learning_rate": 0.00045936521491602616,
      "loss": 7.3574,
      "step": 572
    },
    {
      "epoch": 0.16313167259786476,
      "grad_norm": 0.5444307327270508,
      "learning_rate": 0.00045929405066894393,
      "loss": 7.0332,
      "step": 573
    },
    {
      "epoch": 0.16341637010676158,
      "grad_norm": 0.4734575152397156,
      "learning_rate": 0.00045922288642186165,
      "loss": 7.3379,
      "step": 574
    },
    {
      "epoch": 0.16370106761565836,
      "grad_norm": 0.43693485856056213,
      "learning_rate": 0.00045915172217477943,
      "loss": 7.3506,
      "step": 575
    },
    {
      "epoch": 0.16398576512455515,
      "grad_norm": 0.39955365657806396,
      "learning_rate": 0.00045908055792769715,
      "loss": 7.7363,
      "step": 576
    },
    {
      "epoch": 0.16427046263345196,
      "grad_norm": 0.42568933963775635,
      "learning_rate": 0.0004590093936806149,
      "loss": 7.7568,
      "step": 577
    },
    {
      "epoch": 0.16455516014234875,
      "grad_norm": 0.4571026563644409,
      "learning_rate": 0.0004589382294335326,
      "loss": 7.3809,
      "step": 578
    },
    {
      "epoch": 0.16483985765124556,
      "grad_norm": 0.45719119906425476,
      "learning_rate": 0.0004588670651864503,
      "loss": 7.4697,
      "step": 579
    },
    {
      "epoch": 0.16512455516014235,
      "grad_norm": 0.4227726459503174,
      "learning_rate": 0.00045879590093936805,
      "loss": 7.7959,
      "step": 580
    },
    {
      "epoch": 0.16540925266903914,
      "grad_norm": 0.4999239444732666,
      "learning_rate": 0.0004587247366922858,
      "loss": 7.0615,
      "step": 581
    },
    {
      "epoch": 0.16569395017793595,
      "grad_norm": 0.4378338158130646,
      "learning_rate": 0.00045865357244520355,
      "loss": 7.4492,
      "step": 582
    },
    {
      "epoch": 0.16597864768683274,
      "grad_norm": 0.4626426100730896,
      "learning_rate": 0.00045858240819812127,
      "loss": 7.6074,
      "step": 583
    },
    {
      "epoch": 0.16626334519572955,
      "grad_norm": 0.49946653842926025,
      "learning_rate": 0.000458511243951039,
      "loss": 7.5752,
      "step": 584
    },
    {
      "epoch": 0.16654804270462634,
      "grad_norm": 0.4690173864364624,
      "learning_rate": 0.0004584400797039567,
      "loss": 7.4453,
      "step": 585
    },
    {
      "epoch": 0.16683274021352312,
      "grad_norm": 0.5380011796951294,
      "learning_rate": 0.0004583689154568745,
      "loss": 7.085,
      "step": 586
    },
    {
      "epoch": 0.16711743772241994,
      "grad_norm": 0.47833171486854553,
      "learning_rate": 0.0004582977512097922,
      "loss": 7.1475,
      "step": 587
    },
    {
      "epoch": 0.16740213523131672,
      "grad_norm": 0.46506747603416443,
      "learning_rate": 0.00045822658696271,
      "loss": 6.9707,
      "step": 588
    },
    {
      "epoch": 0.1676868327402135,
      "grad_norm": 0.6595041155815125,
      "learning_rate": 0.00045815542271562766,
      "loss": 6.2119,
      "step": 589
    },
    {
      "epoch": 0.16797153024911032,
      "grad_norm": 0.47286534309387207,
      "learning_rate": 0.0004580842584685454,
      "loss": 7.2656,
      "step": 590
    },
    {
      "epoch": 0.1682562277580071,
      "grad_norm": 0.3845736086368561,
      "learning_rate": 0.00045801309422146316,
      "loss": 7.7588,
      "step": 591
    },
    {
      "epoch": 0.16854092526690392,
      "grad_norm": 0.4788035750389099,
      "learning_rate": 0.0004579419299743809,
      "loss": 7.4238,
      "step": 592
    },
    {
      "epoch": 0.1688256227758007,
      "grad_norm": 0.5757747292518616,
      "learning_rate": 0.00045787076572729866,
      "loss": 7.0264,
      "step": 593
    },
    {
      "epoch": 0.1691103202846975,
      "grad_norm": 0.4455467462539673,
      "learning_rate": 0.0004577996014802163,
      "loss": 7.6641,
      "step": 594
    },
    {
      "epoch": 0.1693950177935943,
      "grad_norm": 0.36923131346702576,
      "learning_rate": 0.00045772843723313405,
      "loss": 7.3965,
      "step": 595
    },
    {
      "epoch": 0.1696797153024911,
      "grad_norm": 0.5825194120407104,
      "learning_rate": 0.0004576572729860518,
      "loss": 7.0059,
      "step": 596
    },
    {
      "epoch": 0.1699644128113879,
      "grad_norm": 0.409291535615921,
      "learning_rate": 0.00045758610873896955,
      "loss": 7.3086,
      "step": 597
    },
    {
      "epoch": 0.1702491103202847,
      "grad_norm": 0.5457488298416138,
      "learning_rate": 0.00045751494449188727,
      "loss": 7.4639,
      "step": 598
    },
    {
      "epoch": 0.17053380782918148,
      "grad_norm": 0.5561560988426208,
      "learning_rate": 0.00045744378024480505,
      "loss": 7.0244,
      "step": 599
    },
    {
      "epoch": 0.1708185053380783,
      "grad_norm": 0.40452277660369873,
      "learning_rate": 0.0004573726159977227,
      "loss": 7.9131,
      "step": 600
    },
    {
      "epoch": 0.1708185053380783,
      "eval_bleu": 0.08136284175108972,
      "eval_loss": 7.046875,
      "eval_runtime": 175.1925,
      "eval_samples_per_second": 1.621,
      "eval_steps_per_second": 0.103,
      "step": 600
    },
    {
      "epoch": 0.17110320284697508,
      "grad_norm": 0.5157926082611084,
      "learning_rate": 0.0004573014517506405,
      "loss": 7.6035,
      "step": 601
    },
    {
      "epoch": 0.1713879003558719,
      "grad_norm": 0.5090907216072083,
      "learning_rate": 0.0004572302875035582,
      "loss": 7.3867,
      "step": 602
    },
    {
      "epoch": 0.17167259786476868,
      "grad_norm": 0.5271131992340088,
      "learning_rate": 0.00045715912325647594,
      "loss": 7.1807,
      "step": 603
    },
    {
      "epoch": 0.17195729537366547,
      "grad_norm": 0.479402631521225,
      "learning_rate": 0.0004570879590093937,
      "loss": 7.1025,
      "step": 604
    },
    {
      "epoch": 0.17224199288256228,
      "grad_norm": 0.4699629247188568,
      "learning_rate": 0.00045701679476231144,
      "loss": 7.4434,
      "step": 605
    },
    {
      "epoch": 0.17252669039145907,
      "grad_norm": 0.549485981464386,
      "learning_rate": 0.00045694563051522916,
      "loss": 7.0225,
      "step": 606
    },
    {
      "epoch": 0.17281138790035588,
      "grad_norm": 0.39508330821990967,
      "learning_rate": 0.0004568744662681469,
      "loss": 7.8623,
      "step": 607
    },
    {
      "epoch": 0.17309608540925267,
      "grad_norm": 0.5417274236679077,
      "learning_rate": 0.0004568033020210646,
      "loss": 6.9346,
      "step": 608
    },
    {
      "epoch": 0.17338078291814946,
      "grad_norm": 0.4323408901691437,
      "learning_rate": 0.0004567321377739824,
      "loss": 7.8262,
      "step": 609
    },
    {
      "epoch": 0.17366548042704627,
      "grad_norm": 0.5999831557273865,
      "learning_rate": 0.0004566609735269001,
      "loss": 7.0322,
      "step": 610
    },
    {
      "epoch": 0.17395017793594306,
      "grad_norm": 0.3982037901878357,
      "learning_rate": 0.0004565898092798178,
      "loss": 7.5947,
      "step": 611
    },
    {
      "epoch": 0.17423487544483987,
      "grad_norm": 0.41008031368255615,
      "learning_rate": 0.00045651864503273555,
      "loss": 8.0742,
      "step": 612
    },
    {
      "epoch": 0.17451957295373666,
      "grad_norm": 0.496019184589386,
      "learning_rate": 0.0004564474807856533,
      "loss": 7.5781,
      "step": 613
    },
    {
      "epoch": 0.17480427046263344,
      "grad_norm": 0.4990009665489197,
      "learning_rate": 0.00045637631653857105,
      "loss": 7.6621,
      "step": 614
    },
    {
      "epoch": 0.17508896797153026,
      "grad_norm": 0.510688304901123,
      "learning_rate": 0.0004563051522914888,
      "loss": 6.4463,
      "step": 615
    },
    {
      "epoch": 0.17537366548042704,
      "grad_norm": 0.4763341248035431,
      "learning_rate": 0.0004562339880444065,
      "loss": 7.5117,
      "step": 616
    },
    {
      "epoch": 0.17565836298932383,
      "grad_norm": 0.4208298623561859,
      "learning_rate": 0.0004561628237973242,
      "loss": 7.2637,
      "step": 617
    },
    {
      "epoch": 0.17594306049822064,
      "grad_norm": 0.4986557066440582,
      "learning_rate": 0.00045609165955024194,
      "loss": 6.9355,
      "step": 618
    },
    {
      "epoch": 0.17622775800711743,
      "grad_norm": 0.4251626431941986,
      "learning_rate": 0.0004560204953031597,
      "loss": 7.6758,
      "step": 619
    },
    {
      "epoch": 0.17651245551601424,
      "grad_norm": 0.5883548855781555,
      "learning_rate": 0.00045594933105607744,
      "loss": 6.8984,
      "step": 620
    },
    {
      "epoch": 0.17679715302491103,
      "grad_norm": 0.45804181694984436,
      "learning_rate": 0.00045587816680899517,
      "loss": 7.5889,
      "step": 621
    },
    {
      "epoch": 0.17708185053380782,
      "grad_norm": 0.5595649480819702,
      "learning_rate": 0.00045580700256191294,
      "loss": 7.3184,
      "step": 622
    },
    {
      "epoch": 0.17736654804270463,
      "grad_norm": 0.41659483313560486,
      "learning_rate": 0.0004557358383148306,
      "loss": 7.333,
      "step": 623
    },
    {
      "epoch": 0.17765124555160142,
      "grad_norm": 0.4967384934425354,
      "learning_rate": 0.0004556646740677484,
      "loss": 7.2559,
      "step": 624
    },
    {
      "epoch": 0.17793594306049823,
      "grad_norm": 0.43483781814575195,
      "learning_rate": 0.0004555935098206661,
      "loss": 7.7031,
      "step": 625
    },
    {
      "epoch": 0.17822064056939502,
      "grad_norm": 0.53510981798172,
      "learning_rate": 0.00045552234557358383,
      "loss": 7.0801,
      "step": 626
    },
    {
      "epoch": 0.1785053380782918,
      "grad_norm": 0.5476201176643372,
      "learning_rate": 0.0004554511813265016,
      "loss": 6.9561,
      "step": 627
    },
    {
      "epoch": 0.17879003558718862,
      "grad_norm": 0.5914725661277771,
      "learning_rate": 0.0004553800170794193,
      "loss": 7.0059,
      "step": 628
    },
    {
      "epoch": 0.1790747330960854,
      "grad_norm": 0.5816411972045898,
      "learning_rate": 0.000455308852832337,
      "loss": 7.1006,
      "step": 629
    },
    {
      "epoch": 0.17935943060498222,
      "grad_norm": 0.503734290599823,
      "learning_rate": 0.0004552376885852548,
      "loss": 7.2002,
      "step": 630
    },
    {
      "epoch": 0.179644128113879,
      "grad_norm": 0.4251323938369751,
      "learning_rate": 0.0004551665243381725,
      "loss": 7.6299,
      "step": 631
    },
    {
      "epoch": 0.1799288256227758,
      "grad_norm": 0.5005379319190979,
      "learning_rate": 0.0004550953600910903,
      "loss": 6.8135,
      "step": 632
    },
    {
      "epoch": 0.1802135231316726,
      "grad_norm": 0.4894733428955078,
      "learning_rate": 0.000455024195844008,
      "loss": 7.0869,
      "step": 633
    },
    {
      "epoch": 0.1804982206405694,
      "grad_norm": 0.40313103795051575,
      "learning_rate": 0.00045495303159692567,
      "loss": 7.4717,
      "step": 634
    },
    {
      "epoch": 0.1807829181494662,
      "grad_norm": 0.4881073832511902,
      "learning_rate": 0.00045488186734984345,
      "loss": 7.3096,
      "step": 635
    },
    {
      "epoch": 0.181067615658363,
      "grad_norm": 0.5019959807395935,
      "learning_rate": 0.00045481070310276117,
      "loss": 7.1348,
      "step": 636
    },
    {
      "epoch": 0.18135231316725978,
      "grad_norm": 0.5300182700157166,
      "learning_rate": 0.00045473953885567895,
      "loss": 7.3125,
      "step": 637
    },
    {
      "epoch": 0.1816370106761566,
      "grad_norm": 0.5444533824920654,
      "learning_rate": 0.00045466837460859667,
      "loss": 7.0107,
      "step": 638
    },
    {
      "epoch": 0.18192170818505338,
      "grad_norm": 0.5647503733634949,
      "learning_rate": 0.00045459721036151434,
      "loss": 7.1396,
      "step": 639
    },
    {
      "epoch": 0.1822064056939502,
      "grad_norm": 0.6619802117347717,
      "learning_rate": 0.0004545260461144321,
      "loss": 6.9766,
      "step": 640
    },
    {
      "epoch": 0.18249110320284698,
      "grad_norm": 0.4294302761554718,
      "learning_rate": 0.00045445488186734984,
      "loss": 7.6309,
      "step": 641
    },
    {
      "epoch": 0.18277580071174376,
      "grad_norm": 0.46216872334480286,
      "learning_rate": 0.0004543837176202676,
      "loss": 7.1006,
      "step": 642
    },
    {
      "epoch": 0.18306049822064058,
      "grad_norm": 0.48695653676986694,
      "learning_rate": 0.00045431255337318534,
      "loss": 7.3574,
      "step": 643
    },
    {
      "epoch": 0.18334519572953736,
      "grad_norm": 0.5963466763496399,
      "learning_rate": 0.00045424138912610306,
      "loss": 7.293,
      "step": 644
    },
    {
      "epoch": 0.18362989323843418,
      "grad_norm": 0.5873686671257019,
      "learning_rate": 0.0004541702248790208,
      "loss": 6.9521,
      "step": 645
    },
    {
      "epoch": 0.18391459074733096,
      "grad_norm": 0.368348628282547,
      "learning_rate": 0.0004540990606319385,
      "loss": 7.6436,
      "step": 646
    },
    {
      "epoch": 0.18419928825622775,
      "grad_norm": 0.37956151366233826,
      "learning_rate": 0.00045402789638485623,
      "loss": 7.9355,
      "step": 647
    },
    {
      "epoch": 0.18448398576512456,
      "grad_norm": 0.5874934792518616,
      "learning_rate": 0.000453956732137774,
      "loss": 7.1172,
      "step": 648
    },
    {
      "epoch": 0.18476868327402135,
      "grad_norm": 0.44219866394996643,
      "learning_rate": 0.00045388556789069173,
      "loss": 7.7051,
      "step": 649
    },
    {
      "epoch": 0.18505338078291814,
      "grad_norm": 0.49062198400497437,
      "learning_rate": 0.0004538144036436095,
      "loss": 7.5488,
      "step": 650
    },
    {
      "epoch": 0.18533807829181495,
      "grad_norm": 0.48055267333984375,
      "learning_rate": 0.0004537432393965272,
      "loss": 7.4219,
      "step": 651
    },
    {
      "epoch": 0.18562277580071174,
      "grad_norm": 0.477851927280426,
      "learning_rate": 0.0004536720751494449,
      "loss": 7.5186,
      "step": 652
    },
    {
      "epoch": 0.18590747330960855,
      "grad_norm": 0.44223421812057495,
      "learning_rate": 0.0004536009109023627,
      "loss": 7.4512,
      "step": 653
    },
    {
      "epoch": 0.18619217081850534,
      "grad_norm": 0.48089858889579773,
      "learning_rate": 0.0004535297466552804,
      "loss": 7.2803,
      "step": 654
    },
    {
      "epoch": 0.18647686832740212,
      "grad_norm": 0.5489603877067566,
      "learning_rate": 0.0004534585824081982,
      "loss": 6.2256,
      "step": 655
    },
    {
      "epoch": 0.18676156583629894,
      "grad_norm": 0.4798690676689148,
      "learning_rate": 0.00045338741816111584,
      "loss": 7.459,
      "step": 656
    },
    {
      "epoch": 0.18704626334519572,
      "grad_norm": 0.5676328539848328,
      "learning_rate": 0.00045331625391403357,
      "loss": 6.7197,
      "step": 657
    },
    {
      "epoch": 0.18733096085409254,
      "grad_norm": 0.5007146596908569,
      "learning_rate": 0.00045324508966695134,
      "loss": 7.3262,
      "step": 658
    },
    {
      "epoch": 0.18761565836298932,
      "grad_norm": 0.6378830075263977,
      "learning_rate": 0.00045317392541986906,
      "loss": 7.1758,
      "step": 659
    },
    {
      "epoch": 0.1879003558718861,
      "grad_norm": 0.4543790817260742,
      "learning_rate": 0.00045310276117278684,
      "loss": 7.1152,
      "step": 660
    },
    {
      "epoch": 0.18818505338078292,
      "grad_norm": 0.43836989998817444,
      "learning_rate": 0.00045303159692570456,
      "loss": 7.6982,
      "step": 661
    },
    {
      "epoch": 0.1884697508896797,
      "grad_norm": 0.48784226179122925,
      "learning_rate": 0.00045296043267862223,
      "loss": 7.4922,
      "step": 662
    },
    {
      "epoch": 0.18875444839857652,
      "grad_norm": 0.501962423324585,
      "learning_rate": 0.00045288926843154,
      "loss": 6.9375,
      "step": 663
    },
    {
      "epoch": 0.1890391459074733,
      "grad_norm": 0.4261062443256378,
      "learning_rate": 0.00045281810418445773,
      "loss": 7.4248,
      "step": 664
    },
    {
      "epoch": 0.1893238434163701,
      "grad_norm": 0.4303380250930786,
      "learning_rate": 0.00045274693993737546,
      "loss": 7.3965,
      "step": 665
    },
    {
      "epoch": 0.1896085409252669,
      "grad_norm": 0.4281022250652313,
      "learning_rate": 0.00045267577569029323,
      "loss": 7.6621,
      "step": 666
    },
    {
      "epoch": 0.1898932384341637,
      "grad_norm": 0.5295163989067078,
      "learning_rate": 0.00045260461144321096,
      "loss": 7.4043,
      "step": 667
    },
    {
      "epoch": 0.1901779359430605,
      "grad_norm": 0.48053741455078125,
      "learning_rate": 0.0004525334471961287,
      "loss": 7.4639,
      "step": 668
    },
    {
      "epoch": 0.1904626334519573,
      "grad_norm": 0.4163114130496979,
      "learning_rate": 0.0004524622829490464,
      "loss": 7.7852,
      "step": 669
    },
    {
      "epoch": 0.19074733096085408,
      "grad_norm": 0.43880659341812134,
      "learning_rate": 0.0004523911187019641,
      "loss": 7.457,
      "step": 670
    },
    {
      "epoch": 0.1910320284697509,
      "grad_norm": 0.4487917423248291,
      "learning_rate": 0.0004523199544548819,
      "loss": 7.665,
      "step": 671
    },
    {
      "epoch": 0.19131672597864768,
      "grad_norm": 0.45230862498283386,
      "learning_rate": 0.0004522487902077996,
      "loss": 7.8857,
      "step": 672
    },
    {
      "epoch": 0.1916014234875445,
      "grad_norm": 0.496181458234787,
      "learning_rate": 0.00045217762596071735,
      "loss": 7.2002,
      "step": 673
    },
    {
      "epoch": 0.19188612099644128,
      "grad_norm": 0.35912343859672546,
      "learning_rate": 0.00045210646171363507,
      "loss": 7.5195,
      "step": 674
    },
    {
      "epoch": 0.19217081850533807,
      "grad_norm": 0.462393194437027,
      "learning_rate": 0.0004520352974665528,
      "loss": 7.5742,
      "step": 675
    },
    {
      "epoch": 0.19245551601423488,
      "grad_norm": 0.5089987516403198,
      "learning_rate": 0.00045196413321947057,
      "loss": 7.1533,
      "step": 676
    },
    {
      "epoch": 0.19274021352313167,
      "grad_norm": 0.5419299602508545,
      "learning_rate": 0.0004518929689723883,
      "loss": 7.0479,
      "step": 677
    },
    {
      "epoch": 0.19302491103202846,
      "grad_norm": 0.47667187452316284,
      "learning_rate": 0.000451821804725306,
      "loss": 7.5176,
      "step": 678
    },
    {
      "epoch": 0.19330960854092527,
      "grad_norm": 0.49150925874710083,
      "learning_rate": 0.00045175064047822374,
      "loss": 7.5088,
      "step": 679
    },
    {
      "epoch": 0.19359430604982206,
      "grad_norm": 0.4631671905517578,
      "learning_rate": 0.00045167947623114146,
      "loss": 7.2822,
      "step": 680
    },
    {
      "epoch": 0.19387900355871887,
      "grad_norm": 0.6819197535514832,
      "learning_rate": 0.00045160831198405924,
      "loss": 6.5312,
      "step": 681
    },
    {
      "epoch": 0.19416370106761566,
      "grad_norm": 0.5060408115386963,
      "learning_rate": 0.00045153714773697696,
      "loss": 7.2939,
      "step": 682
    },
    {
      "epoch": 0.19444839857651244,
      "grad_norm": 0.4816899597644806,
      "learning_rate": 0.0004514659834898947,
      "loss": 7.5371,
      "step": 683
    },
    {
      "epoch": 0.19473309608540926,
      "grad_norm": 0.4419718086719513,
      "learning_rate": 0.0004513948192428124,
      "loss": 7.2891,
      "step": 684
    },
    {
      "epoch": 0.19501779359430604,
      "grad_norm": 0.5301364660263062,
      "learning_rate": 0.00045132365499573013,
      "loss": 6.8311,
      "step": 685
    },
    {
      "epoch": 0.19530249110320286,
      "grad_norm": 0.5797151327133179,
      "learning_rate": 0.0004512524907486479,
      "loss": 7.2871,
      "step": 686
    },
    {
      "epoch": 0.19558718861209964,
      "grad_norm": 0.604928195476532,
      "learning_rate": 0.00045118132650156563,
      "loss": 7.0869,
      "step": 687
    },
    {
      "epoch": 0.19587188612099643,
      "grad_norm": 0.4925019443035126,
      "learning_rate": 0.00045111016225448335,
      "loss": 7.1387,
      "step": 688
    },
    {
      "epoch": 0.19615658362989324,
      "grad_norm": 0.5087660551071167,
      "learning_rate": 0.0004510389980074011,
      "loss": 6.9668,
      "step": 689
    },
    {
      "epoch": 0.19644128113879003,
      "grad_norm": 0.4889131188392639,
      "learning_rate": 0.0004509678337603188,
      "loss": 6.9883,
      "step": 690
    },
    {
      "epoch": 0.19672597864768684,
      "grad_norm": 0.5806876420974731,
      "learning_rate": 0.00045089666951323657,
      "loss": 6.6211,
      "step": 691
    },
    {
      "epoch": 0.19701067615658363,
      "grad_norm": 0.6690099239349365,
      "learning_rate": 0.0004508255052661543,
      "loss": 6.5527,
      "step": 692
    },
    {
      "epoch": 0.19729537366548042,
      "grad_norm": 0.4910028278827667,
      "learning_rate": 0.000450754341019072,
      "loss": 7.0664,
      "step": 693
    },
    {
      "epoch": 0.19758007117437723,
      "grad_norm": 0.49179354310035706,
      "learning_rate": 0.0004506831767719898,
      "loss": 7.1865,
      "step": 694
    },
    {
      "epoch": 0.19786476868327402,
      "grad_norm": 0.638883113861084,
      "learning_rate": 0.0004506120125249075,
      "loss": 7.1006,
      "step": 695
    },
    {
      "epoch": 0.19814946619217083,
      "grad_norm": 0.400945246219635,
      "learning_rate": 0.0004505408482778252,
      "loss": 7.6387,
      "step": 696
    },
    {
      "epoch": 0.19843416370106762,
      "grad_norm": 0.5283869504928589,
      "learning_rate": 0.00045046968403074296,
      "loss": 7.2627,
      "step": 697
    },
    {
      "epoch": 0.1987188612099644,
      "grad_norm": 0.5028197765350342,
      "learning_rate": 0.0004503985197836607,
      "loss": 7.2373,
      "step": 698
    },
    {
      "epoch": 0.19900355871886122,
      "grad_norm": 0.5313175916671753,
      "learning_rate": 0.00045032735553657846,
      "loss": 7.4814,
      "step": 699
    },
    {
      "epoch": 0.199288256227758,
      "grad_norm": 0.5412628054618835,
      "learning_rate": 0.0004502561912894962,
      "loss": 7.584,
      "step": 700
    },
    {
      "epoch": 0.19957295373665482,
      "grad_norm": 0.5525862574577332,
      "learning_rate": 0.00045018502704241385,
      "loss": 6.9756,
      "step": 701
    },
    {
      "epoch": 0.1998576512455516,
      "grad_norm": 0.470053106546402,
      "learning_rate": 0.00045011386279533163,
      "loss": 7.3096,
      "step": 702
    },
    {
      "epoch": 0.2001423487544484,
      "grad_norm": 0.4619928300380707,
      "learning_rate": 0.00045004269854824935,
      "loss": 7.4365,
      "step": 703
    },
    {
      "epoch": 0.2004270462633452,
      "grad_norm": 0.511256754398346,
      "learning_rate": 0.00044997153430116713,
      "loss": 7.3418,
      "step": 704
    },
    {
      "epoch": 0.200711743772242,
      "grad_norm": 0.44021302461624146,
      "learning_rate": 0.00044990037005408485,
      "loss": 7.5215,
      "step": 705
    },
    {
      "epoch": 0.20099644128113878,
      "grad_norm": 0.45131561160087585,
      "learning_rate": 0.0004498292058070026,
      "loss": 7.0215,
      "step": 706
    },
    {
      "epoch": 0.2012811387900356,
      "grad_norm": 0.4091982841491699,
      "learning_rate": 0.0004497580415599203,
      "loss": 7.6855,
      "step": 707
    },
    {
      "epoch": 0.20156583629893238,
      "grad_norm": 0.42062363028526306,
      "learning_rate": 0.000449686877312838,
      "loss": 7.7256,
      "step": 708
    },
    {
      "epoch": 0.2018505338078292,
      "grad_norm": 0.49599114060401917,
      "learning_rate": 0.00044961571306575574,
      "loss": 7.1162,
      "step": 709
    },
    {
      "epoch": 0.20213523131672598,
      "grad_norm": 0.4859379231929779,
      "learning_rate": 0.0004495445488186735,
      "loss": 6.9316,
      "step": 710
    },
    {
      "epoch": 0.20241992882562276,
      "grad_norm": 0.40452441573143005,
      "learning_rate": 0.00044947338457159124,
      "loss": 7.4365,
      "step": 711
    },
    {
      "epoch": 0.20270462633451958,
      "grad_norm": 0.6663489937782288,
      "learning_rate": 0.000449402220324509,
      "loss": 6.9199,
      "step": 712
    },
    {
      "epoch": 0.20298932384341636,
      "grad_norm": 0.8615325093269348,
      "learning_rate": 0.0004493310560774267,
      "loss": 7.3496,
      "step": 713
    },
    {
      "epoch": 0.20327402135231318,
      "grad_norm": 0.4769529700279236,
      "learning_rate": 0.0004492598918303444,
      "loss": 7.4785,
      "step": 714
    },
    {
      "epoch": 0.20355871886120996,
      "grad_norm": 0.36396512389183044,
      "learning_rate": 0.0004491887275832622,
      "loss": 7.6992,
      "step": 715
    },
    {
      "epoch": 0.20384341637010675,
      "grad_norm": 0.43238091468811035,
      "learning_rate": 0.0004491175633361799,
      "loss": 7.6221,
      "step": 716
    },
    {
      "epoch": 0.20412811387900356,
      "grad_norm": 0.5147523283958435,
      "learning_rate": 0.0004490463990890977,
      "loss": 7.3389,
      "step": 717
    },
    {
      "epoch": 0.20441281138790035,
      "grad_norm": 0.47087812423706055,
      "learning_rate": 0.00044897523484201536,
      "loss": 7.5195,
      "step": 718
    },
    {
      "epoch": 0.20469750889679716,
      "grad_norm": 0.5744754672050476,
      "learning_rate": 0.0004489040705949331,
      "loss": 6.8584,
      "step": 719
    },
    {
      "epoch": 0.20498220640569395,
      "grad_norm": 0.7531524300575256,
      "learning_rate": 0.00044883290634785086,
      "loss": 7.6885,
      "step": 720
    },
    {
      "epoch": 0.20526690391459074,
      "grad_norm": 0.6688421964645386,
      "learning_rate": 0.0004487617421007686,
      "loss": 7.0498,
      "step": 721
    },
    {
      "epoch": 0.20555160142348755,
      "grad_norm": 0.41622069478034973,
      "learning_rate": 0.00044869057785368636,
      "loss": 7.5127,
      "step": 722
    },
    {
      "epoch": 0.20583629893238434,
      "grad_norm": 0.6788975596427917,
      "learning_rate": 0.0004486194136066041,
      "loss": 6.959,
      "step": 723
    },
    {
      "epoch": 0.20612099644128115,
      "grad_norm": 0.6270914077758789,
      "learning_rate": 0.00044854824935952175,
      "loss": 6.5908,
      "step": 724
    },
    {
      "epoch": 0.20640569395017794,
      "grad_norm": 0.42413002252578735,
      "learning_rate": 0.0004484770851124395,
      "loss": 7.3301,
      "step": 725
    },
    {
      "epoch": 0.20669039145907472,
      "grad_norm": 0.4253315031528473,
      "learning_rate": 0.00044840592086535725,
      "loss": 7.8818,
      "step": 726
    },
    {
      "epoch": 0.20697508896797154,
      "grad_norm": 0.45278969407081604,
      "learning_rate": 0.00044833475661827497,
      "loss": 7.4268,
      "step": 727
    },
    {
      "epoch": 0.20725978647686832,
      "grad_norm": 0.4045217037200928,
      "learning_rate": 0.00044826359237119275,
      "loss": 7.6025,
      "step": 728
    },
    {
      "epoch": 0.20754448398576514,
      "grad_norm": 0.5309168696403503,
      "learning_rate": 0.0004481924281241104,
      "loss": 7.1768,
      "step": 729
    },
    {
      "epoch": 0.20782918149466192,
      "grad_norm": 0.4788176715373993,
      "learning_rate": 0.0004481212638770282,
      "loss": 7.4551,
      "step": 730
    },
    {
      "epoch": 0.2081138790035587,
      "grad_norm": 0.535114586353302,
      "learning_rate": 0.0004480500996299459,
      "loss": 7.1504,
      "step": 731
    },
    {
      "epoch": 0.20839857651245552,
      "grad_norm": 0.5233950614929199,
      "learning_rate": 0.00044797893538286364,
      "loss": 7.1992,
      "step": 732
    },
    {
      "epoch": 0.2086832740213523,
      "grad_norm": 0.5039567351341248,
      "learning_rate": 0.0004479077711357814,
      "loss": 7.2734,
      "step": 733
    },
    {
      "epoch": 0.2089679715302491,
      "grad_norm": 0.5497581958770752,
      "learning_rate": 0.00044783660688869914,
      "loss": 7.126,
      "step": 734
    },
    {
      "epoch": 0.2092526690391459,
      "grad_norm": 0.5098934173583984,
      "learning_rate": 0.00044776544264161686,
      "loss": 7.2588,
      "step": 735
    },
    {
      "epoch": 0.2095373665480427,
      "grad_norm": 0.45094266533851624,
      "learning_rate": 0.0004476942783945346,
      "loss": 7.3164,
      "step": 736
    },
    {
      "epoch": 0.2098220640569395,
      "grad_norm": 0.46048614382743835,
      "learning_rate": 0.0004476231141474523,
      "loss": 7.7139,
      "step": 737
    },
    {
      "epoch": 0.2101067615658363,
      "grad_norm": 0.4291335642337799,
      "learning_rate": 0.0004475519499003701,
      "loss": 7.6035,
      "step": 738
    },
    {
      "epoch": 0.21039145907473308,
      "grad_norm": 0.3905137777328491,
      "learning_rate": 0.0004474807856532878,
      "loss": 7.9336,
      "step": 739
    },
    {
      "epoch": 0.2106761565836299,
      "grad_norm": 0.4790925085544586,
      "learning_rate": 0.0004474096214062056,
      "loss": 7.4365,
      "step": 740
    },
    {
      "epoch": 0.21096085409252668,
      "grad_norm": 0.6108134984970093,
      "learning_rate": 0.00044733845715912325,
      "loss": 6.9443,
      "step": 741
    },
    {
      "epoch": 0.2112455516014235,
      "grad_norm": 0.5870121121406555,
      "learning_rate": 0.000447267292912041,
      "loss": 7.0771,
      "step": 742
    },
    {
      "epoch": 0.21153024911032028,
      "grad_norm": 0.4234772324562073,
      "learning_rate": 0.00044719612866495875,
      "loss": 7.6436,
      "step": 743
    },
    {
      "epoch": 0.21181494661921707,
      "grad_norm": 0.4352046549320221,
      "learning_rate": 0.0004471249644178765,
      "loss": 7.7432,
      "step": 744
    },
    {
      "epoch": 0.21209964412811388,
      "grad_norm": 0.4731186032295227,
      "learning_rate": 0.0004470538001707942,
      "loss": 7.2842,
      "step": 745
    },
    {
      "epoch": 0.21238434163701067,
      "grad_norm": 0.5395245552062988,
      "learning_rate": 0.0004469826359237119,
      "loss": 6.9424,
      "step": 746
    },
    {
      "epoch": 0.21266903914590748,
      "grad_norm": 0.4573425352573395,
      "learning_rate": 0.00044691147167662964,
      "loss": 7.2559,
      "step": 747
    },
    {
      "epoch": 0.21295373665480427,
      "grad_norm": 0.4909103512763977,
      "learning_rate": 0.0004468403074295474,
      "loss": 7.2422,
      "step": 748
    },
    {
      "epoch": 0.21323843416370106,
      "grad_norm": 0.4994555115699768,
      "learning_rate": 0.00044676914318246514,
      "loss": 7.5713,
      "step": 749
    },
    {
      "epoch": 0.21352313167259787,
      "grad_norm": 0.5618712306022644,
      "learning_rate": 0.00044669797893538287,
      "loss": 6.9062,
      "step": 750
    },
    {
      "epoch": 0.21380782918149466,
      "grad_norm": 0.5334068536758423,
      "learning_rate": 0.00044662681468830064,
      "loss": 6.875,
      "step": 751
    },
    {
      "epoch": 0.21409252669039147,
      "grad_norm": 0.518782913684845,
      "learning_rate": 0.0004465556504412183,
      "loss": 7.1416,
      "step": 752
    },
    {
      "epoch": 0.21437722419928826,
      "grad_norm": 0.48912790417671204,
      "learning_rate": 0.0004464844861941361,
      "loss": 7.5664,
      "step": 753
    },
    {
      "epoch": 0.21466192170818504,
      "grad_norm": 0.38679495453834534,
      "learning_rate": 0.0004464133219470538,
      "loss": 7.5771,
      "step": 754
    },
    {
      "epoch": 0.21494661921708186,
      "grad_norm": 0.4669499099254608,
      "learning_rate": 0.00044634215769997153,
      "loss": 7.0508,
      "step": 755
    },
    {
      "epoch": 0.21523131672597864,
      "grad_norm": 0.5104237794876099,
      "learning_rate": 0.0004462709934528893,
      "loss": 6.9834,
      "step": 756
    },
    {
      "epoch": 0.21551601423487546,
      "grad_norm": 0.5343145728111267,
      "learning_rate": 0.000446199829205807,
      "loss": 7.3018,
      "step": 757
    },
    {
      "epoch": 0.21580071174377224,
      "grad_norm": 2.3170905113220215,
      "learning_rate": 0.0004461286649587247,
      "loss": 6.9307,
      "step": 758
    },
    {
      "epoch": 0.21608540925266903,
      "grad_norm": 0.45728206634521484,
      "learning_rate": 0.0004460575007116425,
      "loss": 7.5264,
      "step": 759
    },
    {
      "epoch": 0.21637010676156584,
      "grad_norm": 0.6728972792625427,
      "learning_rate": 0.0004459863364645602,
      "loss": 6.4639,
      "step": 760
    },
    {
      "epoch": 0.21665480427046263,
      "grad_norm": 0.5580065846443176,
      "learning_rate": 0.000445915172217478,
      "loss": 7.2715,
      "step": 761
    },
    {
      "epoch": 0.21693950177935942,
      "grad_norm": 0.5601818561553955,
      "learning_rate": 0.0004458440079703957,
      "loss": 7.0195,
      "step": 762
    },
    {
      "epoch": 0.21722419928825623,
      "grad_norm": 0.53824383020401,
      "learning_rate": 0.00044577284372331337,
      "loss": 7.1748,
      "step": 763
    },
    {
      "epoch": 0.21750889679715302,
      "grad_norm": 0.5673179030418396,
      "learning_rate": 0.00044570167947623115,
      "loss": 7.3262,
      "step": 764
    },
    {
      "epoch": 0.21779359430604983,
      "grad_norm": 0.5143839120864868,
      "learning_rate": 0.00044563051522914887,
      "loss": 7.2246,
      "step": 765
    },
    {
      "epoch": 0.21807829181494662,
      "grad_norm": 0.479536771774292,
      "learning_rate": 0.00044555935098206665,
      "loss": 7.5,
      "step": 766
    },
    {
      "epoch": 0.2183629893238434,
      "grad_norm": 0.5423471927642822,
      "learning_rate": 0.00044548818673498437,
      "loss": 7.626,
      "step": 767
    },
    {
      "epoch": 0.21864768683274022,
      "grad_norm": 0.43286991119384766,
      "learning_rate": 0.0004454170224879021,
      "loss": 7.3652,
      "step": 768
    },
    {
      "epoch": 0.218932384341637,
      "grad_norm": 0.5857230424880981,
      "learning_rate": 0.0004453458582408198,
      "loss": 7.0664,
      "step": 769
    },
    {
      "epoch": 0.21921708185053382,
      "grad_norm": 0.5000907778739929,
      "learning_rate": 0.00044527469399373754,
      "loss": 7.4561,
      "step": 770
    },
    {
      "epoch": 0.2195017793594306,
      "grad_norm": 0.43097177147865295,
      "learning_rate": 0.0004452035297466553,
      "loss": 7.3828,
      "step": 771
    },
    {
      "epoch": 0.2197864768683274,
      "grad_norm": 0.42178913950920105,
      "learning_rate": 0.00044513236549957304,
      "loss": 7.5723,
      "step": 772
    },
    {
      "epoch": 0.2200711743772242,
      "grad_norm": 0.6063342690467834,
      "learning_rate": 0.00044506120125249076,
      "loss": 7.0381,
      "step": 773
    },
    {
      "epoch": 0.220355871886121,
      "grad_norm": 0.5009235143661499,
      "learning_rate": 0.0004449900370054085,
      "loss": 7.2695,
      "step": 774
    },
    {
      "epoch": 0.2206405693950178,
      "grad_norm": 0.5648031830787659,
      "learning_rate": 0.0004449188727583262,
      "loss": 6.8516,
      "step": 775
    },
    {
      "epoch": 0.2209252669039146,
      "grad_norm": 0.5607783794403076,
      "learning_rate": 0.00044484770851124393,
      "loss": 6.6475,
      "step": 776
    },
    {
      "epoch": 0.22120996441281138,
      "grad_norm": 0.5817128419876099,
      "learning_rate": 0.0004447765442641617,
      "loss": 6.7178,
      "step": 777
    },
    {
      "epoch": 0.2214946619217082,
      "grad_norm": 0.5132507085800171,
      "learning_rate": 0.00044470538001707943,
      "loss": 7.5273,
      "step": 778
    },
    {
      "epoch": 0.22177935943060498,
      "grad_norm": 0.491405725479126,
      "learning_rate": 0.0004446342157699972,
      "loss": 7.3223,
      "step": 779
    },
    {
      "epoch": 0.2220640569395018,
      "grad_norm": 0.49990856647491455,
      "learning_rate": 0.0004445630515229149,
      "loss": 7.0146,
      "step": 780
    },
    {
      "epoch": 0.22234875444839858,
      "grad_norm": 0.598653256893158,
      "learning_rate": 0.0004444918872758326,
      "loss": 6.3633,
      "step": 781
    },
    {
      "epoch": 0.22263345195729536,
      "grad_norm": 0.5837319493293762,
      "learning_rate": 0.0004444207230287504,
      "loss": 7.0635,
      "step": 782
    },
    {
      "epoch": 0.22291814946619218,
      "grad_norm": 0.3937888443470001,
      "learning_rate": 0.0004443495587816681,
      "loss": 7.7715,
      "step": 783
    },
    {
      "epoch": 0.22320284697508896,
      "grad_norm": 0.44455066323280334,
      "learning_rate": 0.0004442783945345859,
      "loss": 7.417,
      "step": 784
    },
    {
      "epoch": 0.22348754448398578,
      "grad_norm": 0.4986377954483032,
      "learning_rate": 0.0004442072302875036,
      "loss": 7.3779,
      "step": 785
    },
    {
      "epoch": 0.22377224199288256,
      "grad_norm": 0.5064935088157654,
      "learning_rate": 0.00044413606604042126,
      "loss": 7.123,
      "step": 786
    },
    {
      "epoch": 0.22405693950177935,
      "grad_norm": 0.44009876251220703,
      "learning_rate": 0.00044406490179333904,
      "loss": 7.709,
      "step": 787
    },
    {
      "epoch": 0.22434163701067616,
      "grad_norm": 0.483054518699646,
      "learning_rate": 0.00044399373754625676,
      "loss": 7.5605,
      "step": 788
    },
    {
      "epoch": 0.22462633451957295,
      "grad_norm": 0.5088704228401184,
      "learning_rate": 0.00044392257329917454,
      "loss": 7.2334,
      "step": 789
    },
    {
      "epoch": 0.22491103202846974,
      "grad_norm": 0.41016024351119995,
      "learning_rate": 0.00044385140905209226,
      "loss": 7.791,
      "step": 790
    },
    {
      "epoch": 0.22519572953736655,
      "grad_norm": 0.5502153038978577,
      "learning_rate": 0.00044378024480500993,
      "loss": 7.3965,
      "step": 791
    },
    {
      "epoch": 0.22548042704626334,
      "grad_norm": 0.543286144733429,
      "learning_rate": 0.0004437090805579277,
      "loss": 7.2197,
      "step": 792
    },
    {
      "epoch": 0.22576512455516015,
      "grad_norm": 0.4524809718132019,
      "learning_rate": 0.00044363791631084543,
      "loss": 7.7568,
      "step": 793
    },
    {
      "epoch": 0.22604982206405694,
      "grad_norm": 0.46553486585617065,
      "learning_rate": 0.00044356675206376315,
      "loss": 7.4297,
      "step": 794
    },
    {
      "epoch": 0.22633451957295372,
      "grad_norm": 0.4544578492641449,
      "learning_rate": 0.00044349558781668093,
      "loss": 7.6367,
      "step": 795
    },
    {
      "epoch": 0.22661921708185054,
      "grad_norm": 0.4988901615142822,
      "learning_rate": 0.00044342442356959865,
      "loss": 7.5498,
      "step": 796
    },
    {
      "epoch": 0.22690391459074732,
      "grad_norm": 0.5461361408233643,
      "learning_rate": 0.0004433532593225164,
      "loss": 7.2773,
      "step": 797
    },
    {
      "epoch": 0.22718861209964414,
      "grad_norm": 0.5461740493774414,
      "learning_rate": 0.0004432820950754341,
      "loss": 7.0732,
      "step": 798
    },
    {
      "epoch": 0.22747330960854092,
      "grad_norm": 0.48787885904312134,
      "learning_rate": 0.0004432109308283518,
      "loss": 7.1338,
      "step": 799
    },
    {
      "epoch": 0.2277580071174377,
      "grad_norm": 0.4876340329647064,
      "learning_rate": 0.0004431397665812696,
      "loss": 7.0635,
      "step": 800
    },
    {
      "epoch": 0.2277580071174377,
      "eval_bleu": 0.10199226196567952,
      "eval_loss": 7.03515625,
      "eval_runtime": 179.5646,
      "eval_samples_per_second": 1.582,
      "eval_steps_per_second": 0.1,
      "step": 800
    },
    {
      "epoch": 0.22804270462633452,
      "grad_norm": 0.4812236428260803,
      "learning_rate": 0.0004430686023341873,
      "loss": 7.4922,
      "step": 801
    },
    {
      "epoch": 0.2283274021352313,
      "grad_norm": 0.5275574922561646,
      "learning_rate": 0.00044299743808710505,
      "loss": 7.2969,
      "step": 802
    },
    {
      "epoch": 0.22861209964412813,
      "grad_norm": 0.6468873620033264,
      "learning_rate": 0.00044292627384002277,
      "loss": 7.0674,
      "step": 803
    },
    {
      "epoch": 0.2288967971530249,
      "grad_norm": 0.5731150507926941,
      "learning_rate": 0.0004428551095929405,
      "loss": 6.8281,
      "step": 804
    },
    {
      "epoch": 0.2291814946619217,
      "grad_norm": 0.3813442587852478,
      "learning_rate": 0.00044278394534585827,
      "loss": 7.7334,
      "step": 805
    },
    {
      "epoch": 0.2294661921708185,
      "grad_norm": 0.4659816026687622,
      "learning_rate": 0.000442712781098776,
      "loss": 7.6846,
      "step": 806
    },
    {
      "epoch": 0.2297508896797153,
      "grad_norm": 0.4053337574005127,
      "learning_rate": 0.0004426416168516937,
      "loss": 7.9707,
      "step": 807
    },
    {
      "epoch": 0.2300355871886121,
      "grad_norm": 0.642076849937439,
      "learning_rate": 0.00044257045260461144,
      "loss": 7.5889,
      "step": 808
    },
    {
      "epoch": 0.2303202846975089,
      "grad_norm": 0.5518573522567749,
      "learning_rate": 0.00044249928835752916,
      "loss": 7.4805,
      "step": 809
    },
    {
      "epoch": 0.23060498220640568,
      "grad_norm": 0.5629279017448425,
      "learning_rate": 0.00044242812411044694,
      "loss": 7.0342,
      "step": 810
    },
    {
      "epoch": 0.2308896797153025,
      "grad_norm": 0.5347673892974854,
      "learning_rate": 0.00044235695986336466,
      "loss": 7.0684,
      "step": 811
    },
    {
      "epoch": 0.23117437722419928,
      "grad_norm": 0.5217247605323792,
      "learning_rate": 0.0004422857956162824,
      "loss": 7.2383,
      "step": 812
    },
    {
      "epoch": 0.2314590747330961,
      "grad_norm": 0.6054062843322754,
      "learning_rate": 0.00044221463136920016,
      "loss": 7.4854,
      "step": 813
    },
    {
      "epoch": 0.23174377224199288,
      "grad_norm": 0.6359828114509583,
      "learning_rate": 0.0004421434671221178,
      "loss": 6.7822,
      "step": 814
    },
    {
      "epoch": 0.23202846975088967,
      "grad_norm": 0.508417010307312,
      "learning_rate": 0.0004420723028750356,
      "loss": 7.4092,
      "step": 815
    },
    {
      "epoch": 0.23231316725978648,
      "grad_norm": 0.47506824135780334,
      "learning_rate": 0.0004420011386279533,
      "loss": 7.3555,
      "step": 816
    },
    {
      "epoch": 0.23259786476868327,
      "grad_norm": 0.5379090905189514,
      "learning_rate": 0.00044192997438087105,
      "loss": 6.8398,
      "step": 817
    },
    {
      "epoch": 0.23288256227758006,
      "grad_norm": 0.49462756514549255,
      "learning_rate": 0.0004418588101337888,
      "loss": 7.1006,
      "step": 818
    },
    {
      "epoch": 0.23316725978647687,
      "grad_norm": 0.4736199676990509,
      "learning_rate": 0.0004417876458867065,
      "loss": 7.082,
      "step": 819
    },
    {
      "epoch": 0.23345195729537366,
      "grad_norm": 0.46984589099884033,
      "learning_rate": 0.00044171648163962427,
      "loss": 7.0547,
      "step": 820
    },
    {
      "epoch": 0.23373665480427047,
      "grad_norm": 0.42229676246643066,
      "learning_rate": 0.000441645317392542,
      "loss": 7.5547,
      "step": 821
    },
    {
      "epoch": 0.23402135231316726,
      "grad_norm": 0.5318613052368164,
      "learning_rate": 0.0004415741531454597,
      "loss": 7.2295,
      "step": 822
    },
    {
      "epoch": 0.23430604982206404,
      "grad_norm": 0.5012297034263611,
      "learning_rate": 0.0004415029888983775,
      "loss": 7.3096,
      "step": 823
    },
    {
      "epoch": 0.23459074733096086,
      "grad_norm": 0.4093872606754303,
      "learning_rate": 0.0004414318246512952,
      "loss": 7.4219,
      "step": 824
    },
    {
      "epoch": 0.23487544483985764,
      "grad_norm": 0.542306661605835,
      "learning_rate": 0.0004413606604042129,
      "loss": 7.1299,
      "step": 825
    },
    {
      "epoch": 0.23516014234875446,
      "grad_norm": 0.5459290742874146,
      "learning_rate": 0.00044128949615713066,
      "loss": 7.2705,
      "step": 826
    },
    {
      "epoch": 0.23544483985765124,
      "grad_norm": 0.4683960974216461,
      "learning_rate": 0.0004412183319100484,
      "loss": 7.1016,
      "step": 827
    },
    {
      "epoch": 0.23572953736654803,
      "grad_norm": 0.4583689570426941,
      "learning_rate": 0.00044114716766296616,
      "loss": 7.5137,
      "step": 828
    },
    {
      "epoch": 0.23601423487544484,
      "grad_norm": 0.48082277178764343,
      "learning_rate": 0.0004410760034158839,
      "loss": 7.4746,
      "step": 829
    },
    {
      "epoch": 0.23629893238434163,
      "grad_norm": 0.4595732092857361,
      "learning_rate": 0.0004410048391688016,
      "loss": 7.3242,
      "step": 830
    },
    {
      "epoch": 0.23658362989323845,
      "grad_norm": 0.8203401565551758,
      "learning_rate": 0.00044093367492171933,
      "loss": 7.3457,
      "step": 831
    },
    {
      "epoch": 0.23686832740213523,
      "grad_norm": 0.6309863924980164,
      "learning_rate": 0.00044086251067463705,
      "loss": 6.5508,
      "step": 832
    },
    {
      "epoch": 0.23715302491103202,
      "grad_norm": 0.5663462281227112,
      "learning_rate": 0.00044079134642755483,
      "loss": 6.9541,
      "step": 833
    },
    {
      "epoch": 0.23743772241992883,
      "grad_norm": 0.5051040649414062,
      "learning_rate": 0.00044072018218047255,
      "loss": 6.9756,
      "step": 834
    },
    {
      "epoch": 0.23772241992882562,
      "grad_norm": 0.47865554690361023,
      "learning_rate": 0.0004406490179333903,
      "loss": 7.3057,
      "step": 835
    },
    {
      "epoch": 0.23800711743772243,
      "grad_norm": 0.43697893619537354,
      "learning_rate": 0.000440577853686308,
      "loss": 7.6318,
      "step": 836
    },
    {
      "epoch": 0.23829181494661922,
      "grad_norm": 0.534851610660553,
      "learning_rate": 0.0004405066894392257,
      "loss": 7.0742,
      "step": 837
    },
    {
      "epoch": 0.238576512455516,
      "grad_norm": 0.5143926739692688,
      "learning_rate": 0.0004404355251921435,
      "loss": 6.8213,
      "step": 838
    },
    {
      "epoch": 0.23886120996441282,
      "grad_norm": 0.5173239707946777,
      "learning_rate": 0.0004403643609450612,
      "loss": 7.1689,
      "step": 839
    },
    {
      "epoch": 0.2391459074733096,
      "grad_norm": 0.44212883710861206,
      "learning_rate": 0.00044029319669797894,
      "loss": 7.6211,
      "step": 840
    },
    {
      "epoch": 0.23943060498220642,
      "grad_norm": 0.4778367877006531,
      "learning_rate": 0.0004402220324508967,
      "loss": 7.2168,
      "step": 841
    },
    {
      "epoch": 0.2397153024911032,
      "grad_norm": 0.6036151051521301,
      "learning_rate": 0.0004401508682038144,
      "loss": 6.9395,
      "step": 842
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.5227299928665161,
      "learning_rate": 0.0004400797039567321,
      "loss": 7.4209,
      "step": 843
    },
    {
      "epoch": 0.2402846975088968,
      "grad_norm": 0.4932726323604584,
      "learning_rate": 0.0004400085397096499,
      "loss": 7.5088,
      "step": 844
    },
    {
      "epoch": 0.2405693950177936,
      "grad_norm": 0.41230401396751404,
      "learning_rate": 0.0004399373754625676,
      "loss": 7.6035,
      "step": 845
    },
    {
      "epoch": 0.24085409252669038,
      "grad_norm": 0.6228237748146057,
      "learning_rate": 0.0004398662112154854,
      "loss": 6.9785,
      "step": 846
    },
    {
      "epoch": 0.2411387900355872,
      "grad_norm": 0.5555920004844666,
      "learning_rate": 0.00043979504696840306,
      "loss": 7.0986,
      "step": 847
    },
    {
      "epoch": 0.24142348754448398,
      "grad_norm": 0.5252820253372192,
      "learning_rate": 0.0004397238827213208,
      "loss": 7.1113,
      "step": 848
    },
    {
      "epoch": 0.2417081850533808,
      "grad_norm": 0.5262016654014587,
      "learning_rate": 0.00043965271847423856,
      "loss": 7.3691,
      "step": 849
    },
    {
      "epoch": 0.24199288256227758,
      "grad_norm": 0.5109695196151733,
      "learning_rate": 0.0004395815542271563,
      "loss": 7.3809,
      "step": 850
    },
    {
      "epoch": 0.24227758007117436,
      "grad_norm": 0.4192237854003906,
      "learning_rate": 0.00043951038998007406,
      "loss": 7.791,
      "step": 851
    },
    {
      "epoch": 0.24256227758007118,
      "grad_norm": 0.5321003794670105,
      "learning_rate": 0.0004394392257329918,
      "loss": 7.1904,
      "step": 852
    },
    {
      "epoch": 0.24284697508896796,
      "grad_norm": 0.5241415500640869,
      "learning_rate": 0.00043936806148590945,
      "loss": 7.2236,
      "step": 853
    },
    {
      "epoch": 0.24313167259786478,
      "grad_norm": 0.4347819685935974,
      "learning_rate": 0.0004392968972388272,
      "loss": 7.6279,
      "step": 854
    },
    {
      "epoch": 0.24341637010676156,
      "grad_norm": 0.5250548720359802,
      "learning_rate": 0.00043922573299174495,
      "loss": 7.4512,
      "step": 855
    },
    {
      "epoch": 0.24370106761565835,
      "grad_norm": 0.5096853971481323,
      "learning_rate": 0.00043915456874466267,
      "loss": 7.3613,
      "step": 856
    },
    {
      "epoch": 0.24398576512455517,
      "grad_norm": 0.5170667767524719,
      "learning_rate": 0.00043908340449758045,
      "loss": 7.2148,
      "step": 857
    },
    {
      "epoch": 0.24427046263345195,
      "grad_norm": 0.4182034730911255,
      "learning_rate": 0.00043901224025049817,
      "loss": 7.5996,
      "step": 858
    },
    {
      "epoch": 0.24455516014234877,
      "grad_norm": 0.47495993971824646,
      "learning_rate": 0.0004389410760034159,
      "loss": 7.1172,
      "step": 859
    },
    {
      "epoch": 0.24483985765124555,
      "grad_norm": 0.425849050283432,
      "learning_rate": 0.0004388699117563336,
      "loss": 7.5752,
      "step": 860
    },
    {
      "epoch": 0.24512455516014234,
      "grad_norm": 0.5022419095039368,
      "learning_rate": 0.00043879874750925134,
      "loss": 7.2393,
      "step": 861
    },
    {
      "epoch": 0.24540925266903915,
      "grad_norm": 0.5683399438858032,
      "learning_rate": 0.0004387275832621691,
      "loss": 7.1514,
      "step": 862
    },
    {
      "epoch": 0.24569395017793594,
      "grad_norm": 0.3528391122817993,
      "learning_rate": 0.00043865641901508684,
      "loss": 7.8232,
      "step": 863
    },
    {
      "epoch": 0.24597864768683275,
      "grad_norm": 0.4852282404899597,
      "learning_rate": 0.00043858525476800456,
      "loss": 7.5166,
      "step": 864
    },
    {
      "epoch": 0.24626334519572954,
      "grad_norm": 0.6018944382667542,
      "learning_rate": 0.0004385140905209223,
      "loss": 7.3467,
      "step": 865
    },
    {
      "epoch": 0.24654804270462632,
      "grad_norm": 0.6870088577270508,
      "learning_rate": 0.00043844292627384,
      "loss": 6.6152,
      "step": 866
    },
    {
      "epoch": 0.24683274021352314,
      "grad_norm": 0.4139854907989502,
      "learning_rate": 0.0004383717620267578,
      "loss": 7.709,
      "step": 867
    },
    {
      "epoch": 0.24711743772241992,
      "grad_norm": 0.576387882232666,
      "learning_rate": 0.0004383005977796755,
      "loss": 6.8809,
      "step": 868
    },
    {
      "epoch": 0.24740213523131674,
      "grad_norm": 0.4755402207374573,
      "learning_rate": 0.0004382294335325933,
      "loss": 7.293,
      "step": 869
    },
    {
      "epoch": 0.24768683274021353,
      "grad_norm": 0.5647282600402832,
      "learning_rate": 0.00043815826928551095,
      "loss": 6.9727,
      "step": 870
    },
    {
      "epoch": 0.2479715302491103,
      "grad_norm": 0.6776756644248962,
      "learning_rate": 0.0004380871050384287,
      "loss": 5.9951,
      "step": 871
    },
    {
      "epoch": 0.24825622775800713,
      "grad_norm": 0.45100125670433044,
      "learning_rate": 0.00043801594079134645,
      "loss": 7.3037,
      "step": 872
    },
    {
      "epoch": 0.2485409252669039,
      "grad_norm": 0.4857039749622345,
      "learning_rate": 0.0004379447765442642,
      "loss": 7.4775,
      "step": 873
    },
    {
      "epoch": 0.24882562277580073,
      "grad_norm": 0.5256150960922241,
      "learning_rate": 0.0004378736122971819,
      "loss": 7.2344,
      "step": 874
    },
    {
      "epoch": 0.2491103202846975,
      "grad_norm": 0.5394523739814758,
      "learning_rate": 0.0004378024480500997,
      "loss": 7.0469,
      "step": 875
    },
    {
      "epoch": 0.2493950177935943,
      "grad_norm": 0.4427834153175354,
      "learning_rate": 0.00043773128380301734,
      "loss": 7.4883,
      "step": 876
    },
    {
      "epoch": 0.2496797153024911,
      "grad_norm": 0.5576911568641663,
      "learning_rate": 0.0004376601195559351,
      "loss": 6.9834,
      "step": 877
    },
    {
      "epoch": 0.2499644128113879,
      "grad_norm": 0.5288885235786438,
      "learning_rate": 0.00043758895530885284,
      "loss": 7.5918,
      "step": 878
    },
    {
      "epoch": 0.2502491103202847,
      "grad_norm": 0.8065580129623413,
      "learning_rate": 0.00043751779106177056,
      "loss": 6.6963,
      "step": 879
    },
    {
      "epoch": 0.2505338078291815,
      "grad_norm": 0.6014062762260437,
      "learning_rate": 0.00043744662681468834,
      "loss": 6.7549,
      "step": 880
    },
    {
      "epoch": 0.2508185053380783,
      "grad_norm": 0.4984685480594635,
      "learning_rate": 0.000437375462567606,
      "loss": 7.5518,
      "step": 881
    },
    {
      "epoch": 0.25110320284697507,
      "grad_norm": 0.47743654251098633,
      "learning_rate": 0.0004373042983205238,
      "loss": 7.4434,
      "step": 882
    },
    {
      "epoch": 0.2513879003558719,
      "grad_norm": 0.5659381151199341,
      "learning_rate": 0.0004372331340734415,
      "loss": 7.1982,
      "step": 883
    },
    {
      "epoch": 0.2516725978647687,
      "grad_norm": 0.521162748336792,
      "learning_rate": 0.00043716196982635923,
      "loss": 7.4102,
      "step": 884
    },
    {
      "epoch": 0.25195729537366546,
      "grad_norm": 0.5566650629043579,
      "learning_rate": 0.000437090805579277,
      "loss": 6.7998,
      "step": 885
    },
    {
      "epoch": 0.25224199288256227,
      "grad_norm": 0.5467177629470825,
      "learning_rate": 0.00043701964133219473,
      "loss": 7.2295,
      "step": 886
    },
    {
      "epoch": 0.2525266903914591,
      "grad_norm": 0.5133892297744751,
      "learning_rate": 0.0004369484770851124,
      "loss": 6.835,
      "step": 887
    },
    {
      "epoch": 0.2528113879003559,
      "grad_norm": 0.4159110486507416,
      "learning_rate": 0.0004368773128380302,
      "loss": 7.7285,
      "step": 888
    },
    {
      "epoch": 0.25309608540925266,
      "grad_norm": 0.443999320268631,
      "learning_rate": 0.0004368061485909479,
      "loss": 7.4121,
      "step": 889
    },
    {
      "epoch": 0.25338078291814947,
      "grad_norm": 0.5295789837837219,
      "learning_rate": 0.0004367349843438657,
      "loss": 7.1299,
      "step": 890
    },
    {
      "epoch": 0.2536654804270463,
      "grad_norm": 0.42882344126701355,
      "learning_rate": 0.0004366638200967834,
      "loss": 7.7754,
      "step": 891
    },
    {
      "epoch": 0.25395017793594304,
      "grad_norm": 0.5075104236602783,
      "learning_rate": 0.00043659265584970107,
      "loss": 6.7295,
      "step": 892
    },
    {
      "epoch": 0.25423487544483986,
      "grad_norm": 0.5219612121582031,
      "learning_rate": 0.00043652149160261885,
      "loss": 7.6309,
      "step": 893
    },
    {
      "epoch": 0.2545195729537367,
      "grad_norm": 0.5781932473182678,
      "learning_rate": 0.00043645032735553657,
      "loss": 6.8701,
      "step": 894
    },
    {
      "epoch": 0.25480427046263343,
      "grad_norm": 0.4656875431537628,
      "learning_rate": 0.00043637916310845435,
      "loss": 7.5107,
      "step": 895
    },
    {
      "epoch": 0.25508896797153024,
      "grad_norm": 0.512679934501648,
      "learning_rate": 0.00043630799886137207,
      "loss": 7.2812,
      "step": 896
    },
    {
      "epoch": 0.25537366548042706,
      "grad_norm": 0.530357301235199,
      "learning_rate": 0.0004362368346142898,
      "loss": 6.9248,
      "step": 897
    },
    {
      "epoch": 0.2556583629893238,
      "grad_norm": 0.44879037141799927,
      "learning_rate": 0.0004361656703672075,
      "loss": 7.5576,
      "step": 898
    },
    {
      "epoch": 0.25594306049822063,
      "grad_norm": 0.4848640263080597,
      "learning_rate": 0.00043609450612012524,
      "loss": 7.3281,
      "step": 899
    },
    {
      "epoch": 0.25622775800711745,
      "grad_norm": 0.43455377221107483,
      "learning_rate": 0.000436023341873043,
      "loss": 7.5635,
      "step": 900
    },
    {
      "epoch": 0.25651245551601426,
      "grad_norm": 0.41088178753852844,
      "learning_rate": 0.00043595217762596074,
      "loss": 7.5537,
      "step": 901
    },
    {
      "epoch": 0.256797153024911,
      "grad_norm": 0.5732548832893372,
      "learning_rate": 0.00043588101337887846,
      "loss": 6.7061,
      "step": 902
    },
    {
      "epoch": 0.25708185053380783,
      "grad_norm": 0.5296939015388489,
      "learning_rate": 0.00043580984913179624,
      "loss": 7.0996,
      "step": 903
    },
    {
      "epoch": 0.25736654804270465,
      "grad_norm": 0.44330182671546936,
      "learning_rate": 0.0004357386848847139,
      "loss": 7.5117,
      "step": 904
    },
    {
      "epoch": 0.2576512455516014,
      "grad_norm": 0.625610888004303,
      "learning_rate": 0.00043566752063763163,
      "loss": 7.1572,
      "step": 905
    },
    {
      "epoch": 0.2579359430604982,
      "grad_norm": 0.5074996948242188,
      "learning_rate": 0.0004355963563905494,
      "loss": 7.4404,
      "step": 906
    },
    {
      "epoch": 0.25822064056939503,
      "grad_norm": 0.4755004942417145,
      "learning_rate": 0.00043552519214346713,
      "loss": 7.374,
      "step": 907
    },
    {
      "epoch": 0.2585053380782918,
      "grad_norm": 0.6408856511116028,
      "learning_rate": 0.0004354540278963849,
      "loss": 7.1172,
      "step": 908
    },
    {
      "epoch": 0.2587900355871886,
      "grad_norm": 0.5064488053321838,
      "learning_rate": 0.00043538286364930257,
      "loss": 7.2129,
      "step": 909
    },
    {
      "epoch": 0.2590747330960854,
      "grad_norm": 0.6024318933486938,
      "learning_rate": 0.0004353116994022203,
      "loss": 7.0586,
      "step": 910
    },
    {
      "epoch": 0.25935943060498223,
      "grad_norm": 0.5472372174263,
      "learning_rate": 0.00043524053515513807,
      "loss": 6.8779,
      "step": 911
    },
    {
      "epoch": 0.259644128113879,
      "grad_norm": 0.4426235258579254,
      "learning_rate": 0.0004351693709080558,
      "loss": 7.7656,
      "step": 912
    },
    {
      "epoch": 0.2599288256227758,
      "grad_norm": 0.47683799266815186,
      "learning_rate": 0.00043509820666097357,
      "loss": 7.9033,
      "step": 913
    },
    {
      "epoch": 0.2602135231316726,
      "grad_norm": 0.4714563488960266,
      "learning_rate": 0.0004350270424138913,
      "loss": 7.6914,
      "step": 914
    },
    {
      "epoch": 0.2604982206405694,
      "grad_norm": 0.5874764323234558,
      "learning_rate": 0.00043495587816680896,
      "loss": 6.9482,
      "step": 915
    },
    {
      "epoch": 0.2607829181494662,
      "grad_norm": 0.46627673506736755,
      "learning_rate": 0.00043488471391972674,
      "loss": 7.5713,
      "step": 916
    },
    {
      "epoch": 0.261067615658363,
      "grad_norm": 0.5269532203674316,
      "learning_rate": 0.00043481354967264446,
      "loss": 7.1367,
      "step": 917
    },
    {
      "epoch": 0.26135231316725976,
      "grad_norm": 0.49821069836616516,
      "learning_rate": 0.00043474238542556224,
      "loss": 7.6846,
      "step": 918
    },
    {
      "epoch": 0.2616370106761566,
      "grad_norm": 0.47218114137649536,
      "learning_rate": 0.00043467122117847996,
      "loss": 7.5557,
      "step": 919
    },
    {
      "epoch": 0.2619217081850534,
      "grad_norm": 0.5291839241981506,
      "learning_rate": 0.0004346000569313977,
      "loss": 7.4102,
      "step": 920
    },
    {
      "epoch": 0.26220640569395015,
      "grad_norm": 0.5340070724487305,
      "learning_rate": 0.0004345288926843154,
      "loss": 6.9697,
      "step": 921
    },
    {
      "epoch": 0.26249110320284696,
      "grad_norm": 0.5339959859848022,
      "learning_rate": 0.00043445772843723313,
      "loss": 7.0928,
      "step": 922
    },
    {
      "epoch": 0.2627758007117438,
      "grad_norm": 0.580223798751831,
      "learning_rate": 0.00043438656419015085,
      "loss": 6.8213,
      "step": 923
    },
    {
      "epoch": 0.2630604982206406,
      "grad_norm": 0.5580481290817261,
      "learning_rate": 0.00043431539994306863,
      "loss": 6.8799,
      "step": 924
    },
    {
      "epoch": 0.26334519572953735,
      "grad_norm": 0.4867836534976959,
      "learning_rate": 0.00043424423569598635,
      "loss": 7.8086,
      "step": 925
    },
    {
      "epoch": 0.26362989323843417,
      "grad_norm": 0.4634561538696289,
      "learning_rate": 0.0004341730714489041,
      "loss": 7.4473,
      "step": 926
    },
    {
      "epoch": 0.263914590747331,
      "grad_norm": 0.5524905920028687,
      "learning_rate": 0.0004341019072018218,
      "loss": 6.9121,
      "step": 927
    },
    {
      "epoch": 0.26419928825622774,
      "grad_norm": 0.45816829800605774,
      "learning_rate": 0.0004340307429547395,
      "loss": 7.3428,
      "step": 928
    },
    {
      "epoch": 0.26448398576512455,
      "grad_norm": 0.6225414276123047,
      "learning_rate": 0.0004339595787076573,
      "loss": 6.8535,
      "step": 929
    },
    {
      "epoch": 0.26476868327402137,
      "grad_norm": 0.4236343801021576,
      "learning_rate": 0.000433888414460575,
      "loss": 8.0156,
      "step": 930
    },
    {
      "epoch": 0.2650533807829181,
      "grad_norm": 0.4699235260486603,
      "learning_rate": 0.0004338172502134928,
      "loss": 7.5518,
      "step": 931
    },
    {
      "epoch": 0.26533807829181494,
      "grad_norm": 0.4837797284126282,
      "learning_rate": 0.00043374608596641047,
      "loss": 7.3555,
      "step": 932
    },
    {
      "epoch": 0.26562277580071175,
      "grad_norm": 0.5280996561050415,
      "learning_rate": 0.0004336749217193282,
      "loss": 6.7998,
      "step": 933
    },
    {
      "epoch": 0.26590747330960857,
      "grad_norm": 0.5825293064117432,
      "learning_rate": 0.00043360375747224597,
      "loss": 7.0225,
      "step": 934
    },
    {
      "epoch": 0.2661921708185053,
      "grad_norm": 0.38843557238578796,
      "learning_rate": 0.0004335325932251637,
      "loss": 7.8369,
      "step": 935
    },
    {
      "epoch": 0.26647686832740214,
      "grad_norm": 0.41565942764282227,
      "learning_rate": 0.00043346142897808147,
      "loss": 7.6914,
      "step": 936
    },
    {
      "epoch": 0.26676156583629895,
      "grad_norm": 0.5718416571617126,
      "learning_rate": 0.00043339026473099914,
      "loss": 6.6875,
      "step": 937
    },
    {
      "epoch": 0.2670462633451957,
      "grad_norm": 0.5091272592544556,
      "learning_rate": 0.00043331910048391686,
      "loss": 7.2822,
      "step": 938
    },
    {
      "epoch": 0.2673309608540925,
      "grad_norm": 0.47761550545692444,
      "learning_rate": 0.00043324793623683463,
      "loss": 7.2646,
      "step": 939
    },
    {
      "epoch": 0.26761565836298934,
      "grad_norm": 0.4958757758140564,
      "learning_rate": 0.00043317677198975236,
      "loss": 7.3691,
      "step": 940
    },
    {
      "epoch": 0.2679003558718861,
      "grad_norm": 0.5092728137969971,
      "learning_rate": 0.0004331056077426701,
      "loss": 7.3887,
      "step": 941
    },
    {
      "epoch": 0.2681850533807829,
      "grad_norm": 0.5320535898208618,
      "learning_rate": 0.00043303444349558786,
      "loss": 7.2588,
      "step": 942
    },
    {
      "epoch": 0.2684697508896797,
      "grad_norm": 0.4988536834716797,
      "learning_rate": 0.0004329632792485055,
      "loss": 7.1562,
      "step": 943
    },
    {
      "epoch": 0.26875444839857654,
      "grad_norm": 0.5935326814651489,
      "learning_rate": 0.0004328921150014233,
      "loss": 6.0732,
      "step": 944
    },
    {
      "epoch": 0.2690391459074733,
      "grad_norm": 0.4439448416233063,
      "learning_rate": 0.000432820950754341,
      "loss": 7.1514,
      "step": 945
    },
    {
      "epoch": 0.2693238434163701,
      "grad_norm": 0.41855907440185547,
      "learning_rate": 0.00043274978650725875,
      "loss": 7.793,
      "step": 946
    },
    {
      "epoch": 0.2696085409252669,
      "grad_norm": 0.4971495568752289,
      "learning_rate": 0.0004326786222601765,
      "loss": 7.2061,
      "step": 947
    },
    {
      "epoch": 0.2698932384341637,
      "grad_norm": 0.5047661662101746,
      "learning_rate": 0.00043260745801309425,
      "loss": 6.791,
      "step": 948
    },
    {
      "epoch": 0.2701779359430605,
      "grad_norm": 0.4491565525531769,
      "learning_rate": 0.00043253629376601197,
      "loss": 7.3984,
      "step": 949
    },
    {
      "epoch": 0.2704626334519573,
      "grad_norm": 0.5155214071273804,
      "learning_rate": 0.0004324651295189297,
      "loss": 7.3164,
      "step": 950
    },
    {
      "epoch": 0.27074733096085407,
      "grad_norm": 0.46427953243255615,
      "learning_rate": 0.0004323939652718474,
      "loss": 7.3857,
      "step": 951
    },
    {
      "epoch": 0.2710320284697509,
      "grad_norm": 0.5414927005767822,
      "learning_rate": 0.0004323228010247652,
      "loss": 7.0195,
      "step": 952
    },
    {
      "epoch": 0.2713167259786477,
      "grad_norm": 0.5723718404769897,
      "learning_rate": 0.0004322516367776829,
      "loss": 7.0186,
      "step": 953
    },
    {
      "epoch": 0.27160142348754446,
      "grad_norm": 0.5131714940071106,
      "learning_rate": 0.0004321804725306006,
      "loss": 7.1494,
      "step": 954
    },
    {
      "epoch": 0.27188612099644127,
      "grad_norm": 0.6219976544380188,
      "learning_rate": 0.00043210930828351836,
      "loss": 6.8535,
      "step": 955
    },
    {
      "epoch": 0.2721708185053381,
      "grad_norm": 0.4529915452003479,
      "learning_rate": 0.0004320381440364361,
      "loss": 7.4473,
      "step": 956
    },
    {
      "epoch": 0.2724555160142349,
      "grad_norm": 0.5033783912658691,
      "learning_rate": 0.00043196697978935386,
      "loss": 7.4102,
      "step": 957
    },
    {
      "epoch": 0.27274021352313166,
      "grad_norm": 0.44644036889076233,
      "learning_rate": 0.0004318958155422716,
      "loss": 7.7686,
      "step": 958
    },
    {
      "epoch": 0.27302491103202847,
      "grad_norm": 0.5473158359527588,
      "learning_rate": 0.0004318246512951893,
      "loss": 7.25,
      "step": 959
    },
    {
      "epoch": 0.2733096085409253,
      "grad_norm": 0.4492653012275696,
      "learning_rate": 0.00043175348704810703,
      "loss": 7.8701,
      "step": 960
    },
    {
      "epoch": 0.27359430604982204,
      "grad_norm": 0.47338056564331055,
      "learning_rate": 0.00043168232280102475,
      "loss": 7.2969,
      "step": 961
    },
    {
      "epoch": 0.27387900355871886,
      "grad_norm": 0.5589231848716736,
      "learning_rate": 0.00043161115855394253,
      "loss": 7.4736,
      "step": 962
    },
    {
      "epoch": 0.2741637010676157,
      "grad_norm": 0.4764990210533142,
      "learning_rate": 0.00043153999430686025,
      "loss": 7.3125,
      "step": 963
    },
    {
      "epoch": 0.27444839857651243,
      "grad_norm": 0.5841454863548279,
      "learning_rate": 0.000431468830059778,
      "loss": 7.0547,
      "step": 964
    },
    {
      "epoch": 0.27473309608540925,
      "grad_norm": 0.52353835105896,
      "learning_rate": 0.0004313976658126957,
      "loss": 7.459,
      "step": 965
    },
    {
      "epoch": 0.27501779359430606,
      "grad_norm": 0.523868978023529,
      "learning_rate": 0.0004313265015656134,
      "loss": 7.3135,
      "step": 966
    },
    {
      "epoch": 0.2753024911032029,
      "grad_norm": 0.4624759554862976,
      "learning_rate": 0.0004312553373185312,
      "loss": 7.6846,
      "step": 967
    },
    {
      "epoch": 0.27558718861209963,
      "grad_norm": 0.5388631224632263,
      "learning_rate": 0.0004311841730714489,
      "loss": 7.3652,
      "step": 968
    },
    {
      "epoch": 0.27587188612099645,
      "grad_norm": 0.5112046599388123,
      "learning_rate": 0.00043111300882436664,
      "loss": 7.6865,
      "step": 969
    },
    {
      "epoch": 0.27615658362989326,
      "grad_norm": 0.40607786178588867,
      "learning_rate": 0.0004310418445772844,
      "loss": 7.4023,
      "step": 970
    },
    {
      "epoch": 0.27644128113879,
      "grad_norm": 0.44282764196395874,
      "learning_rate": 0.0004309706803302021,
      "loss": 7.7432,
      "step": 971
    },
    {
      "epoch": 0.27672597864768683,
      "grad_norm": 0.4683508574962616,
      "learning_rate": 0.0004308995160831198,
      "loss": 7.6504,
      "step": 972
    },
    {
      "epoch": 0.27701067615658365,
      "grad_norm": 0.5277723073959351,
      "learning_rate": 0.0004308283518360376,
      "loss": 7.3008,
      "step": 973
    },
    {
      "epoch": 0.2772953736654804,
      "grad_norm": 0.48368245363235474,
      "learning_rate": 0.0004307571875889553,
      "loss": 7.2158,
      "step": 974
    },
    {
      "epoch": 0.2775800711743772,
      "grad_norm": 0.5163562893867493,
      "learning_rate": 0.0004306860233418731,
      "loss": 7.3232,
      "step": 975
    },
    {
      "epoch": 0.27786476868327403,
      "grad_norm": 0.5331327319145203,
      "learning_rate": 0.0004306148590947908,
      "loss": 6.9707,
      "step": 976
    },
    {
      "epoch": 0.2781494661921708,
      "grad_norm": 0.5096694231033325,
      "learning_rate": 0.0004305436948477085,
      "loss": 7.1387,
      "step": 977
    },
    {
      "epoch": 0.2784341637010676,
      "grad_norm": 0.6267108917236328,
      "learning_rate": 0.00043047253060062626,
      "loss": 6.917,
      "step": 978
    },
    {
      "epoch": 0.2787188612099644,
      "grad_norm": 0.4161131680011749,
      "learning_rate": 0.000430401366353544,
      "loss": 7.3604,
      "step": 979
    },
    {
      "epoch": 0.27900355871886123,
      "grad_norm": 0.41714102029800415,
      "learning_rate": 0.00043033020210646176,
      "loss": 7.6514,
      "step": 980
    },
    {
      "epoch": 0.279288256227758,
      "grad_norm": 0.5368282794952393,
      "learning_rate": 0.0004302590378593795,
      "loss": 7.1318,
      "step": 981
    },
    {
      "epoch": 0.2795729537366548,
      "grad_norm": 0.42313826084136963,
      "learning_rate": 0.00043018787361229715,
      "loss": 7.5117,
      "step": 982
    },
    {
      "epoch": 0.2798576512455516,
      "grad_norm": 0.4829246997833252,
      "learning_rate": 0.0004301167093652149,
      "loss": 7.4912,
      "step": 983
    },
    {
      "epoch": 0.2801423487544484,
      "grad_norm": 0.6406773924827576,
      "learning_rate": 0.00043004554511813265,
      "loss": 6.4014,
      "step": 984
    },
    {
      "epoch": 0.2804270462633452,
      "grad_norm": 0.3736668527126312,
      "learning_rate": 0.0004299743808710504,
      "loss": 7.9062,
      "step": 985
    },
    {
      "epoch": 0.280711743772242,
      "grad_norm": 0.522930383682251,
      "learning_rate": 0.00042990321662396815,
      "loss": 7.0312,
      "step": 986
    },
    {
      "epoch": 0.28099644128113876,
      "grad_norm": 0.4965778887271881,
      "learning_rate": 0.00042983205237688587,
      "loss": 7.1846,
      "step": 987
    },
    {
      "epoch": 0.2812811387900356,
      "grad_norm": 0.45272135734558105,
      "learning_rate": 0.0004297608881298036,
      "loss": 7.2119,
      "step": 988
    },
    {
      "epoch": 0.2815658362989324,
      "grad_norm": 0.5090800523757935,
      "learning_rate": 0.0004296897238827213,
      "loss": 7.0088,
      "step": 989
    },
    {
      "epoch": 0.2818505338078292,
      "grad_norm": 0.6086441874504089,
      "learning_rate": 0.00042961855963563904,
      "loss": 7.2715,
      "step": 990
    },
    {
      "epoch": 0.28213523131672597,
      "grad_norm": 0.7149702906608582,
      "learning_rate": 0.0004295473953885568,
      "loss": 6.3223,
      "step": 991
    },
    {
      "epoch": 0.2824199288256228,
      "grad_norm": 0.574139416217804,
      "learning_rate": 0.00042947623114147454,
      "loss": 7.1348,
      "step": 992
    },
    {
      "epoch": 0.2827046263345196,
      "grad_norm": 0.4520958960056305,
      "learning_rate": 0.0004294050668943923,
      "loss": 7.4697,
      "step": 993
    },
    {
      "epoch": 0.28298932384341635,
      "grad_norm": 0.4637325406074524,
      "learning_rate": 0.00042933390264731,
      "loss": 7.2773,
      "step": 994
    },
    {
      "epoch": 0.28327402135231317,
      "grad_norm": 0.4676058888435364,
      "learning_rate": 0.0004292627384002277,
      "loss": 7.3975,
      "step": 995
    },
    {
      "epoch": 0.28355871886121,
      "grad_norm": 0.42332184314727783,
      "learning_rate": 0.0004291915741531455,
      "loss": 7.6719,
      "step": 996
    },
    {
      "epoch": 0.28384341637010674,
      "grad_norm": 0.47916945815086365,
      "learning_rate": 0.0004291204099060632,
      "loss": 7.3799,
      "step": 997
    },
    {
      "epoch": 0.28412811387900355,
      "grad_norm": 0.5687378644943237,
      "learning_rate": 0.000429049245658981,
      "loss": 7.1699,
      "step": 998
    },
    {
      "epoch": 0.28441281138790037,
      "grad_norm": 0.38771921396255493,
      "learning_rate": 0.00042897808141189865,
      "loss": 7.5986,
      "step": 999
    },
    {
      "epoch": 0.2846975088967972,
      "grad_norm": 0.43714073300361633,
      "learning_rate": 0.0004289069171648164,
      "loss": 7.5859,
      "step": 1000
    },
    {
      "epoch": 0.2846975088967972,
      "eval_bleu": 0.10196535812756952,
      "eval_loss": 7.04296875,
      "eval_runtime": 121.1357,
      "eval_samples_per_second": 2.344,
      "eval_steps_per_second": 0.149,
      "step": 1000
    },
    {
      "epoch": 0.28498220640569394,
      "grad_norm": 0.4797312617301941,
      "learning_rate": 0.00042883575291773415,
      "loss": 7.6777,
      "step": 1001
    },
    {
      "epoch": 0.28526690391459075,
      "grad_norm": 0.47663864493370056,
      "learning_rate": 0.0004287645886706519,
      "loss": 7.6465,
      "step": 1002
    },
    {
      "epoch": 0.28555160142348757,
      "grad_norm": 0.4880309998989105,
      "learning_rate": 0.0004286934244235696,
      "loss": 7.5508,
      "step": 1003
    },
    {
      "epoch": 0.2858362989323843,
      "grad_norm": 0.4995867908000946,
      "learning_rate": 0.0004286222601764874,
      "loss": 7.2236,
      "step": 1004
    },
    {
      "epoch": 0.28612099644128114,
      "grad_norm": 0.5235399603843689,
      "learning_rate": 0.00042855109592940504,
      "loss": 6.8809,
      "step": 1005
    },
    {
      "epoch": 0.28640569395017795,
      "grad_norm": 0.4574708938598633,
      "learning_rate": 0.0004284799316823228,
      "loss": 7.4971,
      "step": 1006
    },
    {
      "epoch": 0.2866903914590747,
      "grad_norm": 0.6360961198806763,
      "learning_rate": 0.00042840876743524054,
      "loss": 7.5049,
      "step": 1007
    },
    {
      "epoch": 0.2869750889679715,
      "grad_norm": 0.5685173869132996,
      "learning_rate": 0.00042833760318815826,
      "loss": 6.958,
      "step": 1008
    },
    {
      "epoch": 0.28725978647686834,
      "grad_norm": 0.4308505654335022,
      "learning_rate": 0.00042826643894107604,
      "loss": 7.6084,
      "step": 1009
    },
    {
      "epoch": 0.2875444839857651,
      "grad_norm": 0.4639454483985901,
      "learning_rate": 0.0004281952746939937,
      "loss": 7.4463,
      "step": 1010
    },
    {
      "epoch": 0.2878291814946619,
      "grad_norm": 0.4709155857563019,
      "learning_rate": 0.0004281241104469115,
      "loss": 7.5322,
      "step": 1011
    },
    {
      "epoch": 0.2881138790035587,
      "grad_norm": 0.43574628233909607,
      "learning_rate": 0.0004280529461998292,
      "loss": 7.5186,
      "step": 1012
    },
    {
      "epoch": 0.28839857651245554,
      "grad_norm": 0.4731388986110687,
      "learning_rate": 0.00042798178195274693,
      "loss": 7.2949,
      "step": 1013
    },
    {
      "epoch": 0.2886832740213523,
      "grad_norm": 0.4204134941101074,
      "learning_rate": 0.0004279106177056647,
      "loss": 7.5859,
      "step": 1014
    },
    {
      "epoch": 0.2889679715302491,
      "grad_norm": 0.5122082829475403,
      "learning_rate": 0.00042783945345858243,
      "loss": 7.2627,
      "step": 1015
    },
    {
      "epoch": 0.2892526690391459,
      "grad_norm": 0.3954521119594574,
      "learning_rate": 0.00042776828921150015,
      "loss": 7.543,
      "step": 1016
    },
    {
      "epoch": 0.2895373665480427,
      "grad_norm": 0.4996376037597656,
      "learning_rate": 0.0004276971249644179,
      "loss": 7.4268,
      "step": 1017
    },
    {
      "epoch": 0.2898220640569395,
      "grad_norm": 0.48199692368507385,
      "learning_rate": 0.0004276259607173356,
      "loss": 7.4111,
      "step": 1018
    },
    {
      "epoch": 0.2901067615658363,
      "grad_norm": 0.4896969497203827,
      "learning_rate": 0.0004275547964702534,
      "loss": 7.5732,
      "step": 1019
    },
    {
      "epoch": 0.29039145907473307,
      "grad_norm": 0.5372595191001892,
      "learning_rate": 0.0004274836322231711,
      "loss": 6.9336,
      "step": 1020
    },
    {
      "epoch": 0.2906761565836299,
      "grad_norm": 0.46799662709236145,
      "learning_rate": 0.0004274124679760888,
      "loss": 7.6201,
      "step": 1021
    },
    {
      "epoch": 0.2909608540925267,
      "grad_norm": 0.47574394941329956,
      "learning_rate": 0.00042734130372900655,
      "loss": 7.2734,
      "step": 1022
    },
    {
      "epoch": 0.2912455516014235,
      "grad_norm": 0.4846518337726593,
      "learning_rate": 0.00042727013948192427,
      "loss": 7.4453,
      "step": 1023
    },
    {
      "epoch": 0.29153024911032027,
      "grad_norm": 0.5557963848114014,
      "learning_rate": 0.00042719897523484205,
      "loss": 7.0713,
      "step": 1024
    },
    {
      "epoch": 0.2918149466192171,
      "grad_norm": 0.4947628080844879,
      "learning_rate": 0.00042712781098775977,
      "loss": 7.5947,
      "step": 1025
    },
    {
      "epoch": 0.2920996441281139,
      "grad_norm": 0.492767333984375,
      "learning_rate": 0.0004270566467406775,
      "loss": 7.252,
      "step": 1026
    },
    {
      "epoch": 0.29238434163701066,
      "grad_norm": 0.6265051960945129,
      "learning_rate": 0.0004269854824935952,
      "loss": 6.3564,
      "step": 1027
    },
    {
      "epoch": 0.2926690391459075,
      "grad_norm": 0.5161956548690796,
      "learning_rate": 0.00042691431824651294,
      "loss": 7.5752,
      "step": 1028
    },
    {
      "epoch": 0.2929537366548043,
      "grad_norm": 0.5737975239753723,
      "learning_rate": 0.0004268431539994307,
      "loss": 7.0615,
      "step": 1029
    },
    {
      "epoch": 0.29323843416370104,
      "grad_norm": 0.4803426265716553,
      "learning_rate": 0.00042677198975234844,
      "loss": 7.5,
      "step": 1030
    },
    {
      "epoch": 0.29352313167259786,
      "grad_norm": 0.4499126076698303,
      "learning_rate": 0.00042670082550526616,
      "loss": 7.2568,
      "step": 1031
    },
    {
      "epoch": 0.2938078291814947,
      "grad_norm": 0.4502011239528656,
      "learning_rate": 0.00042662966125818394,
      "loss": 7.7002,
      "step": 1032
    },
    {
      "epoch": 0.29409252669039143,
      "grad_norm": 0.43873316049575806,
      "learning_rate": 0.0004265584970111016,
      "loss": 7.4287,
      "step": 1033
    },
    {
      "epoch": 0.29437722419928825,
      "grad_norm": 0.4562997817993164,
      "learning_rate": 0.0004264873327640193,
      "loss": 7.5312,
      "step": 1034
    },
    {
      "epoch": 0.29466192170818506,
      "grad_norm": 0.5061038136482239,
      "learning_rate": 0.0004264161685169371,
      "loss": 7.4629,
      "step": 1035
    },
    {
      "epoch": 0.2949466192170819,
      "grad_norm": 0.4653073847293854,
      "learning_rate": 0.0004263450042698548,
      "loss": 7.5996,
      "step": 1036
    },
    {
      "epoch": 0.29523131672597863,
      "grad_norm": 0.6139973402023315,
      "learning_rate": 0.0004262738400227726,
      "loss": 6.3779,
      "step": 1037
    },
    {
      "epoch": 0.29551601423487545,
      "grad_norm": 0.5320224761962891,
      "learning_rate": 0.0004262026757756903,
      "loss": 7.127,
      "step": 1038
    },
    {
      "epoch": 0.29580071174377226,
      "grad_norm": 0.5026519298553467,
      "learning_rate": 0.000426131511528608,
      "loss": 7.3281,
      "step": 1039
    },
    {
      "epoch": 0.296085409252669,
      "grad_norm": 0.5016393065452576,
      "learning_rate": 0.00042606034728152577,
      "loss": 7.0791,
      "step": 1040
    },
    {
      "epoch": 0.29637010676156583,
      "grad_norm": 0.516268253326416,
      "learning_rate": 0.0004259891830344435,
      "loss": 7.041,
      "step": 1041
    },
    {
      "epoch": 0.29665480427046265,
      "grad_norm": 0.48828786611557007,
      "learning_rate": 0.00042591801878736127,
      "loss": 7.4434,
      "step": 1042
    },
    {
      "epoch": 0.2969395017793594,
      "grad_norm": 0.4357978105545044,
      "learning_rate": 0.000425846854540279,
      "loss": 7.1211,
      "step": 1043
    },
    {
      "epoch": 0.2972241992882562,
      "grad_norm": 0.5390711426734924,
      "learning_rate": 0.00042577569029319666,
      "loss": 6.9951,
      "step": 1044
    },
    {
      "epoch": 0.29750889679715303,
      "grad_norm": 0.43850216269493103,
      "learning_rate": 0.00042570452604611444,
      "loss": 7.4346,
      "step": 1045
    },
    {
      "epoch": 0.29779359430604985,
      "grad_norm": 0.5152047276496887,
      "learning_rate": 0.00042563336179903216,
      "loss": 7.2969,
      "step": 1046
    },
    {
      "epoch": 0.2980782918149466,
      "grad_norm": 0.4733029305934906,
      "learning_rate": 0.00042556219755194994,
      "loss": 7.626,
      "step": 1047
    },
    {
      "epoch": 0.2983629893238434,
      "grad_norm": 0.5408203601837158,
      "learning_rate": 0.00042549103330486766,
      "loss": 7.0078,
      "step": 1048
    },
    {
      "epoch": 0.29864768683274023,
      "grad_norm": 0.39213523268699646,
      "learning_rate": 0.0004254198690577854,
      "loss": 7.5322,
      "step": 1049
    },
    {
      "epoch": 0.298932384341637,
      "grad_norm": 0.4816019833087921,
      "learning_rate": 0.0004253487048107031,
      "loss": 7.7979,
      "step": 1050
    },
    {
      "epoch": 0.2992170818505338,
      "grad_norm": 0.47788047790527344,
      "learning_rate": 0.00042527754056362083,
      "loss": 7.3506,
      "step": 1051
    },
    {
      "epoch": 0.2995017793594306,
      "grad_norm": 0.5580766201019287,
      "learning_rate": 0.00042520637631653855,
      "loss": 6.7529,
      "step": 1052
    },
    {
      "epoch": 0.2997864768683274,
      "grad_norm": 0.4852105379104614,
      "learning_rate": 0.00042513521206945633,
      "loss": 7.3311,
      "step": 1053
    },
    {
      "epoch": 0.3000711743772242,
      "grad_norm": 0.5395891070365906,
      "learning_rate": 0.00042506404782237405,
      "loss": 6.998,
      "step": 1054
    },
    {
      "epoch": 0.300355871886121,
      "grad_norm": 0.5457566976547241,
      "learning_rate": 0.0004249928835752918,
      "loss": 7.085,
      "step": 1055
    },
    {
      "epoch": 0.3006405693950178,
      "grad_norm": 0.5181650519371033,
      "learning_rate": 0.0004249217193282095,
      "loss": 7.3096,
      "step": 1056
    },
    {
      "epoch": 0.3009252669039146,
      "grad_norm": 0.5144093036651611,
      "learning_rate": 0.0004248505550811272,
      "loss": 7.0566,
      "step": 1057
    },
    {
      "epoch": 0.3012099644128114,
      "grad_norm": 0.6092381477355957,
      "learning_rate": 0.000424779390834045,
      "loss": 7.3281,
      "step": 1058
    },
    {
      "epoch": 0.3014946619217082,
      "grad_norm": 0.5332916975021362,
      "learning_rate": 0.0004247082265869627,
      "loss": 7.002,
      "step": 1059
    },
    {
      "epoch": 0.30177935943060497,
      "grad_norm": 0.5226554274559021,
      "learning_rate": 0.0004246370623398805,
      "loss": 7.1387,
      "step": 1060
    },
    {
      "epoch": 0.3020640569395018,
      "grad_norm": 0.5610384941101074,
      "learning_rate": 0.00042456589809279817,
      "loss": 7.3506,
      "step": 1061
    },
    {
      "epoch": 0.3023487544483986,
      "grad_norm": 0.42654889822006226,
      "learning_rate": 0.0004244947338457159,
      "loss": 7.5635,
      "step": 1062
    },
    {
      "epoch": 0.30263345195729535,
      "grad_norm": 0.5280149579048157,
      "learning_rate": 0.00042442356959863367,
      "loss": 7.29,
      "step": 1063
    },
    {
      "epoch": 0.30291814946619217,
      "grad_norm": 0.4202621579170227,
      "learning_rate": 0.0004243524053515514,
      "loss": 7.5176,
      "step": 1064
    },
    {
      "epoch": 0.303202846975089,
      "grad_norm": 0.5048372745513916,
      "learning_rate": 0.00042428124110446917,
      "loss": 6.957,
      "step": 1065
    },
    {
      "epoch": 0.30348754448398574,
      "grad_norm": 0.6739668846130371,
      "learning_rate": 0.0004242100768573869,
      "loss": 6.7461,
      "step": 1066
    },
    {
      "epoch": 0.30377224199288255,
      "grad_norm": 0.74714595079422,
      "learning_rate": 0.00042413891261030456,
      "loss": 7.7529,
      "step": 1067
    },
    {
      "epoch": 0.30405693950177937,
      "grad_norm": 0.6046257019042969,
      "learning_rate": 0.00042406774836322233,
      "loss": 7.0068,
      "step": 1068
    },
    {
      "epoch": 0.3043416370106762,
      "grad_norm": 0.5528740882873535,
      "learning_rate": 0.00042399658411614006,
      "loss": 7.2773,
      "step": 1069
    },
    {
      "epoch": 0.30462633451957294,
      "grad_norm": 0.47163739800453186,
      "learning_rate": 0.0004239254198690578,
      "loss": 7.4395,
      "step": 1070
    },
    {
      "epoch": 0.30491103202846975,
      "grad_norm": 0.4603082239627838,
      "learning_rate": 0.00042385425562197556,
      "loss": 7.54,
      "step": 1071
    },
    {
      "epoch": 0.30519572953736657,
      "grad_norm": 0.5182943344116211,
      "learning_rate": 0.0004237830913748932,
      "loss": 7.0479,
      "step": 1072
    },
    {
      "epoch": 0.3054804270462633,
      "grad_norm": 0.4548114240169525,
      "learning_rate": 0.000423711927127811,
      "loss": 7.7168,
      "step": 1073
    },
    {
      "epoch": 0.30576512455516014,
      "grad_norm": 0.5856614708900452,
      "learning_rate": 0.0004236407628807287,
      "loss": 7.1514,
      "step": 1074
    },
    {
      "epoch": 0.30604982206405695,
      "grad_norm": 0.4826880395412445,
      "learning_rate": 0.00042356959863364645,
      "loss": 7.3438,
      "step": 1075
    },
    {
      "epoch": 0.3063345195729537,
      "grad_norm": 0.5586170554161072,
      "learning_rate": 0.0004234984343865642,
      "loss": 7.1416,
      "step": 1076
    },
    {
      "epoch": 0.3066192170818505,
      "grad_norm": 0.4764145612716675,
      "learning_rate": 0.00042342727013948195,
      "loss": 7.0732,
      "step": 1077
    },
    {
      "epoch": 0.30690391459074734,
      "grad_norm": 0.6197493076324463,
      "learning_rate": 0.00042335610589239967,
      "loss": 7.1006,
      "step": 1078
    },
    {
      "epoch": 0.30718861209964415,
      "grad_norm": 0.47980010509490967,
      "learning_rate": 0.0004232849416453174,
      "loss": 7.5361,
      "step": 1079
    },
    {
      "epoch": 0.3074733096085409,
      "grad_norm": 0.5147501826286316,
      "learning_rate": 0.0004232137773982351,
      "loss": 7.2217,
      "step": 1080
    },
    {
      "epoch": 0.3077580071174377,
      "grad_norm": 0.6229455471038818,
      "learning_rate": 0.0004231426131511529,
      "loss": 6.8916,
      "step": 1081
    },
    {
      "epoch": 0.30804270462633454,
      "grad_norm": 0.519205629825592,
      "learning_rate": 0.0004230714489040706,
      "loss": 7.1543,
      "step": 1082
    },
    {
      "epoch": 0.3083274021352313,
      "grad_norm": 0.5824966430664062,
      "learning_rate": 0.0004230002846569884,
      "loss": 6.1338,
      "step": 1083
    },
    {
      "epoch": 0.3086120996441281,
      "grad_norm": 0.5626207590103149,
      "learning_rate": 0.00042292912040990606,
      "loss": 6.9707,
      "step": 1084
    },
    {
      "epoch": 0.3088967971530249,
      "grad_norm": 0.5577024817466736,
      "learning_rate": 0.0004228579561628238,
      "loss": 6.8984,
      "step": 1085
    },
    {
      "epoch": 0.3091814946619217,
      "grad_norm": 0.48237547278404236,
      "learning_rate": 0.00042278679191574156,
      "loss": 7.4697,
      "step": 1086
    },
    {
      "epoch": 0.3094661921708185,
      "grad_norm": 0.4760837256908417,
      "learning_rate": 0.0004227156276686593,
      "loss": 7.3721,
      "step": 1087
    },
    {
      "epoch": 0.3097508896797153,
      "grad_norm": 0.466458797454834,
      "learning_rate": 0.000422644463421577,
      "loss": 7.5654,
      "step": 1088
    },
    {
      "epoch": 0.3100355871886121,
      "grad_norm": 0.5326457023620605,
      "learning_rate": 0.00042257329917449473,
      "loss": 7.2549,
      "step": 1089
    },
    {
      "epoch": 0.3103202846975089,
      "grad_norm": 0.6053488850593567,
      "learning_rate": 0.00042250213492741245,
      "loss": 6.918,
      "step": 1090
    },
    {
      "epoch": 0.3106049822064057,
      "grad_norm": 0.4849885404109955,
      "learning_rate": 0.00042243097068033023,
      "loss": 6.9521,
      "step": 1091
    },
    {
      "epoch": 0.3108896797153025,
      "grad_norm": 0.46493789553642273,
      "learning_rate": 0.00042235980643324795,
      "loss": 7.5,
      "step": 1092
    },
    {
      "epoch": 0.3111743772241993,
      "grad_norm": 0.5597470998764038,
      "learning_rate": 0.0004222886421861657,
      "loss": 7.1045,
      "step": 1093
    },
    {
      "epoch": 0.3114590747330961,
      "grad_norm": 0.46983328461647034,
      "learning_rate": 0.00042221747793908345,
      "loss": 7.376,
      "step": 1094
    },
    {
      "epoch": 0.3117437722419929,
      "grad_norm": 0.7661401033401489,
      "learning_rate": 0.0004221463136920011,
      "loss": 7.9004,
      "step": 1095
    },
    {
      "epoch": 0.31202846975088966,
      "grad_norm": 0.5901150703430176,
      "learning_rate": 0.0004220751494449189,
      "loss": 7.0488,
      "step": 1096
    },
    {
      "epoch": 0.3123131672597865,
      "grad_norm": 0.49586740136146545,
      "learning_rate": 0.0004220039851978366,
      "loss": 7.3682,
      "step": 1097
    },
    {
      "epoch": 0.3125978647686833,
      "grad_norm": 0.42957544326782227,
      "learning_rate": 0.00042193282095075434,
      "loss": 7.3447,
      "step": 1098
    },
    {
      "epoch": 0.31288256227758005,
      "grad_norm": 0.6224203705787659,
      "learning_rate": 0.0004218616567036721,
      "loss": 6.999,
      "step": 1099
    },
    {
      "epoch": 0.31316725978647686,
      "grad_norm": 0.5391501188278198,
      "learning_rate": 0.0004217904924565898,
      "loss": 7.1846,
      "step": 1100
    },
    {
      "epoch": 0.3134519572953737,
      "grad_norm": 0.4944549798965454,
      "learning_rate": 0.0004217193282095075,
      "loss": 7.5273,
      "step": 1101
    },
    {
      "epoch": 0.3137366548042705,
      "grad_norm": 0.5197928547859192,
      "learning_rate": 0.0004216481639624253,
      "loss": 7.1572,
      "step": 1102
    },
    {
      "epoch": 0.31402135231316725,
      "grad_norm": 0.45653462409973145,
      "learning_rate": 0.000421576999715343,
      "loss": 7.1807,
      "step": 1103
    },
    {
      "epoch": 0.31430604982206406,
      "grad_norm": 0.5307984352111816,
      "learning_rate": 0.0004215058354682608,
      "loss": 7.4922,
      "step": 1104
    },
    {
      "epoch": 0.3145907473309609,
      "grad_norm": 0.4564886689186096,
      "learning_rate": 0.0004214346712211785,
      "loss": 7.3174,
      "step": 1105
    },
    {
      "epoch": 0.31487544483985763,
      "grad_norm": 0.5012796521186829,
      "learning_rate": 0.0004213635069740962,
      "loss": 7.165,
      "step": 1106
    },
    {
      "epoch": 0.31516014234875445,
      "grad_norm": 3.3040928840637207,
      "learning_rate": 0.00042129234272701396,
      "loss": 7.4414,
      "step": 1107
    },
    {
      "epoch": 0.31544483985765126,
      "grad_norm": 0.5748804211616516,
      "learning_rate": 0.0004212211784799317,
      "loss": 7.124,
      "step": 1108
    },
    {
      "epoch": 0.315729537366548,
      "grad_norm": 0.565574586391449,
      "learning_rate": 0.00042115001423284946,
      "loss": 6.9238,
      "step": 1109
    },
    {
      "epoch": 0.31601423487544483,
      "grad_norm": 0.5565568208694458,
      "learning_rate": 0.0004210788499857672,
      "loss": 7.1934,
      "step": 1110
    },
    {
      "epoch": 0.31629893238434165,
      "grad_norm": 0.4593667685985565,
      "learning_rate": 0.0004210076857386849,
      "loss": 7.4541,
      "step": 1111
    },
    {
      "epoch": 0.31658362989323846,
      "grad_norm": 0.4457915127277374,
      "learning_rate": 0.0004209365214916026,
      "loss": 7.6582,
      "step": 1112
    },
    {
      "epoch": 0.3168683274021352,
      "grad_norm": 0.40129971504211426,
      "learning_rate": 0.00042086535724452035,
      "loss": 7.7979,
      "step": 1113
    },
    {
      "epoch": 0.31715302491103203,
      "grad_norm": 0.5046826004981995,
      "learning_rate": 0.0004207941929974381,
      "loss": 7.2188,
      "step": 1114
    },
    {
      "epoch": 0.31743772241992885,
      "grad_norm": 0.4885587990283966,
      "learning_rate": 0.00042072302875035585,
      "loss": 7.6162,
      "step": 1115
    },
    {
      "epoch": 0.3177224199288256,
      "grad_norm": 0.4405319392681122,
      "learning_rate": 0.00042065186450327357,
      "loss": 7.4834,
      "step": 1116
    },
    {
      "epoch": 0.3180071174377224,
      "grad_norm": 0.5957425236701965,
      "learning_rate": 0.0004205807002561913,
      "loss": 6.667,
      "step": 1117
    },
    {
      "epoch": 0.31829181494661923,
      "grad_norm": 0.5472882986068726,
      "learning_rate": 0.000420509536009109,
      "loss": 7.4531,
      "step": 1118
    },
    {
      "epoch": 0.318576512455516,
      "grad_norm": 0.4806768596172333,
      "learning_rate": 0.00042043837176202674,
      "loss": 7.4805,
      "step": 1119
    },
    {
      "epoch": 0.3188612099644128,
      "grad_norm": 0.4944848120212555,
      "learning_rate": 0.0004203672075149445,
      "loss": 7.5342,
      "step": 1120
    },
    {
      "epoch": 0.3191459074733096,
      "grad_norm": 0.5867344737052917,
      "learning_rate": 0.00042029604326786224,
      "loss": 6.6504,
      "step": 1121
    },
    {
      "epoch": 0.3194306049822064,
      "grad_norm": 0.45524725317955017,
      "learning_rate": 0.00042022487902078,
      "loss": 7.334,
      "step": 1122
    },
    {
      "epoch": 0.3197153024911032,
      "grad_norm": 0.47168710827827454,
      "learning_rate": 0.0004201537147736977,
      "loss": 7.3643,
      "step": 1123
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.46018290519714355,
      "learning_rate": 0.0004200825505266154,
      "loss": 7.2725,
      "step": 1124
    },
    {
      "epoch": 0.3202846975088968,
      "grad_norm": 0.49334147572517395,
      "learning_rate": 0.0004200113862795332,
      "loss": 7.3291,
      "step": 1125
    },
    {
      "epoch": 0.3205693950177936,
      "grad_norm": 0.6619085073471069,
      "learning_rate": 0.0004199402220324509,
      "loss": 6.3311,
      "step": 1126
    },
    {
      "epoch": 0.3208540925266904,
      "grad_norm": 0.4803929626941681,
      "learning_rate": 0.0004198690577853687,
      "loss": 7.1162,
      "step": 1127
    },
    {
      "epoch": 0.3211387900355872,
      "grad_norm": 0.4872837960720062,
      "learning_rate": 0.00041979789353828635,
      "loss": 7.2471,
      "step": 1128
    },
    {
      "epoch": 0.32142348754448397,
      "grad_norm": 0.5039362907409668,
      "learning_rate": 0.00041972672929120407,
      "loss": 7.9453,
      "step": 1129
    },
    {
      "epoch": 0.3217081850533808,
      "grad_norm": 0.4333154559135437,
      "learning_rate": 0.00041965556504412185,
      "loss": 7.4307,
      "step": 1130
    },
    {
      "epoch": 0.3219928825622776,
      "grad_norm": 0.6478785276412964,
      "learning_rate": 0.00041958440079703957,
      "loss": 7.7002,
      "step": 1131
    },
    {
      "epoch": 0.32227758007117435,
      "grad_norm": 0.5212644934654236,
      "learning_rate": 0.0004195132365499573,
      "loss": 7.1113,
      "step": 1132
    },
    {
      "epoch": 0.32256227758007117,
      "grad_norm": 0.4614885449409485,
      "learning_rate": 0.00041944207230287507,
      "loss": 7.6172,
      "step": 1133
    },
    {
      "epoch": 0.322846975088968,
      "grad_norm": 0.5155771374702454,
      "learning_rate": 0.00041937090805579274,
      "loss": 7.1846,
      "step": 1134
    },
    {
      "epoch": 0.3231316725978648,
      "grad_norm": 0.5528173446655273,
      "learning_rate": 0.0004192997438087105,
      "loss": 7.2109,
      "step": 1135
    },
    {
      "epoch": 0.32341637010676155,
      "grad_norm": 0.505190372467041,
      "learning_rate": 0.00041922857956162824,
      "loss": 6.9854,
      "step": 1136
    },
    {
      "epoch": 0.32370106761565837,
      "grad_norm": 0.4246407747268677,
      "learning_rate": 0.00041915741531454596,
      "loss": 7.6191,
      "step": 1137
    },
    {
      "epoch": 0.3239857651245552,
      "grad_norm": 0.5137282609939575,
      "learning_rate": 0.00041908625106746374,
      "loss": 6.8467,
      "step": 1138
    },
    {
      "epoch": 0.32427046263345194,
      "grad_norm": 0.5498955845832825,
      "learning_rate": 0.00041901508682038146,
      "loss": 7.3867,
      "step": 1139
    },
    {
      "epoch": 0.32455516014234875,
      "grad_norm": 0.46873560547828674,
      "learning_rate": 0.0004189439225732992,
      "loss": 7.5898,
      "step": 1140
    },
    {
      "epoch": 0.32483985765124557,
      "grad_norm": 0.4682499170303345,
      "learning_rate": 0.0004188727583262169,
      "loss": 7.0762,
      "step": 1141
    },
    {
      "epoch": 0.3251245551601423,
      "grad_norm": 0.6929412484169006,
      "learning_rate": 0.00041880159407913463,
      "loss": 6.9707,
      "step": 1142
    },
    {
      "epoch": 0.32540925266903914,
      "grad_norm": 0.5219119787216187,
      "learning_rate": 0.0004187304298320524,
      "loss": 6.9746,
      "step": 1143
    },
    {
      "epoch": 0.32569395017793595,
      "grad_norm": 0.6330713629722595,
      "learning_rate": 0.00041865926558497013,
      "loss": 6.1309,
      "step": 1144
    },
    {
      "epoch": 0.32597864768683277,
      "grad_norm": 0.6545981168746948,
      "learning_rate": 0.00041858810133788785,
      "loss": 6.5879,
      "step": 1145
    },
    {
      "epoch": 0.3262633451957295,
      "grad_norm": 0.5922051668167114,
      "learning_rate": 0.0004185169370908056,
      "loss": 7.0771,
      "step": 1146
    },
    {
      "epoch": 0.32654804270462634,
      "grad_norm": 0.6108818054199219,
      "learning_rate": 0.0004184457728437233,
      "loss": 7.0137,
      "step": 1147
    },
    {
      "epoch": 0.32683274021352315,
      "grad_norm": 0.5289431810379028,
      "learning_rate": 0.0004183746085966411,
      "loss": 7.001,
      "step": 1148
    },
    {
      "epoch": 0.3271174377224199,
      "grad_norm": 0.4609707295894623,
      "learning_rate": 0.0004183034443495588,
      "loss": 7.4082,
      "step": 1149
    },
    {
      "epoch": 0.3274021352313167,
      "grad_norm": 0.4477441608905792,
      "learning_rate": 0.0004182322801024765,
      "loss": 7.6816,
      "step": 1150
    },
    {
      "epoch": 0.32768683274021354,
      "grad_norm": 0.4503907859325409,
      "learning_rate": 0.00041816111585539424,
      "loss": 7.873,
      "step": 1151
    },
    {
      "epoch": 0.3279715302491103,
      "grad_norm": 0.972274661064148,
      "learning_rate": 0.00041808995160831197,
      "loss": 7.2227,
      "step": 1152
    },
    {
      "epoch": 0.3282562277580071,
      "grad_norm": 0.5258875489234924,
      "learning_rate": 0.00041801878736122974,
      "loss": 7.418,
      "step": 1153
    },
    {
      "epoch": 0.3285409252669039,
      "grad_norm": 0.47450560331344604,
      "learning_rate": 0.00041794762311414747,
      "loss": 7.2832,
      "step": 1154
    },
    {
      "epoch": 0.3288256227758007,
      "grad_norm": 0.4881042540073395,
      "learning_rate": 0.0004178764588670652,
      "loss": 7.5781,
      "step": 1155
    },
    {
      "epoch": 0.3291103202846975,
      "grad_norm": 0.5091273784637451,
      "learning_rate": 0.00041780529461998297,
      "loss": 7.2471,
      "step": 1156
    },
    {
      "epoch": 0.3293950177935943,
      "grad_norm": 0.5281254053115845,
      "learning_rate": 0.00041773413037290064,
      "loss": 7.1455,
      "step": 1157
    },
    {
      "epoch": 0.3296797153024911,
      "grad_norm": 0.44808077812194824,
      "learning_rate": 0.0004176629661258184,
      "loss": 7.7734,
      "step": 1158
    },
    {
      "epoch": 0.3299644128113879,
      "grad_norm": 0.49891433119773865,
      "learning_rate": 0.00041759180187873613,
      "loss": 7.4795,
      "step": 1159
    },
    {
      "epoch": 0.3302491103202847,
      "grad_norm": 0.5285683274269104,
      "learning_rate": 0.00041752063763165386,
      "loss": 7.4639,
      "step": 1160
    },
    {
      "epoch": 0.3305338078291815,
      "grad_norm": 0.4684009253978729,
      "learning_rate": 0.00041744947338457163,
      "loss": 7.6631,
      "step": 1161
    },
    {
      "epoch": 0.3308185053380783,
      "grad_norm": 0.711145281791687,
      "learning_rate": 0.0004173783091374893,
      "loss": 7.2764,
      "step": 1162
    },
    {
      "epoch": 0.3311032028469751,
      "grad_norm": 0.49265673756599426,
      "learning_rate": 0.0004173071448904071,
      "loss": 7.21,
      "step": 1163
    },
    {
      "epoch": 0.3313879003558719,
      "grad_norm": 0.6499678492546082,
      "learning_rate": 0.0004172359806433248,
      "loss": 6.4902,
      "step": 1164
    },
    {
      "epoch": 0.33167259786476866,
      "grad_norm": 0.48859643936157227,
      "learning_rate": 0.0004171648163962425,
      "loss": 7.4043,
      "step": 1165
    },
    {
      "epoch": 0.3319572953736655,
      "grad_norm": 0.43188872933387756,
      "learning_rate": 0.0004170936521491603,
      "loss": 7.8252,
      "step": 1166
    },
    {
      "epoch": 0.3322419928825623,
      "grad_norm": 0.4691556990146637,
      "learning_rate": 0.000417022487902078,
      "loss": 7.4482,
      "step": 1167
    },
    {
      "epoch": 0.3325266903914591,
      "grad_norm": 0.582195520401001,
      "learning_rate": 0.0004169513236549957,
      "loss": 7.5371,
      "step": 1168
    },
    {
      "epoch": 0.33281138790035586,
      "grad_norm": 0.4443216919898987,
      "learning_rate": 0.00041688015940791347,
      "loss": 7.7871,
      "step": 1169
    },
    {
      "epoch": 0.3330960854092527,
      "grad_norm": 0.6017194390296936,
      "learning_rate": 0.0004168089951608312,
      "loss": 6.9932,
      "step": 1170
    },
    {
      "epoch": 0.3333807829181495,
      "grad_norm": 0.5376923680305481,
      "learning_rate": 0.00041673783091374897,
      "loss": 7.125,
      "step": 1171
    },
    {
      "epoch": 0.33366548042704625,
      "grad_norm": 0.4595070779323578,
      "learning_rate": 0.0004166666666666667,
      "loss": 7.8262,
      "step": 1172
    },
    {
      "epoch": 0.33395017793594306,
      "grad_norm": 0.5157293081283569,
      "learning_rate": 0.00041659550241958436,
      "loss": 7.165,
      "step": 1173
    },
    {
      "epoch": 0.3342348754448399,
      "grad_norm": 0.6389365196228027,
      "learning_rate": 0.00041652433817250214,
      "loss": 6.8311,
      "step": 1174
    },
    {
      "epoch": 0.33451957295373663,
      "grad_norm": 0.4487728476524353,
      "learning_rate": 0.00041645317392541986,
      "loss": 7.7686,
      "step": 1175
    },
    {
      "epoch": 0.33480427046263345,
      "grad_norm": 0.4286770224571228,
      "learning_rate": 0.00041638200967833764,
      "loss": 7.6963,
      "step": 1176
    },
    {
      "epoch": 0.33508896797153026,
      "grad_norm": 0.4808689057826996,
      "learning_rate": 0.00041631084543125536,
      "loss": 7.1982,
      "step": 1177
    },
    {
      "epoch": 0.335373665480427,
      "grad_norm": 0.5197522044181824,
      "learning_rate": 0.0004162396811841731,
      "loss": 7.3545,
      "step": 1178
    },
    {
      "epoch": 0.33565836298932383,
      "grad_norm": 0.4670189917087555,
      "learning_rate": 0.0004161685169370908,
      "loss": 7.1934,
      "step": 1179
    },
    {
      "epoch": 0.33594306049822065,
      "grad_norm": 0.5172544121742249,
      "learning_rate": 0.00041609735269000853,
      "loss": 7.1943,
      "step": 1180
    },
    {
      "epoch": 0.33622775800711746,
      "grad_norm": 0.4959327280521393,
      "learning_rate": 0.00041602618844292625,
      "loss": 7.3457,
      "step": 1181
    },
    {
      "epoch": 0.3365124555160142,
      "grad_norm": 0.5002378225326538,
      "learning_rate": 0.00041595502419584403,
      "loss": 7.502,
      "step": 1182
    },
    {
      "epoch": 0.33679715302491103,
      "grad_norm": 0.4305514097213745,
      "learning_rate": 0.00041588385994876175,
      "loss": 7.751,
      "step": 1183
    },
    {
      "epoch": 0.33708185053380785,
      "grad_norm": 0.46878859400749207,
      "learning_rate": 0.00041581269570167953,
      "loss": 7.2695,
      "step": 1184
    },
    {
      "epoch": 0.3373665480427046,
      "grad_norm": 0.4970056712627411,
      "learning_rate": 0.0004157415314545972,
      "loss": 7.2119,
      "step": 1185
    },
    {
      "epoch": 0.3376512455516014,
      "grad_norm": 0.5278494358062744,
      "learning_rate": 0.0004156703672075149,
      "loss": 7.2178,
      "step": 1186
    },
    {
      "epoch": 0.33793594306049823,
      "grad_norm": 0.6189301013946533,
      "learning_rate": 0.0004155992029604327,
      "loss": 7.0967,
      "step": 1187
    },
    {
      "epoch": 0.338220640569395,
      "grad_norm": 0.5286704301834106,
      "learning_rate": 0.0004155280387133504,
      "loss": 7.4258,
      "step": 1188
    },
    {
      "epoch": 0.3385053380782918,
      "grad_norm": 0.5079885721206665,
      "learning_rate": 0.0004154568744662682,
      "loss": 7.3896,
      "step": 1189
    },
    {
      "epoch": 0.3387900355871886,
      "grad_norm": 0.6349208950996399,
      "learning_rate": 0.00041538571021918587,
      "loss": 7.4512,
      "step": 1190
    },
    {
      "epoch": 0.33907473309608543,
      "grad_norm": 0.6474052667617798,
      "learning_rate": 0.0004153145459721036,
      "loss": 6.4209,
      "step": 1191
    },
    {
      "epoch": 0.3393594306049822,
      "grad_norm": 0.3994763195514679,
      "learning_rate": 0.00041524338172502137,
      "loss": 7.8379,
      "step": 1192
    },
    {
      "epoch": 0.339644128113879,
      "grad_norm": 0.46896281838417053,
      "learning_rate": 0.0004151722174779391,
      "loss": 7.4766,
      "step": 1193
    },
    {
      "epoch": 0.3399288256227758,
      "grad_norm": 4.865256309509277,
      "learning_rate": 0.00041510105323085687,
      "loss": 7.6611,
      "step": 1194
    },
    {
      "epoch": 0.3402135231316726,
      "grad_norm": 0.5058532953262329,
      "learning_rate": 0.0004150298889837746,
      "loss": 7.6475,
      "step": 1195
    },
    {
      "epoch": 0.3404982206405694,
      "grad_norm": 0.4236146807670593,
      "learning_rate": 0.00041495872473669226,
      "loss": 7.4639,
      "step": 1196
    },
    {
      "epoch": 0.3407829181494662,
      "grad_norm": 0.5200099945068359,
      "learning_rate": 0.00041488756048961003,
      "loss": 6.7969,
      "step": 1197
    },
    {
      "epoch": 0.34106761565836297,
      "grad_norm": 0.5131163597106934,
      "learning_rate": 0.00041481639624252776,
      "loss": 7.0908,
      "step": 1198
    },
    {
      "epoch": 0.3413523131672598,
      "grad_norm": 0.42460203170776367,
      "learning_rate": 0.0004147452319954455,
      "loss": 7.6074,
      "step": 1199
    },
    {
      "epoch": 0.3416370106761566,
      "grad_norm": 0.4523850977420807,
      "learning_rate": 0.00041467406774836326,
      "loss": 7.1504,
      "step": 1200
    },
    {
      "epoch": 0.3416370106761566,
      "eval_bleu": 0.10037787970140617,
      "eval_loss": 7.05078125,
      "eval_runtime": 151.2375,
      "eval_samples_per_second": 1.878,
      "eval_steps_per_second": 0.119,
      "step": 1200
    },
    {
      "epoch": 0.3419217081850534,
      "grad_norm": 0.5223976373672485,
      "learning_rate": 0.000414602903501281,
      "loss": 7.1885,
      "step": 1201
    },
    {
      "epoch": 0.34220640569395017,
      "grad_norm": 0.5459167957305908,
      "learning_rate": 0.0004145317392541987,
      "loss": 7.1562,
      "step": 1202
    },
    {
      "epoch": 0.342491103202847,
      "grad_norm": 0.416970431804657,
      "learning_rate": 0.0004144605750071164,
      "loss": 7.335,
      "step": 1203
    },
    {
      "epoch": 0.3427758007117438,
      "grad_norm": 0.4617096781730652,
      "learning_rate": 0.00041438941076003415,
      "loss": 7.2158,
      "step": 1204
    },
    {
      "epoch": 0.34306049822064055,
      "grad_norm": 0.48640942573547363,
      "learning_rate": 0.0004143182465129519,
      "loss": 7.3984,
      "step": 1205
    },
    {
      "epoch": 0.34334519572953737,
      "grad_norm": 0.5603982210159302,
      "learning_rate": 0.00041424708226586965,
      "loss": 6.6523,
      "step": 1206
    },
    {
      "epoch": 0.3436298932384342,
      "grad_norm": 0.652839720249176,
      "learning_rate": 0.00041417591801878737,
      "loss": 6.833,
      "step": 1207
    },
    {
      "epoch": 0.34391459074733094,
      "grad_norm": 0.5112550258636475,
      "learning_rate": 0.0004141047537717051,
      "loss": 7.3613,
      "step": 1208
    },
    {
      "epoch": 0.34419928825622775,
      "grad_norm": 0.46459585428237915,
      "learning_rate": 0.0004140335895246228,
      "loss": 7.2656,
      "step": 1209
    },
    {
      "epoch": 0.34448398576512457,
      "grad_norm": 0.58054119348526,
      "learning_rate": 0.0004139624252775406,
      "loss": 7.0889,
      "step": 1210
    },
    {
      "epoch": 0.3447686832740213,
      "grad_norm": 0.5896499156951904,
      "learning_rate": 0.0004138912610304583,
      "loss": 6.834,
      "step": 1211
    },
    {
      "epoch": 0.34505338078291814,
      "grad_norm": 0.3972550630569458,
      "learning_rate": 0.0004138200967833761,
      "loss": 7.4492,
      "step": 1212
    },
    {
      "epoch": 0.34533807829181495,
      "grad_norm": 0.5355075597763062,
      "learning_rate": 0.00041374893253629376,
      "loss": 7.2773,
      "step": 1213
    },
    {
      "epoch": 0.34562277580071177,
      "grad_norm": 0.5021422505378723,
      "learning_rate": 0.0004136777682892115,
      "loss": 7.5195,
      "step": 1214
    },
    {
      "epoch": 0.3459074733096085,
      "grad_norm": 0.5513196587562561,
      "learning_rate": 0.00041360660404212926,
      "loss": 7.165,
      "step": 1215
    },
    {
      "epoch": 0.34619217081850534,
      "grad_norm": 0.4936729073524475,
      "learning_rate": 0.000413535439795047,
      "loss": 6.9385,
      "step": 1216
    },
    {
      "epoch": 0.34647686832740215,
      "grad_norm": 0.4948480725288391,
      "learning_rate": 0.0004134642755479647,
      "loss": 7.3184,
      "step": 1217
    },
    {
      "epoch": 0.3467615658362989,
      "grad_norm": 0.49627193808555603,
      "learning_rate": 0.00041339311130088243,
      "loss": 7.248,
      "step": 1218
    },
    {
      "epoch": 0.3470462633451957,
      "grad_norm": 0.4766037166118622,
      "learning_rate": 0.00041332194705380015,
      "loss": 7.3076,
      "step": 1219
    },
    {
      "epoch": 0.34733096085409254,
      "grad_norm": 0.4347938299179077,
      "learning_rate": 0.00041325078280671793,
      "loss": 7.3867,
      "step": 1220
    },
    {
      "epoch": 0.3476156583629893,
      "grad_norm": 0.4287054240703583,
      "learning_rate": 0.00041317961855963565,
      "loss": 7.6777,
      "step": 1221
    },
    {
      "epoch": 0.3479003558718861,
      "grad_norm": 0.5360247492790222,
      "learning_rate": 0.0004131084543125534,
      "loss": 7.0908,
      "step": 1222
    },
    {
      "epoch": 0.3481850533807829,
      "grad_norm": 0.504662811756134,
      "learning_rate": 0.00041303729006547115,
      "loss": 7.0732,
      "step": 1223
    },
    {
      "epoch": 0.34846975088967974,
      "grad_norm": 0.5329994559288025,
      "learning_rate": 0.0004129661258183888,
      "loss": 7.2832,
      "step": 1224
    },
    {
      "epoch": 0.3487544483985765,
      "grad_norm": 0.49108269810676575,
      "learning_rate": 0.0004128949615713066,
      "loss": 7.3379,
      "step": 1225
    },
    {
      "epoch": 0.3490391459074733,
      "grad_norm": 0.43490204215049744,
      "learning_rate": 0.0004128237973242243,
      "loss": 7.5186,
      "step": 1226
    },
    {
      "epoch": 0.34932384341637013,
      "grad_norm": 0.4555596113204956,
      "learning_rate": 0.00041275263307714204,
      "loss": 7.5586,
      "step": 1227
    },
    {
      "epoch": 0.3496085409252669,
      "grad_norm": 0.43048095703125,
      "learning_rate": 0.0004126814688300598,
      "loss": 7.499,
      "step": 1228
    },
    {
      "epoch": 0.3498932384341637,
      "grad_norm": 0.5056287050247192,
      "learning_rate": 0.00041261030458297754,
      "loss": 7.2539,
      "step": 1229
    },
    {
      "epoch": 0.3501779359430605,
      "grad_norm": 0.5323185920715332,
      "learning_rate": 0.0004125391403358952,
      "loss": 7.4971,
      "step": 1230
    },
    {
      "epoch": 0.3504626334519573,
      "grad_norm": 0.4007214307785034,
      "learning_rate": 0.000412467976088813,
      "loss": 8.1367,
      "step": 1231
    },
    {
      "epoch": 0.3507473309608541,
      "grad_norm": 0.49167752265930176,
      "learning_rate": 0.0004123968118417307,
      "loss": 7.5352,
      "step": 1232
    },
    {
      "epoch": 0.3510320284697509,
      "grad_norm": 0.4917040467262268,
      "learning_rate": 0.0004123256475946485,
      "loss": 7.1221,
      "step": 1233
    },
    {
      "epoch": 0.35131672597864766,
      "grad_norm": 0.5261372327804565,
      "learning_rate": 0.0004122544833475662,
      "loss": 6.8887,
      "step": 1234
    },
    {
      "epoch": 0.3516014234875445,
      "grad_norm": 0.43594714999198914,
      "learning_rate": 0.0004121833191004839,
      "loss": 7.6035,
      "step": 1235
    },
    {
      "epoch": 0.3518861209964413,
      "grad_norm": 0.5169447064399719,
      "learning_rate": 0.00041211215485340165,
      "loss": 7.2676,
      "step": 1236
    },
    {
      "epoch": 0.3521708185053381,
      "grad_norm": 0.4773392081260681,
      "learning_rate": 0.0004120409906063194,
      "loss": 7.6768,
      "step": 1237
    },
    {
      "epoch": 0.35245551601423486,
      "grad_norm": 0.4493792951107025,
      "learning_rate": 0.00041196982635923715,
      "loss": 7.4033,
      "step": 1238
    },
    {
      "epoch": 0.3527402135231317,
      "grad_norm": 0.4882626533508301,
      "learning_rate": 0.0004118986621121549,
      "loss": 7.3887,
      "step": 1239
    },
    {
      "epoch": 0.3530249110320285,
      "grad_norm": 0.6581220030784607,
      "learning_rate": 0.0004118274978650726,
      "loss": 6.6924,
      "step": 1240
    },
    {
      "epoch": 0.35330960854092525,
      "grad_norm": 0.541993260383606,
      "learning_rate": 0.0004117563336179903,
      "loss": 6.876,
      "step": 1241
    },
    {
      "epoch": 0.35359430604982206,
      "grad_norm": 0.46581751108169556,
      "learning_rate": 0.00041168516937090805,
      "loss": 7.5488,
      "step": 1242
    },
    {
      "epoch": 0.3538790035587189,
      "grad_norm": 0.46979838609695435,
      "learning_rate": 0.0004116140051238258,
      "loss": 7.6562,
      "step": 1243
    },
    {
      "epoch": 0.35416370106761563,
      "grad_norm": 0.5168439745903015,
      "learning_rate": 0.00041154284087674354,
      "loss": 7.3828,
      "step": 1244
    },
    {
      "epoch": 0.35444839857651245,
      "grad_norm": 0.4394519627094269,
      "learning_rate": 0.00041147167662966127,
      "loss": 7.6738,
      "step": 1245
    },
    {
      "epoch": 0.35473309608540926,
      "grad_norm": 0.41233596205711365,
      "learning_rate": 0.00041140051238257904,
      "loss": 7.7783,
      "step": 1246
    },
    {
      "epoch": 0.3550177935943061,
      "grad_norm": 0.5092318058013916,
      "learning_rate": 0.0004113293481354967,
      "loss": 7.4785,
      "step": 1247
    },
    {
      "epoch": 0.35530249110320283,
      "grad_norm": 0.5250484347343445,
      "learning_rate": 0.00041125818388841444,
      "loss": 6.3848,
      "step": 1248
    },
    {
      "epoch": 0.35558718861209965,
      "grad_norm": 0.46243852376937866,
      "learning_rate": 0.0004111870196413322,
      "loss": 7.7559,
      "step": 1249
    },
    {
      "epoch": 0.35587188612099646,
      "grad_norm": 0.5164723992347717,
      "learning_rate": 0.00041111585539424994,
      "loss": 7.1367,
      "step": 1250
    },
    {
      "epoch": 0.3561565836298932,
      "grad_norm": 0.5358831882476807,
      "learning_rate": 0.0004110446911471677,
      "loss": 7.1572,
      "step": 1251
    },
    {
      "epoch": 0.35644128113879003,
      "grad_norm": 0.5281264185905457,
      "learning_rate": 0.0004109735269000854,
      "loss": 7.5811,
      "step": 1252
    },
    {
      "epoch": 0.35672597864768685,
      "grad_norm": 0.5329515337944031,
      "learning_rate": 0.0004109023626530031,
      "loss": 7.0713,
      "step": 1253
    },
    {
      "epoch": 0.3570106761565836,
      "grad_norm": 0.5931563377380371,
      "learning_rate": 0.0004108311984059209,
      "loss": 7.1826,
      "step": 1254
    },
    {
      "epoch": 0.3572953736654804,
      "grad_norm": 0.481064110994339,
      "learning_rate": 0.0004107600341588386,
      "loss": 6.9893,
      "step": 1255
    },
    {
      "epoch": 0.35758007117437723,
      "grad_norm": 0.45632845163345337,
      "learning_rate": 0.0004106888699117564,
      "loss": 7.4512,
      "step": 1256
    },
    {
      "epoch": 0.35786476868327405,
      "grad_norm": 0.5430977940559387,
      "learning_rate": 0.0004106177056646741,
      "loss": 7.2402,
      "step": 1257
    },
    {
      "epoch": 0.3581494661921708,
      "grad_norm": 1.1921885013580322,
      "learning_rate": 0.00041054654141759177,
      "loss": 7.5723,
      "step": 1258
    },
    {
      "epoch": 0.3584341637010676,
      "grad_norm": 0.5579004287719727,
      "learning_rate": 0.00041047537717050955,
      "loss": 7.0098,
      "step": 1259
    },
    {
      "epoch": 0.35871886120996443,
      "grad_norm": 0.474059522151947,
      "learning_rate": 0.00041040421292342727,
      "loss": 7.6367,
      "step": 1260
    },
    {
      "epoch": 0.3590035587188612,
      "grad_norm": 0.6036648154258728,
      "learning_rate": 0.00041033304867634505,
      "loss": 7.3857,
      "step": 1261
    },
    {
      "epoch": 0.359288256227758,
      "grad_norm": 0.4579026699066162,
      "learning_rate": 0.00041026188442926277,
      "loss": 7.79,
      "step": 1262
    },
    {
      "epoch": 0.3595729537366548,
      "grad_norm": 0.5366966724395752,
      "learning_rate": 0.00041019072018218044,
      "loss": 6.7188,
      "step": 1263
    },
    {
      "epoch": 0.3598576512455516,
      "grad_norm": 0.6088496446609497,
      "learning_rate": 0.0004101195559350982,
      "loss": 6.8174,
      "step": 1264
    },
    {
      "epoch": 0.3601423487544484,
      "grad_norm": 0.5021112561225891,
      "learning_rate": 0.00041004839168801594,
      "loss": 7.3369,
      "step": 1265
    },
    {
      "epoch": 0.3604270462633452,
      "grad_norm": 0.4617200195789337,
      "learning_rate": 0.00040997722744093366,
      "loss": 7.6934,
      "step": 1266
    },
    {
      "epoch": 0.36071174377224197,
      "grad_norm": 0.4734245240688324,
      "learning_rate": 0.00040990606319385144,
      "loss": 7.3154,
      "step": 1267
    },
    {
      "epoch": 0.3609964412811388,
      "grad_norm": 0.621208667755127,
      "learning_rate": 0.00040983489894676916,
      "loss": 7.1494,
      "step": 1268
    },
    {
      "epoch": 0.3612811387900356,
      "grad_norm": 0.58305823802948,
      "learning_rate": 0.0004097637346996869,
      "loss": 6.7168,
      "step": 1269
    },
    {
      "epoch": 0.3615658362989324,
      "grad_norm": 0.4475565254688263,
      "learning_rate": 0.0004096925704526046,
      "loss": 7.6641,
      "step": 1270
    },
    {
      "epoch": 0.36185053380782917,
      "grad_norm": 0.476866215467453,
      "learning_rate": 0.00040962140620552233,
      "loss": 7.5732,
      "step": 1271
    },
    {
      "epoch": 0.362135231316726,
      "grad_norm": 0.4611436426639557,
      "learning_rate": 0.0004095502419584401,
      "loss": 7.8398,
      "step": 1272
    },
    {
      "epoch": 0.3624199288256228,
      "grad_norm": 0.4453016519546509,
      "learning_rate": 0.00040947907771135783,
      "loss": 7.583,
      "step": 1273
    },
    {
      "epoch": 0.36270462633451955,
      "grad_norm": 0.490815132856369,
      "learning_rate": 0.0004094079134642756,
      "loss": 7.207,
      "step": 1274
    },
    {
      "epoch": 0.36298932384341637,
      "grad_norm": 0.48163872957229614,
      "learning_rate": 0.0004093367492171933,
      "loss": 7.8252,
      "step": 1275
    },
    {
      "epoch": 0.3632740213523132,
      "grad_norm": 0.5621169209480286,
      "learning_rate": 0.000409265584970111,
      "loss": 7.1377,
      "step": 1276
    },
    {
      "epoch": 0.36355871886120994,
      "grad_norm": 0.48964807391166687,
      "learning_rate": 0.0004091944207230288,
      "loss": 7.5391,
      "step": 1277
    },
    {
      "epoch": 0.36384341637010675,
      "grad_norm": 0.6166073679924011,
      "learning_rate": 0.0004091232564759465,
      "loss": 6.9941,
      "step": 1278
    },
    {
      "epoch": 0.36412811387900357,
      "grad_norm": 0.5375643968582153,
      "learning_rate": 0.0004090520922288642,
      "loss": 6.9863,
      "step": 1279
    },
    {
      "epoch": 0.3644128113879004,
      "grad_norm": 0.6133733987808228,
      "learning_rate": 0.00040898092798178194,
      "loss": 6.7217,
      "step": 1280
    },
    {
      "epoch": 0.36469750889679714,
      "grad_norm": 0.4984293580055237,
      "learning_rate": 0.00040890976373469967,
      "loss": 7.4521,
      "step": 1281
    },
    {
      "epoch": 0.36498220640569395,
      "grad_norm": 0.4901769459247589,
      "learning_rate": 0.00040883859948761744,
      "loss": 7.166,
      "step": 1282
    },
    {
      "epoch": 0.36526690391459077,
      "grad_norm": 0.48212358355522156,
      "learning_rate": 0.00040876743524053517,
      "loss": 7.2979,
      "step": 1283
    },
    {
      "epoch": 0.3655516014234875,
      "grad_norm": 0.5306196212768555,
      "learning_rate": 0.0004086962709934529,
      "loss": 6.5479,
      "step": 1284
    },
    {
      "epoch": 0.36583629893238434,
      "grad_norm": 0.3485376238822937,
      "learning_rate": 0.00040862510674637067,
      "loss": 7.6035,
      "step": 1285
    },
    {
      "epoch": 0.36612099644128115,
      "grad_norm": 0.6489346623420715,
      "learning_rate": 0.00040855394249928833,
      "loss": 6.4941,
      "step": 1286
    },
    {
      "epoch": 0.3664056939501779,
      "grad_norm": 0.5040377974510193,
      "learning_rate": 0.0004084827782522061,
      "loss": 7.2979,
      "step": 1287
    },
    {
      "epoch": 0.3666903914590747,
      "grad_norm": 0.4830140769481659,
      "learning_rate": 0.00040841161400512383,
      "loss": 7.7559,
      "step": 1288
    },
    {
      "epoch": 0.36697508896797154,
      "grad_norm": 0.44353169202804565,
      "learning_rate": 0.00040834044975804156,
      "loss": 7.3809,
      "step": 1289
    },
    {
      "epoch": 0.36725978647686836,
      "grad_norm": 0.5763711333274841,
      "learning_rate": 0.00040826928551095933,
      "loss": 7.2432,
      "step": 1290
    },
    {
      "epoch": 0.3675444839857651,
      "grad_norm": 0.5355408191680908,
      "learning_rate": 0.000408198121263877,
      "loss": 7.2393,
      "step": 1291
    },
    {
      "epoch": 0.3678291814946619,
      "grad_norm": 0.534460723400116,
      "learning_rate": 0.0004081269570167948,
      "loss": 7.4102,
      "step": 1292
    },
    {
      "epoch": 0.36811387900355874,
      "grad_norm": 0.49930524826049805,
      "learning_rate": 0.0004080557927697125,
      "loss": 7.374,
      "step": 1293
    },
    {
      "epoch": 0.3683985765124555,
      "grad_norm": 0.5417733192443848,
      "learning_rate": 0.0004079846285226302,
      "loss": 7.4746,
      "step": 1294
    },
    {
      "epoch": 0.3686832740213523,
      "grad_norm": 0.48457103967666626,
      "learning_rate": 0.000407913464275548,
      "loss": 7.7334,
      "step": 1295
    },
    {
      "epoch": 0.36896797153024913,
      "grad_norm": 0.5501992106437683,
      "learning_rate": 0.0004078423000284657,
      "loss": 6.957,
      "step": 1296
    },
    {
      "epoch": 0.3692526690391459,
      "grad_norm": 0.49789419770240784,
      "learning_rate": 0.0004077711357813834,
      "loss": 7.3945,
      "step": 1297
    },
    {
      "epoch": 0.3695373665480427,
      "grad_norm": 0.5354185104370117,
      "learning_rate": 0.00040769997153430117,
      "loss": 7.375,
      "step": 1298
    },
    {
      "epoch": 0.3698220640569395,
      "grad_norm": 0.49562764167785645,
      "learning_rate": 0.0004076288072872189,
      "loss": 7.4277,
      "step": 1299
    },
    {
      "epoch": 0.3701067615658363,
      "grad_norm": 0.5128252506256104,
      "learning_rate": 0.00040755764304013667,
      "loss": 7.0391,
      "step": 1300
    },
    {
      "epoch": 0.3703914590747331,
      "grad_norm": 0.5912569165229797,
      "learning_rate": 0.0004074864787930544,
      "loss": 6.8955,
      "step": 1301
    },
    {
      "epoch": 0.3706761565836299,
      "grad_norm": 0.4700371026992798,
      "learning_rate": 0.0004074153145459721,
      "loss": 7.7754,
      "step": 1302
    },
    {
      "epoch": 0.3709608540925267,
      "grad_norm": 0.4822196960449219,
      "learning_rate": 0.00040734415029888984,
      "loss": 7.3359,
      "step": 1303
    },
    {
      "epoch": 0.3712455516014235,
      "grad_norm": 0.49563339352607727,
      "learning_rate": 0.00040727298605180756,
      "loss": 7.3438,
      "step": 1304
    },
    {
      "epoch": 0.3715302491103203,
      "grad_norm": 0.5423407554626465,
      "learning_rate": 0.00040720182180472534,
      "loss": 6.4746,
      "step": 1305
    },
    {
      "epoch": 0.3718149466192171,
      "grad_norm": 0.48336294293403625,
      "learning_rate": 0.00040713065755764306,
      "loss": 7.5684,
      "step": 1306
    },
    {
      "epoch": 0.37209964412811386,
      "grad_norm": 0.4428510069847107,
      "learning_rate": 0.0004070594933105608,
      "loss": 7.6387,
      "step": 1307
    },
    {
      "epoch": 0.3723843416370107,
      "grad_norm": 0.592888355255127,
      "learning_rate": 0.0004069883290634785,
      "loss": 6.9248,
      "step": 1308
    },
    {
      "epoch": 0.3726690391459075,
      "grad_norm": 0.42219093441963196,
      "learning_rate": 0.00040691716481639623,
      "loss": 7.6143,
      "step": 1309
    },
    {
      "epoch": 0.37295373665480425,
      "grad_norm": 0.43672579526901245,
      "learning_rate": 0.000406846000569314,
      "loss": 7.9111,
      "step": 1310
    },
    {
      "epoch": 0.37323843416370106,
      "grad_norm": 0.5268492698669434,
      "learning_rate": 0.00040677483632223173,
      "loss": 7.4766,
      "step": 1311
    },
    {
      "epoch": 0.3735231316725979,
      "grad_norm": 0.5420467853546143,
      "learning_rate": 0.00040670367207514945,
      "loss": 7.2422,
      "step": 1312
    },
    {
      "epoch": 0.3738078291814947,
      "grad_norm": 0.48100414872169495,
      "learning_rate": 0.00040663250782806723,
      "loss": 7.667,
      "step": 1313
    },
    {
      "epoch": 0.37409252669039145,
      "grad_norm": 0.4420747458934784,
      "learning_rate": 0.0004065613435809849,
      "loss": 7.709,
      "step": 1314
    },
    {
      "epoch": 0.37437722419928826,
      "grad_norm": 0.4994819164276123,
      "learning_rate": 0.0004064901793339026,
      "loss": 7.459,
      "step": 1315
    },
    {
      "epoch": 0.3746619217081851,
      "grad_norm": 0.4126765727996826,
      "learning_rate": 0.0004064190150868204,
      "loss": 7.9648,
      "step": 1316
    },
    {
      "epoch": 0.37494661921708183,
      "grad_norm": 0.4801805317401886,
      "learning_rate": 0.0004063478508397381,
      "loss": 7.3936,
      "step": 1317
    },
    {
      "epoch": 0.37523131672597865,
      "grad_norm": 0.5441536903381348,
      "learning_rate": 0.0004062766865926559,
      "loss": 7.6826,
      "step": 1318
    },
    {
      "epoch": 0.37551601423487546,
      "grad_norm": 0.41975390911102295,
      "learning_rate": 0.0004062055223455736,
      "loss": 7.8721,
      "step": 1319
    },
    {
      "epoch": 0.3758007117437722,
      "grad_norm": 0.43222537636756897,
      "learning_rate": 0.0004061343580984913,
      "loss": 7.668,
      "step": 1320
    },
    {
      "epoch": 0.37608540925266903,
      "grad_norm": 0.4959360361099243,
      "learning_rate": 0.00040606319385140906,
      "loss": 7.2529,
      "step": 1321
    },
    {
      "epoch": 0.37637010676156585,
      "grad_norm": 0.5001708269119263,
      "learning_rate": 0.0004059920296043268,
      "loss": 7.8594,
      "step": 1322
    },
    {
      "epoch": 0.3766548042704626,
      "grad_norm": 0.5112770795822144,
      "learning_rate": 0.00040592086535724456,
      "loss": 7.1445,
      "step": 1323
    },
    {
      "epoch": 0.3769395017793594,
      "grad_norm": 0.40957221388816833,
      "learning_rate": 0.0004058497011101623,
      "loss": 7.9492,
      "step": 1324
    },
    {
      "epoch": 0.37722419928825623,
      "grad_norm": 0.4229575991630554,
      "learning_rate": 0.00040577853686307996,
      "loss": 7.4902,
      "step": 1325
    },
    {
      "epoch": 0.37750889679715305,
      "grad_norm": 0.5905504822731018,
      "learning_rate": 0.00040570737261599773,
      "loss": 6.8193,
      "step": 1326
    },
    {
      "epoch": 0.3777935943060498,
      "grad_norm": 0.5057560205459595,
      "learning_rate": 0.00040563620836891546,
      "loss": 7.5215,
      "step": 1327
    },
    {
      "epoch": 0.3780782918149466,
      "grad_norm": 0.536011278629303,
      "learning_rate": 0.0004055650441218332,
      "loss": 7.1104,
      "step": 1328
    },
    {
      "epoch": 0.37836298932384343,
      "grad_norm": 0.46675094962120056,
      "learning_rate": 0.00040549387987475096,
      "loss": 7.6035,
      "step": 1329
    },
    {
      "epoch": 0.3786476868327402,
      "grad_norm": 0.43799978494644165,
      "learning_rate": 0.0004054227156276687,
      "loss": 7.1494,
      "step": 1330
    },
    {
      "epoch": 0.378932384341637,
      "grad_norm": 0.5438700914382935,
      "learning_rate": 0.0004053515513805864,
      "loss": 6.7471,
      "step": 1331
    },
    {
      "epoch": 0.3792170818505338,
      "grad_norm": 0.374488890171051,
      "learning_rate": 0.0004052803871335041,
      "loss": 8.1045,
      "step": 1332
    },
    {
      "epoch": 0.3795017793594306,
      "grad_norm": 0.5037118792533875,
      "learning_rate": 0.00040520922288642185,
      "loss": 7.5791,
      "step": 1333
    },
    {
      "epoch": 0.3797864768683274,
      "grad_norm": 0.6229990720748901,
      "learning_rate": 0.0004051380586393396,
      "loss": 7.2334,
      "step": 1334
    },
    {
      "epoch": 0.3800711743772242,
      "grad_norm": 0.4150471091270447,
      "learning_rate": 0.00040506689439225735,
      "loss": 7.8135,
      "step": 1335
    },
    {
      "epoch": 0.380355871886121,
      "grad_norm": 0.4916723966598511,
      "learning_rate": 0.00040499573014517507,
      "loss": 7.1611,
      "step": 1336
    },
    {
      "epoch": 0.3806405693950178,
      "grad_norm": 0.5903331637382507,
      "learning_rate": 0.0004049245658980928,
      "loss": 7.3105,
      "step": 1337
    },
    {
      "epoch": 0.3809252669039146,
      "grad_norm": 0.5368372797966003,
      "learning_rate": 0.0004048534016510105,
      "loss": 6.752,
      "step": 1338
    },
    {
      "epoch": 0.3812099644128114,
      "grad_norm": 0.570995032787323,
      "learning_rate": 0.0004047822374039283,
      "loss": 7.1406,
      "step": 1339
    },
    {
      "epoch": 0.38149466192170817,
      "grad_norm": 0.5715189576148987,
      "learning_rate": 0.000404711073156846,
      "loss": 7.2061,
      "step": 1340
    },
    {
      "epoch": 0.381779359430605,
      "grad_norm": 0.543644368648529,
      "learning_rate": 0.0004046399089097638,
      "loss": 6.8984,
      "step": 1341
    },
    {
      "epoch": 0.3820640569395018,
      "grad_norm": 0.4701455533504486,
      "learning_rate": 0.00040456874466268146,
      "loss": 7.3135,
      "step": 1342
    },
    {
      "epoch": 0.38234875444839855,
      "grad_norm": 0.47678303718566895,
      "learning_rate": 0.0004044975804155992,
      "loss": 7.6211,
      "step": 1343
    },
    {
      "epoch": 0.38263345195729537,
      "grad_norm": 0.46302834153175354,
      "learning_rate": 0.00040442641616851696,
      "loss": 7.2256,
      "step": 1344
    },
    {
      "epoch": 0.3829181494661922,
      "grad_norm": 0.45586827397346497,
      "learning_rate": 0.0004043552519214347,
      "loss": 8.0332,
      "step": 1345
    },
    {
      "epoch": 0.383202846975089,
      "grad_norm": 0.7867341041564941,
      "learning_rate": 0.0004042840876743524,
      "loss": 6.6299,
      "step": 1346
    },
    {
      "epoch": 0.38348754448398575,
      "grad_norm": 0.5549026131629944,
      "learning_rate": 0.0004042129234272702,
      "loss": 6.9746,
      "step": 1347
    },
    {
      "epoch": 0.38377224199288257,
      "grad_norm": 0.5253206491470337,
      "learning_rate": 0.00040414175918018785,
      "loss": 7.167,
      "step": 1348
    },
    {
      "epoch": 0.3840569395017794,
      "grad_norm": 0.5383652448654175,
      "learning_rate": 0.0004040705949331056,
      "loss": 6.873,
      "step": 1349
    },
    {
      "epoch": 0.38434163701067614,
      "grad_norm": 0.450445294380188,
      "learning_rate": 0.00040399943068602335,
      "loss": 7.584,
      "step": 1350
    },
    {
      "epoch": 0.38462633451957295,
      "grad_norm": 0.5621605515480042,
      "learning_rate": 0.00040392826643894107,
      "loss": 7.1729,
      "step": 1351
    },
    {
      "epoch": 0.38491103202846977,
      "grad_norm": 0.5089342594146729,
      "learning_rate": 0.00040385710219185885,
      "loss": 7.0508,
      "step": 1352
    },
    {
      "epoch": 0.3851957295373665,
      "grad_norm": 0.49221551418304443,
      "learning_rate": 0.0004037859379447765,
      "loss": 7.3994,
      "step": 1353
    },
    {
      "epoch": 0.38548042704626334,
      "grad_norm": 0.49137023091316223,
      "learning_rate": 0.0004037147736976943,
      "loss": 7.2207,
      "step": 1354
    },
    {
      "epoch": 0.38576512455516015,
      "grad_norm": 0.4768388867378235,
      "learning_rate": 0.000403643609450612,
      "loss": 7.3447,
      "step": 1355
    },
    {
      "epoch": 0.3860498220640569,
      "grad_norm": 0.5809038281440735,
      "learning_rate": 0.00040357244520352974,
      "loss": 7.1562,
      "step": 1356
    },
    {
      "epoch": 0.3863345195729537,
      "grad_norm": 0.4786946475505829,
      "learning_rate": 0.0004035012809564475,
      "loss": 7.6953,
      "step": 1357
    },
    {
      "epoch": 0.38661921708185054,
      "grad_norm": 0.48583534359931946,
      "learning_rate": 0.00040343011670936524,
      "loss": 7.4375,
      "step": 1358
    },
    {
      "epoch": 0.38690391459074736,
      "grad_norm": 0.5095043778419495,
      "learning_rate": 0.0004033589524622829,
      "loss": 7.2275,
      "step": 1359
    },
    {
      "epoch": 0.3871886120996441,
      "grad_norm": 0.5062644481658936,
      "learning_rate": 0.0004032877882152007,
      "loss": 7.3193,
      "step": 1360
    },
    {
      "epoch": 0.38747330960854093,
      "grad_norm": 0.5594438314437866,
      "learning_rate": 0.0004032166239681184,
      "loss": 7.0176,
      "step": 1361
    },
    {
      "epoch": 0.38775800711743774,
      "grad_norm": 0.46305960416793823,
      "learning_rate": 0.0004031454597210362,
      "loss": 7.415,
      "step": 1362
    },
    {
      "epoch": 0.3880427046263345,
      "grad_norm": 0.5114954710006714,
      "learning_rate": 0.0004030742954739539,
      "loss": 7.3076,
      "step": 1363
    },
    {
      "epoch": 0.3883274021352313,
      "grad_norm": 0.5575315952301025,
      "learning_rate": 0.00040300313122687163,
      "loss": 7.3623,
      "step": 1364
    },
    {
      "epoch": 0.38861209964412813,
      "grad_norm": 0.5885306596755981,
      "learning_rate": 0.00040293196697978935,
      "loss": 7.3291,
      "step": 1365
    },
    {
      "epoch": 0.3888967971530249,
      "grad_norm": 0.5181400179862976,
      "learning_rate": 0.0004028608027327071,
      "loss": 7.0488,
      "step": 1366
    },
    {
      "epoch": 0.3891814946619217,
      "grad_norm": 0.4886661171913147,
      "learning_rate": 0.00040278963848562485,
      "loss": 7.4795,
      "step": 1367
    },
    {
      "epoch": 0.3894661921708185,
      "grad_norm": 0.49588173627853394,
      "learning_rate": 0.0004027184742385426,
      "loss": 7.75,
      "step": 1368
    },
    {
      "epoch": 0.38975088967971533,
      "grad_norm": 0.44115424156188965,
      "learning_rate": 0.0004026473099914603,
      "loss": 7.5293,
      "step": 1369
    },
    {
      "epoch": 0.3900355871886121,
      "grad_norm": 0.44835931062698364,
      "learning_rate": 0.000402576145744378,
      "loss": 7.5801,
      "step": 1370
    },
    {
      "epoch": 0.3903202846975089,
      "grad_norm": 0.4747038781642914,
      "learning_rate": 0.00040250498149729574,
      "loss": 7.252,
      "step": 1371
    },
    {
      "epoch": 0.3906049822064057,
      "grad_norm": 0.5122354030609131,
      "learning_rate": 0.0004024338172502135,
      "loss": 6.8496,
      "step": 1372
    },
    {
      "epoch": 0.3908896797153025,
      "grad_norm": 0.45732030272483826,
      "learning_rate": 0.00040236265300313124,
      "loss": 7.418,
      "step": 1373
    },
    {
      "epoch": 0.3911743772241993,
      "grad_norm": 0.5770905613899231,
      "learning_rate": 0.00040229148875604897,
      "loss": 7.0566,
      "step": 1374
    },
    {
      "epoch": 0.3914590747330961,
      "grad_norm": 0.4752143621444702,
      "learning_rate": 0.00040222032450896674,
      "loss": 7.585,
      "step": 1375
    },
    {
      "epoch": 0.39174377224199286,
      "grad_norm": 0.5004891753196716,
      "learning_rate": 0.0004021491602618844,
      "loss": 7.1553,
      "step": 1376
    },
    {
      "epoch": 0.3920284697508897,
      "grad_norm": 0.42816469073295593,
      "learning_rate": 0.00040207799601480214,
      "loss": 7.3057,
      "step": 1377
    },
    {
      "epoch": 0.3923131672597865,
      "grad_norm": 0.5445579290390015,
      "learning_rate": 0.0004020068317677199,
      "loss": 7.0078,
      "step": 1378
    },
    {
      "epoch": 0.39259786476868325,
      "grad_norm": 0.492911159992218,
      "learning_rate": 0.00040193566752063763,
      "loss": 7.1426,
      "step": 1379
    },
    {
      "epoch": 0.39288256227758006,
      "grad_norm": 0.5625669956207275,
      "learning_rate": 0.0004018645032735554,
      "loss": 7.3311,
      "step": 1380
    },
    {
      "epoch": 0.3931672597864769,
      "grad_norm": 0.4951213300228119,
      "learning_rate": 0.0004017933390264731,
      "loss": 7.54,
      "step": 1381
    },
    {
      "epoch": 0.3934519572953737,
      "grad_norm": 0.4944069981575012,
      "learning_rate": 0.0004017221747793908,
      "loss": 7.4336,
      "step": 1382
    },
    {
      "epoch": 0.39373665480427045,
      "grad_norm": 0.5445288419723511,
      "learning_rate": 0.0004016510105323086,
      "loss": 6.4658,
      "step": 1383
    },
    {
      "epoch": 0.39402135231316726,
      "grad_norm": 0.4639240801334381,
      "learning_rate": 0.0004015798462852263,
      "loss": 7.458,
      "step": 1384
    },
    {
      "epoch": 0.3943060498220641,
      "grad_norm": 0.8487449884414673,
      "learning_rate": 0.0004015086820381441,
      "loss": 5.751,
      "step": 1385
    },
    {
      "epoch": 0.39459074733096083,
      "grad_norm": 0.5100080370903015,
      "learning_rate": 0.0004014375177910618,
      "loss": 7.7383,
      "step": 1386
    },
    {
      "epoch": 0.39487544483985765,
      "grad_norm": 0.467268168926239,
      "learning_rate": 0.00040136635354397947,
      "loss": 7.4971,
      "step": 1387
    },
    {
      "epoch": 0.39516014234875446,
      "grad_norm": 0.5072725415229797,
      "learning_rate": 0.00040129518929689725,
      "loss": 7.5605,
      "step": 1388
    },
    {
      "epoch": 0.3954448398576512,
      "grad_norm": 0.5850234031677246,
      "learning_rate": 0.00040122402504981497,
      "loss": 7.2188,
      "step": 1389
    },
    {
      "epoch": 0.39572953736654803,
      "grad_norm": 0.39694520831108093,
      "learning_rate": 0.00040115286080273275,
      "loss": 7.916,
      "step": 1390
    },
    {
      "epoch": 0.39601423487544485,
      "grad_norm": 0.5012302398681641,
      "learning_rate": 0.00040108169655565047,
      "loss": 7.2686,
      "step": 1391
    },
    {
      "epoch": 0.39629893238434166,
      "grad_norm": 0.5060039162635803,
      "learning_rate": 0.0004010105323085682,
      "loss": 7.0459,
      "step": 1392
    },
    {
      "epoch": 0.3965836298932384,
      "grad_norm": 0.5460544228553772,
      "learning_rate": 0.0004009393680614859,
      "loss": 7.3105,
      "step": 1393
    },
    {
      "epoch": 0.39686832740213523,
      "grad_norm": 0.5348192453384399,
      "learning_rate": 0.00040086820381440364,
      "loss": 7.2607,
      "step": 1394
    },
    {
      "epoch": 0.39715302491103205,
      "grad_norm": 0.5724747180938721,
      "learning_rate": 0.00040079703956732136,
      "loss": 6.8809,
      "step": 1395
    },
    {
      "epoch": 0.3974377224199288,
      "grad_norm": 0.5098087787628174,
      "learning_rate": 0.00040072587532023914,
      "loss": 7.5469,
      "step": 1396
    },
    {
      "epoch": 0.3977224199288256,
      "grad_norm": 0.4509059488773346,
      "learning_rate": 0.00040065471107315686,
      "loss": 7.6543,
      "step": 1397
    },
    {
      "epoch": 0.39800711743772244,
      "grad_norm": 0.5082734823226929,
      "learning_rate": 0.0004005835468260746,
      "loss": 7.2939,
      "step": 1398
    },
    {
      "epoch": 0.3982918149466192,
      "grad_norm": 0.5169782042503357,
      "learning_rate": 0.0004005123825789923,
      "loss": 7.0498,
      "step": 1399
    },
    {
      "epoch": 0.398576512455516,
      "grad_norm": 0.48001834750175476,
      "learning_rate": 0.00040044121833191003,
      "loss": 7.1455,
      "step": 1400
    },
    {
      "epoch": 0.398576512455516,
      "eval_bleu": 0.10806696460928729,
      "eval_loss": 7.0703125,
      "eval_runtime": 146.4757,
      "eval_samples_per_second": 1.939,
      "eval_steps_per_second": 0.123,
      "step": 1400
    },
    {
      "epoch": 0.3988612099644128,
      "grad_norm": 0.4626038670539856,
      "learning_rate": 0.0004003700540848278,
      "loss": 7.0742,
      "step": 1401
    },
    {
      "epoch": 0.39914590747330964,
      "grad_norm": 0.5733367204666138,
      "learning_rate": 0.00040029888983774553,
      "loss": 7.918,
      "step": 1402
    },
    {
      "epoch": 0.3994306049822064,
      "grad_norm": 0.531499981880188,
      "learning_rate": 0.0004002277255906633,
      "loss": 7.1992,
      "step": 1403
    },
    {
      "epoch": 0.3997153024911032,
      "grad_norm": 0.49822041392326355,
      "learning_rate": 0.000400156561343581,
      "loss": 7.0742,
      "step": 1404
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.5700138211250305,
      "learning_rate": 0.0004000853970964987,
      "loss": 6.8828,
      "step": 1405
    },
    {
      "epoch": 0.4002846975088968,
      "grad_norm": 0.4932331442832947,
      "learning_rate": 0.0004000142328494165,
      "loss": 7.584,
      "step": 1406
    },
    {
      "epoch": 0.4005693950177936,
      "grad_norm": 0.5428732633590698,
      "learning_rate": 0.0003999430686023342,
      "loss": 7.1328,
      "step": 1407
    },
    {
      "epoch": 0.4008540925266904,
      "grad_norm": 0.704805850982666,
      "learning_rate": 0.000399871904355252,
      "loss": 6.8857,
      "step": 1408
    },
    {
      "epoch": 0.40113879003558717,
      "grad_norm": 0.580433189868927,
      "learning_rate": 0.0003998007401081697,
      "loss": 6.9805,
      "step": 1409
    },
    {
      "epoch": 0.401423487544484,
      "grad_norm": 0.42145293951034546,
      "learning_rate": 0.00039972957586108737,
      "loss": 7.6494,
      "step": 1410
    },
    {
      "epoch": 0.4017081850533808,
      "grad_norm": 0.448978066444397,
      "learning_rate": 0.00039965841161400514,
      "loss": 7.7148,
      "step": 1411
    },
    {
      "epoch": 0.40199288256227755,
      "grad_norm": 0.6176373958587646,
      "learning_rate": 0.00039958724736692287,
      "loss": 7.2891,
      "step": 1412
    },
    {
      "epoch": 0.40227758007117437,
      "grad_norm": 0.5092046856880188,
      "learning_rate": 0.0003995160831198406,
      "loss": 7.459,
      "step": 1413
    },
    {
      "epoch": 0.4025622775800712,
      "grad_norm": 0.4859773814678192,
      "learning_rate": 0.00039944491887275837,
      "loss": 7.2979,
      "step": 1414
    },
    {
      "epoch": 0.402846975088968,
      "grad_norm": 0.5634656548500061,
      "learning_rate": 0.00039937375462567603,
      "loss": 7.2861,
      "step": 1415
    },
    {
      "epoch": 0.40313167259786475,
      "grad_norm": 0.5251878499984741,
      "learning_rate": 0.0003993025903785938,
      "loss": 7.1133,
      "step": 1416
    },
    {
      "epoch": 0.40341637010676157,
      "grad_norm": 0.44515395164489746,
      "learning_rate": 0.00039923142613151153,
      "loss": 7.585,
      "step": 1417
    },
    {
      "epoch": 0.4037010676156584,
      "grad_norm": 0.5295915603637695,
      "learning_rate": 0.00039916026188442926,
      "loss": 6.9727,
      "step": 1418
    },
    {
      "epoch": 0.40398576512455514,
      "grad_norm": 0.49705609679222107,
      "learning_rate": 0.00039908909763734703,
      "loss": 7.2217,
      "step": 1419
    },
    {
      "epoch": 0.40427046263345195,
      "grad_norm": 0.5276616811752319,
      "learning_rate": 0.00039901793339026476,
      "loss": 7.3721,
      "step": 1420
    },
    {
      "epoch": 0.40455516014234877,
      "grad_norm": 0.6657406687736511,
      "learning_rate": 0.0003989467691431825,
      "loss": 6.4639,
      "step": 1421
    },
    {
      "epoch": 0.4048398576512455,
      "grad_norm": 0.5318674445152283,
      "learning_rate": 0.0003988756048961002,
      "loss": 7.0811,
      "step": 1422
    },
    {
      "epoch": 0.40512455516014234,
      "grad_norm": 0.5530655980110168,
      "learning_rate": 0.0003988044406490179,
      "loss": 7.0322,
      "step": 1423
    },
    {
      "epoch": 0.40540925266903916,
      "grad_norm": 0.5155173540115356,
      "learning_rate": 0.0003987332764019357,
      "loss": 7.3486,
      "step": 1424
    },
    {
      "epoch": 0.40569395017793597,
      "grad_norm": 0.5686538219451904,
      "learning_rate": 0.0003986621121548534,
      "loss": 7.5049,
      "step": 1425
    },
    {
      "epoch": 0.4059786476868327,
      "grad_norm": 0.5946599841117859,
      "learning_rate": 0.0003985909479077711,
      "loss": 6.7871,
      "step": 1426
    },
    {
      "epoch": 0.40626334519572954,
      "grad_norm": 0.5096083283424377,
      "learning_rate": 0.00039851978366068887,
      "loss": 7.21,
      "step": 1427
    },
    {
      "epoch": 0.40654804270462636,
      "grad_norm": 0.4696735739707947,
      "learning_rate": 0.0003984486194136066,
      "loss": 7.3418,
      "step": 1428
    },
    {
      "epoch": 0.4068327402135231,
      "grad_norm": 0.49570679664611816,
      "learning_rate": 0.00039837745516652437,
      "loss": 7.376,
      "step": 1429
    },
    {
      "epoch": 0.40711743772241993,
      "grad_norm": 0.5150629878044128,
      "learning_rate": 0.0003983062909194421,
      "loss": 7.2188,
      "step": 1430
    },
    {
      "epoch": 0.40740213523131674,
      "grad_norm": 0.49344584345817566,
      "learning_rate": 0.0003982351266723598,
      "loss": 7.7285,
      "step": 1431
    },
    {
      "epoch": 0.4076868327402135,
      "grad_norm": 0.47263434529304504,
      "learning_rate": 0.00039816396242527754,
      "loss": 7.3428,
      "step": 1432
    },
    {
      "epoch": 0.4079715302491103,
      "grad_norm": 0.45633506774902344,
      "learning_rate": 0.00039809279817819526,
      "loss": 7.4014,
      "step": 1433
    },
    {
      "epoch": 0.40825622775800713,
      "grad_norm": 0.45884770154953003,
      "learning_rate": 0.00039802163393111304,
      "loss": 7.8037,
      "step": 1434
    },
    {
      "epoch": 0.4085409252669039,
      "grad_norm": 0.5151818990707397,
      "learning_rate": 0.00039795046968403076,
      "loss": 7.4043,
      "step": 1435
    },
    {
      "epoch": 0.4088256227758007,
      "grad_norm": 0.45263946056365967,
      "learning_rate": 0.0003978793054369485,
      "loss": 7.3467,
      "step": 1436
    },
    {
      "epoch": 0.4091103202846975,
      "grad_norm": 0.47288239002227783,
      "learning_rate": 0.00039780814118986626,
      "loss": 7.1445,
      "step": 1437
    },
    {
      "epoch": 0.40939501779359433,
      "grad_norm": 0.522957444190979,
      "learning_rate": 0.00039773697694278393,
      "loss": 7.4648,
      "step": 1438
    },
    {
      "epoch": 0.4096797153024911,
      "grad_norm": 0.425659716129303,
      "learning_rate": 0.0003976658126957017,
      "loss": 7.6475,
      "step": 1439
    },
    {
      "epoch": 0.4099644128113879,
      "grad_norm": 0.5347933769226074,
      "learning_rate": 0.00039759464844861943,
      "loss": 7.3047,
      "step": 1440
    },
    {
      "epoch": 0.4102491103202847,
      "grad_norm": 0.521828830242157,
      "learning_rate": 0.00039752348420153715,
      "loss": 7.4268,
      "step": 1441
    },
    {
      "epoch": 0.4105338078291815,
      "grad_norm": 0.3580774664878845,
      "learning_rate": 0.00039745231995445493,
      "loss": 8.0146,
      "step": 1442
    },
    {
      "epoch": 0.4108185053380783,
      "grad_norm": 0.4850667417049408,
      "learning_rate": 0.0003973811557073726,
      "loss": 7.6045,
      "step": 1443
    },
    {
      "epoch": 0.4111032028469751,
      "grad_norm": 0.5015649795532227,
      "learning_rate": 0.0003973099914602903,
      "loss": 7.3203,
      "step": 1444
    },
    {
      "epoch": 0.41138790035587186,
      "grad_norm": 0.6262863278388977,
      "learning_rate": 0.0003972388272132081,
      "loss": 6.9189,
      "step": 1445
    },
    {
      "epoch": 0.4116725978647687,
      "grad_norm": 0.7990656495094299,
      "learning_rate": 0.0003971676629661258,
      "loss": 7.4863,
      "step": 1446
    },
    {
      "epoch": 0.4119572953736655,
      "grad_norm": 0.4794697165489197,
      "learning_rate": 0.0003970964987190436,
      "loss": 7.5576,
      "step": 1447
    },
    {
      "epoch": 0.4122419928825623,
      "grad_norm": 0.3931448459625244,
      "learning_rate": 0.0003970253344719613,
      "loss": 7.5928,
      "step": 1448
    },
    {
      "epoch": 0.41252669039145906,
      "grad_norm": 0.5144194960594177,
      "learning_rate": 0.000396954170224879,
      "loss": 7.3867,
      "step": 1449
    },
    {
      "epoch": 0.4128113879003559,
      "grad_norm": 0.4392128586769104,
      "learning_rate": 0.00039688300597779676,
      "loss": 7.7227,
      "step": 1450
    },
    {
      "epoch": 0.4130960854092527,
      "grad_norm": 0.5938143134117126,
      "learning_rate": 0.0003968118417307145,
      "loss": 6.9053,
      "step": 1451
    },
    {
      "epoch": 0.41338078291814945,
      "grad_norm": 0.47164639830589294,
      "learning_rate": 0.00039674067748363226,
      "loss": 7.7539,
      "step": 1452
    },
    {
      "epoch": 0.41366548042704626,
      "grad_norm": 0.5650911927223206,
      "learning_rate": 0.00039666951323655,
      "loss": 6.7578,
      "step": 1453
    },
    {
      "epoch": 0.4139501779359431,
      "grad_norm": 0.5679348111152649,
      "learning_rate": 0.00039659834898946765,
      "loss": 6.8457,
      "step": 1454
    },
    {
      "epoch": 0.41423487544483983,
      "grad_norm": 0.4574567675590515,
      "learning_rate": 0.00039652718474238543,
      "loss": 7.9424,
      "step": 1455
    },
    {
      "epoch": 0.41451957295373665,
      "grad_norm": 0.47063425183296204,
      "learning_rate": 0.00039645602049530315,
      "loss": 7.5273,
      "step": 1456
    },
    {
      "epoch": 0.41480427046263346,
      "grad_norm": 0.5154306888580322,
      "learning_rate": 0.0003963848562482209,
      "loss": 6.8311,
      "step": 1457
    },
    {
      "epoch": 0.4150889679715303,
      "grad_norm": 0.5713111162185669,
      "learning_rate": 0.00039631369200113865,
      "loss": 7.0635,
      "step": 1458
    },
    {
      "epoch": 0.41537366548042703,
      "grad_norm": 0.6168776154518127,
      "learning_rate": 0.0003962425277540564,
      "loss": 7.1387,
      "step": 1459
    },
    {
      "epoch": 0.41565836298932385,
      "grad_norm": 0.6416788697242737,
      "learning_rate": 0.0003961713635069741,
      "loss": 6.4355,
      "step": 1460
    },
    {
      "epoch": 0.41594306049822066,
      "grad_norm": 0.5416299700737,
      "learning_rate": 0.0003961001992598918,
      "loss": 6.8975,
      "step": 1461
    },
    {
      "epoch": 0.4162277580071174,
      "grad_norm": 0.4209768772125244,
      "learning_rate": 0.00039602903501280955,
      "loss": 7.6387,
      "step": 1462
    },
    {
      "epoch": 0.41651245551601424,
      "grad_norm": 0.6386048793792725,
      "learning_rate": 0.0003959578707657273,
      "loss": 7.0059,
      "step": 1463
    },
    {
      "epoch": 0.41679715302491105,
      "grad_norm": 0.46949610114097595,
      "learning_rate": 0.00039588670651864504,
      "loss": 7.5059,
      "step": 1464
    },
    {
      "epoch": 0.4170818505338078,
      "grad_norm": 0.45663750171661377,
      "learning_rate": 0.0003958155422715628,
      "loss": 7.5117,
      "step": 1465
    },
    {
      "epoch": 0.4173665480427046,
      "grad_norm": 0.4578010141849518,
      "learning_rate": 0.0003957443780244805,
      "loss": 7.6816,
      "step": 1466
    },
    {
      "epoch": 0.41765124555160144,
      "grad_norm": 0.5094571113586426,
      "learning_rate": 0.0003956732137773982,
      "loss": 7.5137,
      "step": 1467
    },
    {
      "epoch": 0.4179359430604982,
      "grad_norm": 0.6814231872558594,
      "learning_rate": 0.000395602049530316,
      "loss": 6.5576,
      "step": 1468
    },
    {
      "epoch": 0.418220640569395,
      "grad_norm": 0.5959497094154358,
      "learning_rate": 0.0003955308852832337,
      "loss": 7.0986,
      "step": 1469
    },
    {
      "epoch": 0.4185053380782918,
      "grad_norm": 0.508515477180481,
      "learning_rate": 0.0003954597210361515,
      "loss": 7.166,
      "step": 1470
    },
    {
      "epoch": 0.41879003558718864,
      "grad_norm": 0.5078320503234863,
      "learning_rate": 0.00039538855678906916,
      "loss": 7.4453,
      "step": 1471
    },
    {
      "epoch": 0.4190747330960854,
      "grad_norm": 0.4999483525753021,
      "learning_rate": 0.0003953173925419869,
      "loss": 7.167,
      "step": 1472
    },
    {
      "epoch": 0.4193594306049822,
      "grad_norm": 0.6152355670928955,
      "learning_rate": 0.00039524622829490466,
      "loss": 6.6787,
      "step": 1473
    },
    {
      "epoch": 0.419644128113879,
      "grad_norm": 0.4665818512439728,
      "learning_rate": 0.0003951750640478224,
      "loss": 7.7871,
      "step": 1474
    },
    {
      "epoch": 0.4199288256227758,
      "grad_norm": 0.7211839556694031,
      "learning_rate": 0.0003951038998007401,
      "loss": 6.9883,
      "step": 1475
    },
    {
      "epoch": 0.4202135231316726,
      "grad_norm": 0.5218536257743835,
      "learning_rate": 0.0003950327355536579,
      "loss": 7.1084,
      "step": 1476
    },
    {
      "epoch": 0.4204982206405694,
      "grad_norm": 0.4066094756126404,
      "learning_rate": 0.00039496157130657555,
      "loss": 7.5684,
      "step": 1477
    },
    {
      "epoch": 0.42078291814946617,
      "grad_norm": 0.5480060577392578,
      "learning_rate": 0.0003948904070594933,
      "loss": 7.5,
      "step": 1478
    },
    {
      "epoch": 0.421067615658363,
      "grad_norm": 0.4934498071670532,
      "learning_rate": 0.00039481924281241105,
      "loss": 7.6016,
      "step": 1479
    },
    {
      "epoch": 0.4213523131672598,
      "grad_norm": 0.5548122525215149,
      "learning_rate": 0.00039474807856532877,
      "loss": 7.3652,
      "step": 1480
    },
    {
      "epoch": 0.4216370106761566,
      "grad_norm": 0.45546260476112366,
      "learning_rate": 0.00039467691431824655,
      "loss": 7.5967,
      "step": 1481
    },
    {
      "epoch": 0.42192170818505337,
      "grad_norm": 0.5817190408706665,
      "learning_rate": 0.00039460575007116427,
      "loss": 7.3926,
      "step": 1482
    },
    {
      "epoch": 0.4222064056939502,
      "grad_norm": 0.5597066879272461,
      "learning_rate": 0.000394534585824082,
      "loss": 7.1064,
      "step": 1483
    },
    {
      "epoch": 0.422491103202847,
      "grad_norm": 0.4652703106403351,
      "learning_rate": 0.0003944634215769997,
      "loss": 7.6709,
      "step": 1484
    },
    {
      "epoch": 0.42277580071174375,
      "grad_norm": 0.47021421790122986,
      "learning_rate": 0.00039439225732991744,
      "loss": 7.5459,
      "step": 1485
    },
    {
      "epoch": 0.42306049822064057,
      "grad_norm": 0.49439412355422974,
      "learning_rate": 0.0003943210930828352,
      "loss": 7.3359,
      "step": 1486
    },
    {
      "epoch": 0.4233451957295374,
      "grad_norm": 0.6628631949424744,
      "learning_rate": 0.00039424992883575294,
      "loss": 6.8301,
      "step": 1487
    },
    {
      "epoch": 0.42362989323843414,
      "grad_norm": 0.7028085589408875,
      "learning_rate": 0.00039417876458867066,
      "loss": 6.502,
      "step": 1488
    },
    {
      "epoch": 0.42391459074733095,
      "grad_norm": 0.42867743968963623,
      "learning_rate": 0.0003941076003415884,
      "loss": 7.5254,
      "step": 1489
    },
    {
      "epoch": 0.42419928825622777,
      "grad_norm": 0.48032599687576294,
      "learning_rate": 0.0003940364360945061,
      "loss": 7.3857,
      "step": 1490
    },
    {
      "epoch": 0.4244839857651246,
      "grad_norm": 0.5163261890411377,
      "learning_rate": 0.0003939652718474239,
      "loss": 7.1885,
      "step": 1491
    },
    {
      "epoch": 0.42476868327402134,
      "grad_norm": 0.5880164504051208,
      "learning_rate": 0.0003938941076003416,
      "loss": 6.9902,
      "step": 1492
    },
    {
      "epoch": 0.42505338078291816,
      "grad_norm": 0.5400647521018982,
      "learning_rate": 0.00039382294335325933,
      "loss": 7.2158,
      "step": 1493
    },
    {
      "epoch": 0.42533807829181497,
      "grad_norm": 0.5012080073356628,
      "learning_rate": 0.00039375177910617705,
      "loss": 7.5469,
      "step": 1494
    },
    {
      "epoch": 0.42562277580071173,
      "grad_norm": 0.4507448971271515,
      "learning_rate": 0.0003936806148590948,
      "loss": 7.5957,
      "step": 1495
    },
    {
      "epoch": 0.42590747330960854,
      "grad_norm": 0.4821513295173645,
      "learning_rate": 0.00039360945061201255,
      "loss": 7.6807,
      "step": 1496
    },
    {
      "epoch": 0.42619217081850536,
      "grad_norm": 0.6177121996879578,
      "learning_rate": 0.0003935382863649303,
      "loss": 6.666,
      "step": 1497
    },
    {
      "epoch": 0.4264768683274021,
      "grad_norm": 0.5181967616081238,
      "learning_rate": 0.000393467122117848,
      "loss": 7.3809,
      "step": 1498
    },
    {
      "epoch": 0.42676156583629893,
      "grad_norm": 0.5240229368209839,
      "learning_rate": 0.0003933959578707657,
      "loss": 7.0859,
      "step": 1499
    },
    {
      "epoch": 0.42704626334519574,
      "grad_norm": 0.4757440984249115,
      "learning_rate": 0.00039332479362368344,
      "loss": 7.4932,
      "step": 1500
    },
    {
      "epoch": 0.4273309608540925,
      "grad_norm": 0.4744635224342346,
      "learning_rate": 0.0003932536293766012,
      "loss": 7.5098,
      "step": 1501
    },
    {
      "epoch": 0.4276156583629893,
      "grad_norm": 0.4695112109184265,
      "learning_rate": 0.00039318246512951894,
      "loss": 7.4092,
      "step": 1502
    },
    {
      "epoch": 0.42790035587188613,
      "grad_norm": 0.5622231364250183,
      "learning_rate": 0.00039311130088243667,
      "loss": 7.3037,
      "step": 1503
    },
    {
      "epoch": 0.42818505338078294,
      "grad_norm": 0.5066624283790588,
      "learning_rate": 0.00039304013663535444,
      "loss": 7.25,
      "step": 1504
    },
    {
      "epoch": 0.4284697508896797,
      "grad_norm": 0.5502866506576538,
      "learning_rate": 0.0003929689723882721,
      "loss": 6.9746,
      "step": 1505
    },
    {
      "epoch": 0.4287544483985765,
      "grad_norm": 0.6086217164993286,
      "learning_rate": 0.00039289780814118983,
      "loss": 7.1367,
      "step": 1506
    },
    {
      "epoch": 0.42903914590747333,
      "grad_norm": 0.42815905809402466,
      "learning_rate": 0.0003928266438941076,
      "loss": 7.7324,
      "step": 1507
    },
    {
      "epoch": 0.4293238434163701,
      "grad_norm": 0.6567767858505249,
      "learning_rate": 0.00039275547964702533,
      "loss": 6.5312,
      "step": 1508
    },
    {
      "epoch": 0.4296085409252669,
      "grad_norm": 0.5427162647247314,
      "learning_rate": 0.0003926843153999431,
      "loss": 7.1055,
      "step": 1509
    },
    {
      "epoch": 0.4298932384341637,
      "grad_norm": 0.5855767726898193,
      "learning_rate": 0.00039261315115286083,
      "loss": 7.3848,
      "step": 1510
    },
    {
      "epoch": 0.4301779359430605,
      "grad_norm": 0.5894039273262024,
      "learning_rate": 0.0003925419869057785,
      "loss": 7.0264,
      "step": 1511
    },
    {
      "epoch": 0.4304626334519573,
      "grad_norm": 0.607792317867279,
      "learning_rate": 0.0003924708226586963,
      "loss": 7.2588,
      "step": 1512
    },
    {
      "epoch": 0.4307473309608541,
      "grad_norm": 0.4645512104034424,
      "learning_rate": 0.000392399658411614,
      "loss": 7.5645,
      "step": 1513
    },
    {
      "epoch": 0.4310320284697509,
      "grad_norm": 0.42966175079345703,
      "learning_rate": 0.0003923284941645318,
      "loss": 7.7715,
      "step": 1514
    },
    {
      "epoch": 0.4313167259786477,
      "grad_norm": 0.5350296497344971,
      "learning_rate": 0.0003922573299174495,
      "loss": 7.5469,
      "step": 1515
    },
    {
      "epoch": 0.4316014234875445,
      "grad_norm": 0.6260858774185181,
      "learning_rate": 0.00039218616567036717,
      "loss": 7.2988,
      "step": 1516
    },
    {
      "epoch": 0.4318861209964413,
      "grad_norm": 0.48744967579841614,
      "learning_rate": 0.00039211500142328495,
      "loss": 7.2822,
      "step": 1517
    },
    {
      "epoch": 0.43217081850533806,
      "grad_norm": 0.49149957299232483,
      "learning_rate": 0.00039204383717620267,
      "loss": 7.3467,
      "step": 1518
    },
    {
      "epoch": 0.4324555160142349,
      "grad_norm": 0.455314964056015,
      "learning_rate": 0.00039197267292912045,
      "loss": 7.1279,
      "step": 1519
    },
    {
      "epoch": 0.4327402135231317,
      "grad_norm": 0.7547210454940796,
      "learning_rate": 0.00039190150868203817,
      "loss": 6.4346,
      "step": 1520
    },
    {
      "epoch": 0.43302491103202845,
      "grad_norm": 0.46810033917427063,
      "learning_rate": 0.0003918303444349559,
      "loss": 7.1934,
      "step": 1521
    },
    {
      "epoch": 0.43330960854092526,
      "grad_norm": 0.645000696182251,
      "learning_rate": 0.0003917591801878736,
      "loss": 6.6836,
      "step": 1522
    },
    {
      "epoch": 0.4335943060498221,
      "grad_norm": 0.4427584111690521,
      "learning_rate": 0.00039168801594079134,
      "loss": 7.6494,
      "step": 1523
    },
    {
      "epoch": 0.43387900355871883,
      "grad_norm": 0.5031856894493103,
      "learning_rate": 0.00039161685169370906,
      "loss": 7.501,
      "step": 1524
    },
    {
      "epoch": 0.43416370106761565,
      "grad_norm": 0.5083847045898438,
      "learning_rate": 0.00039154568744662684,
      "loss": 7.3271,
      "step": 1525
    },
    {
      "epoch": 0.43444839857651246,
      "grad_norm": 0.5034371018409729,
      "learning_rate": 0.00039147452319954456,
      "loss": 7.4863,
      "step": 1526
    },
    {
      "epoch": 0.4347330960854093,
      "grad_norm": 0.5857424736022949,
      "learning_rate": 0.00039140335895246234,
      "loss": 6.9111,
      "step": 1527
    },
    {
      "epoch": 0.43501779359430603,
      "grad_norm": 0.5124900341033936,
      "learning_rate": 0.00039133219470538,
      "loss": 7.4219,
      "step": 1528
    },
    {
      "epoch": 0.43530249110320285,
      "grad_norm": 0.5162451863288879,
      "learning_rate": 0.00039126103045829773,
      "loss": 7.8906,
      "step": 1529
    },
    {
      "epoch": 0.43558718861209966,
      "grad_norm": 0.5202774405479431,
      "learning_rate": 0.0003911898662112155,
      "loss": 7.2754,
      "step": 1530
    },
    {
      "epoch": 0.4358718861209964,
      "grad_norm": 0.470385879278183,
      "learning_rate": 0.00039111870196413323,
      "loss": 7.3652,
      "step": 1531
    },
    {
      "epoch": 0.43615658362989324,
      "grad_norm": 0.6605905890464783,
      "learning_rate": 0.000391047537717051,
      "loss": 6.9004,
      "step": 1532
    },
    {
      "epoch": 0.43644128113879005,
      "grad_norm": 0.6666309237480164,
      "learning_rate": 0.0003909763734699687,
      "loss": 6.8594,
      "step": 1533
    },
    {
      "epoch": 0.4367259786476868,
      "grad_norm": 0.5365574359893799,
      "learning_rate": 0.0003909052092228864,
      "loss": 7.0547,
      "step": 1534
    },
    {
      "epoch": 0.4370106761565836,
      "grad_norm": 0.4991326332092285,
      "learning_rate": 0.0003908340449758042,
      "loss": 7.8955,
      "step": 1535
    },
    {
      "epoch": 0.43729537366548044,
      "grad_norm": 0.4271744191646576,
      "learning_rate": 0.0003907628807287219,
      "loss": 7.7021,
      "step": 1536
    },
    {
      "epoch": 0.43758007117437725,
      "grad_norm": 0.5445852875709534,
      "learning_rate": 0.0003906917164816397,
      "loss": 7.208,
      "step": 1537
    },
    {
      "epoch": 0.437864768683274,
      "grad_norm": 0.5331472754478455,
      "learning_rate": 0.0003906205522345574,
      "loss": 6.9561,
      "step": 1538
    },
    {
      "epoch": 0.4381494661921708,
      "grad_norm": 0.5976002812385559,
      "learning_rate": 0.00039054938798747506,
      "loss": 7.3389,
      "step": 1539
    },
    {
      "epoch": 0.43843416370106764,
      "grad_norm": 0.5062530636787415,
      "learning_rate": 0.00039047822374039284,
      "loss": 7.3623,
      "step": 1540
    },
    {
      "epoch": 0.4387188612099644,
      "grad_norm": 0.5322418808937073,
      "learning_rate": 0.00039040705949331056,
      "loss": 7.1709,
      "step": 1541
    },
    {
      "epoch": 0.4390035587188612,
      "grad_norm": 0.5205498337745667,
      "learning_rate": 0.0003903358952462283,
      "loss": 7.2744,
      "step": 1542
    },
    {
      "epoch": 0.439288256227758,
      "grad_norm": 0.45532262325286865,
      "learning_rate": 0.00039026473099914606,
      "loss": 7.79,
      "step": 1543
    },
    {
      "epoch": 0.4395729537366548,
      "grad_norm": 0.48682892322540283,
      "learning_rate": 0.00039019356675206373,
      "loss": 7.4336,
      "step": 1544
    },
    {
      "epoch": 0.4398576512455516,
      "grad_norm": 0.579499363899231,
      "learning_rate": 0.0003901224025049815,
      "loss": 7.6016,
      "step": 1545
    },
    {
      "epoch": 0.4401423487544484,
      "grad_norm": 0.7260848879814148,
      "learning_rate": 0.00039005123825789923,
      "loss": 6.5781,
      "step": 1546
    },
    {
      "epoch": 0.4404270462633452,
      "grad_norm": 0.5014261603355408,
      "learning_rate": 0.00038998007401081696,
      "loss": 7.0049,
      "step": 1547
    },
    {
      "epoch": 0.440711743772242,
      "grad_norm": 0.4954442083835602,
      "learning_rate": 0.00038990890976373473,
      "loss": 7.2334,
      "step": 1548
    },
    {
      "epoch": 0.4409964412811388,
      "grad_norm": 0.6689090728759766,
      "learning_rate": 0.00038983774551665246,
      "loss": 7.0127,
      "step": 1549
    },
    {
      "epoch": 0.4412811387900356,
      "grad_norm": 0.4490669369697571,
      "learning_rate": 0.0003897665812695702,
      "loss": 7.6104,
      "step": 1550
    },
    {
      "epoch": 0.44156583629893237,
      "grad_norm": 0.532595694065094,
      "learning_rate": 0.0003896954170224879,
      "loss": 7.668,
      "step": 1551
    },
    {
      "epoch": 0.4418505338078292,
      "grad_norm": 0.4709828495979309,
      "learning_rate": 0.0003896242527754056,
      "loss": 7.4961,
      "step": 1552
    },
    {
      "epoch": 0.442135231316726,
      "grad_norm": 0.40507403016090393,
      "learning_rate": 0.0003895530885283234,
      "loss": 7.627,
      "step": 1553
    },
    {
      "epoch": 0.44241992882562275,
      "grad_norm": 0.6264590620994568,
      "learning_rate": 0.0003894819242812411,
      "loss": 6.8613,
      "step": 1554
    },
    {
      "epoch": 0.44270462633451957,
      "grad_norm": 0.5518805384635925,
      "learning_rate": 0.0003894107600341589,
      "loss": 6.7607,
      "step": 1555
    },
    {
      "epoch": 0.4429893238434164,
      "grad_norm": 0.5427720546722412,
      "learning_rate": 0.00038933959578707657,
      "loss": 7.5371,
      "step": 1556
    },
    {
      "epoch": 0.44327402135231314,
      "grad_norm": 0.533429741859436,
      "learning_rate": 0.0003892684315399943,
      "loss": 7.2744,
      "step": 1557
    },
    {
      "epoch": 0.44355871886120996,
      "grad_norm": 0.4740807116031647,
      "learning_rate": 0.00038919726729291207,
      "loss": 7.752,
      "step": 1558
    },
    {
      "epoch": 0.44384341637010677,
      "grad_norm": 0.613650381565094,
      "learning_rate": 0.0003891261030458298,
      "loss": 6.8564,
      "step": 1559
    },
    {
      "epoch": 0.4441281138790036,
      "grad_norm": 0.49877262115478516,
      "learning_rate": 0.0003890549387987475,
      "loss": 7.0342,
      "step": 1560
    },
    {
      "epoch": 0.44441281138790034,
      "grad_norm": 0.5290449261665344,
      "learning_rate": 0.00038898377455166524,
      "loss": 7.1807,
      "step": 1561
    },
    {
      "epoch": 0.44469750889679716,
      "grad_norm": 0.44291484355926514,
      "learning_rate": 0.00038891261030458296,
      "loss": 7.6875,
      "step": 1562
    },
    {
      "epoch": 0.44498220640569397,
      "grad_norm": 0.5553268194198608,
      "learning_rate": 0.00038884144605750074,
      "loss": 7.2188,
      "step": 1563
    },
    {
      "epoch": 0.44526690391459073,
      "grad_norm": 0.5795010328292847,
      "learning_rate": 0.00038877028181041846,
      "loss": 7.207,
      "step": 1564
    },
    {
      "epoch": 0.44555160142348754,
      "grad_norm": 0.5112793445587158,
      "learning_rate": 0.0003886991175633362,
      "loss": 7.2705,
      "step": 1565
    },
    {
      "epoch": 0.44583629893238436,
      "grad_norm": 0.48628485202789307,
      "learning_rate": 0.00038862795331625396,
      "loss": 8.0352,
      "step": 1566
    },
    {
      "epoch": 0.4461209964412811,
      "grad_norm": 0.48861849308013916,
      "learning_rate": 0.00038855678906917163,
      "loss": 7.5625,
      "step": 1567
    },
    {
      "epoch": 0.44640569395017793,
      "grad_norm": 0.6439377665519714,
      "learning_rate": 0.0003884856248220894,
      "loss": 6.5352,
      "step": 1568
    },
    {
      "epoch": 0.44669039145907474,
      "grad_norm": 0.569162905216217,
      "learning_rate": 0.0003884144605750071,
      "loss": 7.5254,
      "step": 1569
    },
    {
      "epoch": 0.44697508896797156,
      "grad_norm": 0.4747311770915985,
      "learning_rate": 0.00038834329632792485,
      "loss": 7.7129,
      "step": 1570
    },
    {
      "epoch": 0.4472597864768683,
      "grad_norm": 0.600536584854126,
      "learning_rate": 0.0003882721320808426,
      "loss": 7.1709,
      "step": 1571
    },
    {
      "epoch": 0.44754448398576513,
      "grad_norm": 0.54041588306427,
      "learning_rate": 0.00038820096783376035,
      "loss": 7.3242,
      "step": 1572
    },
    {
      "epoch": 0.44782918149466194,
      "grad_norm": 0.6432653665542603,
      "learning_rate": 0.000388129803586678,
      "loss": 6.4092,
      "step": 1573
    },
    {
      "epoch": 0.4481138790035587,
      "grad_norm": 0.49148261547088623,
      "learning_rate": 0.0003880586393395958,
      "loss": 8.0059,
      "step": 1574
    },
    {
      "epoch": 0.4483985765124555,
      "grad_norm": 0.5798601508140564,
      "learning_rate": 0.0003879874750925135,
      "loss": 6.9951,
      "step": 1575
    },
    {
      "epoch": 0.44868327402135233,
      "grad_norm": 0.5091765522956848,
      "learning_rate": 0.0003879163108454313,
      "loss": 7.8105,
      "step": 1576
    },
    {
      "epoch": 0.4489679715302491,
      "grad_norm": 0.5045468211174011,
      "learning_rate": 0.000387845146598349,
      "loss": 7.3867,
      "step": 1577
    },
    {
      "epoch": 0.4492526690391459,
      "grad_norm": 0.49986931681632996,
      "learning_rate": 0.0003877739823512667,
      "loss": 7.3086,
      "step": 1578
    },
    {
      "epoch": 0.4495373665480427,
      "grad_norm": 0.5203457474708557,
      "learning_rate": 0.00038770281810418446,
      "loss": 7.582,
      "step": 1579
    },
    {
      "epoch": 0.4498220640569395,
      "grad_norm": 0.4249582290649414,
      "learning_rate": 0.0003876316538571022,
      "loss": 7.7676,
      "step": 1580
    },
    {
      "epoch": 0.4501067615658363,
      "grad_norm": 0.4739021360874176,
      "learning_rate": 0.00038756048961001996,
      "loss": 7.583,
      "step": 1581
    },
    {
      "epoch": 0.4503914590747331,
      "grad_norm": 0.6087887287139893,
      "learning_rate": 0.0003874893253629377,
      "loss": 6.8906,
      "step": 1582
    },
    {
      "epoch": 0.4506761565836299,
      "grad_norm": 0.550430417060852,
      "learning_rate": 0.0003874181611158554,
      "loss": 7.0439,
      "step": 1583
    },
    {
      "epoch": 0.4509608540925267,
      "grad_norm": 0.4809916615486145,
      "learning_rate": 0.00038734699686877313,
      "loss": 7.4092,
      "step": 1584
    },
    {
      "epoch": 0.4512455516014235,
      "grad_norm": 0.40640443563461304,
      "learning_rate": 0.00038727583262169085,
      "loss": 8.0088,
      "step": 1585
    },
    {
      "epoch": 0.4515302491103203,
      "grad_norm": 0.5101658701896667,
      "learning_rate": 0.00038720466837460863,
      "loss": 7.2812,
      "step": 1586
    },
    {
      "epoch": 0.45181494661921706,
      "grad_norm": 0.46021097898483276,
      "learning_rate": 0.00038713350412752635,
      "loss": 7.6826,
      "step": 1587
    },
    {
      "epoch": 0.4520996441281139,
      "grad_norm": 0.5049157738685608,
      "learning_rate": 0.0003870623398804441,
      "loss": 6.8799,
      "step": 1588
    },
    {
      "epoch": 0.4523843416370107,
      "grad_norm": 0.48057305812835693,
      "learning_rate": 0.0003869911756333618,
      "loss": 7.1172,
      "step": 1589
    },
    {
      "epoch": 0.45266903914590745,
      "grad_norm": 0.5754631161689758,
      "learning_rate": 0.0003869200113862795,
      "loss": 7.0146,
      "step": 1590
    },
    {
      "epoch": 0.45295373665480426,
      "grad_norm": 0.5763919353485107,
      "learning_rate": 0.00038684884713919724,
      "loss": 6.6592,
      "step": 1591
    },
    {
      "epoch": 0.4532384341637011,
      "grad_norm": 0.4379666745662689,
      "learning_rate": 0.000386777682892115,
      "loss": 7.7998,
      "step": 1592
    },
    {
      "epoch": 0.4535231316725979,
      "grad_norm": 0.4938500225543976,
      "learning_rate": 0.00038670651864503274,
      "loss": 7.5752,
      "step": 1593
    },
    {
      "epoch": 0.45380782918149465,
      "grad_norm": 0.5131168961524963,
      "learning_rate": 0.0003866353543979505,
      "loss": 7.5557,
      "step": 1594
    },
    {
      "epoch": 0.45409252669039146,
      "grad_norm": 0.5030447840690613,
      "learning_rate": 0.0003865641901508682,
      "loss": 7.2998,
      "step": 1595
    },
    {
      "epoch": 0.4543772241992883,
      "grad_norm": 0.4757480025291443,
      "learning_rate": 0.0003864930259037859,
      "loss": 7.2979,
      "step": 1596
    },
    {
      "epoch": 0.45466192170818504,
      "grad_norm": 0.495022714138031,
      "learning_rate": 0.0003864218616567037,
      "loss": 7.6387,
      "step": 1597
    },
    {
      "epoch": 0.45494661921708185,
      "grad_norm": 0.5130190253257751,
      "learning_rate": 0.0003863506974096214,
      "loss": 7.1055,
      "step": 1598
    },
    {
      "epoch": 0.45523131672597866,
      "grad_norm": 0.5624128580093384,
      "learning_rate": 0.0003862795331625392,
      "loss": 7.293,
      "step": 1599
    },
    {
      "epoch": 0.4555160142348754,
      "grad_norm": 0.5088523626327515,
      "learning_rate": 0.0003862083689154569,
      "loss": 7.1807,
      "step": 1600
    },
    {
      "epoch": 0.4555160142348754,
      "eval_bleu": 0.10999045970759502,
      "eval_loss": 7.0859375,
      "eval_runtime": 125.4975,
      "eval_samples_per_second": 2.263,
      "eval_steps_per_second": 0.143,
      "step": 1600
    },
    {
      "epoch": 0.45580071174377224,
      "grad_norm": 0.4664638340473175,
      "learning_rate": 0.0003861372046683746,
      "loss": 7.7344,
      "step": 1601
    },
    {
      "epoch": 0.45608540925266905,
      "grad_norm": 0.6145609617233276,
      "learning_rate": 0.00038606604042129236,
      "loss": 7.334,
      "step": 1602
    },
    {
      "epoch": 0.45637010676156586,
      "grad_norm": 0.5187442898750305,
      "learning_rate": 0.0003859948761742101,
      "loss": 7.3242,
      "step": 1603
    },
    {
      "epoch": 0.4566548042704626,
      "grad_norm": 0.5328884720802307,
      "learning_rate": 0.0003859237119271278,
      "loss": 7.293,
      "step": 1604
    },
    {
      "epoch": 0.45693950177935944,
      "grad_norm": 0.5432795286178589,
      "learning_rate": 0.0003858525476800456,
      "loss": 6.9834,
      "step": 1605
    },
    {
      "epoch": 0.45722419928825625,
      "grad_norm": 0.4709322154521942,
      "learning_rate": 0.00038578138343296325,
      "loss": 7.627,
      "step": 1606
    },
    {
      "epoch": 0.457508896797153,
      "grad_norm": 0.7863417267799377,
      "learning_rate": 0.000385710219185881,
      "loss": 6.5352,
      "step": 1607
    },
    {
      "epoch": 0.4577935943060498,
      "grad_norm": 0.4933783710002899,
      "learning_rate": 0.00038563905493879875,
      "loss": 7.1816,
      "step": 1608
    },
    {
      "epoch": 0.45807829181494664,
      "grad_norm": 0.4733075797557831,
      "learning_rate": 0.00038556789069171647,
      "loss": 7.4834,
      "step": 1609
    },
    {
      "epoch": 0.4583629893238434,
      "grad_norm": 0.5065616369247437,
      "learning_rate": 0.00038549672644463425,
      "loss": 7.6025,
      "step": 1610
    },
    {
      "epoch": 0.4586476868327402,
      "grad_norm": 0.5557234287261963,
      "learning_rate": 0.00038542556219755197,
      "loss": 7.5264,
      "step": 1611
    },
    {
      "epoch": 0.458932384341637,
      "grad_norm": 0.5921558737754822,
      "learning_rate": 0.0003853543979504697,
      "loss": 7.2539,
      "step": 1612
    },
    {
      "epoch": 0.4592170818505338,
      "grad_norm": 0.5364576578140259,
      "learning_rate": 0.0003852832337033874,
      "loss": 7.6895,
      "step": 1613
    },
    {
      "epoch": 0.4595017793594306,
      "grad_norm": 0.53724205493927,
      "learning_rate": 0.00038521206945630514,
      "loss": 7.1143,
      "step": 1614
    },
    {
      "epoch": 0.4597864768683274,
      "grad_norm": 0.44680672883987427,
      "learning_rate": 0.0003851409052092229,
      "loss": 7.8203,
      "step": 1615
    },
    {
      "epoch": 0.4600711743772242,
      "grad_norm": 0.4798159599304199,
      "learning_rate": 0.00038506974096214064,
      "loss": 7.5967,
      "step": 1616
    },
    {
      "epoch": 0.460355871886121,
      "grad_norm": 0.5024503469467163,
      "learning_rate": 0.00038499857671505836,
      "loss": 7.6113,
      "step": 1617
    },
    {
      "epoch": 0.4606405693950178,
      "grad_norm": 0.4705354869365692,
      "learning_rate": 0.0003849274124679761,
      "loss": 7.5576,
      "step": 1618
    },
    {
      "epoch": 0.4609252669039146,
      "grad_norm": 0.47796255350112915,
      "learning_rate": 0.0003848562482208938,
      "loss": 7.627,
      "step": 1619
    },
    {
      "epoch": 0.46120996441281137,
      "grad_norm": 0.5365163683891296,
      "learning_rate": 0.0003847850839738116,
      "loss": 7.332,
      "step": 1620
    },
    {
      "epoch": 0.4614946619217082,
      "grad_norm": 0.607476532459259,
      "learning_rate": 0.0003847139197267293,
      "loss": 6.9092,
      "step": 1621
    },
    {
      "epoch": 0.461779359430605,
      "grad_norm": 0.6723403334617615,
      "learning_rate": 0.00038464275547964703,
      "loss": 7.25,
      "step": 1622
    },
    {
      "epoch": 0.46206405693950175,
      "grad_norm": 0.5916773080825806,
      "learning_rate": 0.00038457159123256475,
      "loss": 7.1523,
      "step": 1623
    },
    {
      "epoch": 0.46234875444839857,
      "grad_norm": 0.6792402267456055,
      "learning_rate": 0.0003845004269854825,
      "loss": 6.9072,
      "step": 1624
    },
    {
      "epoch": 0.4626334519572954,
      "grad_norm": 0.5384190678596497,
      "learning_rate": 0.00038442926273840025,
      "loss": 6.8887,
      "step": 1625
    },
    {
      "epoch": 0.4629181494661922,
      "grad_norm": 0.49065250158309937,
      "learning_rate": 0.000384358098491318,
      "loss": 7.7617,
      "step": 1626
    },
    {
      "epoch": 0.46320284697508896,
      "grad_norm": 0.5000590085983276,
      "learning_rate": 0.0003842869342442357,
      "loss": 7.2842,
      "step": 1627
    },
    {
      "epoch": 0.46348754448398577,
      "grad_norm": 0.5557690858840942,
      "learning_rate": 0.0003842157699971535,
      "loss": 7.0381,
      "step": 1628
    },
    {
      "epoch": 0.4637722419928826,
      "grad_norm": 0.5086784362792969,
      "learning_rate": 0.00038414460575007114,
      "loss": 7.4062,
      "step": 1629
    },
    {
      "epoch": 0.46405693950177934,
      "grad_norm": 0.43616223335266113,
      "learning_rate": 0.0003840734415029889,
      "loss": 7.4316,
      "step": 1630
    },
    {
      "epoch": 0.46434163701067616,
      "grad_norm": 0.582670271396637,
      "learning_rate": 0.00038400227725590664,
      "loss": 6.8467,
      "step": 1631
    },
    {
      "epoch": 0.46462633451957297,
      "grad_norm": 0.42955803871154785,
      "learning_rate": 0.00038393111300882437,
      "loss": 7.8594,
      "step": 1632
    },
    {
      "epoch": 0.46491103202846973,
      "grad_norm": 0.45816558599472046,
      "learning_rate": 0.00038385994876174214,
      "loss": 7.2773,
      "step": 1633
    },
    {
      "epoch": 0.46519572953736654,
      "grad_norm": 0.571272075176239,
      "learning_rate": 0.0003837887845146598,
      "loss": 7.5977,
      "step": 1634
    },
    {
      "epoch": 0.46548042704626336,
      "grad_norm": 0.5401399731636047,
      "learning_rate": 0.00038371762026757753,
      "loss": 7.1367,
      "step": 1635
    },
    {
      "epoch": 0.4657651245551601,
      "grad_norm": 0.5665181875228882,
      "learning_rate": 0.0003836464560204953,
      "loss": 6.7285,
      "step": 1636
    },
    {
      "epoch": 0.46604982206405693,
      "grad_norm": 0.4491924047470093,
      "learning_rate": 0.00038357529177341303,
      "loss": 7.5703,
      "step": 1637
    },
    {
      "epoch": 0.46633451957295374,
      "grad_norm": 0.424701452255249,
      "learning_rate": 0.0003835041275263308,
      "loss": 7.5449,
      "step": 1638
    },
    {
      "epoch": 0.46661921708185056,
      "grad_norm": 0.6199308633804321,
      "learning_rate": 0.00038343296327924853,
      "loss": 6.6602,
      "step": 1639
    },
    {
      "epoch": 0.4669039145907473,
      "grad_norm": 0.4524313807487488,
      "learning_rate": 0.0003833617990321662,
      "loss": 7.5479,
      "step": 1640
    },
    {
      "epoch": 0.46718861209964413,
      "grad_norm": 0.5219374895095825,
      "learning_rate": 0.000383290634785084,
      "loss": 7.3516,
      "step": 1641
    },
    {
      "epoch": 0.46747330960854094,
      "grad_norm": 0.5036229491233826,
      "learning_rate": 0.0003832194705380017,
      "loss": 7.2031,
      "step": 1642
    },
    {
      "epoch": 0.4677580071174377,
      "grad_norm": 0.5226691961288452,
      "learning_rate": 0.0003831483062909195,
      "loss": 7.0127,
      "step": 1643
    },
    {
      "epoch": 0.4680427046263345,
      "grad_norm": 0.47675246000289917,
      "learning_rate": 0.0003830771420438372,
      "loss": 7.7695,
      "step": 1644
    },
    {
      "epoch": 0.46832740213523133,
      "grad_norm": 0.5894948840141296,
      "learning_rate": 0.0003830059777967549,
      "loss": 6.9688,
      "step": 1645
    },
    {
      "epoch": 0.4686120996441281,
      "grad_norm": 0.4946599304676056,
      "learning_rate": 0.00038293481354967265,
      "loss": 6.9648,
      "step": 1646
    },
    {
      "epoch": 0.4688967971530249,
      "grad_norm": 0.49283403158187866,
      "learning_rate": 0.00038286364930259037,
      "loss": 7.6992,
      "step": 1647
    },
    {
      "epoch": 0.4691814946619217,
      "grad_norm": 0.494332492351532,
      "learning_rate": 0.00038279248505550815,
      "loss": 7.5107,
      "step": 1648
    },
    {
      "epoch": 0.46946619217081853,
      "grad_norm": 0.39411231875419617,
      "learning_rate": 0.00038272132080842587,
      "loss": 7.6553,
      "step": 1649
    },
    {
      "epoch": 0.4697508896797153,
      "grad_norm": 0.5392952561378479,
      "learning_rate": 0.0003826501565613436,
      "loss": 7.2207,
      "step": 1650
    },
    {
      "epoch": 0.4700355871886121,
      "grad_norm": 0.5077725052833557,
      "learning_rate": 0.0003825789923142613,
      "loss": 7.1738,
      "step": 1651
    },
    {
      "epoch": 0.4703202846975089,
      "grad_norm": 0.4558878242969513,
      "learning_rate": 0.00038250782806717904,
      "loss": 7.7305,
      "step": 1652
    },
    {
      "epoch": 0.4706049822064057,
      "grad_norm": 0.5433383584022522,
      "learning_rate": 0.00038243666382009676,
      "loss": 6.9561,
      "step": 1653
    },
    {
      "epoch": 0.4708896797153025,
      "grad_norm": 0.5727225542068481,
      "learning_rate": 0.00038236549957301454,
      "loss": 7.082,
      "step": 1654
    },
    {
      "epoch": 0.4711743772241993,
      "grad_norm": 0.5149667859077454,
      "learning_rate": 0.00038229433532593226,
      "loss": 7.3779,
      "step": 1655
    },
    {
      "epoch": 0.47145907473309606,
      "grad_norm": 0.5559431314468384,
      "learning_rate": 0.00038222317107885004,
      "loss": 6.8467,
      "step": 1656
    },
    {
      "epoch": 0.4717437722419929,
      "grad_norm": 0.5958291888237,
      "learning_rate": 0.0003821520068317677,
      "loss": 7.0537,
      "step": 1657
    },
    {
      "epoch": 0.4720284697508897,
      "grad_norm": 0.3918918967247009,
      "learning_rate": 0.00038208084258468543,
      "loss": 7.9854,
      "step": 1658
    },
    {
      "epoch": 0.4723131672597865,
      "grad_norm": 0.4432237148284912,
      "learning_rate": 0.0003820096783376032,
      "loss": 7.4531,
      "step": 1659
    },
    {
      "epoch": 0.47259786476868326,
      "grad_norm": 0.5828916430473328,
      "learning_rate": 0.00038193851409052093,
      "loss": 7.4658,
      "step": 1660
    },
    {
      "epoch": 0.4728825622775801,
      "grad_norm": 0.5251741409301758,
      "learning_rate": 0.0003818673498434387,
      "loss": 7.6777,
      "step": 1661
    },
    {
      "epoch": 0.4731672597864769,
      "grad_norm": 0.5652466416358948,
      "learning_rate": 0.0003817961855963564,
      "loss": 7.1152,
      "step": 1662
    },
    {
      "epoch": 0.47345195729537365,
      "grad_norm": 0.43186524510383606,
      "learning_rate": 0.0003817250213492741,
      "loss": 7.9424,
      "step": 1663
    },
    {
      "epoch": 0.47373665480427046,
      "grad_norm": 0.4190675914287567,
      "learning_rate": 0.0003816538571021919,
      "loss": 7.9873,
      "step": 1664
    },
    {
      "epoch": 0.4740213523131673,
      "grad_norm": 0.35119152069091797,
      "learning_rate": 0.0003815826928551096,
      "loss": 8.0195,
      "step": 1665
    },
    {
      "epoch": 0.47430604982206404,
      "grad_norm": 0.5248192548751831,
      "learning_rate": 0.00038151152860802737,
      "loss": 7.5029,
      "step": 1666
    },
    {
      "epoch": 0.47459074733096085,
      "grad_norm": 0.5171993374824524,
      "learning_rate": 0.0003814403643609451,
      "loss": 7.8926,
      "step": 1667
    },
    {
      "epoch": 0.47487544483985766,
      "grad_norm": 0.5714148879051208,
      "learning_rate": 0.00038136920011386276,
      "loss": 6.8262,
      "step": 1668
    },
    {
      "epoch": 0.4751601423487544,
      "grad_norm": 0.5456072092056274,
      "learning_rate": 0.00038129803586678054,
      "loss": 6.9072,
      "step": 1669
    },
    {
      "epoch": 0.47544483985765124,
      "grad_norm": 0.6678564548492432,
      "learning_rate": 0.00038122687161969826,
      "loss": 6.8818,
      "step": 1670
    },
    {
      "epoch": 0.47572953736654805,
      "grad_norm": 0.4060457646846771,
      "learning_rate": 0.000381155707372616,
      "loss": 7.6504,
      "step": 1671
    },
    {
      "epoch": 0.47601423487544486,
      "grad_norm": 0.5769488215446472,
      "learning_rate": 0.00038108454312553376,
      "loss": 7.1602,
      "step": 1672
    },
    {
      "epoch": 0.4762989323843416,
      "grad_norm": 0.5093549489974976,
      "learning_rate": 0.0003810133788784515,
      "loss": 7.5371,
      "step": 1673
    },
    {
      "epoch": 0.47658362989323844,
      "grad_norm": 0.5345686674118042,
      "learning_rate": 0.0003809422146313692,
      "loss": 7.4883,
      "step": 1674
    },
    {
      "epoch": 0.47686832740213525,
      "grad_norm": 0.6932775378227234,
      "learning_rate": 0.00038087105038428693,
      "loss": 6.6279,
      "step": 1675
    },
    {
      "epoch": 0.477153024911032,
      "grad_norm": 0.5208515524864197,
      "learning_rate": 0.00038079988613720465,
      "loss": 7.4189,
      "step": 1676
    },
    {
      "epoch": 0.4774377224199288,
      "grad_norm": 0.6215210556983948,
      "learning_rate": 0.00038072872189012243,
      "loss": 6.8779,
      "step": 1677
    },
    {
      "epoch": 0.47772241992882564,
      "grad_norm": 0.48564231395721436,
      "learning_rate": 0.00038065755764304015,
      "loss": 7.7676,
      "step": 1678
    },
    {
      "epoch": 0.4780071174377224,
      "grad_norm": 0.5211162567138672,
      "learning_rate": 0.0003805863933959579,
      "loss": 7.375,
      "step": 1679
    },
    {
      "epoch": 0.4782918149466192,
      "grad_norm": 0.634146511554718,
      "learning_rate": 0.0003805152291488756,
      "loss": 6.2031,
      "step": 1680
    },
    {
      "epoch": 0.478576512455516,
      "grad_norm": 0.57793790102005,
      "learning_rate": 0.0003804440649017933,
      "loss": 7.4961,
      "step": 1681
    },
    {
      "epoch": 0.47886120996441284,
      "grad_norm": 0.613333523273468,
      "learning_rate": 0.0003803729006547111,
      "loss": 6.4297,
      "step": 1682
    },
    {
      "epoch": 0.4791459074733096,
      "grad_norm": 0.5153689384460449,
      "learning_rate": 0.0003803017364076288,
      "loss": 7.415,
      "step": 1683
    },
    {
      "epoch": 0.4794306049822064,
      "grad_norm": 0.4538441300392151,
      "learning_rate": 0.0003802305721605466,
      "loss": 7.1904,
      "step": 1684
    },
    {
      "epoch": 0.4797153024911032,
      "grad_norm": 0.5165327191352844,
      "learning_rate": 0.00038015940791346427,
      "loss": 7.7002,
      "step": 1685
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.45390772819519043,
      "learning_rate": 0.000380088243666382,
      "loss": 7.4102,
      "step": 1686
    },
    {
      "epoch": 0.4802846975088968,
      "grad_norm": 0.502204179763794,
      "learning_rate": 0.00038001707941929977,
      "loss": 7.25,
      "step": 1687
    },
    {
      "epoch": 0.4805693950177936,
      "grad_norm": 0.5534747838973999,
      "learning_rate": 0.0003799459151722175,
      "loss": 7.2061,
      "step": 1688
    },
    {
      "epoch": 0.48085409252669037,
      "grad_norm": 0.5134608149528503,
      "learning_rate": 0.0003798747509251352,
      "loss": 7.4482,
      "step": 1689
    },
    {
      "epoch": 0.4811387900355872,
      "grad_norm": 0.5647716522216797,
      "learning_rate": 0.000379803586678053,
      "loss": 7.4502,
      "step": 1690
    },
    {
      "epoch": 0.481423487544484,
      "grad_norm": 0.5849151611328125,
      "learning_rate": 0.00037973242243097066,
      "loss": 7.2373,
      "step": 1691
    },
    {
      "epoch": 0.48170818505338076,
      "grad_norm": 0.5531435608863831,
      "learning_rate": 0.00037966125818388844,
      "loss": 6.8877,
      "step": 1692
    },
    {
      "epoch": 0.48199288256227757,
      "grad_norm": 0.4758262634277344,
      "learning_rate": 0.00037959009393680616,
      "loss": 7.3408,
      "step": 1693
    },
    {
      "epoch": 0.4822775800711744,
      "grad_norm": 0.5585567951202393,
      "learning_rate": 0.0003795189296897239,
      "loss": 7.0674,
      "step": 1694
    },
    {
      "epoch": 0.4825622775800712,
      "grad_norm": 0.463369756937027,
      "learning_rate": 0.00037944776544264166,
      "loss": 7.5537,
      "step": 1695
    },
    {
      "epoch": 0.48284697508896796,
      "grad_norm": 0.5086468458175659,
      "learning_rate": 0.0003793766011955593,
      "loss": 7.6162,
      "step": 1696
    },
    {
      "epoch": 0.48313167259786477,
      "grad_norm": 0.5536462068557739,
      "learning_rate": 0.0003793054369484771,
      "loss": 7.332,
      "step": 1697
    },
    {
      "epoch": 0.4834163701067616,
      "grad_norm": 0.4514314532279968,
      "learning_rate": 0.0003792342727013948,
      "loss": 7.6406,
      "step": 1698
    },
    {
      "epoch": 0.48370106761565834,
      "grad_norm": 0.4876021444797516,
      "learning_rate": 0.00037916310845431255,
      "loss": 7.4336,
      "step": 1699
    },
    {
      "epoch": 0.48398576512455516,
      "grad_norm": 0.5244501829147339,
      "learning_rate": 0.0003790919442072303,
      "loss": 7.7041,
      "step": 1700
    },
    {
      "epoch": 0.48427046263345197,
      "grad_norm": 0.6171659827232361,
      "learning_rate": 0.00037902077996014805,
      "loss": 7.0674,
      "step": 1701
    },
    {
      "epoch": 0.48455516014234873,
      "grad_norm": 0.5354804992675781,
      "learning_rate": 0.0003789496157130657,
      "loss": 7.0459,
      "step": 1702
    },
    {
      "epoch": 0.48483985765124554,
      "grad_norm": 0.49222564697265625,
      "learning_rate": 0.0003788784514659835,
      "loss": 7.5684,
      "step": 1703
    },
    {
      "epoch": 0.48512455516014236,
      "grad_norm": 0.5722427368164062,
      "learning_rate": 0.0003788072872189012,
      "loss": 6.9424,
      "step": 1704
    },
    {
      "epoch": 0.48540925266903917,
      "grad_norm": 0.3971741497516632,
      "learning_rate": 0.000378736122971819,
      "loss": 7.8623,
      "step": 1705
    },
    {
      "epoch": 0.48569395017793593,
      "grad_norm": 0.5367334485054016,
      "learning_rate": 0.0003786649587247367,
      "loss": 7.6836,
      "step": 1706
    },
    {
      "epoch": 0.48597864768683274,
      "grad_norm": 0.458075612783432,
      "learning_rate": 0.0003785937944776544,
      "loss": 7.7031,
      "step": 1707
    },
    {
      "epoch": 0.48626334519572956,
      "grad_norm": 0.611355721950531,
      "learning_rate": 0.00037852263023057216,
      "loss": 7.7764,
      "step": 1708
    },
    {
      "epoch": 0.4865480427046263,
      "grad_norm": 0.5967645645141602,
      "learning_rate": 0.0003784514659834899,
      "loss": 6.9072,
      "step": 1709
    },
    {
      "epoch": 0.48683274021352313,
      "grad_norm": 0.5037733316421509,
      "learning_rate": 0.00037838030173640766,
      "loss": 7.5635,
      "step": 1710
    },
    {
      "epoch": 0.48711743772241994,
      "grad_norm": 0.5649775862693787,
      "learning_rate": 0.0003783091374893254,
      "loss": 7.2119,
      "step": 1711
    },
    {
      "epoch": 0.4874021352313167,
      "grad_norm": 0.6592603325843811,
      "learning_rate": 0.0003782379732422431,
      "loss": 7.0742,
      "step": 1712
    },
    {
      "epoch": 0.4876868327402135,
      "grad_norm": 0.41307011246681213,
      "learning_rate": 0.00037816680899516083,
      "loss": 7.9648,
      "step": 1713
    },
    {
      "epoch": 0.48797153024911033,
      "grad_norm": 0.5472896099090576,
      "learning_rate": 0.00037809564474807855,
      "loss": 7.0195,
      "step": 1714
    },
    {
      "epoch": 0.48825622775800714,
      "grad_norm": 0.5462066531181335,
      "learning_rate": 0.00037802448050099633,
      "loss": 7.6846,
      "step": 1715
    },
    {
      "epoch": 0.4885409252669039,
      "grad_norm": 0.5576370358467102,
      "learning_rate": 0.00037795331625391405,
      "loss": 7.0869,
      "step": 1716
    },
    {
      "epoch": 0.4888256227758007,
      "grad_norm": 0.500810980796814,
      "learning_rate": 0.0003778821520068318,
      "loss": 7.4414,
      "step": 1717
    },
    {
      "epoch": 0.48911032028469753,
      "grad_norm": 0.48971080780029297,
      "learning_rate": 0.00037781098775974955,
      "loss": 7.5107,
      "step": 1718
    },
    {
      "epoch": 0.4893950177935943,
      "grad_norm": 0.566116452217102,
      "learning_rate": 0.0003777398235126672,
      "loss": 7.4121,
      "step": 1719
    },
    {
      "epoch": 0.4896797153024911,
      "grad_norm": 0.42671483755111694,
      "learning_rate": 0.00037766865926558494,
      "loss": 7.6367,
      "step": 1720
    },
    {
      "epoch": 0.4899644128113879,
      "grad_norm": 0.590766966342926,
      "learning_rate": 0.0003775974950185027,
      "loss": 7.1406,
      "step": 1721
    },
    {
      "epoch": 0.4902491103202847,
      "grad_norm": 0.5169826149940491,
      "learning_rate": 0.00037752633077142044,
      "loss": 7.3428,
      "step": 1722
    },
    {
      "epoch": 0.4905338078291815,
      "grad_norm": 0.6001588702201843,
      "learning_rate": 0.0003774551665243382,
      "loss": 7.1094,
      "step": 1723
    },
    {
      "epoch": 0.4908185053380783,
      "grad_norm": 0.5002423524856567,
      "learning_rate": 0.0003773840022772559,
      "loss": 7.2109,
      "step": 1724
    },
    {
      "epoch": 0.49110320284697506,
      "grad_norm": 0.5491111278533936,
      "learning_rate": 0.0003773128380301736,
      "loss": 6.79,
      "step": 1725
    },
    {
      "epoch": 0.4913879003558719,
      "grad_norm": 0.5625193119049072,
      "learning_rate": 0.0003772416737830914,
      "loss": 7.3906,
      "step": 1726
    },
    {
      "epoch": 0.4916725978647687,
      "grad_norm": 0.522409200668335,
      "learning_rate": 0.0003771705095360091,
      "loss": 7.3896,
      "step": 1727
    },
    {
      "epoch": 0.4919572953736655,
      "grad_norm": 0.5947089791297913,
      "learning_rate": 0.0003770993452889269,
      "loss": 7.7861,
      "step": 1728
    },
    {
      "epoch": 0.49224199288256226,
      "grad_norm": 0.441991925239563,
      "learning_rate": 0.0003770281810418446,
      "loss": 7.9551,
      "step": 1729
    },
    {
      "epoch": 0.4925266903914591,
      "grad_norm": 0.49497219920158386,
      "learning_rate": 0.0003769570167947623,
      "loss": 7.6113,
      "step": 1730
    },
    {
      "epoch": 0.4928113879003559,
      "grad_norm": 0.4308784306049347,
      "learning_rate": 0.00037688585254768006,
      "loss": 7.9043,
      "step": 1731
    },
    {
      "epoch": 0.49309608540925265,
      "grad_norm": 0.5549896955490112,
      "learning_rate": 0.0003768146883005978,
      "loss": 7.252,
      "step": 1732
    },
    {
      "epoch": 0.49338078291814946,
      "grad_norm": 0.6144887804985046,
      "learning_rate": 0.00037674352405351556,
      "loss": 7.1465,
      "step": 1733
    },
    {
      "epoch": 0.4936654804270463,
      "grad_norm": 0.6344668865203857,
      "learning_rate": 0.0003766723598064333,
      "loss": 6.4443,
      "step": 1734
    },
    {
      "epoch": 0.49395017793594304,
      "grad_norm": 0.5163561105728149,
      "learning_rate": 0.000376601195559351,
      "loss": 7.4385,
      "step": 1735
    },
    {
      "epoch": 0.49423487544483985,
      "grad_norm": 0.511461615562439,
      "learning_rate": 0.0003765300313122687,
      "loss": 7.7744,
      "step": 1736
    },
    {
      "epoch": 0.49451957295373666,
      "grad_norm": 0.5111595392227173,
      "learning_rate": 0.00037645886706518645,
      "loss": 7.3789,
      "step": 1737
    },
    {
      "epoch": 0.4948042704626335,
      "grad_norm": 0.49170342087745667,
      "learning_rate": 0.00037638770281810417,
      "loss": 7.4453,
      "step": 1738
    },
    {
      "epoch": 0.49508896797153024,
      "grad_norm": 0.508344292640686,
      "learning_rate": 0.00037631653857102195,
      "loss": 7.5684,
      "step": 1739
    },
    {
      "epoch": 0.49537366548042705,
      "grad_norm": 0.511880099773407,
      "learning_rate": 0.00037624537432393967,
      "loss": 7.5498,
      "step": 1740
    },
    {
      "epoch": 0.49565836298932386,
      "grad_norm": 0.6036074757575989,
      "learning_rate": 0.0003761742100768574,
      "loss": 6.7861,
      "step": 1741
    },
    {
      "epoch": 0.4959430604982206,
      "grad_norm": 0.5811192989349365,
      "learning_rate": 0.0003761030458297751,
      "loss": 6.9082,
      "step": 1742
    },
    {
      "epoch": 0.49622775800711744,
      "grad_norm": 0.5931008458137512,
      "learning_rate": 0.00037603188158269284,
      "loss": 6.9551,
      "step": 1743
    },
    {
      "epoch": 0.49651245551601425,
      "grad_norm": 0.5401119589805603,
      "learning_rate": 0.0003759607173356106,
      "loss": 7.4736,
      "step": 1744
    },
    {
      "epoch": 0.496797153024911,
      "grad_norm": 0.4910326898097992,
      "learning_rate": 0.00037588955308852834,
      "loss": 7.3223,
      "step": 1745
    },
    {
      "epoch": 0.4970818505338078,
      "grad_norm": 0.5661277770996094,
      "learning_rate": 0.0003758183888414461,
      "loss": 7.1172,
      "step": 1746
    },
    {
      "epoch": 0.49736654804270464,
      "grad_norm": 0.5483824014663696,
      "learning_rate": 0.0003757472245943638,
      "loss": 7.3525,
      "step": 1747
    },
    {
      "epoch": 0.49765124555160145,
      "grad_norm": 0.5940328240394592,
      "learning_rate": 0.0003756760603472815,
      "loss": 7.2393,
      "step": 1748
    },
    {
      "epoch": 0.4979359430604982,
      "grad_norm": 0.6712566614151001,
      "learning_rate": 0.0003756048961001993,
      "loss": 6.5723,
      "step": 1749
    },
    {
      "epoch": 0.498220640569395,
      "grad_norm": 0.5339024066925049,
      "learning_rate": 0.000375533731853117,
      "loss": 7.2012,
      "step": 1750
    },
    {
      "epoch": 0.49850533807829184,
      "grad_norm": 0.492465615272522,
      "learning_rate": 0.00037546256760603473,
      "loss": 7.5957,
      "step": 1751
    },
    {
      "epoch": 0.4987900355871886,
      "grad_norm": 0.5669850707054138,
      "learning_rate": 0.00037539140335895245,
      "loss": 6.9922,
      "step": 1752
    },
    {
      "epoch": 0.4990747330960854,
      "grad_norm": 0.5506566166877747,
      "learning_rate": 0.0003753202391118702,
      "loss": 7.2881,
      "step": 1753
    },
    {
      "epoch": 0.4993594306049822,
      "grad_norm": 0.45686304569244385,
      "learning_rate": 0.00037524907486478795,
      "loss": 7.4141,
      "step": 1754
    },
    {
      "epoch": 0.499644128113879,
      "grad_norm": 0.5495540499687195,
      "learning_rate": 0.0003751779106177057,
      "loss": 7.1133,
      "step": 1755
    },
    {
      "epoch": 0.4999288256227758,
      "grad_norm": 0.5432637333869934,
      "learning_rate": 0.0003751067463706234,
      "loss": 7.041,
      "step": 1756
    },
    {
      "epoch": 0.5002135231316726,
      "grad_norm": 0.4298688769340515,
      "learning_rate": 0.0003750355821235412,
      "loss": 7.498,
      "step": 1757
    },
    {
      "epoch": 0.5004982206405694,
      "grad_norm": 0.5609787106513977,
      "learning_rate": 0.00037496441787645884,
      "loss": 7.1523,
      "step": 1758
    },
    {
      "epoch": 0.5007829181494662,
      "grad_norm": 0.6223969459533691,
      "learning_rate": 0.0003748932536293766,
      "loss": 6.4463,
      "step": 1759
    },
    {
      "epoch": 0.501067615658363,
      "grad_norm": 0.6767446398735046,
      "learning_rate": 0.00037482208938229434,
      "loss": 6.4258,
      "step": 1760
    },
    {
      "epoch": 0.5013523131672598,
      "grad_norm": 0.5012683868408203,
      "learning_rate": 0.00037475092513521206,
      "loss": 7.6631,
      "step": 1761
    },
    {
      "epoch": 0.5016370106761566,
      "grad_norm": 0.545363187789917,
      "learning_rate": 0.00037467976088812984,
      "loss": 7.1758,
      "step": 1762
    },
    {
      "epoch": 0.5019217081850533,
      "grad_norm": 0.5195574760437012,
      "learning_rate": 0.00037460859664104756,
      "loss": 7.4082,
      "step": 1763
    },
    {
      "epoch": 0.5022064056939501,
      "grad_norm": 0.6033876538276672,
      "learning_rate": 0.0003745374323939653,
      "loss": 6.9688,
      "step": 1764
    },
    {
      "epoch": 0.502491103202847,
      "grad_norm": 0.5066131353378296,
      "learning_rate": 0.000374466268146883,
      "loss": 7.2031,
      "step": 1765
    },
    {
      "epoch": 0.5027758007117438,
      "grad_norm": 0.5280950665473938,
      "learning_rate": 0.00037439510389980073,
      "loss": 7.5674,
      "step": 1766
    },
    {
      "epoch": 0.5030604982206406,
      "grad_norm": 0.4802970886230469,
      "learning_rate": 0.0003743239396527185,
      "loss": 7.6758,
      "step": 1767
    },
    {
      "epoch": 0.5033451957295374,
      "grad_norm": 0.4784156084060669,
      "learning_rate": 0.00037425277540563623,
      "loss": 7.4297,
      "step": 1768
    },
    {
      "epoch": 0.5036298932384342,
      "grad_norm": 0.6867071986198425,
      "learning_rate": 0.0003741816111585539,
      "loss": 6.541,
      "step": 1769
    },
    {
      "epoch": 0.5039145907473309,
      "grad_norm": 0.5525267720222473,
      "learning_rate": 0.0003741104469114717,
      "loss": 7.4209,
      "step": 1770
    },
    {
      "epoch": 0.5041992882562277,
      "grad_norm": 0.44265449047088623,
      "learning_rate": 0.0003740392826643894,
      "loss": 7.5107,
      "step": 1771
    },
    {
      "epoch": 0.5044839857651245,
      "grad_norm": 0.4428139925003052,
      "learning_rate": 0.0003739681184173072,
      "loss": 7.5918,
      "step": 1772
    },
    {
      "epoch": 0.5047686832740214,
      "grad_norm": 0.6008185744285583,
      "learning_rate": 0.0003738969541702249,
      "loss": 7.0234,
      "step": 1773
    },
    {
      "epoch": 0.5050533807829182,
      "grad_norm": 0.5214946866035461,
      "learning_rate": 0.0003738257899231426,
      "loss": 7.1064,
      "step": 1774
    },
    {
      "epoch": 0.505338078291815,
      "grad_norm": 0.49460625648498535,
      "learning_rate": 0.00037375462567606035,
      "loss": 7.6562,
      "step": 1775
    },
    {
      "epoch": 0.5056227758007118,
      "grad_norm": 0.5446761250495911,
      "learning_rate": 0.00037368346142897807,
      "loss": 7.3613,
      "step": 1776
    },
    {
      "epoch": 0.5059074733096085,
      "grad_norm": 0.5687732100486755,
      "learning_rate": 0.00037361229718189585,
      "loss": 6.4512,
      "step": 1777
    },
    {
      "epoch": 0.5061921708185053,
      "grad_norm": 0.5303898453712463,
      "learning_rate": 0.00037354113293481357,
      "loss": 7.7227,
      "step": 1778
    },
    {
      "epoch": 0.5064768683274021,
      "grad_norm": 0.4860377013683319,
      "learning_rate": 0.0003734699686877313,
      "loss": 7.4395,
      "step": 1779
    },
    {
      "epoch": 0.5067615658362989,
      "grad_norm": 0.4877166748046875,
      "learning_rate": 0.000373398804440649,
      "loss": 7.7168,
      "step": 1780
    },
    {
      "epoch": 0.5070462633451958,
      "grad_norm": 0.6578726768493652,
      "learning_rate": 0.00037332764019356674,
      "loss": 7.2344,
      "step": 1781
    },
    {
      "epoch": 0.5073309608540926,
      "grad_norm": 0.436906635761261,
      "learning_rate": 0.00037325647594648446,
      "loss": 7.8311,
      "step": 1782
    },
    {
      "epoch": 0.5076156583629893,
      "grad_norm": 0.5282929539680481,
      "learning_rate": 0.00037318531169940224,
      "loss": 7.1514,
      "step": 1783
    },
    {
      "epoch": 0.5079003558718861,
      "grad_norm": 0.5723035335540771,
      "learning_rate": 0.00037311414745231996,
      "loss": 7.4482,
      "step": 1784
    },
    {
      "epoch": 0.5081850533807829,
      "grad_norm": 0.5254671573638916,
      "learning_rate": 0.00037304298320523774,
      "loss": 7.542,
      "step": 1785
    },
    {
      "epoch": 0.5084697508896797,
      "grad_norm": 0.506375789642334,
      "learning_rate": 0.0003729718189581554,
      "loss": 7.1572,
      "step": 1786
    },
    {
      "epoch": 0.5087544483985765,
      "grad_norm": 0.5270366668701172,
      "learning_rate": 0.00037290065471107313,
      "loss": 7.7188,
      "step": 1787
    },
    {
      "epoch": 0.5090391459074733,
      "grad_norm": 0.43990278244018555,
      "learning_rate": 0.0003728294904639909,
      "loss": 7.5264,
      "step": 1788
    },
    {
      "epoch": 0.5093238434163702,
      "grad_norm": 0.5385646224021912,
      "learning_rate": 0.0003727583262169086,
      "loss": 7.2461,
      "step": 1789
    },
    {
      "epoch": 0.5096085409252669,
      "grad_norm": 0.3960351347923279,
      "learning_rate": 0.0003726871619698264,
      "loss": 7.8682,
      "step": 1790
    },
    {
      "epoch": 0.5098932384341637,
      "grad_norm": 0.5219964385032654,
      "learning_rate": 0.0003726159977227441,
      "loss": 7.375,
      "step": 1791
    },
    {
      "epoch": 0.5101779359430605,
      "grad_norm": 0.5305919647216797,
      "learning_rate": 0.0003725448334756618,
      "loss": 7.4053,
      "step": 1792
    },
    {
      "epoch": 0.5104626334519573,
      "grad_norm": 0.5393206477165222,
      "learning_rate": 0.00037247366922857957,
      "loss": 7.4043,
      "step": 1793
    },
    {
      "epoch": 0.5107473309608541,
      "grad_norm": 0.5216227173805237,
      "learning_rate": 0.0003724025049814973,
      "loss": 7.3125,
      "step": 1794
    },
    {
      "epoch": 0.5110320284697509,
      "grad_norm": 0.4426933526992798,
      "learning_rate": 0.00037233134073441507,
      "loss": 7.4648,
      "step": 1795
    },
    {
      "epoch": 0.5113167259786476,
      "grad_norm": 0.5463060736656189,
      "learning_rate": 0.0003722601764873328,
      "loss": 7.6865,
      "step": 1796
    },
    {
      "epoch": 0.5116014234875444,
      "grad_norm": 0.41957664489746094,
      "learning_rate": 0.00037218901224025046,
      "loss": 7.5635,
      "step": 1797
    },
    {
      "epoch": 0.5118861209964413,
      "grad_norm": 0.5480870008468628,
      "learning_rate": 0.00037211784799316824,
      "loss": 6.6904,
      "step": 1798
    },
    {
      "epoch": 0.5121708185053381,
      "grad_norm": 0.6035405397415161,
      "learning_rate": 0.00037204668374608596,
      "loss": 5.7183,
      "step": 1799
    },
    {
      "epoch": 0.5124555160142349,
      "grad_norm": 0.5898758769035339,
      "learning_rate": 0.0003719755194990037,
      "loss": 6.9951,
      "step": 1800
    },
    {
      "epoch": 0.5124555160142349,
      "eval_bleu": 0.10834418317524194,
      "eval_loss": 7.09765625,
      "eval_runtime": 150.1062,
      "eval_samples_per_second": 1.892,
      "eval_steps_per_second": 0.12,
      "step": 1800
    },
    {
      "epoch": 0.5127402135231317,
      "grad_norm": 0.4677562415599823,
      "learning_rate": 0.00037190435525192146,
      "loss": 7.7871,
      "step": 1801
    },
    {
      "epoch": 0.5130249110320285,
      "grad_norm": 0.4639126658439636,
      "learning_rate": 0.0003718331910048392,
      "loss": 7.6602,
      "step": 1802
    },
    {
      "epoch": 0.5133096085409252,
      "grad_norm": 0.5188493728637695,
      "learning_rate": 0.0003717620267577569,
      "loss": 7.7012,
      "step": 1803
    },
    {
      "epoch": 0.513594306049822,
      "grad_norm": 0.5586040616035461,
      "learning_rate": 0.00037169086251067463,
      "loss": 7.1836,
      "step": 1804
    },
    {
      "epoch": 0.5138790035587188,
      "grad_norm": 0.5589001774787903,
      "learning_rate": 0.00037161969826359235,
      "loss": 7.2881,
      "step": 1805
    },
    {
      "epoch": 0.5141637010676157,
      "grad_norm": 0.4635283052921295,
      "learning_rate": 0.00037154853401651013,
      "loss": 7.6494,
      "step": 1806
    },
    {
      "epoch": 0.5144483985765125,
      "grad_norm": 0.4774576723575592,
      "learning_rate": 0.00037147736976942785,
      "loss": 7.5195,
      "step": 1807
    },
    {
      "epoch": 0.5147330960854093,
      "grad_norm": 0.5103232264518738,
      "learning_rate": 0.00037140620552234563,
      "loss": 7.6201,
      "step": 1808
    },
    {
      "epoch": 0.515017793594306,
      "grad_norm": 0.6217721700668335,
      "learning_rate": 0.0003713350412752633,
      "loss": 7.0146,
      "step": 1809
    },
    {
      "epoch": 0.5153024911032028,
      "grad_norm": 0.4739395081996918,
      "learning_rate": 0.000371263877028181,
      "loss": 7.7031,
      "step": 1810
    },
    {
      "epoch": 0.5155871886120996,
      "grad_norm": 0.5560252666473389,
      "learning_rate": 0.0003711927127810988,
      "loss": 7.1279,
      "step": 1811
    },
    {
      "epoch": 0.5158718861209964,
      "grad_norm": 0.5433075428009033,
      "learning_rate": 0.0003711215485340165,
      "loss": 7.6104,
      "step": 1812
    },
    {
      "epoch": 0.5161565836298933,
      "grad_norm": 0.4654752016067505,
      "learning_rate": 0.0003710503842869343,
      "loss": 7.5918,
      "step": 1813
    },
    {
      "epoch": 0.5164412811387901,
      "grad_norm": 0.5429745316505432,
      "learning_rate": 0.00037097922003985197,
      "loss": 7.4805,
      "step": 1814
    },
    {
      "epoch": 0.5167259786476869,
      "grad_norm": 0.5006376504898071,
      "learning_rate": 0.0003709080557927697,
      "loss": 7.3047,
      "step": 1815
    },
    {
      "epoch": 0.5170106761565836,
      "grad_norm": 0.5047767758369446,
      "learning_rate": 0.00037083689154568747,
      "loss": 7.5781,
      "step": 1816
    },
    {
      "epoch": 0.5172953736654804,
      "grad_norm": 0.5081997513771057,
      "learning_rate": 0.0003707657272986052,
      "loss": 7.457,
      "step": 1817
    },
    {
      "epoch": 0.5175800711743772,
      "grad_norm": 0.5185098052024841,
      "learning_rate": 0.0003706945630515229,
      "loss": 7.4219,
      "step": 1818
    },
    {
      "epoch": 0.517864768683274,
      "grad_norm": 0.4926905035972595,
      "learning_rate": 0.0003706233988044407,
      "loss": 7.3555,
      "step": 1819
    },
    {
      "epoch": 0.5181494661921708,
      "grad_norm": 0.5320389270782471,
      "learning_rate": 0.00037055223455735836,
      "loss": 7.2461,
      "step": 1820
    },
    {
      "epoch": 0.5184341637010677,
      "grad_norm": 0.49548694491386414,
      "learning_rate": 0.00037048107031027613,
      "loss": 7.6895,
      "step": 1821
    },
    {
      "epoch": 0.5187188612099645,
      "grad_norm": 0.43033069372177124,
      "learning_rate": 0.00037040990606319386,
      "loss": 7.4385,
      "step": 1822
    },
    {
      "epoch": 0.5190035587188612,
      "grad_norm": 0.48232313990592957,
      "learning_rate": 0.0003703387418161116,
      "loss": 7.4141,
      "step": 1823
    },
    {
      "epoch": 0.519288256227758,
      "grad_norm": 0.557569146156311,
      "learning_rate": 0.00037026757756902936,
      "loss": 7.2666,
      "step": 1824
    },
    {
      "epoch": 0.5195729537366548,
      "grad_norm": 0.5144730806350708,
      "learning_rate": 0.000370196413321947,
      "loss": 7.6348,
      "step": 1825
    },
    {
      "epoch": 0.5198576512455516,
      "grad_norm": 0.48285073041915894,
      "learning_rate": 0.0003701252490748648,
      "loss": 7.6338,
      "step": 1826
    },
    {
      "epoch": 0.5201423487544484,
      "grad_norm": 0.4901992380619049,
      "learning_rate": 0.0003700540848277825,
      "loss": 7.6816,
      "step": 1827
    },
    {
      "epoch": 0.5204270462633452,
      "grad_norm": 0.5319448709487915,
      "learning_rate": 0.00036998292058070025,
      "loss": 7.1611,
      "step": 1828
    },
    {
      "epoch": 0.5207117437722419,
      "grad_norm": 0.5035426020622253,
      "learning_rate": 0.000369911756333618,
      "loss": 7.4648,
      "step": 1829
    },
    {
      "epoch": 0.5209964412811388,
      "grad_norm": 0.6145444512367249,
      "learning_rate": 0.00036984059208653575,
      "loss": 7.1445,
      "step": 1830
    },
    {
      "epoch": 0.5212811387900356,
      "grad_norm": 0.41609200835227966,
      "learning_rate": 0.0003697694278394534,
      "loss": 7.9121,
      "step": 1831
    },
    {
      "epoch": 0.5215658362989324,
      "grad_norm": 0.4411139190196991,
      "learning_rate": 0.0003696982635923712,
      "loss": 7.4531,
      "step": 1832
    },
    {
      "epoch": 0.5218505338078292,
      "grad_norm": 0.5839465260505676,
      "learning_rate": 0.0003696270993452889,
      "loss": 7.499,
      "step": 1833
    },
    {
      "epoch": 0.522135231316726,
      "grad_norm": 0.4684014618396759,
      "learning_rate": 0.0003695559350982067,
      "loss": 7.4766,
      "step": 1834
    },
    {
      "epoch": 0.5224199288256228,
      "grad_norm": 0.5281928777694702,
      "learning_rate": 0.0003694847708511244,
      "loss": 7.5312,
      "step": 1835
    },
    {
      "epoch": 0.5227046263345195,
      "grad_norm": 0.5026902556419373,
      "learning_rate": 0.00036941360660404214,
      "loss": 7.2012,
      "step": 1836
    },
    {
      "epoch": 0.5229893238434163,
      "grad_norm": 0.4886755645275116,
      "learning_rate": 0.00036934244235695986,
      "loss": 7.4756,
      "step": 1837
    },
    {
      "epoch": 0.5232740213523132,
      "grad_norm": 0.6905612349510193,
      "learning_rate": 0.0003692712781098776,
      "loss": 6.6426,
      "step": 1838
    },
    {
      "epoch": 0.52355871886121,
      "grad_norm": 0.6627303957939148,
      "learning_rate": 0.00036920011386279536,
      "loss": 6.9521,
      "step": 1839
    },
    {
      "epoch": 0.5238434163701068,
      "grad_norm": 0.4939972162246704,
      "learning_rate": 0.0003691289496157131,
      "loss": 7.0068,
      "step": 1840
    },
    {
      "epoch": 0.5241281138790036,
      "grad_norm": 0.5928308963775635,
      "learning_rate": 0.0003690577853686308,
      "loss": 7.4785,
      "step": 1841
    },
    {
      "epoch": 0.5244128113879003,
      "grad_norm": 0.5538703203201294,
      "learning_rate": 0.00036898662112154853,
      "loss": 7.6748,
      "step": 1842
    },
    {
      "epoch": 0.5246975088967971,
      "grad_norm": 0.6582218408584595,
      "learning_rate": 0.00036891545687446625,
      "loss": 7.1357,
      "step": 1843
    },
    {
      "epoch": 0.5249822064056939,
      "grad_norm": 0.5117785930633545,
      "learning_rate": 0.00036884429262738403,
      "loss": 7.2305,
      "step": 1844
    },
    {
      "epoch": 0.5252669039145907,
      "grad_norm": 0.4714382588863373,
      "learning_rate": 0.00036877312838030175,
      "loss": 7.7305,
      "step": 1845
    },
    {
      "epoch": 0.5255516014234876,
      "grad_norm": 0.5107897520065308,
      "learning_rate": 0.0003687019641332195,
      "loss": 7.6045,
      "step": 1846
    },
    {
      "epoch": 0.5258362989323844,
      "grad_norm": 0.5239918828010559,
      "learning_rate": 0.00036863079988613725,
      "loss": 7.3604,
      "step": 1847
    },
    {
      "epoch": 0.5261209964412812,
      "grad_norm": 0.4463542401790619,
      "learning_rate": 0.0003685596356390549,
      "loss": 7.6875,
      "step": 1848
    },
    {
      "epoch": 0.5264056939501779,
      "grad_norm": 0.45635050535202026,
      "learning_rate": 0.00036848847139197264,
      "loss": 7.4707,
      "step": 1849
    },
    {
      "epoch": 0.5266903914590747,
      "grad_norm": 0.6467781066894531,
      "learning_rate": 0.0003684173071448904,
      "loss": 6.9404,
      "step": 1850
    },
    {
      "epoch": 0.5269750889679715,
      "grad_norm": 0.4470195472240448,
      "learning_rate": 0.00036834614289780814,
      "loss": 7.584,
      "step": 1851
    },
    {
      "epoch": 0.5272597864768683,
      "grad_norm": 0.49318236112594604,
      "learning_rate": 0.0003682749786507259,
      "loss": 7.3008,
      "step": 1852
    },
    {
      "epoch": 0.5275444839857651,
      "grad_norm": 0.3832341432571411,
      "learning_rate": 0.00036820381440364364,
      "loss": 7.8809,
      "step": 1853
    },
    {
      "epoch": 0.527829181494662,
      "grad_norm": 0.5127120018005371,
      "learning_rate": 0.0003681326501565613,
      "loss": 7.2793,
      "step": 1854
    },
    {
      "epoch": 0.5281138790035588,
      "grad_norm": 0.46850189566612244,
      "learning_rate": 0.0003680614859094791,
      "loss": 7.3555,
      "step": 1855
    },
    {
      "epoch": 0.5283985765124555,
      "grad_norm": 0.5280941724777222,
      "learning_rate": 0.0003679903216623968,
      "loss": 7.2598,
      "step": 1856
    },
    {
      "epoch": 0.5286832740213523,
      "grad_norm": 0.4804897606372833,
      "learning_rate": 0.0003679191574153146,
      "loss": 7.7119,
      "step": 1857
    },
    {
      "epoch": 0.5289679715302491,
      "grad_norm": 0.5771774053573608,
      "learning_rate": 0.0003678479931682323,
      "loss": 7.1006,
      "step": 1858
    },
    {
      "epoch": 0.5292526690391459,
      "grad_norm": 0.6702505350112915,
      "learning_rate": 0.00036777682892115,
      "loss": 7.2842,
      "step": 1859
    },
    {
      "epoch": 0.5295373665480427,
      "grad_norm": 0.5188344120979309,
      "learning_rate": 0.00036770566467406776,
      "loss": 7.1113,
      "step": 1860
    },
    {
      "epoch": 0.5298220640569395,
      "grad_norm": 0.5821831822395325,
      "learning_rate": 0.0003676345004269855,
      "loss": 7.0547,
      "step": 1861
    },
    {
      "epoch": 0.5301067615658362,
      "grad_norm": 0.4173790514469147,
      "learning_rate": 0.00036756333617990326,
      "loss": 7.6582,
      "step": 1862
    },
    {
      "epoch": 0.5303914590747331,
      "grad_norm": 0.44476157426834106,
      "learning_rate": 0.000367492171932821,
      "loss": 7.8896,
      "step": 1863
    },
    {
      "epoch": 0.5306761565836299,
      "grad_norm": 0.5345341563224792,
      "learning_rate": 0.0003674210076857387,
      "loss": 7.2656,
      "step": 1864
    },
    {
      "epoch": 0.5309608540925267,
      "grad_norm": 0.5184802412986755,
      "learning_rate": 0.0003673498434386564,
      "loss": 7.6143,
      "step": 1865
    },
    {
      "epoch": 0.5312455516014235,
      "grad_norm": 0.4787604510784149,
      "learning_rate": 0.00036727867919157415,
      "loss": 7.2578,
      "step": 1866
    },
    {
      "epoch": 0.5315302491103203,
      "grad_norm": 0.5310441851615906,
      "learning_rate": 0.00036720751494449187,
      "loss": 7.4033,
      "step": 1867
    },
    {
      "epoch": 0.5318149466192171,
      "grad_norm": 0.4596840441226959,
      "learning_rate": 0.00036713635069740965,
      "loss": 7.8955,
      "step": 1868
    },
    {
      "epoch": 0.5320996441281138,
      "grad_norm": 0.4832497239112854,
      "learning_rate": 0.00036706518645032737,
      "loss": 7.4756,
      "step": 1869
    },
    {
      "epoch": 0.5323843416370106,
      "grad_norm": 0.5431188941001892,
      "learning_rate": 0.0003669940222032451,
      "loss": 7.5264,
      "step": 1870
    },
    {
      "epoch": 0.5326690391459075,
      "grad_norm": 0.5058256983757019,
      "learning_rate": 0.0003669228579561628,
      "loss": 7.1895,
      "step": 1871
    },
    {
      "epoch": 0.5329537366548043,
      "grad_norm": 0.4994313716888428,
      "learning_rate": 0.00036685169370908054,
      "loss": 7.4639,
      "step": 1872
    },
    {
      "epoch": 0.5332384341637011,
      "grad_norm": 0.4851476848125458,
      "learning_rate": 0.0003667805294619983,
      "loss": 7.5654,
      "step": 1873
    },
    {
      "epoch": 0.5335231316725979,
      "grad_norm": 0.6031661033630371,
      "learning_rate": 0.00036670936521491604,
      "loss": 6.8779,
      "step": 1874
    },
    {
      "epoch": 0.5338078291814946,
      "grad_norm": 0.42537492513656616,
      "learning_rate": 0.0003666382009678338,
      "loss": 7.6953,
      "step": 1875
    },
    {
      "epoch": 0.5340925266903914,
      "grad_norm": 0.5109694600105286,
      "learning_rate": 0.0003665670367207515,
      "loss": 7.6777,
      "step": 1876
    },
    {
      "epoch": 0.5343772241992882,
      "grad_norm": 0.6188481450080872,
      "learning_rate": 0.0003664958724736692,
      "loss": 7.0898,
      "step": 1877
    },
    {
      "epoch": 0.534661921708185,
      "grad_norm": 0.4916169345378876,
      "learning_rate": 0.000366424708226587,
      "loss": 7.7979,
      "step": 1878
    },
    {
      "epoch": 0.5349466192170819,
      "grad_norm": 0.4360571503639221,
      "learning_rate": 0.0003663535439795047,
      "loss": 7.5176,
      "step": 1879
    },
    {
      "epoch": 0.5352313167259787,
      "grad_norm": 0.4968567192554474,
      "learning_rate": 0.0003662823797324225,
      "loss": 7.4512,
      "step": 1880
    },
    {
      "epoch": 0.5355160142348755,
      "grad_norm": 0.5011583566665649,
      "learning_rate": 0.0003662112154853402,
      "loss": 7.5146,
      "step": 1881
    },
    {
      "epoch": 0.5358007117437722,
      "grad_norm": 0.46517226099967957,
      "learning_rate": 0.0003661400512382579,
      "loss": 7.4463,
      "step": 1882
    },
    {
      "epoch": 0.536085409252669,
      "grad_norm": 0.4221740961074829,
      "learning_rate": 0.00036606888699117565,
      "loss": 7.3955,
      "step": 1883
    },
    {
      "epoch": 0.5363701067615658,
      "grad_norm": 0.5024994611740112,
      "learning_rate": 0.0003659977227440934,
      "loss": 7.4414,
      "step": 1884
    },
    {
      "epoch": 0.5366548042704626,
      "grad_norm": 0.46109068393707275,
      "learning_rate": 0.0003659265584970111,
      "loss": 7.8184,
      "step": 1885
    },
    {
      "epoch": 0.5369395017793595,
      "grad_norm": 0.4218958616256714,
      "learning_rate": 0.00036585539424992887,
      "loss": 7.7148,
      "step": 1886
    },
    {
      "epoch": 0.5372241992882563,
      "grad_norm": 0.522912323474884,
      "learning_rate": 0.00036578423000284654,
      "loss": 7.5303,
      "step": 1887
    },
    {
      "epoch": 0.5375088967971531,
      "grad_norm": 0.5935214757919312,
      "learning_rate": 0.0003657130657557643,
      "loss": 7.1064,
      "step": 1888
    },
    {
      "epoch": 0.5377935943060498,
      "grad_norm": 0.5334514379501343,
      "learning_rate": 0.00036564190150868204,
      "loss": 7.3984,
      "step": 1889
    },
    {
      "epoch": 0.5380782918149466,
      "grad_norm": 0.5348649621009827,
      "learning_rate": 0.00036557073726159976,
      "loss": 7.0674,
      "step": 1890
    },
    {
      "epoch": 0.5383629893238434,
      "grad_norm": 0.4524238407611847,
      "learning_rate": 0.00036549957301451754,
      "loss": 7.7676,
      "step": 1891
    },
    {
      "epoch": 0.5386476868327402,
      "grad_norm": 0.5088392496109009,
      "learning_rate": 0.00036542840876743526,
      "loss": 7.6836,
      "step": 1892
    },
    {
      "epoch": 0.538932384341637,
      "grad_norm": 0.47894662618637085,
      "learning_rate": 0.000365357244520353,
      "loss": 7.7031,
      "step": 1893
    },
    {
      "epoch": 0.5392170818505339,
      "grad_norm": 0.5764287114143372,
      "learning_rate": 0.0003652860802732707,
      "loss": 7.1602,
      "step": 1894
    },
    {
      "epoch": 0.5395017793594306,
      "grad_norm": 0.6556904911994934,
      "learning_rate": 0.00036521491602618843,
      "loss": 6.2539,
      "step": 1895
    },
    {
      "epoch": 0.5397864768683274,
      "grad_norm": 0.5637615323066711,
      "learning_rate": 0.0003651437517791062,
      "loss": 7.1055,
      "step": 1896
    },
    {
      "epoch": 0.5400711743772242,
      "grad_norm": 0.4705369770526886,
      "learning_rate": 0.00036507258753202393,
      "loss": 7.375,
      "step": 1897
    },
    {
      "epoch": 0.540355871886121,
      "grad_norm": 0.5638760328292847,
      "learning_rate": 0.00036500142328494165,
      "loss": 7.2764,
      "step": 1898
    },
    {
      "epoch": 0.5406405693950178,
      "grad_norm": 0.46844178438186646,
      "learning_rate": 0.0003649302590378594,
      "loss": 7.5635,
      "step": 1899
    },
    {
      "epoch": 0.5409252669039146,
      "grad_norm": 0.5277328491210938,
      "learning_rate": 0.0003648590947907771,
      "loss": 7.3311,
      "step": 1900
    },
    {
      "epoch": 0.5412099644128114,
      "grad_norm": 0.4768332242965698,
      "learning_rate": 0.0003647879305436949,
      "loss": 7.4766,
      "step": 1901
    },
    {
      "epoch": 0.5414946619217081,
      "grad_norm": 0.5177027583122253,
      "learning_rate": 0.0003647167662966126,
      "loss": 7.3652,
      "step": 1902
    },
    {
      "epoch": 0.541779359430605,
      "grad_norm": 0.5060975551605225,
      "learning_rate": 0.0003646456020495303,
      "loss": 7.5723,
      "step": 1903
    },
    {
      "epoch": 0.5420640569395018,
      "grad_norm": 0.6366326212882996,
      "learning_rate": 0.00036457443780244804,
      "loss": 6.9111,
      "step": 1904
    },
    {
      "epoch": 0.5423487544483986,
      "grad_norm": 0.5358245372772217,
      "learning_rate": 0.00036450327355536577,
      "loss": 6.9463,
      "step": 1905
    },
    {
      "epoch": 0.5426334519572954,
      "grad_norm": 0.4724561274051666,
      "learning_rate": 0.00036443210930828354,
      "loss": 7.4346,
      "step": 1906
    },
    {
      "epoch": 0.5429181494661922,
      "grad_norm": 0.4824996292591095,
      "learning_rate": 0.00036436094506120127,
      "loss": 7.7168,
      "step": 1907
    },
    {
      "epoch": 0.5432028469750889,
      "grad_norm": 0.45312535762786865,
      "learning_rate": 0.000364289780814119,
      "loss": 7.7764,
      "step": 1908
    },
    {
      "epoch": 0.5434875444839857,
      "grad_norm": 0.587820827960968,
      "learning_rate": 0.00036421861656703677,
      "loss": 7.2695,
      "step": 1909
    },
    {
      "epoch": 0.5437722419928825,
      "grad_norm": 0.4031955301761627,
      "learning_rate": 0.00036414745231995444,
      "loss": 7.582,
      "step": 1910
    },
    {
      "epoch": 0.5440569395017794,
      "grad_norm": 0.49873512983322144,
      "learning_rate": 0.0003640762880728722,
      "loss": 7.5537,
      "step": 1911
    },
    {
      "epoch": 0.5443416370106762,
      "grad_norm": 0.5303727388381958,
      "learning_rate": 0.00036400512382578994,
      "loss": 7.1777,
      "step": 1912
    },
    {
      "epoch": 0.544626334519573,
      "grad_norm": 0.511758029460907,
      "learning_rate": 0.00036393395957870766,
      "loss": 7.4238,
      "step": 1913
    },
    {
      "epoch": 0.5449110320284698,
      "grad_norm": 0.44237908720970154,
      "learning_rate": 0.00036386279533162544,
      "loss": 7.5283,
      "step": 1914
    },
    {
      "epoch": 0.5451957295373665,
      "grad_norm": 0.5043278932571411,
      "learning_rate": 0.0003637916310845431,
      "loss": 7.5625,
      "step": 1915
    },
    {
      "epoch": 0.5454804270462633,
      "grad_norm": 0.49663665890693665,
      "learning_rate": 0.0003637204668374608,
      "loss": 7.6689,
      "step": 1916
    },
    {
      "epoch": 0.5457651245551601,
      "grad_norm": 0.45969969034194946,
      "learning_rate": 0.0003636493025903786,
      "loss": 7.6162,
      "step": 1917
    },
    {
      "epoch": 0.5460498220640569,
      "grad_norm": 0.545724093914032,
      "learning_rate": 0.0003635781383432963,
      "loss": 6.9424,
      "step": 1918
    },
    {
      "epoch": 0.5463345195729538,
      "grad_norm": 0.4870263636112213,
      "learning_rate": 0.0003635069740962141,
      "loss": 7.4805,
      "step": 1919
    },
    {
      "epoch": 0.5466192170818506,
      "grad_norm": 0.4136676490306854,
      "learning_rate": 0.0003634358098491318,
      "loss": 7.9678,
      "step": 1920
    },
    {
      "epoch": 0.5469039145907474,
      "grad_norm": 0.4635253846645355,
      "learning_rate": 0.0003633646456020495,
      "loss": 7.2422,
      "step": 1921
    },
    {
      "epoch": 0.5471886120996441,
      "grad_norm": 0.5168067216873169,
      "learning_rate": 0.00036329348135496727,
      "loss": 7.2363,
      "step": 1922
    },
    {
      "epoch": 0.5474733096085409,
      "grad_norm": 0.3811019957065582,
      "learning_rate": 0.000363222317107885,
      "loss": 7.7578,
      "step": 1923
    },
    {
      "epoch": 0.5477580071174377,
      "grad_norm": 0.5115704536437988,
      "learning_rate": 0.00036315115286080277,
      "loss": 7.3984,
      "step": 1924
    },
    {
      "epoch": 0.5480427046263345,
      "grad_norm": 0.5071365833282471,
      "learning_rate": 0.0003630799886137205,
      "loss": 7.3779,
      "step": 1925
    },
    {
      "epoch": 0.5483274021352313,
      "grad_norm": 0.44345128536224365,
      "learning_rate": 0.0003630088243666382,
      "loss": 7.666,
      "step": 1926
    },
    {
      "epoch": 0.5486120996441282,
      "grad_norm": 0.5061864852905273,
      "learning_rate": 0.00036293766011955594,
      "loss": 7.5371,
      "step": 1927
    },
    {
      "epoch": 0.5488967971530249,
      "grad_norm": 0.5193682312965393,
      "learning_rate": 0.00036286649587247366,
      "loss": 7.2842,
      "step": 1928
    },
    {
      "epoch": 0.5491814946619217,
      "grad_norm": 0.4672448933124542,
      "learning_rate": 0.0003627953316253914,
      "loss": 7.8135,
      "step": 1929
    },
    {
      "epoch": 0.5494661921708185,
      "grad_norm": 0.45348474383354187,
      "learning_rate": 0.00036272416737830916,
      "loss": 7.5537,
      "step": 1930
    },
    {
      "epoch": 0.5497508896797153,
      "grad_norm": 0.5304293632507324,
      "learning_rate": 0.0003626530031312269,
      "loss": 7.2041,
      "step": 1931
    },
    {
      "epoch": 0.5500355871886121,
      "grad_norm": 0.4106883704662323,
      "learning_rate": 0.0003625818388841446,
      "loss": 8.0264,
      "step": 1932
    },
    {
      "epoch": 0.5503202846975089,
      "grad_norm": 0.5388475656509399,
      "learning_rate": 0.00036251067463706233,
      "loss": 7.4434,
      "step": 1933
    },
    {
      "epoch": 0.5506049822064057,
      "grad_norm": 0.5221579074859619,
      "learning_rate": 0.00036243951038998005,
      "loss": 7.3984,
      "step": 1934
    },
    {
      "epoch": 0.5508896797153024,
      "grad_norm": 0.5995771884918213,
      "learning_rate": 0.00036236834614289783,
      "loss": 7.1641,
      "step": 1935
    },
    {
      "epoch": 0.5511743772241993,
      "grad_norm": 0.5584063529968262,
      "learning_rate": 0.00036229718189581555,
      "loss": 7.5605,
      "step": 1936
    },
    {
      "epoch": 0.5514590747330961,
      "grad_norm": 0.5710662007331848,
      "learning_rate": 0.00036222601764873333,
      "loss": 7.1553,
      "step": 1937
    },
    {
      "epoch": 0.5517437722419929,
      "grad_norm": 0.5953046083450317,
      "learning_rate": 0.000362154853401651,
      "loss": 7.0371,
      "step": 1938
    },
    {
      "epoch": 0.5520284697508897,
      "grad_norm": 0.5294259786605835,
      "learning_rate": 0.0003620836891545687,
      "loss": 7.4941,
      "step": 1939
    },
    {
      "epoch": 0.5523131672597865,
      "grad_norm": 0.665874719619751,
      "learning_rate": 0.0003620125249074865,
      "loss": 6.9414,
      "step": 1940
    },
    {
      "epoch": 0.5525978647686832,
      "grad_norm": 0.48795419931411743,
      "learning_rate": 0.0003619413606604042,
      "loss": 7.4199,
      "step": 1941
    },
    {
      "epoch": 0.55288256227758,
      "grad_norm": 0.5871965885162354,
      "learning_rate": 0.000361870196413322,
      "loss": 6.8369,
      "step": 1942
    },
    {
      "epoch": 0.5531672597864769,
      "grad_norm": 0.5023308992385864,
      "learning_rate": 0.00036179903216623967,
      "loss": 7.1133,
      "step": 1943
    },
    {
      "epoch": 0.5534519572953737,
      "grad_norm": 0.497126042842865,
      "learning_rate": 0.0003617278679191574,
      "loss": 7.3896,
      "step": 1944
    },
    {
      "epoch": 0.5537366548042705,
      "grad_norm": 0.6035934686660767,
      "learning_rate": 0.00036165670367207517,
      "loss": 7.0957,
      "step": 1945
    },
    {
      "epoch": 0.5540213523131673,
      "grad_norm": 0.520519495010376,
      "learning_rate": 0.0003615855394249929,
      "loss": 7.1953,
      "step": 1946
    },
    {
      "epoch": 0.5543060498220641,
      "grad_norm": 0.5469746589660645,
      "learning_rate": 0.0003615143751779106,
      "loss": 7.4717,
      "step": 1947
    },
    {
      "epoch": 0.5545907473309608,
      "grad_norm": 0.4506682753562927,
      "learning_rate": 0.0003614432109308284,
      "loss": 8.0293,
      "step": 1948
    },
    {
      "epoch": 0.5548754448398576,
      "grad_norm": 0.4993910789489746,
      "learning_rate": 0.00036137204668374606,
      "loss": 7.3896,
      "step": 1949
    },
    {
      "epoch": 0.5551601423487544,
      "grad_norm": 0.5269621014595032,
      "learning_rate": 0.00036130088243666383,
      "loss": 7.4609,
      "step": 1950
    },
    {
      "epoch": 0.5554448398576513,
      "grad_norm": 0.5039801001548767,
      "learning_rate": 0.00036122971818958156,
      "loss": 7.3369,
      "step": 1951
    },
    {
      "epoch": 0.5557295373665481,
      "grad_norm": 0.5588239431381226,
      "learning_rate": 0.0003611585539424993,
      "loss": 6.7295,
      "step": 1952
    },
    {
      "epoch": 0.5560142348754449,
      "grad_norm": 0.37530943751335144,
      "learning_rate": 0.00036108738969541706,
      "loss": 7.9775,
      "step": 1953
    },
    {
      "epoch": 0.5562989323843416,
      "grad_norm": 0.4834645390510559,
      "learning_rate": 0.0003610162254483348,
      "loss": 7.6377,
      "step": 1954
    },
    {
      "epoch": 0.5565836298932384,
      "grad_norm": 0.44427239894866943,
      "learning_rate": 0.0003609450612012525,
      "loss": 7.6768,
      "step": 1955
    },
    {
      "epoch": 0.5568683274021352,
      "grad_norm": 0.46818944811820984,
      "learning_rate": 0.0003608738969541702,
      "loss": 7.1377,
      "step": 1956
    },
    {
      "epoch": 0.557153024911032,
      "grad_norm": 0.5476382970809937,
      "learning_rate": 0.00036080273270708795,
      "loss": 7.2188,
      "step": 1957
    },
    {
      "epoch": 0.5574377224199288,
      "grad_norm": 0.7069761157035828,
      "learning_rate": 0.0003607315684600057,
      "loss": 7.2842,
      "step": 1958
    },
    {
      "epoch": 0.5577224199288257,
      "grad_norm": 0.48670628666877747,
      "learning_rate": 0.00036066040421292345,
      "loss": 7.3574,
      "step": 1959
    },
    {
      "epoch": 0.5580071174377225,
      "grad_norm": 0.49129214882850647,
      "learning_rate": 0.0003605892399658411,
      "loss": 7.2822,
      "step": 1960
    },
    {
      "epoch": 0.5582918149466192,
      "grad_norm": 0.43926045298576355,
      "learning_rate": 0.0003605180757187589,
      "loss": 7.708,
      "step": 1961
    },
    {
      "epoch": 0.558576512455516,
      "grad_norm": 0.5371343493461609,
      "learning_rate": 0.0003604469114716766,
      "loss": 7.1279,
      "step": 1962
    },
    {
      "epoch": 0.5588612099644128,
      "grad_norm": 0.5935686230659485,
      "learning_rate": 0.0003603757472245944,
      "loss": 6.9062,
      "step": 1963
    },
    {
      "epoch": 0.5591459074733096,
      "grad_norm": 0.5080186724662781,
      "learning_rate": 0.0003603045829775121,
      "loss": 7.4346,
      "step": 1964
    },
    {
      "epoch": 0.5594306049822064,
      "grad_norm": 0.5738562941551208,
      "learning_rate": 0.00036023341873042984,
      "loss": 6.8496,
      "step": 1965
    },
    {
      "epoch": 0.5597153024911032,
      "grad_norm": 0.545665442943573,
      "learning_rate": 0.00036016225448334756,
      "loss": 7.5244,
      "step": 1966
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.39548519253730774,
      "learning_rate": 0.0003600910902362653,
      "loss": 7.7207,
      "step": 1967
    },
    {
      "epoch": 0.5602846975088968,
      "grad_norm": 0.5748059749603271,
      "learning_rate": 0.00036001992598918306,
      "loss": 7.085,
      "step": 1968
    },
    {
      "epoch": 0.5605693950177936,
      "grad_norm": 0.45681309700012207,
      "learning_rate": 0.0003599487617421008,
      "loss": 7.8096,
      "step": 1969
    },
    {
      "epoch": 0.5608540925266904,
      "grad_norm": 0.4927469491958618,
      "learning_rate": 0.0003598775974950185,
      "loss": 7.0459,
      "step": 1970
    },
    {
      "epoch": 0.5611387900355872,
      "grad_norm": 0.46010443568229675,
      "learning_rate": 0.0003598064332479363,
      "loss": 7.4414,
      "step": 1971
    },
    {
      "epoch": 0.561423487544484,
      "grad_norm": 0.6437321901321411,
      "learning_rate": 0.00035973526900085395,
      "loss": 6.9727,
      "step": 1972
    },
    {
      "epoch": 0.5617081850533808,
      "grad_norm": 0.46185731887817383,
      "learning_rate": 0.00035966410475377173,
      "loss": 7.6182,
      "step": 1973
    },
    {
      "epoch": 0.5619928825622775,
      "grad_norm": 0.5001309514045715,
      "learning_rate": 0.00035959294050668945,
      "loss": 7.8613,
      "step": 1974
    },
    {
      "epoch": 0.5622775800711743,
      "grad_norm": 0.46464210748672485,
      "learning_rate": 0.0003595217762596072,
      "loss": 7.7021,
      "step": 1975
    },
    {
      "epoch": 0.5625622775800712,
      "grad_norm": 0.5368632078170776,
      "learning_rate": 0.00035945061201252495,
      "loss": 7.3047,
      "step": 1976
    },
    {
      "epoch": 0.562846975088968,
      "grad_norm": 0.49685120582580566,
      "learning_rate": 0.0003593794477654426,
      "loss": 7.4932,
      "step": 1977
    },
    {
      "epoch": 0.5631316725978648,
      "grad_norm": 0.6346001029014587,
      "learning_rate": 0.00035930828351836034,
      "loss": 7.0449,
      "step": 1978
    },
    {
      "epoch": 0.5634163701067616,
      "grad_norm": 0.5482520461082458,
      "learning_rate": 0.0003592371192712781,
      "loss": 7.3398,
      "step": 1979
    },
    {
      "epoch": 0.5637010676156584,
      "grad_norm": 0.5402587056159973,
      "learning_rate": 0.00035916595502419584,
      "loss": 7.4434,
      "step": 1980
    },
    {
      "epoch": 0.5639857651245551,
      "grad_norm": 0.48446568846702576,
      "learning_rate": 0.0003590947907771136,
      "loss": 7.4541,
      "step": 1981
    },
    {
      "epoch": 0.5642704626334519,
      "grad_norm": 0.5069032311439514,
      "learning_rate": 0.00035902362653003134,
      "loss": 7.5781,
      "step": 1982
    },
    {
      "epoch": 0.5645551601423487,
      "grad_norm": 0.5194056034088135,
      "learning_rate": 0.000358952462282949,
      "loss": 7.2754,
      "step": 1983
    },
    {
      "epoch": 0.5648398576512456,
      "grad_norm": 0.4448651671409607,
      "learning_rate": 0.0003588812980358668,
      "loss": 8.0352,
      "step": 1984
    },
    {
      "epoch": 0.5651245551601424,
      "grad_norm": 0.48044419288635254,
      "learning_rate": 0.0003588101337887845,
      "loss": 7.2314,
      "step": 1985
    },
    {
      "epoch": 0.5654092526690392,
      "grad_norm": 0.3711092472076416,
      "learning_rate": 0.0003587389695417023,
      "loss": 8.0391,
      "step": 1986
    },
    {
      "epoch": 0.5656939501779359,
      "grad_norm": 0.5697209239006042,
      "learning_rate": 0.00035866780529462,
      "loss": 6.9863,
      "step": 1987
    },
    {
      "epoch": 0.5659786476868327,
      "grad_norm": 0.4593607187271118,
      "learning_rate": 0.0003585966410475377,
      "loss": 7.2432,
      "step": 1988
    },
    {
      "epoch": 0.5662633451957295,
      "grad_norm": 0.514837384223938,
      "learning_rate": 0.00035852547680045546,
      "loss": 7.3799,
      "step": 1989
    },
    {
      "epoch": 0.5665480427046263,
      "grad_norm": 0.41861602663993835,
      "learning_rate": 0.0003584543125533732,
      "loss": 7.792,
      "step": 1990
    },
    {
      "epoch": 0.5668327402135231,
      "grad_norm": 0.6247495412826538,
      "learning_rate": 0.00035838314830629095,
      "loss": 6.7305,
      "step": 1991
    },
    {
      "epoch": 0.56711743772242,
      "grad_norm": 0.4983992576599121,
      "learning_rate": 0.0003583119840592087,
      "loss": 7.624,
      "step": 1992
    },
    {
      "epoch": 0.5674021352313168,
      "grad_norm": 0.6223182082176208,
      "learning_rate": 0.0003582408198121264,
      "loss": 6.5537,
      "step": 1993
    },
    {
      "epoch": 0.5676868327402135,
      "grad_norm": 0.5390810966491699,
      "learning_rate": 0.0003581696555650441,
      "loss": 6.7236,
      "step": 1994
    },
    {
      "epoch": 0.5679715302491103,
      "grad_norm": 0.6671931743621826,
      "learning_rate": 0.00035809849131796185,
      "loss": 6.7148,
      "step": 1995
    },
    {
      "epoch": 0.5682562277580071,
      "grad_norm": 0.5253502726554871,
      "learning_rate": 0.00035802732707087957,
      "loss": 7.7129,
      "step": 1996
    },
    {
      "epoch": 0.5685409252669039,
      "grad_norm": 0.5754931569099426,
      "learning_rate": 0.00035795616282379735,
      "loss": 7.0928,
      "step": 1997
    },
    {
      "epoch": 0.5688256227758007,
      "grad_norm": 0.7119153141975403,
      "learning_rate": 0.00035788499857671507,
      "loss": 6.7354,
      "step": 1998
    },
    {
      "epoch": 0.5691103202846975,
      "grad_norm": 0.4391777217388153,
      "learning_rate": 0.00035781383432963285,
      "loss": 7.5645,
      "step": 1999
    },
    {
      "epoch": 0.5693950177935944,
      "grad_norm": 0.45925286412239075,
      "learning_rate": 0.0003577426700825505,
      "loss": 7.5547,
      "step": 2000
    },
    {
      "epoch": 0.5693950177935944,
      "eval_bleu": 0.12240902102387125,
      "eval_loss": 7.11328125,
      "eval_runtime": 124.8971,
      "eval_samples_per_second": 2.274,
      "eval_steps_per_second": 0.144,
      "step": 2000
    },
    {
      "epoch": 0.5696797153024911,
      "grad_norm": 0.4213367998600006,
      "learning_rate": 0.00035767150583546824,
      "loss": 7.6436,
      "step": 2001
    },
    {
      "epoch": 0.5699644128113879,
      "grad_norm": 0.6510480642318726,
      "learning_rate": 0.000357600341588386,
      "loss": 6.791,
      "step": 2002
    },
    {
      "epoch": 0.5702491103202847,
      "grad_norm": 0.40787938237190247,
      "learning_rate": 0.00035752917734130374,
      "loss": 7.9385,
      "step": 2003
    },
    {
      "epoch": 0.5705338078291815,
      "grad_norm": 0.5310803651809692,
      "learning_rate": 0.0003574580130942215,
      "loss": 7.3613,
      "step": 2004
    },
    {
      "epoch": 0.5708185053380783,
      "grad_norm": 0.5553985834121704,
      "learning_rate": 0.0003573868488471392,
      "loss": 7.2148,
      "step": 2005
    },
    {
      "epoch": 0.5711032028469751,
      "grad_norm": 0.5080181360244751,
      "learning_rate": 0.0003573156846000569,
      "loss": 7.6328,
      "step": 2006
    },
    {
      "epoch": 0.5713879003558718,
      "grad_norm": 0.5772024989128113,
      "learning_rate": 0.0003572445203529747,
      "loss": 7.3027,
      "step": 2007
    },
    {
      "epoch": 0.5716725978647687,
      "grad_norm": 0.5602935552597046,
      "learning_rate": 0.0003571733561058924,
      "loss": 7.0781,
      "step": 2008
    },
    {
      "epoch": 0.5719572953736655,
      "grad_norm": 0.46387121081352234,
      "learning_rate": 0.0003571021918588102,
      "loss": 7.3994,
      "step": 2009
    },
    {
      "epoch": 0.5722419928825623,
      "grad_norm": 0.46233439445495605,
      "learning_rate": 0.0003570310276117279,
      "loss": 7.667,
      "step": 2010
    },
    {
      "epoch": 0.5725266903914591,
      "grad_norm": 0.5186634063720703,
      "learning_rate": 0.00035695986336464557,
      "loss": 7.2578,
      "step": 2011
    },
    {
      "epoch": 0.5728113879003559,
      "grad_norm": 0.5802475810050964,
      "learning_rate": 0.00035688869911756335,
      "loss": 7.0547,
      "step": 2012
    },
    {
      "epoch": 0.5730960854092527,
      "grad_norm": 0.4938744008541107,
      "learning_rate": 0.00035681753487048107,
      "loss": 7.5059,
      "step": 2013
    },
    {
      "epoch": 0.5733807829181494,
      "grad_norm": 0.5719893574714661,
      "learning_rate": 0.0003567463706233988,
      "loss": 7.0898,
      "step": 2014
    },
    {
      "epoch": 0.5736654804270462,
      "grad_norm": 0.5488351583480835,
      "learning_rate": 0.00035667520637631657,
      "loss": 7.416,
      "step": 2015
    },
    {
      "epoch": 0.573950177935943,
      "grad_norm": 0.6115978956222534,
      "learning_rate": 0.0003566040421292343,
      "loss": 7.3984,
      "step": 2016
    },
    {
      "epoch": 0.5742348754448399,
      "grad_norm": 0.5848609209060669,
      "learning_rate": 0.000356532877882152,
      "loss": 7.1094,
      "step": 2017
    },
    {
      "epoch": 0.5745195729537367,
      "grad_norm": 0.5114953517913818,
      "learning_rate": 0.00035646171363506974,
      "loss": 7.1885,
      "step": 2018
    },
    {
      "epoch": 0.5748042704626335,
      "grad_norm": 0.4265839159488678,
      "learning_rate": 0.00035639054938798746,
      "loss": 7.2188,
      "step": 2019
    },
    {
      "epoch": 0.5750889679715302,
      "grad_norm": 0.5009260773658752,
      "learning_rate": 0.00035631938514090524,
      "loss": 7.377,
      "step": 2020
    },
    {
      "epoch": 0.575373665480427,
      "grad_norm": 0.5544488430023193,
      "learning_rate": 0.00035624822089382296,
      "loss": 7.8457,
      "step": 2021
    },
    {
      "epoch": 0.5756583629893238,
      "grad_norm": 0.4924667477607727,
      "learning_rate": 0.0003561770566467407,
      "loss": 7.2812,
      "step": 2022
    },
    {
      "epoch": 0.5759430604982206,
      "grad_norm": 0.5703074932098389,
      "learning_rate": 0.0003561058923996584,
      "loss": 7.168,
      "step": 2023
    },
    {
      "epoch": 0.5762277580071175,
      "grad_norm": 0.5206242203712463,
      "learning_rate": 0.00035603472815257613,
      "loss": 7.4453,
      "step": 2024
    },
    {
      "epoch": 0.5765124555160143,
      "grad_norm": 0.4663465619087219,
      "learning_rate": 0.0003559635639054939,
      "loss": 7.9004,
      "step": 2025
    },
    {
      "epoch": 0.5767971530249111,
      "grad_norm": 0.43252623081207275,
      "learning_rate": 0.00035589239965841163,
      "loss": 7.8994,
      "step": 2026
    },
    {
      "epoch": 0.5770818505338078,
      "grad_norm": 0.6330854296684265,
      "learning_rate": 0.00035582123541132935,
      "loss": 6.9521,
      "step": 2027
    },
    {
      "epoch": 0.5773665480427046,
      "grad_norm": 0.46247154474258423,
      "learning_rate": 0.0003557500711642471,
      "loss": 7.5449,
      "step": 2028
    },
    {
      "epoch": 0.5776512455516014,
      "grad_norm": 0.49696725606918335,
      "learning_rate": 0.0003556789069171648,
      "loss": 7.582,
      "step": 2029
    },
    {
      "epoch": 0.5779359430604982,
      "grad_norm": 0.6059892177581787,
      "learning_rate": 0.0003556077426700826,
      "loss": 7.46,
      "step": 2030
    },
    {
      "epoch": 0.578220640569395,
      "grad_norm": 0.5297976732254028,
      "learning_rate": 0.0003555365784230003,
      "loss": 7.0938,
      "step": 2031
    },
    {
      "epoch": 0.5785053380782919,
      "grad_norm": 0.5169646739959717,
      "learning_rate": 0.000355465414175918,
      "loss": 7.21,
      "step": 2032
    },
    {
      "epoch": 0.5787900355871887,
      "grad_norm": 0.4276336133480072,
      "learning_rate": 0.00035539424992883574,
      "loss": 7.5605,
      "step": 2033
    },
    {
      "epoch": 0.5790747330960854,
      "grad_norm": 0.46051013469696045,
      "learning_rate": 0.00035532308568175347,
      "loss": 7.7881,
      "step": 2034
    },
    {
      "epoch": 0.5793594306049822,
      "grad_norm": 0.4332405924797058,
      "learning_rate": 0.00035525192143467124,
      "loss": 7.6035,
      "step": 2035
    },
    {
      "epoch": 0.579644128113879,
      "grad_norm": 0.45131269097328186,
      "learning_rate": 0.00035518075718758897,
      "loss": 7.7773,
      "step": 2036
    },
    {
      "epoch": 0.5799288256227758,
      "grad_norm": 0.5939685702323914,
      "learning_rate": 0.0003551095929405067,
      "loss": 6.9141,
      "step": 2037
    },
    {
      "epoch": 0.5802135231316726,
      "grad_norm": 0.5040512681007385,
      "learning_rate": 0.00035503842869342447,
      "loss": 7.583,
      "step": 2038
    },
    {
      "epoch": 0.5804982206405694,
      "grad_norm": 0.5753424167633057,
      "learning_rate": 0.00035496726444634213,
      "loss": 7.1895,
      "step": 2039
    },
    {
      "epoch": 0.5807829181494661,
      "grad_norm": 0.5665147304534912,
      "learning_rate": 0.0003548961001992599,
      "loss": 7.0928,
      "step": 2040
    },
    {
      "epoch": 0.581067615658363,
      "grad_norm": 0.7562616467475891,
      "learning_rate": 0.00035482493595217763,
      "loss": 7.5742,
      "step": 2041
    },
    {
      "epoch": 0.5813523131672598,
      "grad_norm": 0.6012423038482666,
      "learning_rate": 0.00035475377170509536,
      "loss": 6.4844,
      "step": 2042
    },
    {
      "epoch": 0.5816370106761566,
      "grad_norm": 0.5107037425041199,
      "learning_rate": 0.00035468260745801313,
      "loss": 7.2783,
      "step": 2043
    },
    {
      "epoch": 0.5819217081850534,
      "grad_norm": 0.5033462643623352,
      "learning_rate": 0.00035461144321093086,
      "loss": 7.4502,
      "step": 2044
    },
    {
      "epoch": 0.5822064056939502,
      "grad_norm": 0.40743377804756165,
      "learning_rate": 0.0003545402789638485,
      "loss": 7.8535,
      "step": 2045
    },
    {
      "epoch": 0.582491103202847,
      "grad_norm": 0.4928213059902191,
      "learning_rate": 0.0003544691147167663,
      "loss": 7.7344,
      "step": 2046
    },
    {
      "epoch": 0.5827758007117437,
      "grad_norm": 0.4693673253059387,
      "learning_rate": 0.000354397950469684,
      "loss": 7.9561,
      "step": 2047
    },
    {
      "epoch": 0.5830604982206405,
      "grad_norm": 0.7379196286201477,
      "learning_rate": 0.0003543267862226018,
      "loss": 6.7002,
      "step": 2048
    },
    {
      "epoch": 0.5833451957295374,
      "grad_norm": 0.4803205728530884,
      "learning_rate": 0.0003542556219755195,
      "loss": 7.4229,
      "step": 2049
    },
    {
      "epoch": 0.5836298932384342,
      "grad_norm": 0.5716486573219299,
      "learning_rate": 0.0003541844577284372,
      "loss": 6.9502,
      "step": 2050
    },
    {
      "epoch": 0.583914590747331,
      "grad_norm": 0.45838019251823425,
      "learning_rate": 0.00035411329348135497,
      "loss": 7.7568,
      "step": 2051
    },
    {
      "epoch": 0.5841992882562278,
      "grad_norm": 0.49467402696609497,
      "learning_rate": 0.0003540421292342727,
      "loss": 7.2793,
      "step": 2052
    },
    {
      "epoch": 0.5844839857651245,
      "grad_norm": 0.5489658713340759,
      "learning_rate": 0.00035397096498719047,
      "loss": 7.4727,
      "step": 2053
    },
    {
      "epoch": 0.5847686832740213,
      "grad_norm": 0.5034195780754089,
      "learning_rate": 0.0003538998007401082,
      "loss": 6.8535,
      "step": 2054
    },
    {
      "epoch": 0.5850533807829181,
      "grad_norm": 0.46720364689826965,
      "learning_rate": 0.0003538286364930259,
      "loss": 7.6396,
      "step": 2055
    },
    {
      "epoch": 0.585338078291815,
      "grad_norm": 0.5128947496414185,
      "learning_rate": 0.00035375747224594364,
      "loss": 7.3818,
      "step": 2056
    },
    {
      "epoch": 0.5856227758007118,
      "grad_norm": 0.5748533010482788,
      "learning_rate": 0.00035368630799886136,
      "loss": 6.8867,
      "step": 2057
    },
    {
      "epoch": 0.5859074733096086,
      "grad_norm": 0.5973435044288635,
      "learning_rate": 0.00035361514375177914,
      "loss": 7.2441,
      "step": 2058
    },
    {
      "epoch": 0.5861921708185054,
      "grad_norm": 0.49531224370002747,
      "learning_rate": 0.00035354397950469686,
      "loss": 7.3135,
      "step": 2059
    },
    {
      "epoch": 0.5864768683274021,
      "grad_norm": 0.48562198877334595,
      "learning_rate": 0.0003534728152576146,
      "loss": 7.6074,
      "step": 2060
    },
    {
      "epoch": 0.5867615658362989,
      "grad_norm": 0.5165866017341614,
      "learning_rate": 0.00035340165101053236,
      "loss": 7.5791,
      "step": 2061
    },
    {
      "epoch": 0.5870462633451957,
      "grad_norm": 0.45740577578544617,
      "learning_rate": 0.00035333048676345003,
      "loss": 7.4385,
      "step": 2062
    },
    {
      "epoch": 0.5873309608540925,
      "grad_norm": 0.5447055101394653,
      "learning_rate": 0.00035325932251636775,
      "loss": 7.8857,
      "step": 2063
    },
    {
      "epoch": 0.5876156583629893,
      "grad_norm": 0.6106885075569153,
      "learning_rate": 0.00035318815826928553,
      "loss": 6.9482,
      "step": 2064
    },
    {
      "epoch": 0.5879003558718862,
      "grad_norm": 0.5842505693435669,
      "learning_rate": 0.00035311699402220325,
      "loss": 6.8926,
      "step": 2065
    },
    {
      "epoch": 0.5881850533807829,
      "grad_norm": 0.49692773818969727,
      "learning_rate": 0.00035304582977512103,
      "loss": 7.5605,
      "step": 2066
    },
    {
      "epoch": 0.5884697508896797,
      "grad_norm": 0.6139780879020691,
      "learning_rate": 0.0003529746655280387,
      "loss": 6.7373,
      "step": 2067
    },
    {
      "epoch": 0.5887544483985765,
      "grad_norm": 0.5743252635002136,
      "learning_rate": 0.0003529035012809564,
      "loss": 7.2646,
      "step": 2068
    },
    {
      "epoch": 0.5890391459074733,
      "grad_norm": 0.5680800080299377,
      "learning_rate": 0.0003528323370338742,
      "loss": 7.2871,
      "step": 2069
    },
    {
      "epoch": 0.5893238434163701,
      "grad_norm": 0.5095707774162292,
      "learning_rate": 0.0003527611727867919,
      "loss": 7.4971,
      "step": 2070
    },
    {
      "epoch": 0.5896085409252669,
      "grad_norm": 0.896400511264801,
      "learning_rate": 0.0003526900085397097,
      "loss": 6.6104,
      "step": 2071
    },
    {
      "epoch": 0.5898932384341637,
      "grad_norm": 0.4993736445903778,
      "learning_rate": 0.0003526188442926274,
      "loss": 6.9717,
      "step": 2072
    },
    {
      "epoch": 0.5901779359430604,
      "grad_norm": 0.5368587374687195,
      "learning_rate": 0.0003525476800455451,
      "loss": 6.9668,
      "step": 2073
    },
    {
      "epoch": 0.5904626334519573,
      "grad_norm": 0.8063827753067017,
      "learning_rate": 0.00035247651579846287,
      "loss": 6.4941,
      "step": 2074
    },
    {
      "epoch": 0.5907473309608541,
      "grad_norm": 0.5529154539108276,
      "learning_rate": 0.0003524053515513806,
      "loss": 7.1484,
      "step": 2075
    },
    {
      "epoch": 0.5910320284697509,
      "grad_norm": 0.47062167525291443,
      "learning_rate": 0.0003523341873042983,
      "loss": 7.8848,
      "step": 2076
    },
    {
      "epoch": 0.5913167259786477,
      "grad_norm": 0.4873555600643158,
      "learning_rate": 0.0003522630230572161,
      "loss": 7.2012,
      "step": 2077
    },
    {
      "epoch": 0.5916014234875445,
      "grad_norm": 0.6792465448379517,
      "learning_rate": 0.00035219185881013376,
      "loss": 6.8574,
      "step": 2078
    },
    {
      "epoch": 0.5918861209964413,
      "grad_norm": 0.5873688459396362,
      "learning_rate": 0.00035212069456305153,
      "loss": 7.0488,
      "step": 2079
    },
    {
      "epoch": 0.592170818505338,
      "grad_norm": 0.39383015036582947,
      "learning_rate": 0.00035204953031596926,
      "loss": 8.1514,
      "step": 2080
    },
    {
      "epoch": 0.5924555160142349,
      "grad_norm": 0.5332582592964172,
      "learning_rate": 0.000351978366068887,
      "loss": 7.3662,
      "step": 2081
    },
    {
      "epoch": 0.5927402135231317,
      "grad_norm": 0.5531021952629089,
      "learning_rate": 0.00035190720182180476,
      "loss": 7.2773,
      "step": 2082
    },
    {
      "epoch": 0.5930249110320285,
      "grad_norm": 0.5053843855857849,
      "learning_rate": 0.0003518360375747225,
      "loss": 7.1855,
      "step": 2083
    },
    {
      "epoch": 0.5933096085409253,
      "grad_norm": 0.45989909768104553,
      "learning_rate": 0.0003517648733276402,
      "loss": 7.4287,
      "step": 2084
    },
    {
      "epoch": 0.5935943060498221,
      "grad_norm": 0.4866672456264496,
      "learning_rate": 0.0003516937090805579,
      "loss": 7.585,
      "step": 2085
    },
    {
      "epoch": 0.5938790035587188,
      "grad_norm": 0.5517799854278564,
      "learning_rate": 0.00035162254483347565,
      "loss": 7.2832,
      "step": 2086
    },
    {
      "epoch": 0.5941637010676156,
      "grad_norm": 0.5337896347045898,
      "learning_rate": 0.0003515513805863934,
      "loss": 7.7295,
      "step": 2087
    },
    {
      "epoch": 0.5944483985765124,
      "grad_norm": 0.4845709800720215,
      "learning_rate": 0.00035148021633931115,
      "loss": 7.6777,
      "step": 2088
    },
    {
      "epoch": 0.5947330960854093,
      "grad_norm": 0.6379998326301575,
      "learning_rate": 0.0003514090520922289,
      "loss": 7.0889,
      "step": 2089
    },
    {
      "epoch": 0.5950177935943061,
      "grad_norm": 0.47309526801109314,
      "learning_rate": 0.0003513378878451466,
      "loss": 7.207,
      "step": 2090
    },
    {
      "epoch": 0.5953024911032029,
      "grad_norm": 0.5866497755050659,
      "learning_rate": 0.0003512667235980643,
      "loss": 7.5488,
      "step": 2091
    },
    {
      "epoch": 0.5955871886120997,
      "grad_norm": 0.42400386929512024,
      "learning_rate": 0.0003511955593509821,
      "loss": 7.8086,
      "step": 2092
    },
    {
      "epoch": 0.5958718861209964,
      "grad_norm": 0.491928368806839,
      "learning_rate": 0.0003511243951038998,
      "loss": 7.5957,
      "step": 2093
    },
    {
      "epoch": 0.5961565836298932,
      "grad_norm": 0.5717054605484009,
      "learning_rate": 0.00035105323085681754,
      "loss": 7.4238,
      "step": 2094
    },
    {
      "epoch": 0.59644128113879,
      "grad_norm": 0.5866520404815674,
      "learning_rate": 0.00035098206660973526,
      "loss": 6.9717,
      "step": 2095
    },
    {
      "epoch": 0.5967259786476868,
      "grad_norm": 0.5479311347007751,
      "learning_rate": 0.000350910902362653,
      "loss": 7.3965,
      "step": 2096
    },
    {
      "epoch": 0.5970106761565837,
      "grad_norm": 0.6484185457229614,
      "learning_rate": 0.00035083973811557076,
      "loss": 6.4531,
      "step": 2097
    },
    {
      "epoch": 0.5972953736654805,
      "grad_norm": 0.48248234391212463,
      "learning_rate": 0.0003507685738684885,
      "loss": 7.6416,
      "step": 2098
    },
    {
      "epoch": 0.5975800711743772,
      "grad_norm": 0.5445560812950134,
      "learning_rate": 0.0003506974096214062,
      "loss": 7.3369,
      "step": 2099
    },
    {
      "epoch": 0.597864768683274,
      "grad_norm": 0.5570351481437683,
      "learning_rate": 0.000350626245374324,
      "loss": 7.1934,
      "step": 2100
    },
    {
      "epoch": 0.5981494661921708,
      "grad_norm": 0.5478289723396301,
      "learning_rate": 0.00035055508112724165,
      "loss": 7.2461,
      "step": 2101
    },
    {
      "epoch": 0.5984341637010676,
      "grad_norm": 0.5014203786849976,
      "learning_rate": 0.00035048391688015943,
      "loss": 7.3496,
      "step": 2102
    },
    {
      "epoch": 0.5987188612099644,
      "grad_norm": 0.6860536932945251,
      "learning_rate": 0.00035041275263307715,
      "loss": 7.1865,
      "step": 2103
    },
    {
      "epoch": 0.5990035587188612,
      "grad_norm": 0.6474937796592712,
      "learning_rate": 0.0003503415883859949,
      "loss": 6.8711,
      "step": 2104
    },
    {
      "epoch": 0.599288256227758,
      "grad_norm": 0.47246089577674866,
      "learning_rate": 0.00035027042413891265,
      "loss": 7.2148,
      "step": 2105
    },
    {
      "epoch": 0.5995729537366548,
      "grad_norm": 0.5702275037765503,
      "learning_rate": 0.0003501992598918303,
      "loss": 6.8945,
      "step": 2106
    },
    {
      "epoch": 0.5998576512455516,
      "grad_norm": 0.40596503019332886,
      "learning_rate": 0.00035012809564474804,
      "loss": 7.9541,
      "step": 2107
    },
    {
      "epoch": 0.6001423487544484,
      "grad_norm": 0.5930145382881165,
      "learning_rate": 0.0003500569313976658,
      "loss": 6.9639,
      "step": 2108
    },
    {
      "epoch": 0.6004270462633452,
      "grad_norm": 0.617525577545166,
      "learning_rate": 0.00034998576715058354,
      "loss": 7.0625,
      "step": 2109
    },
    {
      "epoch": 0.600711743772242,
      "grad_norm": 0.6312153935432434,
      "learning_rate": 0.0003499146029035013,
      "loss": 7.5361,
      "step": 2110
    },
    {
      "epoch": 0.6009964412811388,
      "grad_norm": 0.5454759001731873,
      "learning_rate": 0.00034984343865641904,
      "loss": 7.3086,
      "step": 2111
    },
    {
      "epoch": 0.6012811387900356,
      "grad_norm": 0.5005685091018677,
      "learning_rate": 0.0003497722744093367,
      "loss": 7.5273,
      "step": 2112
    },
    {
      "epoch": 0.6015658362989323,
      "grad_norm": 0.6094414591789246,
      "learning_rate": 0.0003497011101622545,
      "loss": 7.1025,
      "step": 2113
    },
    {
      "epoch": 0.6018505338078292,
      "grad_norm": 0.6047765016555786,
      "learning_rate": 0.0003496299459151722,
      "loss": 7.0469,
      "step": 2114
    },
    {
      "epoch": 0.602135231316726,
      "grad_norm": 0.5072075128555298,
      "learning_rate": 0.00034955878166809,
      "loss": 7.8574,
      "step": 2115
    },
    {
      "epoch": 0.6024199288256228,
      "grad_norm": 0.6358774304389954,
      "learning_rate": 0.0003494876174210077,
      "loss": 7.5156,
      "step": 2116
    },
    {
      "epoch": 0.6027046263345196,
      "grad_norm": 0.5350812077522278,
      "learning_rate": 0.00034941645317392543,
      "loss": 7.4121,
      "step": 2117
    },
    {
      "epoch": 0.6029893238434164,
      "grad_norm": 0.486637145280838,
      "learning_rate": 0.00034934528892684315,
      "loss": 7.4844,
      "step": 2118
    },
    {
      "epoch": 0.6032740213523131,
      "grad_norm": 0.5685145258903503,
      "learning_rate": 0.0003492741246797609,
      "loss": 6.7461,
      "step": 2119
    },
    {
      "epoch": 0.6035587188612099,
      "grad_norm": 0.47627735137939453,
      "learning_rate": 0.00034920296043267865,
      "loss": 7.2236,
      "step": 2120
    },
    {
      "epoch": 0.6038434163701067,
      "grad_norm": 0.44288116693496704,
      "learning_rate": 0.0003491317961855964,
      "loss": 7.7178,
      "step": 2121
    },
    {
      "epoch": 0.6041281138790036,
      "grad_norm": 0.45927396416664124,
      "learning_rate": 0.0003490606319385141,
      "loss": 7.6787,
      "step": 2122
    },
    {
      "epoch": 0.6044128113879004,
      "grad_norm": 0.6424807906150818,
      "learning_rate": 0.0003489894676914318,
      "loss": 7.0068,
      "step": 2123
    },
    {
      "epoch": 0.6046975088967972,
      "grad_norm": 0.49789246916770935,
      "learning_rate": 0.00034891830344434954,
      "loss": 7.5869,
      "step": 2124
    },
    {
      "epoch": 0.604982206405694,
      "grad_norm": 0.5254243016242981,
      "learning_rate": 0.00034884713919726727,
      "loss": 7.6221,
      "step": 2125
    },
    {
      "epoch": 0.6052669039145907,
      "grad_norm": 0.6453412771224976,
      "learning_rate": 0.00034877597495018504,
      "loss": 6.4648,
      "step": 2126
    },
    {
      "epoch": 0.6055516014234875,
      "grad_norm": 0.516929030418396,
      "learning_rate": 0.00034870481070310277,
      "loss": 7.4844,
      "step": 2127
    },
    {
      "epoch": 0.6058362989323843,
      "grad_norm": 0.4611877501010895,
      "learning_rate": 0.00034863364645602054,
      "loss": 7.6621,
      "step": 2128
    },
    {
      "epoch": 0.6061209964412811,
      "grad_norm": 0.46393656730651855,
      "learning_rate": 0.0003485624822089382,
      "loss": 7.6914,
      "step": 2129
    },
    {
      "epoch": 0.606405693950178,
      "grad_norm": 0.5048043727874756,
      "learning_rate": 0.00034849131796185594,
      "loss": 7.5166,
      "step": 2130
    },
    {
      "epoch": 0.6066903914590748,
      "grad_norm": 0.5202438235282898,
      "learning_rate": 0.0003484201537147737,
      "loss": 7.1406,
      "step": 2131
    },
    {
      "epoch": 0.6069750889679715,
      "grad_norm": 0.5707917809486389,
      "learning_rate": 0.00034834898946769144,
      "loss": 7.1123,
      "step": 2132
    },
    {
      "epoch": 0.6072597864768683,
      "grad_norm": 0.5565769076347351,
      "learning_rate": 0.0003482778252206092,
      "loss": 6.6875,
      "step": 2133
    },
    {
      "epoch": 0.6075444839857651,
      "grad_norm": 0.4910077452659607,
      "learning_rate": 0.00034820666097352694,
      "loss": 7.4756,
      "step": 2134
    },
    {
      "epoch": 0.6078291814946619,
      "grad_norm": 0.5626629590988159,
      "learning_rate": 0.0003481354967264446,
      "loss": 7.5127,
      "step": 2135
    },
    {
      "epoch": 0.6081138790035587,
      "grad_norm": 0.5074114203453064,
      "learning_rate": 0.0003480643324793624,
      "loss": 7.2012,
      "step": 2136
    },
    {
      "epoch": 0.6083985765124555,
      "grad_norm": 0.5298428535461426,
      "learning_rate": 0.0003479931682322801,
      "loss": 7.7109,
      "step": 2137
    },
    {
      "epoch": 0.6086832740213524,
      "grad_norm": 0.6115362644195557,
      "learning_rate": 0.0003479220039851979,
      "loss": 7.0977,
      "step": 2138
    },
    {
      "epoch": 0.6089679715302491,
      "grad_norm": 0.431292861700058,
      "learning_rate": 0.0003478508397381156,
      "loss": 7.6045,
      "step": 2139
    },
    {
      "epoch": 0.6092526690391459,
      "grad_norm": 0.5121554136276245,
      "learning_rate": 0.00034777967549103327,
      "loss": 7.334,
      "step": 2140
    },
    {
      "epoch": 0.6095373665480427,
      "grad_norm": 0.5342856049537659,
      "learning_rate": 0.00034770851124395105,
      "loss": 7.2197,
      "step": 2141
    },
    {
      "epoch": 0.6098220640569395,
      "grad_norm": 0.4256729185581207,
      "learning_rate": 0.00034763734699686877,
      "loss": 7.5889,
      "step": 2142
    },
    {
      "epoch": 0.6101067615658363,
      "grad_norm": 0.5610166192054749,
      "learning_rate": 0.0003475661827497865,
      "loss": 7.1924,
      "step": 2143
    },
    {
      "epoch": 0.6103914590747331,
      "grad_norm": 0.5923747420310974,
      "learning_rate": 0.00034749501850270427,
      "loss": 6.7246,
      "step": 2144
    },
    {
      "epoch": 0.61067615658363,
      "grad_norm": 0.4393155574798584,
      "learning_rate": 0.000347423854255622,
      "loss": 7.5312,
      "step": 2145
    },
    {
      "epoch": 0.6109608540925267,
      "grad_norm": 0.5192463397979736,
      "learning_rate": 0.0003473526900085397,
      "loss": 7.1416,
      "step": 2146
    },
    {
      "epoch": 0.6112455516014235,
      "grad_norm": 0.5738014578819275,
      "learning_rate": 0.00034728152576145744,
      "loss": 7.5859,
      "step": 2147
    },
    {
      "epoch": 0.6115302491103203,
      "grad_norm": 0.43368011713027954,
      "learning_rate": 0.00034721036151437516,
      "loss": 7.5889,
      "step": 2148
    },
    {
      "epoch": 0.6118149466192171,
      "grad_norm": 0.5562120079994202,
      "learning_rate": 0.00034713919726729294,
      "loss": 7.1768,
      "step": 2149
    },
    {
      "epoch": 0.6120996441281139,
      "grad_norm": 0.41656413674354553,
      "learning_rate": 0.00034706803302021066,
      "loss": 7.8721,
      "step": 2150
    },
    {
      "epoch": 0.6123843416370107,
      "grad_norm": 0.5791816711425781,
      "learning_rate": 0.0003469968687731284,
      "loss": 7.4785,
      "step": 2151
    },
    {
      "epoch": 0.6126690391459074,
      "grad_norm": 0.4928486943244934,
      "learning_rate": 0.0003469257045260461,
      "loss": 6.998,
      "step": 2152
    },
    {
      "epoch": 0.6129537366548042,
      "grad_norm": 0.5168572068214417,
      "learning_rate": 0.00034685454027896383,
      "loss": 7.4395,
      "step": 2153
    },
    {
      "epoch": 0.613238434163701,
      "grad_norm": 0.5523490309715271,
      "learning_rate": 0.0003467833760318816,
      "loss": 7.2715,
      "step": 2154
    },
    {
      "epoch": 0.6135231316725979,
      "grad_norm": 0.5481563210487366,
      "learning_rate": 0.00034671221178479933,
      "loss": 7.4609,
      "step": 2155
    },
    {
      "epoch": 0.6138078291814947,
      "grad_norm": 0.5097735524177551,
      "learning_rate": 0.0003466410475377171,
      "loss": 7.2139,
      "step": 2156
    },
    {
      "epoch": 0.6140925266903915,
      "grad_norm": 0.5688567757606506,
      "learning_rate": 0.0003465698832906348,
      "loss": 7.7695,
      "step": 2157
    },
    {
      "epoch": 0.6143772241992883,
      "grad_norm": 0.6080267429351807,
      "learning_rate": 0.0003464987190435525,
      "loss": 7.4521,
      "step": 2158
    },
    {
      "epoch": 0.614661921708185,
      "grad_norm": 0.5230960845947266,
      "learning_rate": 0.0003464275547964703,
      "loss": 7.5762,
      "step": 2159
    },
    {
      "epoch": 0.6149466192170818,
      "grad_norm": 0.5455117225646973,
      "learning_rate": 0.000346356390549388,
      "loss": 7.3916,
      "step": 2160
    },
    {
      "epoch": 0.6152313167259786,
      "grad_norm": 0.5547946095466614,
      "learning_rate": 0.0003462852263023057,
      "loss": 7.0488,
      "step": 2161
    },
    {
      "epoch": 0.6155160142348755,
      "grad_norm": 0.5069556832313538,
      "learning_rate": 0.0003462140620552235,
      "loss": 7.2734,
      "step": 2162
    },
    {
      "epoch": 0.6158007117437723,
      "grad_norm": 0.4696637690067291,
      "learning_rate": 0.00034614289780814117,
      "loss": 7.7881,
      "step": 2163
    },
    {
      "epoch": 0.6160854092526691,
      "grad_norm": 0.5057565569877625,
      "learning_rate": 0.00034607173356105894,
      "loss": 7.5703,
      "step": 2164
    },
    {
      "epoch": 0.6163701067615658,
      "grad_norm": 0.4836486279964447,
      "learning_rate": 0.00034600056931397667,
      "loss": 7.7119,
      "step": 2165
    },
    {
      "epoch": 0.6166548042704626,
      "grad_norm": 0.6151525378227234,
      "learning_rate": 0.0003459294050668944,
      "loss": 7.0391,
      "step": 2166
    },
    {
      "epoch": 0.6169395017793594,
      "grad_norm": 0.5067072510719299,
      "learning_rate": 0.00034585824081981217,
      "loss": 7.459,
      "step": 2167
    },
    {
      "epoch": 0.6172241992882562,
      "grad_norm": 0.6069993376731873,
      "learning_rate": 0.00034578707657272983,
      "loss": 6.3975,
      "step": 2168
    },
    {
      "epoch": 0.617508896797153,
      "grad_norm": 0.5203698873519897,
      "learning_rate": 0.0003457159123256476,
      "loss": 7.2441,
      "step": 2169
    },
    {
      "epoch": 0.6177935943060499,
      "grad_norm": 0.3850257098674774,
      "learning_rate": 0.00034564474807856533,
      "loss": 8.2393,
      "step": 2170
    },
    {
      "epoch": 0.6180782918149467,
      "grad_norm": 0.4491412043571472,
      "learning_rate": 0.00034557358383148306,
      "loss": 7.6846,
      "step": 2171
    },
    {
      "epoch": 0.6183629893238434,
      "grad_norm": 0.6185278296470642,
      "learning_rate": 0.00034550241958440083,
      "loss": 7.0449,
      "step": 2172
    },
    {
      "epoch": 0.6186476868327402,
      "grad_norm": 0.5225209593772888,
      "learning_rate": 0.00034543125533731856,
      "loss": 7.5029,
      "step": 2173
    },
    {
      "epoch": 0.618932384341637,
      "grad_norm": 0.6062451601028442,
      "learning_rate": 0.0003453600910902362,
      "loss": 6.9746,
      "step": 2174
    },
    {
      "epoch": 0.6192170818505338,
      "grad_norm": 0.5692175626754761,
      "learning_rate": 0.000345288926843154,
      "loss": 7.3057,
      "step": 2175
    },
    {
      "epoch": 0.6195017793594306,
      "grad_norm": 0.7507825493812561,
      "learning_rate": 0.0003452177625960717,
      "loss": 7.0664,
      "step": 2176
    },
    {
      "epoch": 0.6197864768683274,
      "grad_norm": 0.5483091473579407,
      "learning_rate": 0.0003451465983489895,
      "loss": 7.5576,
      "step": 2177
    },
    {
      "epoch": 0.6200711743772243,
      "grad_norm": 0.5689990520477295,
      "learning_rate": 0.0003450754341019072,
      "loss": 7.3818,
      "step": 2178
    },
    {
      "epoch": 0.620355871886121,
      "grad_norm": 0.5737230777740479,
      "learning_rate": 0.00034500426985482495,
      "loss": 7.168,
      "step": 2179
    },
    {
      "epoch": 0.6206405693950178,
      "grad_norm": 0.6294044852256775,
      "learning_rate": 0.00034493310560774267,
      "loss": 7.2705,
      "step": 2180
    },
    {
      "epoch": 0.6209252669039146,
      "grad_norm": 0.5089132189750671,
      "learning_rate": 0.0003448619413606604,
      "loss": 7.3428,
      "step": 2181
    },
    {
      "epoch": 0.6212099644128114,
      "grad_norm": 0.4789276421070099,
      "learning_rate": 0.00034479077711357817,
      "loss": 7.416,
      "step": 2182
    },
    {
      "epoch": 0.6214946619217082,
      "grad_norm": 0.5327168107032776,
      "learning_rate": 0.0003447196128664959,
      "loss": 7.1934,
      "step": 2183
    },
    {
      "epoch": 0.621779359430605,
      "grad_norm": 0.6093951463699341,
      "learning_rate": 0.0003446484486194136,
      "loss": 7.8867,
      "step": 2184
    },
    {
      "epoch": 0.6220640569395017,
      "grad_norm": 0.41979315876960754,
      "learning_rate": 0.00034457728437233134,
      "loss": 7.4541,
      "step": 2185
    },
    {
      "epoch": 0.6223487544483985,
      "grad_norm": 0.4761776328086853,
      "learning_rate": 0.00034450612012524906,
      "loss": 7.668,
      "step": 2186
    },
    {
      "epoch": 0.6226334519572954,
      "grad_norm": 0.49217426776885986,
      "learning_rate": 0.00034443495587816684,
      "loss": 7.2471,
      "step": 2187
    },
    {
      "epoch": 0.6229181494661922,
      "grad_norm": 0.44071730971336365,
      "learning_rate": 0.00034436379163108456,
      "loss": 7.9873,
      "step": 2188
    },
    {
      "epoch": 0.623202846975089,
      "grad_norm": 0.501250147819519,
      "learning_rate": 0.0003442926273840023,
      "loss": 6.8828,
      "step": 2189
    },
    {
      "epoch": 0.6234875444839858,
      "grad_norm": 0.5736002326011658,
      "learning_rate": 0.00034422146313692006,
      "loss": 7.1855,
      "step": 2190
    },
    {
      "epoch": 0.6237722419928826,
      "grad_norm": 0.49344316124916077,
      "learning_rate": 0.00034415029888983773,
      "loss": 7.4307,
      "step": 2191
    },
    {
      "epoch": 0.6240569395017793,
      "grad_norm": 0.5500075221061707,
      "learning_rate": 0.00034407913464275545,
      "loss": 7.2852,
      "step": 2192
    },
    {
      "epoch": 0.6243416370106761,
      "grad_norm": 0.46079370379447937,
      "learning_rate": 0.00034400797039567323,
      "loss": 7.2939,
      "step": 2193
    },
    {
      "epoch": 0.624626334519573,
      "grad_norm": 0.40938979387283325,
      "learning_rate": 0.00034393680614859095,
      "loss": 7.7061,
      "step": 2194
    },
    {
      "epoch": 0.6249110320284698,
      "grad_norm": 0.6053853631019592,
      "learning_rate": 0.00034386564190150873,
      "loss": 6.2139,
      "step": 2195
    },
    {
      "epoch": 0.6251957295373666,
      "grad_norm": 0.552024245262146,
      "learning_rate": 0.0003437944776544264,
      "loss": 7.1475,
      "step": 2196
    },
    {
      "epoch": 0.6254804270462634,
      "grad_norm": 0.45515403151512146,
      "learning_rate": 0.0003437233134073441,
      "loss": 7.8721,
      "step": 2197
    },
    {
      "epoch": 0.6257651245551601,
      "grad_norm": 0.47008493542671204,
      "learning_rate": 0.0003436521491602619,
      "loss": 7.4482,
      "step": 2198
    },
    {
      "epoch": 0.6260498220640569,
      "grad_norm": 0.5757606625556946,
      "learning_rate": 0.0003435809849131796,
      "loss": 7.3613,
      "step": 2199
    },
    {
      "epoch": 0.6263345195729537,
      "grad_norm": 0.5597023367881775,
      "learning_rate": 0.0003435098206660974,
      "loss": 7.0918,
      "step": 2200
    },
    {
      "epoch": 0.6263345195729537,
      "eval_bleu": 0.13287560068230989,
      "eval_loss": 7.1328125,
      "eval_runtime": 100.6647,
      "eval_samples_per_second": 2.821,
      "eval_steps_per_second": 0.179,
      "step": 2200
    },
    {
      "epoch": 0.6266192170818505,
      "grad_norm": 0.5530959367752075,
      "learning_rate": 0.0003434386564190151,
      "loss": 7.7461,
      "step": 2201
    },
    {
      "epoch": 0.6269039145907473,
      "grad_norm": 0.5981701612472534,
      "learning_rate": 0.0003433674921719328,
      "loss": 6.9199,
      "step": 2202
    },
    {
      "epoch": 0.6271886120996442,
      "grad_norm": 0.5055157542228699,
      "learning_rate": 0.00034329632792485056,
      "loss": 7.335,
      "step": 2203
    },
    {
      "epoch": 0.627473309608541,
      "grad_norm": 0.5935801267623901,
      "learning_rate": 0.0003432251636777683,
      "loss": 7.5439,
      "step": 2204
    },
    {
      "epoch": 0.6277580071174377,
      "grad_norm": 0.45453959703445435,
      "learning_rate": 0.000343153999430686,
      "loss": 8.2197,
      "step": 2205
    },
    {
      "epoch": 0.6280427046263345,
      "grad_norm": 0.55586177110672,
      "learning_rate": 0.0003430828351836038,
      "loss": 7.2695,
      "step": 2206
    },
    {
      "epoch": 0.6283274021352313,
      "grad_norm": 0.4503067433834076,
      "learning_rate": 0.0003430116709365215,
      "loss": 7.7305,
      "step": 2207
    },
    {
      "epoch": 0.6286120996441281,
      "grad_norm": 0.5060966610908508,
      "learning_rate": 0.00034294050668943923,
      "loss": 7.5537,
      "step": 2208
    },
    {
      "epoch": 0.6288967971530249,
      "grad_norm": 0.44377487897872925,
      "learning_rate": 0.00034286934244235696,
      "loss": 7.7207,
      "step": 2209
    },
    {
      "epoch": 0.6291814946619217,
      "grad_norm": 0.5119345188140869,
      "learning_rate": 0.0003427981781952747,
      "loss": 7.5625,
      "step": 2210
    },
    {
      "epoch": 0.6294661921708185,
      "grad_norm": 0.566845715045929,
      "learning_rate": 0.00034272701394819245,
      "loss": 7.4189,
      "step": 2211
    },
    {
      "epoch": 0.6297508896797153,
      "grad_norm": 0.5611362457275391,
      "learning_rate": 0.0003426558497011102,
      "loss": 7.4414,
      "step": 2212
    },
    {
      "epoch": 0.6300355871886121,
      "grad_norm": 0.6418138146400452,
      "learning_rate": 0.0003425846854540279,
      "loss": 6.8125,
      "step": 2213
    },
    {
      "epoch": 0.6303202846975089,
      "grad_norm": 0.48487016558647156,
      "learning_rate": 0.0003425135212069456,
      "loss": 7.6846,
      "step": 2214
    },
    {
      "epoch": 0.6306049822064057,
      "grad_norm": 0.5055760145187378,
      "learning_rate": 0.00034244235695986335,
      "loss": 7.7559,
      "step": 2215
    },
    {
      "epoch": 0.6308896797153025,
      "grad_norm": 0.7017578482627869,
      "learning_rate": 0.0003423711927127811,
      "loss": 6.5195,
      "step": 2216
    },
    {
      "epoch": 0.6311743772241993,
      "grad_norm": 0.6525343060493469,
      "learning_rate": 0.00034230002846569885,
      "loss": 6.7949,
      "step": 2217
    },
    {
      "epoch": 0.631459074733096,
      "grad_norm": 0.5343315005302429,
      "learning_rate": 0.0003422288642186166,
      "loss": 7.1582,
      "step": 2218
    },
    {
      "epoch": 0.6317437722419929,
      "grad_norm": 0.5380028486251831,
      "learning_rate": 0.0003421576999715343,
      "loss": 7.5547,
      "step": 2219
    },
    {
      "epoch": 0.6320284697508897,
      "grad_norm": 0.8130743503570557,
      "learning_rate": 0.000342086535724452,
      "loss": 5.9453,
      "step": 2220
    },
    {
      "epoch": 0.6323131672597865,
      "grad_norm": 0.5291591286659241,
      "learning_rate": 0.0003420153714773698,
      "loss": 6.9434,
      "step": 2221
    },
    {
      "epoch": 0.6325978647686833,
      "grad_norm": 0.45236918330192566,
      "learning_rate": 0.0003419442072302875,
      "loss": 7.7422,
      "step": 2222
    },
    {
      "epoch": 0.6328825622775801,
      "grad_norm": 0.7175799012184143,
      "learning_rate": 0.00034187304298320524,
      "loss": 6.8174,
      "step": 2223
    },
    {
      "epoch": 0.6331672597864769,
      "grad_norm": 0.527629017829895,
      "learning_rate": 0.000341801878736123,
      "loss": 7.5654,
      "step": 2224
    },
    {
      "epoch": 0.6334519572953736,
      "grad_norm": 0.5250352025032043,
      "learning_rate": 0.0003417307144890407,
      "loss": 7.7236,
      "step": 2225
    },
    {
      "epoch": 0.6337366548042704,
      "grad_norm": 0.5464637279510498,
      "learning_rate": 0.00034165955024195846,
      "loss": 7.5723,
      "step": 2226
    },
    {
      "epoch": 0.6340213523131673,
      "grad_norm": 0.47365373373031616,
      "learning_rate": 0.0003415883859948762,
      "loss": 7.835,
      "step": 2227
    },
    {
      "epoch": 0.6343060498220641,
      "grad_norm": 0.5280548334121704,
      "learning_rate": 0.0003415172217477939,
      "loss": 7.2344,
      "step": 2228
    },
    {
      "epoch": 0.6345907473309609,
      "grad_norm": 0.4568774700164795,
      "learning_rate": 0.0003414460575007117,
      "loss": 7.9023,
      "step": 2229
    },
    {
      "epoch": 0.6348754448398577,
      "grad_norm": 0.42387446761131287,
      "learning_rate": 0.00034137489325362935,
      "loss": 7.8271,
      "step": 2230
    },
    {
      "epoch": 0.6351601423487544,
      "grad_norm": 0.4962710738182068,
      "learning_rate": 0.0003413037290065471,
      "loss": 7.6387,
      "step": 2231
    },
    {
      "epoch": 0.6354448398576512,
      "grad_norm": 0.5621127486228943,
      "learning_rate": 0.00034123256475946485,
      "loss": 7.5977,
      "step": 2232
    },
    {
      "epoch": 0.635729537366548,
      "grad_norm": 0.6523125767707825,
      "learning_rate": 0.00034116140051238257,
      "loss": 7.0479,
      "step": 2233
    },
    {
      "epoch": 0.6360142348754448,
      "grad_norm": 0.5323299169540405,
      "learning_rate": 0.00034109023626530035,
      "loss": 6.9795,
      "step": 2234
    },
    {
      "epoch": 0.6362989323843417,
      "grad_norm": 0.5165249109268188,
      "learning_rate": 0.00034101907201821807,
      "loss": 7.1914,
      "step": 2235
    },
    {
      "epoch": 0.6365836298932385,
      "grad_norm": 0.49533525109291077,
      "learning_rate": 0.0003409479077711358,
      "loss": 7.6328,
      "step": 2236
    },
    {
      "epoch": 0.6368683274021353,
      "grad_norm": 0.47130367159843445,
      "learning_rate": 0.0003408767435240535,
      "loss": 7.6123,
      "step": 2237
    },
    {
      "epoch": 0.637153024911032,
      "grad_norm": 0.8031613826751709,
      "learning_rate": 0.00034080557927697124,
      "loss": 7.6816,
      "step": 2238
    },
    {
      "epoch": 0.6374377224199288,
      "grad_norm": 0.5580613613128662,
      "learning_rate": 0.000340734415029889,
      "loss": 7.3926,
      "step": 2239
    },
    {
      "epoch": 0.6377224199288256,
      "grad_norm": 0.6967948079109192,
      "learning_rate": 0.00034066325078280674,
      "loss": 6.4297,
      "step": 2240
    },
    {
      "epoch": 0.6380071174377224,
      "grad_norm": 0.5278670787811279,
      "learning_rate": 0.0003405920865357244,
      "loss": 6.8213,
      "step": 2241
    },
    {
      "epoch": 0.6382918149466192,
      "grad_norm": 0.49892371892929077,
      "learning_rate": 0.0003405209222886422,
      "loss": 7.2061,
      "step": 2242
    },
    {
      "epoch": 0.638576512455516,
      "grad_norm": 0.5185858011245728,
      "learning_rate": 0.0003404497580415599,
      "loss": 6.7139,
      "step": 2243
    },
    {
      "epoch": 0.6388612099644128,
      "grad_norm": 0.5726151466369629,
      "learning_rate": 0.0003403785937944777,
      "loss": 7.3594,
      "step": 2244
    },
    {
      "epoch": 0.6391459074733096,
      "grad_norm": 0.5117450952529907,
      "learning_rate": 0.0003403074295473954,
      "loss": 7.7832,
      "step": 2245
    },
    {
      "epoch": 0.6394306049822064,
      "grad_norm": 0.5724362134933472,
      "learning_rate": 0.00034023626530031313,
      "loss": 6.9775,
      "step": 2246
    },
    {
      "epoch": 0.6397153024911032,
      "grad_norm": 0.5516958832740784,
      "learning_rate": 0.00034016510105323085,
      "loss": 6.9697,
      "step": 2247
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.42785122990608215,
      "learning_rate": 0.0003400939368061486,
      "loss": 7.8418,
      "step": 2248
    },
    {
      "epoch": 0.6402846975088968,
      "grad_norm": 0.4782722294330597,
      "learning_rate": 0.00034002277255906635,
      "loss": 8.1426,
      "step": 2249
    },
    {
      "epoch": 0.6405693950177936,
      "grad_norm": 0.5566619038581848,
      "learning_rate": 0.0003399516083119841,
      "loss": 7.4961,
      "step": 2250
    },
    {
      "epoch": 0.6408540925266903,
      "grad_norm": 0.5516657829284668,
      "learning_rate": 0.0003398804440649018,
      "loss": 7.2812,
      "step": 2251
    },
    {
      "epoch": 0.6411387900355872,
      "grad_norm": 0.5913874506950378,
      "learning_rate": 0.0003398092798178196,
      "loss": 7.0244,
      "step": 2252
    },
    {
      "epoch": 0.641423487544484,
      "grad_norm": 0.5177945494651794,
      "learning_rate": 0.00033973811557073724,
      "loss": 7.3984,
      "step": 2253
    },
    {
      "epoch": 0.6417081850533808,
      "grad_norm": 0.49120068550109863,
      "learning_rate": 0.00033966695132365497,
      "loss": 7.5352,
      "step": 2254
    },
    {
      "epoch": 0.6419928825622776,
      "grad_norm": 0.550446093082428,
      "learning_rate": 0.00033959578707657274,
      "loss": 7.5801,
      "step": 2255
    },
    {
      "epoch": 0.6422775800711744,
      "grad_norm": 0.4962588846683502,
      "learning_rate": 0.00033952462282949047,
      "loss": 7.876,
      "step": 2256
    },
    {
      "epoch": 0.6425622775800712,
      "grad_norm": 0.8149594068527222,
      "learning_rate": 0.00033945345858240824,
      "loss": 7.1533,
      "step": 2257
    },
    {
      "epoch": 0.6428469750889679,
      "grad_norm": 0.530551552772522,
      "learning_rate": 0.0003393822943353259,
      "loss": 7.5137,
      "step": 2258
    },
    {
      "epoch": 0.6431316725978647,
      "grad_norm": 0.6212481260299683,
      "learning_rate": 0.00033931113008824363,
      "loss": 7.4053,
      "step": 2259
    },
    {
      "epoch": 0.6434163701067616,
      "grad_norm": 0.5940155386924744,
      "learning_rate": 0.0003392399658411614,
      "loss": 7.0625,
      "step": 2260
    },
    {
      "epoch": 0.6437010676156584,
      "grad_norm": 0.491619348526001,
      "learning_rate": 0.00033916880159407913,
      "loss": 7.4209,
      "step": 2261
    },
    {
      "epoch": 0.6439857651245552,
      "grad_norm": 0.49928030371665955,
      "learning_rate": 0.0003390976373469969,
      "loss": 8.0742,
      "step": 2262
    },
    {
      "epoch": 0.644270462633452,
      "grad_norm": 0.5286344885826111,
      "learning_rate": 0.00033902647309991463,
      "loss": 7.1387,
      "step": 2263
    },
    {
      "epoch": 0.6445551601423487,
      "grad_norm": 0.4610708951950073,
      "learning_rate": 0.0003389553088528323,
      "loss": 7.998,
      "step": 2264
    },
    {
      "epoch": 0.6448398576512455,
      "grad_norm": 0.5358865857124329,
      "learning_rate": 0.0003388841446057501,
      "loss": 7.3457,
      "step": 2265
    },
    {
      "epoch": 0.6451245551601423,
      "grad_norm": 0.598233699798584,
      "learning_rate": 0.0003388129803586678,
      "loss": 7.7861,
      "step": 2266
    },
    {
      "epoch": 0.6454092526690391,
      "grad_norm": 0.49692031741142273,
      "learning_rate": 0.0003387418161115856,
      "loss": 7.6465,
      "step": 2267
    },
    {
      "epoch": 0.645693950177936,
      "grad_norm": 0.5108835101127625,
      "learning_rate": 0.0003386706518645033,
      "loss": 7.7324,
      "step": 2268
    },
    {
      "epoch": 0.6459786476868328,
      "grad_norm": 0.6088387966156006,
      "learning_rate": 0.00033859948761742097,
      "loss": 6.9102,
      "step": 2269
    },
    {
      "epoch": 0.6462633451957296,
      "grad_norm": 0.4589509963989258,
      "learning_rate": 0.00033852832337033875,
      "loss": 7.6445,
      "step": 2270
    },
    {
      "epoch": 0.6465480427046263,
      "grad_norm": 0.5209418535232544,
      "learning_rate": 0.00033845715912325647,
      "loss": 7.4287,
      "step": 2271
    },
    {
      "epoch": 0.6468327402135231,
      "grad_norm": 0.4534274637699127,
      "learning_rate": 0.0003383859948761742,
      "loss": 7.7842,
      "step": 2272
    },
    {
      "epoch": 0.6471174377224199,
      "grad_norm": 1.020326852798462,
      "learning_rate": 0.00033831483062909197,
      "loss": 7.1016,
      "step": 2273
    },
    {
      "epoch": 0.6474021352313167,
      "grad_norm": 0.5115575790405273,
      "learning_rate": 0.0003382436663820097,
      "loss": 7.6416,
      "step": 2274
    },
    {
      "epoch": 0.6476868327402135,
      "grad_norm": 0.5519798994064331,
      "learning_rate": 0.0003381725021349274,
      "loss": 7.1357,
      "step": 2275
    },
    {
      "epoch": 0.6479715302491104,
      "grad_norm": 0.5168785452842712,
      "learning_rate": 0.00033810133788784514,
      "loss": 7.3428,
      "step": 2276
    },
    {
      "epoch": 0.6482562277580071,
      "grad_norm": 0.4821089208126068,
      "learning_rate": 0.00033803017364076286,
      "loss": 7.5107,
      "step": 2277
    },
    {
      "epoch": 0.6485409252669039,
      "grad_norm": 0.5485820770263672,
      "learning_rate": 0.00033795900939368064,
      "loss": 7.0635,
      "step": 2278
    },
    {
      "epoch": 0.6488256227758007,
      "grad_norm": 0.36052268743515015,
      "learning_rate": 0.00033788784514659836,
      "loss": 8.0645,
      "step": 2279
    },
    {
      "epoch": 0.6491103202846975,
      "grad_norm": 0.42885997891426086,
      "learning_rate": 0.00033781668089951614,
      "loss": 8.1523,
      "step": 2280
    },
    {
      "epoch": 0.6493950177935943,
      "grad_norm": 0.49766457080841064,
      "learning_rate": 0.0003377455166524338,
      "loss": 7.8027,
      "step": 2281
    },
    {
      "epoch": 0.6496797153024911,
      "grad_norm": 0.47775396704673767,
      "learning_rate": 0.00033767435240535153,
      "loss": 7.5195,
      "step": 2282
    },
    {
      "epoch": 0.649964412811388,
      "grad_norm": 0.4830781817436218,
      "learning_rate": 0.0003376031881582693,
      "loss": 7.6436,
      "step": 2283
    },
    {
      "epoch": 0.6502491103202847,
      "grad_norm": 0.49363309144973755,
      "learning_rate": 0.00033753202391118703,
      "loss": 7.3682,
      "step": 2284
    },
    {
      "epoch": 0.6505338078291815,
      "grad_norm": 0.4887140393257141,
      "learning_rate": 0.0003374608596641048,
      "loss": 7.3818,
      "step": 2285
    },
    {
      "epoch": 0.6508185053380783,
      "grad_norm": 0.4715929627418518,
      "learning_rate": 0.0003373896954170225,
      "loss": 7.8271,
      "step": 2286
    },
    {
      "epoch": 0.6511032028469751,
      "grad_norm": 0.5423536896705627,
      "learning_rate": 0.0003373185311699402,
      "loss": 7.4238,
      "step": 2287
    },
    {
      "epoch": 0.6513879003558719,
      "grad_norm": 0.47211354970932007,
      "learning_rate": 0.000337247366922858,
      "loss": 7.3379,
      "step": 2288
    },
    {
      "epoch": 0.6516725978647687,
      "grad_norm": 0.48700150847435,
      "learning_rate": 0.0003371762026757757,
      "loss": 7.5566,
      "step": 2289
    },
    {
      "epoch": 0.6519572953736655,
      "grad_norm": 0.51788330078125,
      "learning_rate": 0.0003371050384286934,
      "loss": 7.874,
      "step": 2290
    },
    {
      "epoch": 0.6522419928825622,
      "grad_norm": 0.5355291366577148,
      "learning_rate": 0.0003370338741816112,
      "loss": 7.3291,
      "step": 2291
    },
    {
      "epoch": 0.652526690391459,
      "grad_norm": 0.5080185532569885,
      "learning_rate": 0.00033696270993452887,
      "loss": 7.418,
      "step": 2292
    },
    {
      "epoch": 0.6528113879003559,
      "grad_norm": 0.5415621399879456,
      "learning_rate": 0.00033689154568744664,
      "loss": 7.501,
      "step": 2293
    },
    {
      "epoch": 0.6530960854092527,
      "grad_norm": 0.47751662135124207,
      "learning_rate": 0.00033682038144036437,
      "loss": 7.4209,
      "step": 2294
    },
    {
      "epoch": 0.6533807829181495,
      "grad_norm": 0.6230096817016602,
      "learning_rate": 0.0003367492171932821,
      "loss": 6.7041,
      "step": 2295
    },
    {
      "epoch": 0.6536654804270463,
      "grad_norm": 0.49479925632476807,
      "learning_rate": 0.00033667805294619986,
      "loss": 7.1641,
      "step": 2296
    },
    {
      "epoch": 0.653950177935943,
      "grad_norm": 0.6173176169395447,
      "learning_rate": 0.0003366068886991176,
      "loss": 6.8145,
      "step": 2297
    },
    {
      "epoch": 0.6542348754448398,
      "grad_norm": 0.6523106098175049,
      "learning_rate": 0.0003365357244520353,
      "loss": 7.2803,
      "step": 2298
    },
    {
      "epoch": 0.6545195729537366,
      "grad_norm": 0.6193196773529053,
      "learning_rate": 0.00033646456020495303,
      "loss": 6.7588,
      "step": 2299
    },
    {
      "epoch": 0.6548042704626335,
      "grad_norm": 0.5244598388671875,
      "learning_rate": 0.00033639339595787076,
      "loss": 7.2598,
      "step": 2300
    },
    {
      "epoch": 0.6550889679715303,
      "grad_norm": 0.659014105796814,
      "learning_rate": 0.00033632223171078853,
      "loss": 7.1836,
      "step": 2301
    },
    {
      "epoch": 0.6553736654804271,
      "grad_norm": 0.5242417454719543,
      "learning_rate": 0.00033625106746370626,
      "loss": 7.6309,
      "step": 2302
    },
    {
      "epoch": 0.6556583629893239,
      "grad_norm": 0.5243130922317505,
      "learning_rate": 0.0003361799032166239,
      "loss": 7.2646,
      "step": 2303
    },
    {
      "epoch": 0.6559430604982206,
      "grad_norm": 0.5155357718467712,
      "learning_rate": 0.0003361087389695417,
      "loss": 7.0674,
      "step": 2304
    },
    {
      "epoch": 0.6562277580071174,
      "grad_norm": 0.449266254901886,
      "learning_rate": 0.0003360375747224594,
      "loss": 7.5293,
      "step": 2305
    },
    {
      "epoch": 0.6565124555160142,
      "grad_norm": 0.4096229076385498,
      "learning_rate": 0.0003359664104753772,
      "loss": 7.9707,
      "step": 2306
    },
    {
      "epoch": 0.656797153024911,
      "grad_norm": 0.5661504864692688,
      "learning_rate": 0.0003358952462282949,
      "loss": 6.6533,
      "step": 2307
    },
    {
      "epoch": 0.6570818505338079,
      "grad_norm": 0.526755690574646,
      "learning_rate": 0.00033582408198121265,
      "loss": 7.3135,
      "step": 2308
    },
    {
      "epoch": 0.6573665480427047,
      "grad_norm": 0.5567693114280701,
      "learning_rate": 0.00033575291773413037,
      "loss": 7.1904,
      "step": 2309
    },
    {
      "epoch": 0.6576512455516014,
      "grad_norm": 0.4437178373336792,
      "learning_rate": 0.0003356817534870481,
      "loss": 7.2275,
      "step": 2310
    },
    {
      "epoch": 0.6579359430604982,
      "grad_norm": 0.4830579161643982,
      "learning_rate": 0.00033561058923996587,
      "loss": 7.1758,
      "step": 2311
    },
    {
      "epoch": 0.658220640569395,
      "grad_norm": 0.4752644896507263,
      "learning_rate": 0.0003355394249928836,
      "loss": 7.04,
      "step": 2312
    },
    {
      "epoch": 0.6585053380782918,
      "grad_norm": 0.4864455461502075,
      "learning_rate": 0.0003354682607458013,
      "loss": 7.5352,
      "step": 2313
    },
    {
      "epoch": 0.6587900355871886,
      "grad_norm": 0.5474832653999329,
      "learning_rate": 0.00033539709649871904,
      "loss": 7.1387,
      "step": 2314
    },
    {
      "epoch": 0.6590747330960854,
      "grad_norm": 0.41237911581993103,
      "learning_rate": 0.00033532593225163676,
      "loss": 7.6631,
      "step": 2315
    },
    {
      "epoch": 0.6593594306049823,
      "grad_norm": 0.6190535426139832,
      "learning_rate": 0.00033525476800455454,
      "loss": 6.917,
      "step": 2316
    },
    {
      "epoch": 0.659644128113879,
      "grad_norm": 0.46857327222824097,
      "learning_rate": 0.00033518360375747226,
      "loss": 7.6875,
      "step": 2317
    },
    {
      "epoch": 0.6599288256227758,
      "grad_norm": 0.6233308911323547,
      "learning_rate": 0.00033511243951039,
      "loss": 6.7314,
      "step": 2318
    },
    {
      "epoch": 0.6602135231316726,
      "grad_norm": 0.5557500123977661,
      "learning_rate": 0.00033504127526330776,
      "loss": 7.2695,
      "step": 2319
    },
    {
      "epoch": 0.6604982206405694,
      "grad_norm": 0.6447112560272217,
      "learning_rate": 0.00033497011101622543,
      "loss": 6.9551,
      "step": 2320
    },
    {
      "epoch": 0.6607829181494662,
      "grad_norm": 0.47477272152900696,
      "learning_rate": 0.00033489894676914315,
      "loss": 7.8281,
      "step": 2321
    },
    {
      "epoch": 0.661067615658363,
      "grad_norm": 0.4724421203136444,
      "learning_rate": 0.00033482778252206093,
      "loss": 7.751,
      "step": 2322
    },
    {
      "epoch": 0.6613523131672597,
      "grad_norm": 0.46375802159309387,
      "learning_rate": 0.00033475661827497865,
      "loss": 7.0107,
      "step": 2323
    },
    {
      "epoch": 0.6616370106761565,
      "grad_norm": 0.524549663066864,
      "learning_rate": 0.00033468545402789643,
      "loss": 7.3418,
      "step": 2324
    },
    {
      "epoch": 0.6619217081850534,
      "grad_norm": 0.5469258427619934,
      "learning_rate": 0.00033461428978081415,
      "loss": 7.6543,
      "step": 2325
    },
    {
      "epoch": 0.6622064056939502,
      "grad_norm": 0.5319260954856873,
      "learning_rate": 0.0003345431255337318,
      "loss": 7.3936,
      "step": 2326
    },
    {
      "epoch": 0.662491103202847,
      "grad_norm": 0.47819387912750244,
      "learning_rate": 0.0003344719612866496,
      "loss": 7.4814,
      "step": 2327
    },
    {
      "epoch": 0.6627758007117438,
      "grad_norm": 0.5634894967079163,
      "learning_rate": 0.0003344007970395673,
      "loss": 7.3096,
      "step": 2328
    },
    {
      "epoch": 0.6630604982206406,
      "grad_norm": 0.803426206111908,
      "learning_rate": 0.0003343296327924851,
      "loss": 7.9717,
      "step": 2329
    },
    {
      "epoch": 0.6633451957295373,
      "grad_norm": 0.5465373396873474,
      "learning_rate": 0.0003342584685454028,
      "loss": 7.2686,
      "step": 2330
    },
    {
      "epoch": 0.6636298932384341,
      "grad_norm": 0.4493846595287323,
      "learning_rate": 0.0003341873042983205,
      "loss": 7.9785,
      "step": 2331
    },
    {
      "epoch": 0.663914590747331,
      "grad_norm": 0.48055505752563477,
      "learning_rate": 0.00033411614005123826,
      "loss": 7.2207,
      "step": 2332
    },
    {
      "epoch": 0.6641992882562278,
      "grad_norm": 0.5335696339607239,
      "learning_rate": 0.000334044975804156,
      "loss": 7.3945,
      "step": 2333
    },
    {
      "epoch": 0.6644839857651246,
      "grad_norm": 0.4912032186985016,
      "learning_rate": 0.00033397381155707376,
      "loss": 7.9902,
      "step": 2334
    },
    {
      "epoch": 0.6647686832740214,
      "grad_norm": 0.34603360295295715,
      "learning_rate": 0.0003339026473099915,
      "loss": 7.8994,
      "step": 2335
    },
    {
      "epoch": 0.6650533807829182,
      "grad_norm": 0.47904396057128906,
      "learning_rate": 0.0003338314830629092,
      "loss": 7.6074,
      "step": 2336
    },
    {
      "epoch": 0.6653380782918149,
      "grad_norm": 0.48204609751701355,
      "learning_rate": 0.00033376031881582693,
      "loss": 7.3301,
      "step": 2337
    },
    {
      "epoch": 0.6656227758007117,
      "grad_norm": 0.5332012176513672,
      "learning_rate": 0.00033368915456874465,
      "loss": 7.2656,
      "step": 2338
    },
    {
      "epoch": 0.6659074733096085,
      "grad_norm": 0.540007472038269,
      "learning_rate": 0.0003336179903216624,
      "loss": 7.5908,
      "step": 2339
    },
    {
      "epoch": 0.6661921708185053,
      "grad_norm": 0.474321573972702,
      "learning_rate": 0.00033354682607458015,
      "loss": 7.4619,
      "step": 2340
    },
    {
      "epoch": 0.6664768683274022,
      "grad_norm": 0.5654955506324768,
      "learning_rate": 0.0003334756618274979,
      "loss": 7.5283,
      "step": 2341
    },
    {
      "epoch": 0.666761565836299,
      "grad_norm": 0.5407078862190247,
      "learning_rate": 0.00033340449758041565,
      "loss": 7.1494,
      "step": 2342
    },
    {
      "epoch": 0.6670462633451957,
      "grad_norm": 0.4155261814594269,
      "learning_rate": 0.0003333333333333333,
      "loss": 7.4814,
      "step": 2343
    },
    {
      "epoch": 0.6673309608540925,
      "grad_norm": 0.6322360038757324,
      "learning_rate": 0.00033326216908625104,
      "loss": 6.8779,
      "step": 2344
    },
    {
      "epoch": 0.6676156583629893,
      "grad_norm": 0.6207045912742615,
      "learning_rate": 0.0003331910048391688,
      "loss": 6.6729,
      "step": 2345
    },
    {
      "epoch": 0.6679003558718861,
      "grad_norm": 0.4776933491230011,
      "learning_rate": 0.00033311984059208654,
      "loss": 7.9014,
      "step": 2346
    },
    {
      "epoch": 0.6681850533807829,
      "grad_norm": 0.5356349349021912,
      "learning_rate": 0.0003330486763450043,
      "loss": 7.3896,
      "step": 2347
    },
    {
      "epoch": 0.6684697508896797,
      "grad_norm": 0.4867721498012543,
      "learning_rate": 0.000332977512097922,
      "loss": 7.873,
      "step": 2348
    },
    {
      "epoch": 0.6687544483985766,
      "grad_norm": 0.48482412099838257,
      "learning_rate": 0.0003329063478508397,
      "loss": 7.2061,
      "step": 2349
    },
    {
      "epoch": 0.6690391459074733,
      "grad_norm": 0.6009427905082703,
      "learning_rate": 0.0003328351836037575,
      "loss": 6.9131,
      "step": 2350
    },
    {
      "epoch": 0.6693238434163701,
      "grad_norm": 0.5925540328025818,
      "learning_rate": 0.0003327640193566752,
      "loss": 7.1953,
      "step": 2351
    },
    {
      "epoch": 0.6696085409252669,
      "grad_norm": 0.5818936228752136,
      "learning_rate": 0.00033269285510959294,
      "loss": 7.5547,
      "step": 2352
    },
    {
      "epoch": 0.6698932384341637,
      "grad_norm": 0.6883563995361328,
      "learning_rate": 0.0003326216908625107,
      "loss": 6.5547,
      "step": 2353
    },
    {
      "epoch": 0.6701779359430605,
      "grad_norm": 0.6582491993904114,
      "learning_rate": 0.0003325505266154284,
      "loss": 7.2998,
      "step": 2354
    },
    {
      "epoch": 0.6704626334519573,
      "grad_norm": 0.5136357545852661,
      "learning_rate": 0.00033247936236834616,
      "loss": 7.0244,
      "step": 2355
    },
    {
      "epoch": 0.670747330960854,
      "grad_norm": 0.48090219497680664,
      "learning_rate": 0.0003324081981212639,
      "loss": 7.8398,
      "step": 2356
    },
    {
      "epoch": 0.6710320284697509,
      "grad_norm": 0.46920713782310486,
      "learning_rate": 0.0003323370338741816,
      "loss": 7.8672,
      "step": 2357
    },
    {
      "epoch": 0.6713167259786477,
      "grad_norm": 0.4962335526943207,
      "learning_rate": 0.0003322658696270994,
      "loss": 7.3828,
      "step": 2358
    },
    {
      "epoch": 0.6716014234875445,
      "grad_norm": 0.46902936697006226,
      "learning_rate": 0.00033219470538001705,
      "loss": 7.5742,
      "step": 2359
    },
    {
      "epoch": 0.6718861209964413,
      "grad_norm": 0.46318742632865906,
      "learning_rate": 0.0003321235411329348,
      "loss": 7.6436,
      "step": 2360
    },
    {
      "epoch": 0.6721708185053381,
      "grad_norm": 0.3906967043876648,
      "learning_rate": 0.00033205237688585255,
      "loss": 7.7422,
      "step": 2361
    },
    {
      "epoch": 0.6724555160142349,
      "grad_norm": 0.5068019032478333,
      "learning_rate": 0.00033198121263877027,
      "loss": 7.4404,
      "step": 2362
    },
    {
      "epoch": 0.6727402135231316,
      "grad_norm": 0.44605696201324463,
      "learning_rate": 0.00033191004839168805,
      "loss": 7.8076,
      "step": 2363
    },
    {
      "epoch": 0.6730249110320284,
      "grad_norm": 0.5342697501182556,
      "learning_rate": 0.00033183888414460577,
      "loss": 7.4365,
      "step": 2364
    },
    {
      "epoch": 0.6733096085409253,
      "grad_norm": 0.5412485003471375,
      "learning_rate": 0.0003317677198975235,
      "loss": 7.7148,
      "step": 2365
    },
    {
      "epoch": 0.6735943060498221,
      "grad_norm": 0.5002887845039368,
      "learning_rate": 0.0003316965556504412,
      "loss": 7.4385,
      "step": 2366
    },
    {
      "epoch": 0.6738790035587189,
      "grad_norm": 0.45079633593559265,
      "learning_rate": 0.00033162539140335894,
      "loss": 7.9336,
      "step": 2367
    },
    {
      "epoch": 0.6741637010676157,
      "grad_norm": 0.5976175665855408,
      "learning_rate": 0.0003315542271562767,
      "loss": 6.8652,
      "step": 2368
    },
    {
      "epoch": 0.6744483985765125,
      "grad_norm": 0.6301334500312805,
      "learning_rate": 0.00033148306290919444,
      "loss": 7.5537,
      "step": 2369
    },
    {
      "epoch": 0.6747330960854092,
      "grad_norm": 0.5371695756912231,
      "learning_rate": 0.00033141189866211216,
      "loss": 7.5791,
      "step": 2370
    },
    {
      "epoch": 0.675017793594306,
      "grad_norm": 0.5207507610321045,
      "learning_rate": 0.0003313407344150299,
      "loss": 7.8486,
      "step": 2371
    },
    {
      "epoch": 0.6753024911032028,
      "grad_norm": 0.5720896124839783,
      "learning_rate": 0.0003312695701679476,
      "loss": 7.3232,
      "step": 2372
    },
    {
      "epoch": 0.6755871886120997,
      "grad_norm": 0.4481121897697449,
      "learning_rate": 0.0003311984059208654,
      "loss": 8.0625,
      "step": 2373
    },
    {
      "epoch": 0.6758718861209965,
      "grad_norm": 0.42948395013809204,
      "learning_rate": 0.0003311272416737831,
      "loss": 7.8193,
      "step": 2374
    },
    {
      "epoch": 0.6761565836298933,
      "grad_norm": 0.4894871711730957,
      "learning_rate": 0.00033105607742670083,
      "loss": 7.3154,
      "step": 2375
    },
    {
      "epoch": 0.67644128113879,
      "grad_norm": 0.6354722380638123,
      "learning_rate": 0.00033098491317961855,
      "loss": 6.915,
      "step": 2376
    },
    {
      "epoch": 0.6767259786476868,
      "grad_norm": 0.4813879132270813,
      "learning_rate": 0.0003309137489325363,
      "loss": 7.8115,
      "step": 2377
    },
    {
      "epoch": 0.6770106761565836,
      "grad_norm": 0.474843829870224,
      "learning_rate": 0.00033084258468545405,
      "loss": 7.5869,
      "step": 2378
    },
    {
      "epoch": 0.6772953736654804,
      "grad_norm": 0.5708481073379517,
      "learning_rate": 0.0003307714204383718,
      "loss": 7.252,
      "step": 2379
    },
    {
      "epoch": 0.6775800711743772,
      "grad_norm": 0.6503724455833435,
      "learning_rate": 0.0003307002561912895,
      "loss": 6.8975,
      "step": 2380
    },
    {
      "epoch": 0.677864768683274,
      "grad_norm": 0.4870607554912567,
      "learning_rate": 0.0003306290919442073,
      "loss": 7.4883,
      "step": 2381
    },
    {
      "epoch": 0.6781494661921709,
      "grad_norm": 0.571067214012146,
      "learning_rate": 0.00033055792769712494,
      "loss": 7.6104,
      "step": 2382
    },
    {
      "epoch": 0.6784341637010676,
      "grad_norm": 0.45373737812042236,
      "learning_rate": 0.0003304867634500427,
      "loss": 7.873,
      "step": 2383
    },
    {
      "epoch": 0.6787188612099644,
      "grad_norm": 0.5081170201301575,
      "learning_rate": 0.00033041559920296044,
      "loss": 7.5107,
      "step": 2384
    },
    {
      "epoch": 0.6790035587188612,
      "grad_norm": 0.5405246019363403,
      "learning_rate": 0.00033034443495587817,
      "loss": 6.9736,
      "step": 2385
    },
    {
      "epoch": 0.679288256227758,
      "grad_norm": 0.5232670307159424,
      "learning_rate": 0.00033027327070879594,
      "loss": 6.9502,
      "step": 2386
    },
    {
      "epoch": 0.6795729537366548,
      "grad_norm": 0.5093205571174622,
      "learning_rate": 0.00033020210646171367,
      "loss": 7.7061,
      "step": 2387
    },
    {
      "epoch": 0.6798576512455516,
      "grad_norm": 0.5074747800827026,
      "learning_rate": 0.00033013094221463133,
      "loss": 7.2803,
      "step": 2388
    },
    {
      "epoch": 0.6801423487544483,
      "grad_norm": 0.5424448847770691,
      "learning_rate": 0.0003300597779675491,
      "loss": 7.0615,
      "step": 2389
    },
    {
      "epoch": 0.6804270462633452,
      "grad_norm": 0.5697972178459167,
      "learning_rate": 0.00032998861372046683,
      "loss": 7.3779,
      "step": 2390
    },
    {
      "epoch": 0.680711743772242,
      "grad_norm": 0.525036096572876,
      "learning_rate": 0.0003299174494733846,
      "loss": 7.8457,
      "step": 2391
    },
    {
      "epoch": 0.6809964412811388,
      "grad_norm": 0.5301199555397034,
      "learning_rate": 0.00032984628522630233,
      "loss": 7.1494,
      "step": 2392
    },
    {
      "epoch": 0.6812811387900356,
      "grad_norm": 0.4997997581958771,
      "learning_rate": 0.00032977512097922,
      "loss": 7.5947,
      "step": 2393
    },
    {
      "epoch": 0.6815658362989324,
      "grad_norm": 0.4766846001148224,
      "learning_rate": 0.0003297039567321378,
      "loss": 7.7344,
      "step": 2394
    },
    {
      "epoch": 0.6818505338078292,
      "grad_norm": 0.5259794592857361,
      "learning_rate": 0.0003296327924850555,
      "loss": 7.4971,
      "step": 2395
    },
    {
      "epoch": 0.6821352313167259,
      "grad_norm": 0.462907999753952,
      "learning_rate": 0.0003295616282379733,
      "loss": 7.8066,
      "step": 2396
    },
    {
      "epoch": 0.6824199288256227,
      "grad_norm": 0.4880430996417999,
      "learning_rate": 0.000329490463990891,
      "loss": 7.5781,
      "step": 2397
    },
    {
      "epoch": 0.6827046263345196,
      "grad_norm": 0.4985828995704651,
      "learning_rate": 0.0003294192997438087,
      "loss": 7.4785,
      "step": 2398
    },
    {
      "epoch": 0.6829893238434164,
      "grad_norm": 0.5203863382339478,
      "learning_rate": 0.00032934813549672645,
      "loss": 7.4844,
      "step": 2399
    },
    {
      "epoch": 0.6832740213523132,
      "grad_norm": 4.599032402038574,
      "learning_rate": 0.00032927697124964417,
      "loss": 7.6904,
      "step": 2400
    },
    {
      "epoch": 0.6832740213523132,
      "eval_bleu": 0.13088770903999547,
      "eval_loss": 7.140625,
      "eval_runtime": 120.9258,
      "eval_samples_per_second": 2.349,
      "eval_steps_per_second": 0.149,
      "step": 2400
    },
    {
      "epoch": 0.68355871886121,
      "grad_norm": 0.5291078090667725,
      "learning_rate": 0.0003292058070025619,
      "loss": 7.1123,
      "step": 2401
    },
    {
      "epoch": 0.6838434163701068,
      "grad_norm": 0.5695016980171204,
      "learning_rate": 0.00032913464275547967,
      "loss": 7.2393,
      "step": 2402
    },
    {
      "epoch": 0.6841281138790035,
      "grad_norm": 0.4912911653518677,
      "learning_rate": 0.0003290634785083974,
      "loss": 7.8594,
      "step": 2403
    },
    {
      "epoch": 0.6844128113879003,
      "grad_norm": 0.48669469356536865,
      "learning_rate": 0.0003289923142613151,
      "loss": 7.7432,
      "step": 2404
    },
    {
      "epoch": 0.6846975088967971,
      "grad_norm": 0.5981013178825378,
      "learning_rate": 0.00032892115001423284,
      "loss": 8.0547,
      "step": 2405
    },
    {
      "epoch": 0.684982206405694,
      "grad_norm": 0.5165833234786987,
      "learning_rate": 0.00032884998576715056,
      "loss": 7.4111,
      "step": 2406
    },
    {
      "epoch": 0.6852669039145908,
      "grad_norm": 0.489007830619812,
      "learning_rate": 0.00032877882152006834,
      "loss": 7.5986,
      "step": 2407
    },
    {
      "epoch": 0.6855516014234876,
      "grad_norm": 0.5957891941070557,
      "learning_rate": 0.00032870765727298606,
      "loss": 6.9951,
      "step": 2408
    },
    {
      "epoch": 0.6858362989323843,
      "grad_norm": 0.5551978945732117,
      "learning_rate": 0.00032863649302590384,
      "loss": 7.7207,
      "step": 2409
    },
    {
      "epoch": 0.6861209964412811,
      "grad_norm": 0.5670446157455444,
      "learning_rate": 0.0003285653287788215,
      "loss": 7.7109,
      "step": 2410
    },
    {
      "epoch": 0.6864056939501779,
      "grad_norm": 0.5335172414779663,
      "learning_rate": 0.00032849416453173923,
      "loss": 7.3105,
      "step": 2411
    },
    {
      "epoch": 0.6866903914590747,
      "grad_norm": 0.5178844928741455,
      "learning_rate": 0.000328423000284657,
      "loss": 7.4385,
      "step": 2412
    },
    {
      "epoch": 0.6869750889679715,
      "grad_norm": 0.5215638279914856,
      "learning_rate": 0.00032835183603757473,
      "loss": 7.6895,
      "step": 2413
    },
    {
      "epoch": 0.6872597864768684,
      "grad_norm": 0.5652980804443359,
      "learning_rate": 0.0003282806717904925,
      "loss": 7.584,
      "step": 2414
    },
    {
      "epoch": 0.6875444839857652,
      "grad_norm": 0.5067718625068665,
      "learning_rate": 0.00032820950754341023,
      "loss": 7.5527,
      "step": 2415
    },
    {
      "epoch": 0.6878291814946619,
      "grad_norm": 0.5221681594848633,
      "learning_rate": 0.0003281383432963279,
      "loss": 7.2822,
      "step": 2416
    },
    {
      "epoch": 0.6881138790035587,
      "grad_norm": 0.5655103325843811,
      "learning_rate": 0.0003280671790492457,
      "loss": 7.3877,
      "step": 2417
    },
    {
      "epoch": 0.6883985765124555,
      "grad_norm": 0.5567444562911987,
      "learning_rate": 0.0003279960148021634,
      "loss": 6.9941,
      "step": 2418
    },
    {
      "epoch": 0.6886832740213523,
      "grad_norm": 0.5423716902732849,
      "learning_rate": 0.0003279248505550811,
      "loss": 7.3525,
      "step": 2419
    },
    {
      "epoch": 0.6889679715302491,
      "grad_norm": 0.6028076410293579,
      "learning_rate": 0.0003278536863079989,
      "loss": 7.2041,
      "step": 2420
    },
    {
      "epoch": 0.689252669039146,
      "grad_norm": 0.40490609407424927,
      "learning_rate": 0.00032778252206091656,
      "loss": 7.9053,
      "step": 2421
    },
    {
      "epoch": 0.6895373665480427,
      "grad_norm": 0.4942556917667389,
      "learning_rate": 0.00032771135781383434,
      "loss": 7.4385,
      "step": 2422
    },
    {
      "epoch": 0.6898220640569395,
      "grad_norm": 0.580564022064209,
      "learning_rate": 0.00032764019356675206,
      "loss": 6.8105,
      "step": 2423
    },
    {
      "epoch": 0.6901067615658363,
      "grad_norm": 0.4961944818496704,
      "learning_rate": 0.0003275690293196698,
      "loss": 7.5088,
      "step": 2424
    },
    {
      "epoch": 0.6903914590747331,
      "grad_norm": 0.5567737817764282,
      "learning_rate": 0.00032749786507258756,
      "loss": 7.043,
      "step": 2425
    },
    {
      "epoch": 0.6906761565836299,
      "grad_norm": 0.45999664068222046,
      "learning_rate": 0.0003274267008255053,
      "loss": 7.4668,
      "step": 2426
    },
    {
      "epoch": 0.6909608540925267,
      "grad_norm": 0.5199651122093201,
      "learning_rate": 0.000327355536578423,
      "loss": 7.5986,
      "step": 2427
    },
    {
      "epoch": 0.6912455516014235,
      "grad_norm": 0.5514554977416992,
      "learning_rate": 0.00032728437233134073,
      "loss": 7.0938,
      "step": 2428
    },
    {
      "epoch": 0.6915302491103202,
      "grad_norm": 0.5455131530761719,
      "learning_rate": 0.00032721320808425846,
      "loss": 7.417,
      "step": 2429
    },
    {
      "epoch": 0.691814946619217,
      "grad_norm": 0.6098621487617493,
      "learning_rate": 0.00032714204383717623,
      "loss": 7.1562,
      "step": 2430
    },
    {
      "epoch": 0.6920996441281139,
      "grad_norm": 0.5698403716087341,
      "learning_rate": 0.00032707087959009395,
      "loss": 7.0664,
      "step": 2431
    },
    {
      "epoch": 0.6923843416370107,
      "grad_norm": 0.5050579905509949,
      "learning_rate": 0.0003269997153430116,
      "loss": 7.4766,
      "step": 2432
    },
    {
      "epoch": 0.6926690391459075,
      "grad_norm": 0.6395690441131592,
      "learning_rate": 0.0003269285510959294,
      "loss": 6.9932,
      "step": 2433
    },
    {
      "epoch": 0.6929537366548043,
      "grad_norm": 0.4442979097366333,
      "learning_rate": 0.0003268573868488471,
      "loss": 7.8506,
      "step": 2434
    },
    {
      "epoch": 0.6932384341637011,
      "grad_norm": 0.4568585157394409,
      "learning_rate": 0.0003267862226017649,
      "loss": 7.9463,
      "step": 2435
    },
    {
      "epoch": 0.6935231316725978,
      "grad_norm": 0.5049718618392944,
      "learning_rate": 0.0003267150583546826,
      "loss": 7.4805,
      "step": 2436
    },
    {
      "epoch": 0.6938078291814946,
      "grad_norm": 0.487659215927124,
      "learning_rate": 0.00032664389410760035,
      "loss": 7.4424,
      "step": 2437
    },
    {
      "epoch": 0.6940925266903915,
      "grad_norm": 0.5680638551712036,
      "learning_rate": 0.00032657272986051807,
      "loss": 7.4688,
      "step": 2438
    },
    {
      "epoch": 0.6943772241992883,
      "grad_norm": 0.5356339812278748,
      "learning_rate": 0.0003265015656134358,
      "loss": 7.5352,
      "step": 2439
    },
    {
      "epoch": 0.6946619217081851,
      "grad_norm": 0.6688348650932312,
      "learning_rate": 0.00032643040136635357,
      "loss": 7.3721,
      "step": 2440
    },
    {
      "epoch": 0.6949466192170819,
      "grad_norm": 0.47483405470848083,
      "learning_rate": 0.0003263592371192713,
      "loss": 7.9307,
      "step": 2441
    },
    {
      "epoch": 0.6952313167259786,
      "grad_norm": 0.5095844864845276,
      "learning_rate": 0.000326288072872189,
      "loss": 7.6348,
      "step": 2442
    },
    {
      "epoch": 0.6955160142348754,
      "grad_norm": 0.46317967772483826,
      "learning_rate": 0.0003262169086251068,
      "loss": 7.5244,
      "step": 2443
    },
    {
      "epoch": 0.6958007117437722,
      "grad_norm": 0.4732758104801178,
      "learning_rate": 0.00032614574437802446,
      "loss": 7.2275,
      "step": 2444
    },
    {
      "epoch": 0.696085409252669,
      "grad_norm": 0.45463210344314575,
      "learning_rate": 0.00032607458013094224,
      "loss": 8.0078,
      "step": 2445
    },
    {
      "epoch": 0.6963701067615659,
      "grad_norm": 0.4577997326850891,
      "learning_rate": 0.00032600341588385996,
      "loss": 7.374,
      "step": 2446
    },
    {
      "epoch": 0.6966548042704627,
      "grad_norm": 0.5834449529647827,
      "learning_rate": 0.0003259322516367777,
      "loss": 7.0254,
      "step": 2447
    },
    {
      "epoch": 0.6969395017793595,
      "grad_norm": 0.5038715600967407,
      "learning_rate": 0.00032586108738969546,
      "loss": 7.7227,
      "step": 2448
    },
    {
      "epoch": 0.6972241992882562,
      "grad_norm": 0.5771576762199402,
      "learning_rate": 0.0003257899231426131,
      "loss": 7.3682,
      "step": 2449
    },
    {
      "epoch": 0.697508896797153,
      "grad_norm": 0.5379785895347595,
      "learning_rate": 0.00032571875889553085,
      "loss": 7.3193,
      "step": 2450
    },
    {
      "epoch": 0.6977935943060498,
      "grad_norm": 0.5493350028991699,
      "learning_rate": 0.0003256475946484486,
      "loss": 7.6426,
      "step": 2451
    },
    {
      "epoch": 0.6980782918149466,
      "grad_norm": 0.4517982006072998,
      "learning_rate": 0.00032557643040136635,
      "loss": 7.6719,
      "step": 2452
    },
    {
      "epoch": 0.6983629893238434,
      "grad_norm": 0.6194322109222412,
      "learning_rate": 0.0003255052661542841,
      "loss": 6.9062,
      "step": 2453
    },
    {
      "epoch": 0.6986476868327403,
      "grad_norm": 0.5672539472579956,
      "learning_rate": 0.00032543410190720185,
      "loss": 7.418,
      "step": 2454
    },
    {
      "epoch": 0.698932384341637,
      "grad_norm": 0.5473609566688538,
      "learning_rate": 0.0003253629376601195,
      "loss": 7.4854,
      "step": 2455
    },
    {
      "epoch": 0.6992170818505338,
      "grad_norm": 0.5478382706642151,
      "learning_rate": 0.0003252917734130373,
      "loss": 7.0479,
      "step": 2456
    },
    {
      "epoch": 0.6995017793594306,
      "grad_norm": 0.4817988872528076,
      "learning_rate": 0.000325220609165955,
      "loss": 7.4961,
      "step": 2457
    },
    {
      "epoch": 0.6997864768683274,
      "grad_norm": 0.5195891261100769,
      "learning_rate": 0.0003251494449188728,
      "loss": 7.4434,
      "step": 2458
    },
    {
      "epoch": 0.7000711743772242,
      "grad_norm": 0.4249008297920227,
      "learning_rate": 0.0003250782806717905,
      "loss": 7.9424,
      "step": 2459
    },
    {
      "epoch": 0.700355871886121,
      "grad_norm": 0.4435999393463135,
      "learning_rate": 0.00032500711642470824,
      "loss": 7.8525,
      "step": 2460
    },
    {
      "epoch": 0.7006405693950178,
      "grad_norm": 0.4299977123737335,
      "learning_rate": 0.00032493595217762596,
      "loss": 7.6748,
      "step": 2461
    },
    {
      "epoch": 0.7009252669039145,
      "grad_norm": 0.5531622767448425,
      "learning_rate": 0.0003248647879305437,
      "loss": 7.4512,
      "step": 2462
    },
    {
      "epoch": 0.7012099644128114,
      "grad_norm": 0.48327934741973877,
      "learning_rate": 0.00032479362368346146,
      "loss": 7.3096,
      "step": 2463
    },
    {
      "epoch": 0.7014946619217082,
      "grad_norm": 0.44077467918395996,
      "learning_rate": 0.0003247224594363792,
      "loss": 7.4961,
      "step": 2464
    },
    {
      "epoch": 0.701779359430605,
      "grad_norm": 0.5207807421684265,
      "learning_rate": 0.0003246512951892969,
      "loss": 7.4551,
      "step": 2465
    },
    {
      "epoch": 0.7020640569395018,
      "grad_norm": 0.45951128005981445,
      "learning_rate": 0.00032458013094221463,
      "loss": 7.6992,
      "step": 2466
    },
    {
      "epoch": 0.7023487544483986,
      "grad_norm": 0.5466511249542236,
      "learning_rate": 0.00032450896669513235,
      "loss": 7.2676,
      "step": 2467
    },
    {
      "epoch": 0.7026334519572953,
      "grad_norm": 0.48531726002693176,
      "learning_rate": 0.0003244378024480501,
      "loss": 7.3965,
      "step": 2468
    },
    {
      "epoch": 0.7029181494661921,
      "grad_norm": 0.49602001905441284,
      "learning_rate": 0.00032436663820096785,
      "loss": 7.2119,
      "step": 2469
    },
    {
      "epoch": 0.703202846975089,
      "grad_norm": 0.612349271774292,
      "learning_rate": 0.0003242954739538856,
      "loss": 6.5303,
      "step": 2470
    },
    {
      "epoch": 0.7034875444839858,
      "grad_norm": 0.4321024417877197,
      "learning_rate": 0.00032422430970680335,
      "loss": 7.9688,
      "step": 2471
    },
    {
      "epoch": 0.7037722419928826,
      "grad_norm": 0.5489293336868286,
      "learning_rate": 0.000324153145459721,
      "loss": 7.6045,
      "step": 2472
    },
    {
      "epoch": 0.7040569395017794,
      "grad_norm": 0.588467538356781,
      "learning_rate": 0.00032408198121263874,
      "loss": 7.0908,
      "step": 2473
    },
    {
      "epoch": 0.7043416370106762,
      "grad_norm": 0.5416799187660217,
      "learning_rate": 0.0003240108169655565,
      "loss": 7.2705,
      "step": 2474
    },
    {
      "epoch": 0.7046263345195729,
      "grad_norm": 0.8922178745269775,
      "learning_rate": 0.00032393965271847424,
      "loss": 6.3242,
      "step": 2475
    },
    {
      "epoch": 0.7049110320284697,
      "grad_norm": 0.5479971766471863,
      "learning_rate": 0.000323868488471392,
      "loss": 7.3818,
      "step": 2476
    },
    {
      "epoch": 0.7051957295373665,
      "grad_norm": 0.5427135825157166,
      "learning_rate": 0.0003237973242243097,
      "loss": 7.585,
      "step": 2477
    },
    {
      "epoch": 0.7054804270462633,
      "grad_norm": 0.5067457556724548,
      "learning_rate": 0.0003237261599772274,
      "loss": 7.4941,
      "step": 2478
    },
    {
      "epoch": 0.7057651245551602,
      "grad_norm": 0.5363481044769287,
      "learning_rate": 0.0003236549957301452,
      "loss": 7.2266,
      "step": 2479
    },
    {
      "epoch": 0.706049822064057,
      "grad_norm": 0.5168928503990173,
      "learning_rate": 0.0003235838314830629,
      "loss": 7.7686,
      "step": 2480
    },
    {
      "epoch": 0.7063345195729538,
      "grad_norm": 0.5572430491447449,
      "learning_rate": 0.0003235126672359807,
      "loss": 7.4053,
      "step": 2481
    },
    {
      "epoch": 0.7066192170818505,
      "grad_norm": 0.5149341821670532,
      "learning_rate": 0.0003234415029888984,
      "loss": 7.2969,
      "step": 2482
    },
    {
      "epoch": 0.7069039145907473,
      "grad_norm": 0.6848782300949097,
      "learning_rate": 0.0003233703387418161,
      "loss": 6.874,
      "step": 2483
    },
    {
      "epoch": 0.7071886120996441,
      "grad_norm": 0.6058078408241272,
      "learning_rate": 0.00032329917449473386,
      "loss": 7.3066,
      "step": 2484
    },
    {
      "epoch": 0.7074733096085409,
      "grad_norm": 0.54443359375,
      "learning_rate": 0.0003232280102476516,
      "loss": 7.5088,
      "step": 2485
    },
    {
      "epoch": 0.7077580071174377,
      "grad_norm": 0.4545068144798279,
      "learning_rate": 0.0003231568460005693,
      "loss": 7.332,
      "step": 2486
    },
    {
      "epoch": 0.7080427046263346,
      "grad_norm": 0.6422168612480164,
      "learning_rate": 0.0003230856817534871,
      "loss": 6.9941,
      "step": 2487
    },
    {
      "epoch": 0.7083274021352313,
      "grad_norm": 0.5583098530769348,
      "learning_rate": 0.0003230145175064048,
      "loss": 7.3994,
      "step": 2488
    },
    {
      "epoch": 0.7086120996441281,
      "grad_norm": 0.5985742211341858,
      "learning_rate": 0.0003229433532593225,
      "loss": 7.3682,
      "step": 2489
    },
    {
      "epoch": 0.7088967971530249,
      "grad_norm": 0.47018465399742126,
      "learning_rate": 0.00032287218901224025,
      "loss": 7.6973,
      "step": 2490
    },
    {
      "epoch": 0.7091814946619217,
      "grad_norm": 0.4802987277507782,
      "learning_rate": 0.00032280102476515797,
      "loss": 7.6484,
      "step": 2491
    },
    {
      "epoch": 0.7094661921708185,
      "grad_norm": 0.658785343170166,
      "learning_rate": 0.00032272986051807575,
      "loss": 6.5713,
      "step": 2492
    },
    {
      "epoch": 0.7097508896797153,
      "grad_norm": 0.5003043413162231,
      "learning_rate": 0.00032265869627099347,
      "loss": 7.7158,
      "step": 2493
    },
    {
      "epoch": 0.7100355871886121,
      "grad_norm": 0.5012388825416565,
      "learning_rate": 0.0003225875320239112,
      "loss": 8.001,
      "step": 2494
    },
    {
      "epoch": 0.7103202846975089,
      "grad_norm": 0.5033847093582153,
      "learning_rate": 0.0003225163677768289,
      "loss": 7.2197,
      "step": 2495
    },
    {
      "epoch": 0.7106049822064057,
      "grad_norm": 0.5201261043548584,
      "learning_rate": 0.00032244520352974664,
      "loss": 7.4658,
      "step": 2496
    },
    {
      "epoch": 0.7108896797153025,
      "grad_norm": 0.5782834887504578,
      "learning_rate": 0.0003223740392826644,
      "loss": 7.5674,
      "step": 2497
    },
    {
      "epoch": 0.7111743772241993,
      "grad_norm": 0.5645745396614075,
      "learning_rate": 0.00032230287503558214,
      "loss": 7.2188,
      "step": 2498
    },
    {
      "epoch": 0.7114590747330961,
      "grad_norm": 0.49483171105384827,
      "learning_rate": 0.00032223171078849986,
      "loss": 7.1914,
      "step": 2499
    },
    {
      "epoch": 0.7117437722419929,
      "grad_norm": 0.4906511604785919,
      "learning_rate": 0.0003221605465414176,
      "loss": 7.4727,
      "step": 2500
    },
    {
      "epoch": 0.7120284697508896,
      "grad_norm": 0.4382246136665344,
      "learning_rate": 0.0003220893822943353,
      "loss": 7.6748,
      "step": 2501
    },
    {
      "epoch": 0.7123131672597864,
      "grad_norm": 0.4665907919406891,
      "learning_rate": 0.0003220182180472531,
      "loss": 7.6875,
      "step": 2502
    },
    {
      "epoch": 0.7125978647686833,
      "grad_norm": 0.552628219127655,
      "learning_rate": 0.0003219470538001708,
      "loss": 7.1357,
      "step": 2503
    },
    {
      "epoch": 0.7128825622775801,
      "grad_norm": 0.5044735670089722,
      "learning_rate": 0.00032187588955308853,
      "loss": 7.5371,
      "step": 2504
    },
    {
      "epoch": 0.7131672597864769,
      "grad_norm": 0.6448603868484497,
      "learning_rate": 0.0003218047253060063,
      "loss": 7.0977,
      "step": 2505
    },
    {
      "epoch": 0.7134519572953737,
      "grad_norm": 0.5014495253562927,
      "learning_rate": 0.000321733561058924,
      "loss": 7.6699,
      "step": 2506
    },
    {
      "epoch": 0.7137366548042705,
      "grad_norm": 0.5269450545310974,
      "learning_rate": 0.00032166239681184175,
      "loss": 8.0605,
      "step": 2507
    },
    {
      "epoch": 0.7140213523131672,
      "grad_norm": 0.4552445411682129,
      "learning_rate": 0.0003215912325647595,
      "loss": 7.9111,
      "step": 2508
    },
    {
      "epoch": 0.714306049822064,
      "grad_norm": 0.50946044921875,
      "learning_rate": 0.0003215200683176772,
      "loss": 7.8047,
      "step": 2509
    },
    {
      "epoch": 0.7145907473309608,
      "grad_norm": 0.5101521611213684,
      "learning_rate": 0.000321448904070595,
      "loss": 7.3662,
      "step": 2510
    },
    {
      "epoch": 0.7148754448398577,
      "grad_norm": 0.48125940561294556,
      "learning_rate": 0.00032137773982351264,
      "loss": 7.3232,
      "step": 2511
    },
    {
      "epoch": 0.7151601423487545,
      "grad_norm": 0.4662749767303467,
      "learning_rate": 0.0003213065755764304,
      "loss": 7.793,
      "step": 2512
    },
    {
      "epoch": 0.7154448398576513,
      "grad_norm": 0.5242834687232971,
      "learning_rate": 0.00032123541132934814,
      "loss": 7.4775,
      "step": 2513
    },
    {
      "epoch": 0.7157295373665481,
      "grad_norm": 0.5214455127716064,
      "learning_rate": 0.00032116424708226587,
      "loss": 7.7207,
      "step": 2514
    },
    {
      "epoch": 0.7160142348754448,
      "grad_norm": 0.5187191367149353,
      "learning_rate": 0.00032109308283518364,
      "loss": 7.3887,
      "step": 2515
    },
    {
      "epoch": 0.7162989323843416,
      "grad_norm": 1.1979396343231201,
      "learning_rate": 0.00032102191858810136,
      "loss": 7.7852,
      "step": 2516
    },
    {
      "epoch": 0.7165836298932384,
      "grad_norm": 0.546038806438446,
      "learning_rate": 0.00032095075434101903,
      "loss": 7.6377,
      "step": 2517
    },
    {
      "epoch": 0.7168683274021352,
      "grad_norm": 0.5767659544944763,
      "learning_rate": 0.0003208795900939368,
      "loss": 6.6592,
      "step": 2518
    },
    {
      "epoch": 0.717153024911032,
      "grad_norm": 0.526856005191803,
      "learning_rate": 0.00032080842584685453,
      "loss": 7.6436,
      "step": 2519
    },
    {
      "epoch": 0.7174377224199289,
      "grad_norm": 0.704961359500885,
      "learning_rate": 0.0003207372615997723,
      "loss": 7.2441,
      "step": 2520
    },
    {
      "epoch": 0.7177224199288256,
      "grad_norm": 0.45226889848709106,
      "learning_rate": 0.00032066609735269003,
      "loss": 7.9951,
      "step": 2521
    },
    {
      "epoch": 0.7180071174377224,
      "grad_norm": 0.5204233527183533,
      "learning_rate": 0.0003205949331056077,
      "loss": 7.4854,
      "step": 2522
    },
    {
      "epoch": 0.7182918149466192,
      "grad_norm": 0.49523013830184937,
      "learning_rate": 0.0003205237688585255,
      "loss": 7.3008,
      "step": 2523
    },
    {
      "epoch": 0.718576512455516,
      "grad_norm": 0.5574997663497925,
      "learning_rate": 0.0003204526046114432,
      "loss": 7.0771,
      "step": 2524
    },
    {
      "epoch": 0.7188612099644128,
      "grad_norm": 0.43019574880599976,
      "learning_rate": 0.000320381440364361,
      "loss": 7.7607,
      "step": 2525
    },
    {
      "epoch": 0.7191459074733096,
      "grad_norm": 0.4437674283981323,
      "learning_rate": 0.0003203102761172787,
      "loss": 7.7451,
      "step": 2526
    },
    {
      "epoch": 0.7194306049822065,
      "grad_norm": 0.5373892784118652,
      "learning_rate": 0.0003202391118701964,
      "loss": 7.8223,
      "step": 2527
    },
    {
      "epoch": 0.7197153024911032,
      "grad_norm": 0.6886398792266846,
      "learning_rate": 0.00032016794762311415,
      "loss": 7.3906,
      "step": 2528
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.4852343201637268,
      "learning_rate": 0.00032009678337603187,
      "loss": 7.7832,
      "step": 2529
    },
    {
      "epoch": 0.7202846975088968,
      "grad_norm": 0.4960104525089264,
      "learning_rate": 0.0003200256191289496,
      "loss": 7.3652,
      "step": 2530
    },
    {
      "epoch": 0.7205693950177936,
      "grad_norm": 0.5244605541229248,
      "learning_rate": 0.00031995445488186737,
      "loss": 7.4326,
      "step": 2531
    },
    {
      "epoch": 0.7208540925266904,
      "grad_norm": 0.510108232498169,
      "learning_rate": 0.0003198832906347851,
      "loss": 7.4443,
      "step": 2532
    },
    {
      "epoch": 0.7211387900355872,
      "grad_norm": 0.6572321057319641,
      "learning_rate": 0.00031981212638770287,
      "loss": 6.5303,
      "step": 2533
    },
    {
      "epoch": 0.7214234875444839,
      "grad_norm": 0.4834159314632416,
      "learning_rate": 0.00031974096214062054,
      "loss": 7.5742,
      "step": 2534
    },
    {
      "epoch": 0.7217081850533807,
      "grad_norm": 0.5568831562995911,
      "learning_rate": 0.00031966979789353826,
      "loss": 7.4502,
      "step": 2535
    },
    {
      "epoch": 0.7219928825622776,
      "grad_norm": 0.5373832583427429,
      "learning_rate": 0.00031959863364645604,
      "loss": 7.3809,
      "step": 2536
    },
    {
      "epoch": 0.7222775800711744,
      "grad_norm": 0.4732987880706787,
      "learning_rate": 0.00031952746939937376,
      "loss": 7.7334,
      "step": 2537
    },
    {
      "epoch": 0.7225622775800712,
      "grad_norm": 0.4523523449897766,
      "learning_rate": 0.00031945630515229154,
      "loss": 7.748,
      "step": 2538
    },
    {
      "epoch": 0.722846975088968,
      "grad_norm": 0.5473159551620483,
      "learning_rate": 0.0003193851409052092,
      "loss": 7.1377,
      "step": 2539
    },
    {
      "epoch": 0.7231316725978648,
      "grad_norm": 0.4055388569831848,
      "learning_rate": 0.00031931397665812693,
      "loss": 7.4297,
      "step": 2540
    },
    {
      "epoch": 0.7234163701067615,
      "grad_norm": 0.7909030914306641,
      "learning_rate": 0.0003192428124110447,
      "loss": 6.8271,
      "step": 2541
    },
    {
      "epoch": 0.7237010676156583,
      "grad_norm": 0.5730153918266296,
      "learning_rate": 0.00031917164816396243,
      "loss": 7.167,
      "step": 2542
    },
    {
      "epoch": 0.7239857651245551,
      "grad_norm": 0.45755961537361145,
      "learning_rate": 0.0003191004839168802,
      "loss": 7.9287,
      "step": 2543
    },
    {
      "epoch": 0.724270462633452,
      "grad_norm": 0.5494441986083984,
      "learning_rate": 0.00031902931966979793,
      "loss": 7.4141,
      "step": 2544
    },
    {
      "epoch": 0.7245551601423488,
      "grad_norm": 0.572081983089447,
      "learning_rate": 0.0003189581554227156,
      "loss": 6.7646,
      "step": 2545
    },
    {
      "epoch": 0.7248398576512456,
      "grad_norm": 0.5914965867996216,
      "learning_rate": 0.00031888699117563337,
      "loss": 7.2705,
      "step": 2546
    },
    {
      "epoch": 0.7251245551601424,
      "grad_norm": 0.5520585179328918,
      "learning_rate": 0.0003188158269285511,
      "loss": 7.7949,
      "step": 2547
    },
    {
      "epoch": 0.7254092526690391,
      "grad_norm": 0.5735935568809509,
      "learning_rate": 0.0003187446626814688,
      "loss": 7.124,
      "step": 2548
    },
    {
      "epoch": 0.7256939501779359,
      "grad_norm": 0.4828546345233917,
      "learning_rate": 0.0003186734984343866,
      "loss": 7.7803,
      "step": 2549
    },
    {
      "epoch": 0.7259786476868327,
      "grad_norm": 0.48443323373794556,
      "learning_rate": 0.0003186023341873043,
      "loss": 7.6084,
      "step": 2550
    },
    {
      "epoch": 0.7262633451957295,
      "grad_norm": 0.48279136419296265,
      "learning_rate": 0.00031853116994022204,
      "loss": 7.541,
      "step": 2551
    },
    {
      "epoch": 0.7265480427046264,
      "grad_norm": 0.5407781004905701,
      "learning_rate": 0.00031846000569313976,
      "loss": 6.9199,
      "step": 2552
    },
    {
      "epoch": 0.7268327402135232,
      "grad_norm": 0.484910249710083,
      "learning_rate": 0.0003183888414460575,
      "loss": 7.9697,
      "step": 2553
    },
    {
      "epoch": 0.7271174377224199,
      "grad_norm": 0.4652705788612366,
      "learning_rate": 0.00031831767719897526,
      "loss": 7.4219,
      "step": 2554
    },
    {
      "epoch": 0.7274021352313167,
      "grad_norm": 0.6890383958816528,
      "learning_rate": 0.000318246512951893,
      "loss": 6.9521,
      "step": 2555
    },
    {
      "epoch": 0.7276868327402135,
      "grad_norm": 0.4598889648914337,
      "learning_rate": 0.0003181753487048107,
      "loss": 7.7344,
      "step": 2556
    },
    {
      "epoch": 0.7279715302491103,
      "grad_norm": 0.4337534010410309,
      "learning_rate": 0.00031810418445772843,
      "loss": 7.8535,
      "step": 2557
    },
    {
      "epoch": 0.7282562277580071,
      "grad_norm": 0.5306317806243896,
      "learning_rate": 0.00031803302021064615,
      "loss": 7.6562,
      "step": 2558
    },
    {
      "epoch": 0.728540925266904,
      "grad_norm": 0.5442900061607361,
      "learning_rate": 0.00031796185596356393,
      "loss": 7.4395,
      "step": 2559
    },
    {
      "epoch": 0.7288256227758008,
      "grad_norm": 0.5354232788085938,
      "learning_rate": 0.00031789069171648165,
      "loss": 7.7178,
      "step": 2560
    },
    {
      "epoch": 0.7291103202846975,
      "grad_norm": 0.5294575095176697,
      "learning_rate": 0.00031781952746939943,
      "loss": 7.5322,
      "step": 2561
    },
    {
      "epoch": 0.7293950177935943,
      "grad_norm": 0.5500622987747192,
      "learning_rate": 0.0003177483632223171,
      "loss": 7.4443,
      "step": 2562
    },
    {
      "epoch": 0.7296797153024911,
      "grad_norm": 0.5544888377189636,
      "learning_rate": 0.0003176771989752348,
      "loss": 7.4824,
      "step": 2563
    },
    {
      "epoch": 0.7299644128113879,
      "grad_norm": 0.6098049879074097,
      "learning_rate": 0.0003176060347281526,
      "loss": 7.2559,
      "step": 2564
    },
    {
      "epoch": 0.7302491103202847,
      "grad_norm": 0.48214465379714966,
      "learning_rate": 0.0003175348704810703,
      "loss": 7.5166,
      "step": 2565
    },
    {
      "epoch": 0.7305338078291815,
      "grad_norm": 0.4993290603160858,
      "learning_rate": 0.00031746370623398804,
      "loss": 7.4863,
      "step": 2566
    },
    {
      "epoch": 0.7308185053380782,
      "grad_norm": 0.5877825021743774,
      "learning_rate": 0.00031739254198690577,
      "loss": 6.6904,
      "step": 2567
    },
    {
      "epoch": 0.731103202846975,
      "grad_norm": 0.5146313309669495,
      "learning_rate": 0.0003173213777398235,
      "loss": 7.7646,
      "step": 2568
    },
    {
      "epoch": 0.7313879003558719,
      "grad_norm": 0.5577600598335266,
      "learning_rate": 0.00031725021349274127,
      "loss": 7.9033,
      "step": 2569
    },
    {
      "epoch": 0.7316725978647687,
      "grad_norm": 0.5315368175506592,
      "learning_rate": 0.000317179049245659,
      "loss": 7.6758,
      "step": 2570
    },
    {
      "epoch": 0.7319572953736655,
      "grad_norm": 0.5878903269767761,
      "learning_rate": 0.0003171078849985767,
      "loss": 7.541,
      "step": 2571
    },
    {
      "epoch": 0.7322419928825623,
      "grad_norm": 0.5676193833351135,
      "learning_rate": 0.0003170367207514945,
      "loss": 7.0918,
      "step": 2572
    },
    {
      "epoch": 0.7325266903914591,
      "grad_norm": 0.6809705495834351,
      "learning_rate": 0.00031696555650441216,
      "loss": 7.0068,
      "step": 2573
    },
    {
      "epoch": 0.7328113879003558,
      "grad_norm": 0.5574617385864258,
      "learning_rate": 0.00031689439225732994,
      "loss": 6.8799,
      "step": 2574
    },
    {
      "epoch": 0.7330960854092526,
      "grad_norm": 0.49689921736717224,
      "learning_rate": 0.00031682322801024766,
      "loss": 7.208,
      "step": 2575
    },
    {
      "epoch": 0.7333807829181495,
      "grad_norm": 0.5566771626472473,
      "learning_rate": 0.0003167520637631654,
      "loss": 7.4482,
      "step": 2576
    },
    {
      "epoch": 0.7336654804270463,
      "grad_norm": 0.48025646805763245,
      "learning_rate": 0.00031668089951608316,
      "loss": 8.2402,
      "step": 2577
    },
    {
      "epoch": 0.7339501779359431,
      "grad_norm": 0.5314444899559021,
      "learning_rate": 0.0003166097352690009,
      "loss": 7.2295,
      "step": 2578
    },
    {
      "epoch": 0.7342348754448399,
      "grad_norm": 0.453149676322937,
      "learning_rate": 0.00031653857102191855,
      "loss": 7.834,
      "step": 2579
    },
    {
      "epoch": 0.7345195729537367,
      "grad_norm": 0.708128809928894,
      "learning_rate": 0.0003164674067748363,
      "loss": 6.0342,
      "step": 2580
    },
    {
      "epoch": 0.7348042704626334,
      "grad_norm": 0.4753882586956024,
      "learning_rate": 0.00031639624252775405,
      "loss": 8.1387,
      "step": 2581
    },
    {
      "epoch": 0.7350889679715302,
      "grad_norm": 0.4411731958389282,
      "learning_rate": 0.0003163250782806718,
      "loss": 7.5264,
      "step": 2582
    },
    {
      "epoch": 0.735373665480427,
      "grad_norm": 0.5455582737922668,
      "learning_rate": 0.00031625391403358955,
      "loss": 7.3516,
      "step": 2583
    },
    {
      "epoch": 0.7356583629893239,
      "grad_norm": 0.45193761587142944,
      "learning_rate": 0.0003161827497865072,
      "loss": 7.8564,
      "step": 2584
    },
    {
      "epoch": 0.7359430604982207,
      "grad_norm": 0.5696684718132019,
      "learning_rate": 0.000316111585539425,
      "loss": 7.6006,
      "step": 2585
    },
    {
      "epoch": 0.7362277580071175,
      "grad_norm": 0.5321219563484192,
      "learning_rate": 0.0003160404212923427,
      "loss": 7.4746,
      "step": 2586
    },
    {
      "epoch": 0.7365124555160142,
      "grad_norm": 0.4532681405544281,
      "learning_rate": 0.0003159692570452605,
      "loss": 7.9062,
      "step": 2587
    },
    {
      "epoch": 0.736797153024911,
      "grad_norm": 0.4724501967430115,
      "learning_rate": 0.0003158980927981782,
      "loss": 7.835,
      "step": 2588
    },
    {
      "epoch": 0.7370818505338078,
      "grad_norm": 0.4478871822357178,
      "learning_rate": 0.00031582692855109594,
      "loss": 7.8555,
      "step": 2589
    },
    {
      "epoch": 0.7373665480427046,
      "grad_norm": 0.48950639367103577,
      "learning_rate": 0.00031575576430401366,
      "loss": 7.9385,
      "step": 2590
    },
    {
      "epoch": 0.7376512455516014,
      "grad_norm": 0.5294865965843201,
      "learning_rate": 0.0003156846000569314,
      "loss": 7.2168,
      "step": 2591
    },
    {
      "epoch": 0.7379359430604983,
      "grad_norm": 0.4785686433315277,
      "learning_rate": 0.00031561343580984916,
      "loss": 7.5938,
      "step": 2592
    },
    {
      "epoch": 0.7382206405693951,
      "grad_norm": 0.5543127655982971,
      "learning_rate": 0.0003155422715627669,
      "loss": 7.5117,
      "step": 2593
    },
    {
      "epoch": 0.7385053380782918,
      "grad_norm": 0.5411946773529053,
      "learning_rate": 0.0003154711073156846,
      "loss": 7.3301,
      "step": 2594
    },
    {
      "epoch": 0.7387900355871886,
      "grad_norm": 0.5306877493858337,
      "learning_rate": 0.00031539994306860233,
      "loss": 7.3652,
      "step": 2595
    },
    {
      "epoch": 0.7390747330960854,
      "grad_norm": 0.3654478192329407,
      "learning_rate": 0.00031532877882152005,
      "loss": 8.4248,
      "step": 2596
    },
    {
      "epoch": 0.7393594306049822,
      "grad_norm": 0.5097606778144836,
      "learning_rate": 0.0003152576145744378,
      "loss": 7.5879,
      "step": 2597
    },
    {
      "epoch": 0.739644128113879,
      "grad_norm": 0.5534260869026184,
      "learning_rate": 0.00031518645032735555,
      "loss": 7.5234,
      "step": 2598
    },
    {
      "epoch": 0.7399288256227758,
      "grad_norm": 0.5957140922546387,
      "learning_rate": 0.0003151152860802733,
      "loss": 7.0273,
      "step": 2599
    },
    {
      "epoch": 0.7402135231316725,
      "grad_norm": 0.444867342710495,
      "learning_rate": 0.00031504412183319105,
      "loss": 7.6514,
      "step": 2600
    },
    {
      "epoch": 0.7402135231316725,
      "eval_bleu": 0.1311964785011227,
      "eval_loss": 7.15625,
      "eval_runtime": 131.8878,
      "eval_samples_per_second": 2.153,
      "eval_steps_per_second": 0.136,
      "step": 2600
    },
    {
      "epoch": 0.7404982206405694,
      "grad_norm": 0.5156901478767395,
      "learning_rate": 0.0003149729575861087,
      "loss": 7.2305,
      "step": 2601
    },
    {
      "epoch": 0.7407829181494662,
      "grad_norm": 0.5503838658332825,
      "learning_rate": 0.00031490179333902644,
      "loss": 7.4941,
      "step": 2602
    },
    {
      "epoch": 0.741067615658363,
      "grad_norm": 0.5789117217063904,
      "learning_rate": 0.0003148306290919442,
      "loss": 6.9873,
      "step": 2603
    },
    {
      "epoch": 0.7413523131672598,
      "grad_norm": 0.5701901316642761,
      "learning_rate": 0.00031475946484486194,
      "loss": 7.6084,
      "step": 2604
    },
    {
      "epoch": 0.7416370106761566,
      "grad_norm": 0.5211567282676697,
      "learning_rate": 0.0003146883005977797,
      "loss": 7.6279,
      "step": 2605
    },
    {
      "epoch": 0.7419217081850534,
      "grad_norm": 0.49420586228370667,
      "learning_rate": 0.00031461713635069744,
      "loss": 7.8633,
      "step": 2606
    },
    {
      "epoch": 0.7422064056939501,
      "grad_norm": 0.46195220947265625,
      "learning_rate": 0.0003145459721036151,
      "loss": 7.4355,
      "step": 2607
    },
    {
      "epoch": 0.742491103202847,
      "grad_norm": 0.535139799118042,
      "learning_rate": 0.0003144748078565329,
      "loss": 7.1406,
      "step": 2608
    },
    {
      "epoch": 0.7427758007117438,
      "grad_norm": 0.4500882923603058,
      "learning_rate": 0.0003144036436094506,
      "loss": 7.4297,
      "step": 2609
    },
    {
      "epoch": 0.7430604982206406,
      "grad_norm": 0.3977426588535309,
      "learning_rate": 0.0003143324793623684,
      "loss": 7.9404,
      "step": 2610
    },
    {
      "epoch": 0.7433451957295374,
      "grad_norm": 0.5018517971038818,
      "learning_rate": 0.0003142613151152861,
      "loss": 7.6504,
      "step": 2611
    },
    {
      "epoch": 0.7436298932384342,
      "grad_norm": 0.47059011459350586,
      "learning_rate": 0.0003141901508682038,
      "loss": 7.6875,
      "step": 2612
    },
    {
      "epoch": 0.7439145907473309,
      "grad_norm": 0.38504770398139954,
      "learning_rate": 0.00031411898662112156,
      "loss": 7.7979,
      "step": 2613
    },
    {
      "epoch": 0.7441992882562277,
      "grad_norm": 0.45052361488342285,
      "learning_rate": 0.0003140478223740393,
      "loss": 8.085,
      "step": 2614
    },
    {
      "epoch": 0.7444839857651245,
      "grad_norm": 0.5560654997825623,
      "learning_rate": 0.000313976658126957,
      "loss": 7.0352,
      "step": 2615
    },
    {
      "epoch": 0.7447686832740213,
      "grad_norm": 0.4338613450527191,
      "learning_rate": 0.0003139054938798748,
      "loss": 7.6807,
      "step": 2616
    },
    {
      "epoch": 0.7450533807829182,
      "grad_norm": 0.46781688928604126,
      "learning_rate": 0.0003138343296327925,
      "loss": 8.0508,
      "step": 2617
    },
    {
      "epoch": 0.745338078291815,
      "grad_norm": 0.5154721736907959,
      "learning_rate": 0.0003137631653857102,
      "loss": 7.5654,
      "step": 2618
    },
    {
      "epoch": 0.7456227758007118,
      "grad_norm": 0.6442460417747498,
      "learning_rate": 0.00031369200113862795,
      "loss": 7.0449,
      "step": 2619
    },
    {
      "epoch": 0.7459074733096085,
      "grad_norm": 0.6043844819068909,
      "learning_rate": 0.00031362083689154567,
      "loss": 7.3721,
      "step": 2620
    },
    {
      "epoch": 0.7461921708185053,
      "grad_norm": 0.5274696350097656,
      "learning_rate": 0.00031354967264446345,
      "loss": 7.5938,
      "step": 2621
    },
    {
      "epoch": 0.7464768683274021,
      "grad_norm": 0.5526018738746643,
      "learning_rate": 0.00031347850839738117,
      "loss": 7.4717,
      "step": 2622
    },
    {
      "epoch": 0.7467615658362989,
      "grad_norm": 0.49888187646865845,
      "learning_rate": 0.00031340734415029895,
      "loss": 7.3164,
      "step": 2623
    },
    {
      "epoch": 0.7470462633451957,
      "grad_norm": 0.5705715417861938,
      "learning_rate": 0.0003133361799032166,
      "loss": 7.4746,
      "step": 2624
    },
    {
      "epoch": 0.7473309608540926,
      "grad_norm": 0.6312611699104309,
      "learning_rate": 0.00031326501565613434,
      "loss": 7.2734,
      "step": 2625
    },
    {
      "epoch": 0.7476156583629894,
      "grad_norm": 0.46278733015060425,
      "learning_rate": 0.0003131938514090521,
      "loss": 7.7891,
      "step": 2626
    },
    {
      "epoch": 0.7479003558718861,
      "grad_norm": 0.6118682622909546,
      "learning_rate": 0.00031312268716196984,
      "loss": 6.9541,
      "step": 2627
    },
    {
      "epoch": 0.7481850533807829,
      "grad_norm": 0.45542994141578674,
      "learning_rate": 0.0003130515229148876,
      "loss": 7.9307,
      "step": 2628
    },
    {
      "epoch": 0.7484697508896797,
      "grad_norm": 0.5137469172477722,
      "learning_rate": 0.0003129803586678053,
      "loss": 7.3584,
      "step": 2629
    },
    {
      "epoch": 0.7487544483985765,
      "grad_norm": 0.585006058216095,
      "learning_rate": 0.000312909194420723,
      "loss": 6.8936,
      "step": 2630
    },
    {
      "epoch": 0.7490391459074733,
      "grad_norm": 0.5330140590667725,
      "learning_rate": 0.0003128380301736408,
      "loss": 7.0742,
      "step": 2631
    },
    {
      "epoch": 0.7493238434163702,
      "grad_norm": 0.5616894960403442,
      "learning_rate": 0.0003127668659265585,
      "loss": 7.4502,
      "step": 2632
    },
    {
      "epoch": 0.7496085409252669,
      "grad_norm": 0.4760160446166992,
      "learning_rate": 0.00031269570167947623,
      "loss": 7.541,
      "step": 2633
    },
    {
      "epoch": 0.7498932384341637,
      "grad_norm": 0.6454730033874512,
      "learning_rate": 0.000312624537432394,
      "loss": 6.9785,
      "step": 2634
    },
    {
      "epoch": 0.7501779359430605,
      "grad_norm": 0.49260658025741577,
      "learning_rate": 0.0003125533731853117,
      "loss": 7.6104,
      "step": 2635
    },
    {
      "epoch": 0.7504626334519573,
      "grad_norm": 0.5707533359527588,
      "learning_rate": 0.00031248220893822945,
      "loss": 7.4336,
      "step": 2636
    },
    {
      "epoch": 0.7507473309608541,
      "grad_norm": 0.5255804061889648,
      "learning_rate": 0.0003124110446911472,
      "loss": 7.3428,
      "step": 2637
    },
    {
      "epoch": 0.7510320284697509,
      "grad_norm": 0.45832380652427673,
      "learning_rate": 0.0003123398804440649,
      "loss": 7.4775,
      "step": 2638
    },
    {
      "epoch": 0.7513167259786477,
      "grad_norm": 0.8930573463439941,
      "learning_rate": 0.0003122687161969827,
      "loss": 7.3213,
      "step": 2639
    },
    {
      "epoch": 0.7516014234875444,
      "grad_norm": 0.48408377170562744,
      "learning_rate": 0.00031219755194990034,
      "loss": 7.5352,
      "step": 2640
    },
    {
      "epoch": 0.7518861209964413,
      "grad_norm": 0.5931205153465271,
      "learning_rate": 0.0003121263877028181,
      "loss": 7.0576,
      "step": 2641
    },
    {
      "epoch": 0.7521708185053381,
      "grad_norm": 0.6086013913154602,
      "learning_rate": 0.00031205522345573584,
      "loss": 6.9355,
      "step": 2642
    },
    {
      "epoch": 0.7524555160142349,
      "grad_norm": 0.5471476912498474,
      "learning_rate": 0.00031198405920865356,
      "loss": 7.5527,
      "step": 2643
    },
    {
      "epoch": 0.7527402135231317,
      "grad_norm": 0.5162563323974609,
      "learning_rate": 0.00031191289496157134,
      "loss": 7.3232,
      "step": 2644
    },
    {
      "epoch": 0.7530249110320285,
      "grad_norm": 0.4846716821193695,
      "learning_rate": 0.00031184173071448906,
      "loss": 6.8857,
      "step": 2645
    },
    {
      "epoch": 0.7533096085409252,
      "grad_norm": 0.48896706104278564,
      "learning_rate": 0.00031177056646740673,
      "loss": 7.875,
      "step": 2646
    },
    {
      "epoch": 0.753594306049822,
      "grad_norm": 0.5543652176856995,
      "learning_rate": 0.0003116994022203245,
      "loss": 7.666,
      "step": 2647
    },
    {
      "epoch": 0.7538790035587188,
      "grad_norm": 0.553112268447876,
      "learning_rate": 0.00031162823797324223,
      "loss": 6.7373,
      "step": 2648
    },
    {
      "epoch": 0.7541637010676157,
      "grad_norm": 0.4474167227745056,
      "learning_rate": 0.00031155707372616,
      "loss": 7.9023,
      "step": 2649
    },
    {
      "epoch": 0.7544483985765125,
      "grad_norm": 0.4880124628543854,
      "learning_rate": 0.00031148590947907773,
      "loss": 7.4043,
      "step": 2650
    },
    {
      "epoch": 0.7547330960854093,
      "grad_norm": 0.5141882300376892,
      "learning_rate": 0.00031141474523199545,
      "loss": 7.3291,
      "step": 2651
    },
    {
      "epoch": 0.7550177935943061,
      "grad_norm": 0.5146457552909851,
      "learning_rate": 0.0003113435809849132,
      "loss": 7.7734,
      "step": 2652
    },
    {
      "epoch": 0.7553024911032028,
      "grad_norm": 0.5177709460258484,
      "learning_rate": 0.0003112724167378309,
      "loss": 7.4248,
      "step": 2653
    },
    {
      "epoch": 0.7555871886120996,
      "grad_norm": 0.5161314010620117,
      "learning_rate": 0.0003112012524907487,
      "loss": 7.6396,
      "step": 2654
    },
    {
      "epoch": 0.7558718861209964,
      "grad_norm": 0.4534965753555298,
      "learning_rate": 0.0003111300882436664,
      "loss": 7.7861,
      "step": 2655
    },
    {
      "epoch": 0.7561565836298932,
      "grad_norm": 0.5427484512329102,
      "learning_rate": 0.0003110589239965841,
      "loss": 7.0625,
      "step": 2656
    },
    {
      "epoch": 0.7564412811387901,
      "grad_norm": 0.5938054323196411,
      "learning_rate": 0.00031098775974950185,
      "loss": 7.043,
      "step": 2657
    },
    {
      "epoch": 0.7567259786476869,
      "grad_norm": 0.6743369698524475,
      "learning_rate": 0.00031091659550241957,
      "loss": 7.1709,
      "step": 2658
    },
    {
      "epoch": 0.7570106761565837,
      "grad_norm": 1.3058817386627197,
      "learning_rate": 0.00031084543125533735,
      "loss": 7.2109,
      "step": 2659
    },
    {
      "epoch": 0.7572953736654804,
      "grad_norm": 0.5520133972167969,
      "learning_rate": 0.00031077426700825507,
      "loss": 7.3994,
      "step": 2660
    },
    {
      "epoch": 0.7575800711743772,
      "grad_norm": 0.5254921913146973,
      "learning_rate": 0.0003107031027611728,
      "loss": 7.335,
      "step": 2661
    },
    {
      "epoch": 0.757864768683274,
      "grad_norm": 0.5458098649978638,
      "learning_rate": 0.00031063193851409057,
      "loss": 6.959,
      "step": 2662
    },
    {
      "epoch": 0.7581494661921708,
      "grad_norm": 0.5257382392883301,
      "learning_rate": 0.00031056077426700824,
      "loss": 7.1221,
      "step": 2663
    },
    {
      "epoch": 0.7584341637010676,
      "grad_norm": 0.611581027507782,
      "learning_rate": 0.00031048961001992596,
      "loss": 6.6846,
      "step": 2664
    },
    {
      "epoch": 0.7587188612099645,
      "grad_norm": 0.5874609351158142,
      "learning_rate": 0.00031041844577284374,
      "loss": 7.2686,
      "step": 2665
    },
    {
      "epoch": 0.7590035587188612,
      "grad_norm": 0.4901139438152313,
      "learning_rate": 0.00031034728152576146,
      "loss": 7.2773,
      "step": 2666
    },
    {
      "epoch": 0.759288256227758,
      "grad_norm": 0.6847594380378723,
      "learning_rate": 0.00031027611727867924,
      "loss": 7.0762,
      "step": 2667
    },
    {
      "epoch": 0.7595729537366548,
      "grad_norm": 0.5506797432899475,
      "learning_rate": 0.00031020495303159696,
      "loss": 7.0957,
      "step": 2668
    },
    {
      "epoch": 0.7598576512455516,
      "grad_norm": 0.5196370482444763,
      "learning_rate": 0.0003101337887845146,
      "loss": 7.4619,
      "step": 2669
    },
    {
      "epoch": 0.7601423487544484,
      "grad_norm": 0.434251606464386,
      "learning_rate": 0.0003100626245374324,
      "loss": 7.9062,
      "step": 2670
    },
    {
      "epoch": 0.7604270462633452,
      "grad_norm": 0.6190574169158936,
      "learning_rate": 0.0003099914602903501,
      "loss": 6.7988,
      "step": 2671
    },
    {
      "epoch": 0.760711743772242,
      "grad_norm": 0.705678403377533,
      "learning_rate": 0.0003099202960432679,
      "loss": 6.9961,
      "step": 2672
    },
    {
      "epoch": 0.7609964412811387,
      "grad_norm": 0.4898715019226074,
      "learning_rate": 0.0003098491317961856,
      "loss": 7.3662,
      "step": 2673
    },
    {
      "epoch": 0.7612811387900356,
      "grad_norm": 0.46896737813949585,
      "learning_rate": 0.0003097779675491033,
      "loss": 7.5527,
      "step": 2674
    },
    {
      "epoch": 0.7615658362989324,
      "grad_norm": 0.41878241300582886,
      "learning_rate": 0.00030970680330202107,
      "loss": 7.6797,
      "step": 2675
    },
    {
      "epoch": 0.7618505338078292,
      "grad_norm": 0.635953426361084,
      "learning_rate": 0.0003096356390549388,
      "loss": 6.8652,
      "step": 2676
    },
    {
      "epoch": 0.762135231316726,
      "grad_norm": 0.4901335537433624,
      "learning_rate": 0.0003095644748078565,
      "loss": 7.6826,
      "step": 2677
    },
    {
      "epoch": 0.7624199288256228,
      "grad_norm": 0.7121919989585876,
      "learning_rate": 0.0003094933105607743,
      "loss": 6.334,
      "step": 2678
    },
    {
      "epoch": 0.7627046263345195,
      "grad_norm": 0.5046708583831787,
      "learning_rate": 0.000309422146313692,
      "loss": 7.6523,
      "step": 2679
    },
    {
      "epoch": 0.7629893238434163,
      "grad_norm": 0.5050914287567139,
      "learning_rate": 0.00030935098206660974,
      "loss": 7.6406,
      "step": 2680
    },
    {
      "epoch": 0.7632740213523131,
      "grad_norm": 0.516160249710083,
      "learning_rate": 0.00030927981781952746,
      "loss": 7.5088,
      "step": 2681
    },
    {
      "epoch": 0.76355871886121,
      "grad_norm": 0.5769726634025574,
      "learning_rate": 0.0003092086535724452,
      "loss": 7.0059,
      "step": 2682
    },
    {
      "epoch": 0.7638434163701068,
      "grad_norm": 0.6104977130889893,
      "learning_rate": 0.00030913748932536296,
      "loss": 7.7363,
      "step": 2683
    },
    {
      "epoch": 0.7641281138790036,
      "grad_norm": 0.49138209223747253,
      "learning_rate": 0.0003090663250782807,
      "loss": 7.4453,
      "step": 2684
    },
    {
      "epoch": 0.7644128113879004,
      "grad_norm": 0.5323702096939087,
      "learning_rate": 0.0003089951608311984,
      "loss": 7.6797,
      "step": 2685
    },
    {
      "epoch": 0.7646975088967971,
      "grad_norm": 0.5771242380142212,
      "learning_rate": 0.00030892399658411613,
      "loss": 6.9863,
      "step": 2686
    },
    {
      "epoch": 0.7649822064056939,
      "grad_norm": 1.952633023262024,
      "learning_rate": 0.00030885283233703385,
      "loss": 7.2695,
      "step": 2687
    },
    {
      "epoch": 0.7652669039145907,
      "grad_norm": 0.5692996382713318,
      "learning_rate": 0.00030878166808995163,
      "loss": 6.9746,
      "step": 2688
    },
    {
      "epoch": 0.7655516014234875,
      "grad_norm": 0.5239670872688293,
      "learning_rate": 0.00030871050384286935,
      "loss": 7.4033,
      "step": 2689
    },
    {
      "epoch": 0.7658362989323844,
      "grad_norm": 0.5654290914535522,
      "learning_rate": 0.00030863933959578713,
      "loss": 7.291,
      "step": 2690
    },
    {
      "epoch": 0.7661209964412812,
      "grad_norm": 0.5034224390983582,
      "learning_rate": 0.0003085681753487048,
      "loss": 7.3838,
      "step": 2691
    },
    {
      "epoch": 0.766405693950178,
      "grad_norm": 0.41423240303993225,
      "learning_rate": 0.0003084970111016225,
      "loss": 7.8447,
      "step": 2692
    },
    {
      "epoch": 0.7666903914590747,
      "grad_norm": 0.515184760093689,
      "learning_rate": 0.0003084258468545403,
      "loss": 7.8477,
      "step": 2693
    },
    {
      "epoch": 0.7669750889679715,
      "grad_norm": 0.5560903549194336,
      "learning_rate": 0.000308354682607458,
      "loss": 7.2402,
      "step": 2694
    },
    {
      "epoch": 0.7672597864768683,
      "grad_norm": 0.5613119602203369,
      "learning_rate": 0.00030828351836037574,
      "loss": 7.2803,
      "step": 2695
    },
    {
      "epoch": 0.7675444839857651,
      "grad_norm": 0.4810265898704529,
      "learning_rate": 0.0003082123541132935,
      "loss": 7.6846,
      "step": 2696
    },
    {
      "epoch": 0.767829181494662,
      "grad_norm": 0.5044460892677307,
      "learning_rate": 0.0003081411898662112,
      "loss": 7.2402,
      "step": 2697
    },
    {
      "epoch": 0.7681138790035588,
      "grad_norm": 0.4652932584285736,
      "learning_rate": 0.00030807002561912897,
      "loss": 7.915,
      "step": 2698
    },
    {
      "epoch": 0.7683985765124555,
      "grad_norm": 0.5949245691299438,
      "learning_rate": 0.0003079988613720467,
      "loss": 7.3535,
      "step": 2699
    },
    {
      "epoch": 0.7686832740213523,
      "grad_norm": 0.5232352614402771,
      "learning_rate": 0.0003079276971249644,
      "loss": 7.4131,
      "step": 2700
    },
    {
      "epoch": 0.7689679715302491,
      "grad_norm": 0.5790106058120728,
      "learning_rate": 0.0003078565328778822,
      "loss": 7.2266,
      "step": 2701
    },
    {
      "epoch": 0.7692526690391459,
      "grad_norm": 0.5667076706886292,
      "learning_rate": 0.00030778536863079986,
      "loss": 6.917,
      "step": 2702
    },
    {
      "epoch": 0.7695373665480427,
      "grad_norm": 0.4900321364402771,
      "learning_rate": 0.00030771420438371763,
      "loss": 7.0889,
      "step": 2703
    },
    {
      "epoch": 0.7698220640569395,
      "grad_norm": 0.5022786855697632,
      "learning_rate": 0.00030764304013663536,
      "loss": 7.6719,
      "step": 2704
    },
    {
      "epoch": 0.7701067615658364,
      "grad_norm": 0.4413602650165558,
      "learning_rate": 0.0003075718758895531,
      "loss": 7.6396,
      "step": 2705
    },
    {
      "epoch": 0.770391459074733,
      "grad_norm": 0.5268474221229553,
      "learning_rate": 0.00030750071164247086,
      "loss": 7.1738,
      "step": 2706
    },
    {
      "epoch": 0.7706761565836299,
      "grad_norm": 0.48097822070121765,
      "learning_rate": 0.0003074295473953886,
      "loss": 7.7969,
      "step": 2707
    },
    {
      "epoch": 0.7709608540925267,
      "grad_norm": 0.5247225165367126,
      "learning_rate": 0.0003073583831483063,
      "loss": 7.2803,
      "step": 2708
    },
    {
      "epoch": 0.7712455516014235,
      "grad_norm": 0.6115455627441406,
      "learning_rate": 0.000307287218901224,
      "loss": 6.2559,
      "step": 2709
    },
    {
      "epoch": 0.7715302491103203,
      "grad_norm": 0.5317800045013428,
      "learning_rate": 0.00030721605465414175,
      "loss": 7.7441,
      "step": 2710
    },
    {
      "epoch": 0.7718149466192171,
      "grad_norm": 0.5750504732131958,
      "learning_rate": 0.0003071448904070595,
      "loss": 7.7236,
      "step": 2711
    },
    {
      "epoch": 0.7720996441281138,
      "grad_norm": 0.55418860912323,
      "learning_rate": 0.00030707372615997725,
      "loss": 7.4961,
      "step": 2712
    },
    {
      "epoch": 0.7723843416370106,
      "grad_norm": 0.449197381734848,
      "learning_rate": 0.00030700256191289497,
      "loss": 7.9756,
      "step": 2713
    },
    {
      "epoch": 0.7726690391459075,
      "grad_norm": 0.6202036142349243,
      "learning_rate": 0.0003069313976658127,
      "loss": 7.3857,
      "step": 2714
    },
    {
      "epoch": 0.7729537366548043,
      "grad_norm": 0.5838592052459717,
      "learning_rate": 0.0003068602334187304,
      "loss": 7.8164,
      "step": 2715
    },
    {
      "epoch": 0.7732384341637011,
      "grad_norm": 0.5207149982452393,
      "learning_rate": 0.0003067890691716482,
      "loss": 7.7314,
      "step": 2716
    },
    {
      "epoch": 0.7735231316725979,
      "grad_norm": 0.7960397601127625,
      "learning_rate": 0.0003067179049245659,
      "loss": 6.3457,
      "step": 2717
    },
    {
      "epoch": 0.7738078291814947,
      "grad_norm": 0.5746844410896301,
      "learning_rate": 0.00030664674067748364,
      "loss": 7.3242,
      "step": 2718
    },
    {
      "epoch": 0.7740925266903914,
      "grad_norm": 0.4666804373264313,
      "learning_rate": 0.00030657557643040136,
      "loss": 7.5459,
      "step": 2719
    },
    {
      "epoch": 0.7743772241992882,
      "grad_norm": 0.48003408312797546,
      "learning_rate": 0.0003065044121833191,
      "loss": 7.7871,
      "step": 2720
    },
    {
      "epoch": 0.774661921708185,
      "grad_norm": 0.5610489249229431,
      "learning_rate": 0.00030643324793623686,
      "loss": 7.3984,
      "step": 2721
    },
    {
      "epoch": 0.7749466192170819,
      "grad_norm": 0.41973844170570374,
      "learning_rate": 0.0003063620836891546,
      "loss": 7.7754,
      "step": 2722
    },
    {
      "epoch": 0.7752313167259787,
      "grad_norm": 0.4759311378002167,
      "learning_rate": 0.0003062909194420723,
      "loss": 7.6963,
      "step": 2723
    },
    {
      "epoch": 0.7755160142348755,
      "grad_norm": 0.6884262561798096,
      "learning_rate": 0.0003062197551949901,
      "loss": 6.6533,
      "step": 2724
    },
    {
      "epoch": 0.7758007117437722,
      "grad_norm": 0.4731769561767578,
      "learning_rate": 0.00030614859094790775,
      "loss": 7.7754,
      "step": 2725
    },
    {
      "epoch": 0.776085409252669,
      "grad_norm": 0.5525245666503906,
      "learning_rate": 0.0003060774267008255,
      "loss": 7.7852,
      "step": 2726
    },
    {
      "epoch": 0.7763701067615658,
      "grad_norm": 0.5242894291877747,
      "learning_rate": 0.00030600626245374325,
      "loss": 7.4531,
      "step": 2727
    },
    {
      "epoch": 0.7766548042704626,
      "grad_norm": 0.7139949202537537,
      "learning_rate": 0.000305935098206661,
      "loss": 6.2715,
      "step": 2728
    },
    {
      "epoch": 0.7769395017793594,
      "grad_norm": 0.5211818218231201,
      "learning_rate": 0.00030586393395957875,
      "loss": 7.6797,
      "step": 2729
    },
    {
      "epoch": 0.7772241992882563,
      "grad_norm": 0.4806353747844696,
      "learning_rate": 0.0003057927697124964,
      "loss": 7.6074,
      "step": 2730
    },
    {
      "epoch": 0.7775088967971531,
      "grad_norm": 0.5261585712432861,
      "learning_rate": 0.00030572160546541414,
      "loss": 7.6553,
      "step": 2731
    },
    {
      "epoch": 0.7777935943060498,
      "grad_norm": 0.5786540508270264,
      "learning_rate": 0.0003056504412183319,
      "loss": 7.0723,
      "step": 2732
    },
    {
      "epoch": 0.7780782918149466,
      "grad_norm": 0.5983730554580688,
      "learning_rate": 0.00030557927697124964,
      "loss": 7.3467,
      "step": 2733
    },
    {
      "epoch": 0.7783629893238434,
      "grad_norm": 0.5314611196517944,
      "learning_rate": 0.0003055081127241674,
      "loss": 7.7627,
      "step": 2734
    },
    {
      "epoch": 0.7786476868327402,
      "grad_norm": 0.5728591680526733,
      "learning_rate": 0.00030543694847708514,
      "loss": 7.1016,
      "step": 2735
    },
    {
      "epoch": 0.778932384341637,
      "grad_norm": 0.5319963693618774,
      "learning_rate": 0.0003053657842300028,
      "loss": 7.5293,
      "step": 2736
    },
    {
      "epoch": 0.7792170818505338,
      "grad_norm": 0.49272421002388,
      "learning_rate": 0.0003052946199829206,
      "loss": 7.6855,
      "step": 2737
    },
    {
      "epoch": 0.7795017793594307,
      "grad_norm": 0.5124198198318481,
      "learning_rate": 0.0003052234557358383,
      "loss": 7.3438,
      "step": 2738
    },
    {
      "epoch": 0.7797864768683274,
      "grad_norm": 0.7765278816223145,
      "learning_rate": 0.0003051522914887561,
      "loss": 7.0674,
      "step": 2739
    },
    {
      "epoch": 0.7800711743772242,
      "grad_norm": 0.44822877645492554,
      "learning_rate": 0.0003050811272416738,
      "loss": 7.8789,
      "step": 2740
    },
    {
      "epoch": 0.780355871886121,
      "grad_norm": 0.47381624579429626,
      "learning_rate": 0.00030500996299459153,
      "loss": 7.9199,
      "step": 2741
    },
    {
      "epoch": 0.7806405693950178,
      "grad_norm": 0.5750789642333984,
      "learning_rate": 0.00030493879874750926,
      "loss": 6.9893,
      "step": 2742
    },
    {
      "epoch": 0.7809252669039146,
      "grad_norm": 0.6386646628379822,
      "learning_rate": 0.000304867634500427,
      "loss": 7.0859,
      "step": 2743
    },
    {
      "epoch": 0.7812099644128114,
      "grad_norm": 0.5404934883117676,
      "learning_rate": 0.0003047964702533447,
      "loss": 7.4775,
      "step": 2744
    },
    {
      "epoch": 0.7814946619217081,
      "grad_norm": 0.5139842629432678,
      "learning_rate": 0.0003047253060062625,
      "loss": 7.4541,
      "step": 2745
    },
    {
      "epoch": 0.781779359430605,
      "grad_norm": 0.5306956171989441,
      "learning_rate": 0.0003046541417591802,
      "loss": 6.9766,
      "step": 2746
    },
    {
      "epoch": 0.7820640569395018,
      "grad_norm": 0.511721134185791,
      "learning_rate": 0.0003045829775120979,
      "loss": 7.417,
      "step": 2747
    },
    {
      "epoch": 0.7823487544483986,
      "grad_norm": 0.6567382216453552,
      "learning_rate": 0.00030451181326501565,
      "loss": 7.0059,
      "step": 2748
    },
    {
      "epoch": 0.7826334519572954,
      "grad_norm": 0.5798191428184509,
      "learning_rate": 0.00030444064901793337,
      "loss": 7.4072,
      "step": 2749
    },
    {
      "epoch": 0.7829181494661922,
      "grad_norm": 0.3993433117866516,
      "learning_rate": 0.00030436948477085115,
      "loss": 8.0039,
      "step": 2750
    },
    {
      "epoch": 0.783202846975089,
      "grad_norm": 0.64380943775177,
      "learning_rate": 0.00030429832052376887,
      "loss": 6.7822,
      "step": 2751
    },
    {
      "epoch": 0.7834875444839857,
      "grad_norm": 0.6142843961715698,
      "learning_rate": 0.00030422715627668665,
      "loss": 6.8584,
      "step": 2752
    },
    {
      "epoch": 0.7837722419928825,
      "grad_norm": 0.5545031428337097,
      "learning_rate": 0.0003041559920296043,
      "loss": 7.5127,
      "step": 2753
    },
    {
      "epoch": 0.7840569395017793,
      "grad_norm": 0.4792509377002716,
      "learning_rate": 0.00030408482778252204,
      "loss": 7.6289,
      "step": 2754
    },
    {
      "epoch": 0.7843416370106762,
      "grad_norm": 0.44345778226852417,
      "learning_rate": 0.0003040136635354398,
      "loss": 7.4033,
      "step": 2755
    },
    {
      "epoch": 0.784626334519573,
      "grad_norm": 0.4535204768180847,
      "learning_rate": 0.00030394249928835754,
      "loss": 7.7656,
      "step": 2756
    },
    {
      "epoch": 0.7849110320284698,
      "grad_norm": 0.42417848110198975,
      "learning_rate": 0.0003038713350412753,
      "loss": 7.9355,
      "step": 2757
    },
    {
      "epoch": 0.7851957295373665,
      "grad_norm": 0.46319493651390076,
      "learning_rate": 0.00030380017079419304,
      "loss": 7.5439,
      "step": 2758
    },
    {
      "epoch": 0.7854804270462633,
      "grad_norm": 0.5648340582847595,
      "learning_rate": 0.0003037290065471107,
      "loss": 7.5469,
      "step": 2759
    },
    {
      "epoch": 0.7857651245551601,
      "grad_norm": 0.5665770173072815,
      "learning_rate": 0.0003036578423000285,
      "loss": 6.7529,
      "step": 2760
    },
    {
      "epoch": 0.7860498220640569,
      "grad_norm": 0.5870423913002014,
      "learning_rate": 0.0003035866780529462,
      "loss": 7.2734,
      "step": 2761
    },
    {
      "epoch": 0.7863345195729537,
      "grad_norm": 0.6042966246604919,
      "learning_rate": 0.00030351551380586393,
      "loss": 6.9131,
      "step": 2762
    },
    {
      "epoch": 0.7866192170818506,
      "grad_norm": 0.3829229176044464,
      "learning_rate": 0.0003034443495587817,
      "loss": 7.877,
      "step": 2763
    },
    {
      "epoch": 0.7869039145907474,
      "grad_norm": 0.5886365175247192,
      "learning_rate": 0.0003033731853116994,
      "loss": 6.9404,
      "step": 2764
    },
    {
      "epoch": 0.7871886120996441,
      "grad_norm": 0.5557928085327148,
      "learning_rate": 0.00030330202106461715,
      "loss": 7.334,
      "step": 2765
    },
    {
      "epoch": 0.7874733096085409,
      "grad_norm": 0.564155101776123,
      "learning_rate": 0.00030323085681753487,
      "loss": 7.2803,
      "step": 2766
    },
    {
      "epoch": 0.7877580071174377,
      "grad_norm": 0.4728769361972809,
      "learning_rate": 0.0003031596925704526,
      "loss": 7.7861,
      "step": 2767
    },
    {
      "epoch": 0.7880427046263345,
      "grad_norm": 0.5383067727088928,
      "learning_rate": 0.00030308852832337037,
      "loss": 7.04,
      "step": 2768
    },
    {
      "epoch": 0.7883274021352313,
      "grad_norm": 0.5692144632339478,
      "learning_rate": 0.0003030173640762881,
      "loss": 7.4453,
      "step": 2769
    },
    {
      "epoch": 0.7886120996441282,
      "grad_norm": 0.3872782289981842,
      "learning_rate": 0.0003029461998292058,
      "loss": 8.0547,
      "step": 2770
    },
    {
      "epoch": 0.788896797153025,
      "grad_norm": 0.48957905173301697,
      "learning_rate": 0.00030287503558212354,
      "loss": 7.752,
      "step": 2771
    },
    {
      "epoch": 0.7891814946619217,
      "grad_norm": 0.498666912317276,
      "learning_rate": 0.00030280387133504126,
      "loss": 7.5547,
      "step": 2772
    },
    {
      "epoch": 0.7894661921708185,
      "grad_norm": 0.5136122107505798,
      "learning_rate": 0.00030273270708795904,
      "loss": 7.8721,
      "step": 2773
    },
    {
      "epoch": 0.7897508896797153,
      "grad_norm": 0.5858666300773621,
      "learning_rate": 0.00030266154284087676,
      "loss": 7.5459,
      "step": 2774
    },
    {
      "epoch": 0.7900355871886121,
      "grad_norm": 0.5174405574798584,
      "learning_rate": 0.00030259037859379443,
      "loss": 8.0557,
      "step": 2775
    },
    {
      "epoch": 0.7903202846975089,
      "grad_norm": 0.49345019459724426,
      "learning_rate": 0.0003025192143467122,
      "loss": 7.5293,
      "step": 2776
    },
    {
      "epoch": 0.7906049822064057,
      "grad_norm": 0.47130119800567627,
      "learning_rate": 0.00030244805009962993,
      "loss": 7.6543,
      "step": 2777
    },
    {
      "epoch": 0.7908896797153024,
      "grad_norm": 0.6088831424713135,
      "learning_rate": 0.0003023768858525477,
      "loss": 7.04,
      "step": 2778
    },
    {
      "epoch": 0.7911743772241993,
      "grad_norm": 0.631427526473999,
      "learning_rate": 0.00030230572160546543,
      "loss": 6.2588,
      "step": 2779
    },
    {
      "epoch": 0.7914590747330961,
      "grad_norm": 0.6025518178939819,
      "learning_rate": 0.00030223455735838315,
      "loss": 7.3643,
      "step": 2780
    },
    {
      "epoch": 0.7917437722419929,
      "grad_norm": 0.5602458715438843,
      "learning_rate": 0.0003021633931113009,
      "loss": 7.2012,
      "step": 2781
    },
    {
      "epoch": 0.7920284697508897,
      "grad_norm": 0.6382846236228943,
      "learning_rate": 0.0003020922288642186,
      "loss": 6.9961,
      "step": 2782
    },
    {
      "epoch": 0.7923131672597865,
      "grad_norm": 0.45227593183517456,
      "learning_rate": 0.0003020210646171364,
      "loss": 7.876,
      "step": 2783
    },
    {
      "epoch": 0.7925978647686833,
      "grad_norm": 4.088345050811768,
      "learning_rate": 0.0003019499003700541,
      "loss": 7.4541,
      "step": 2784
    },
    {
      "epoch": 0.79288256227758,
      "grad_norm": 0.5664983987808228,
      "learning_rate": 0.0003018787361229718,
      "loss": 7.5137,
      "step": 2785
    },
    {
      "epoch": 0.7931672597864768,
      "grad_norm": 0.6085575222969055,
      "learning_rate": 0.0003018075718758896,
      "loss": 6.9219,
      "step": 2786
    },
    {
      "epoch": 0.7934519572953737,
      "grad_norm": 0.49479979276657104,
      "learning_rate": 0.00030173640762880727,
      "loss": 7.4229,
      "step": 2787
    },
    {
      "epoch": 0.7937366548042705,
      "grad_norm": 0.4929417073726654,
      "learning_rate": 0.00030166524338172504,
      "loss": 7.377,
      "step": 2788
    },
    {
      "epoch": 0.7940213523131673,
      "grad_norm": 0.5337846279144287,
      "learning_rate": 0.00030159407913464277,
      "loss": 7.4072,
      "step": 2789
    },
    {
      "epoch": 0.7943060498220641,
      "grad_norm": 0.5195541381835938,
      "learning_rate": 0.0003015229148875605,
      "loss": 7.6143,
      "step": 2790
    },
    {
      "epoch": 0.7945907473309608,
      "grad_norm": 0.5316729545593262,
      "learning_rate": 0.00030145175064047827,
      "loss": 7.5078,
      "step": 2791
    },
    {
      "epoch": 0.7948754448398576,
      "grad_norm": 0.5704250931739807,
      "learning_rate": 0.00030138058639339594,
      "loss": 7.2812,
      "step": 2792
    },
    {
      "epoch": 0.7951601423487544,
      "grad_norm": 0.5056561827659607,
      "learning_rate": 0.00030130942214631366,
      "loss": 7.4941,
      "step": 2793
    },
    {
      "epoch": 0.7954448398576512,
      "grad_norm": 0.661120593547821,
      "learning_rate": 0.00030123825789923144,
      "loss": 7.541,
      "step": 2794
    },
    {
      "epoch": 0.7957295373665481,
      "grad_norm": 0.5003844499588013,
      "learning_rate": 0.00030116709365214916,
      "loss": 7.5176,
      "step": 2795
    },
    {
      "epoch": 0.7960142348754449,
      "grad_norm": 0.48833969235420227,
      "learning_rate": 0.00030109592940506693,
      "loss": 7.5186,
      "step": 2796
    },
    {
      "epoch": 0.7962989323843417,
      "grad_norm": 0.4091466963291168,
      "learning_rate": 0.00030102476515798466,
      "loss": 8.1641,
      "step": 2797
    },
    {
      "epoch": 0.7965836298932384,
      "grad_norm": 0.5882683992385864,
      "learning_rate": 0.0003009536009109023,
      "loss": 7.6104,
      "step": 2798
    },
    {
      "epoch": 0.7968683274021352,
      "grad_norm": 0.49873751401901245,
      "learning_rate": 0.0003008824366638201,
      "loss": 7.8359,
      "step": 2799
    },
    {
      "epoch": 0.797153024911032,
      "grad_norm": 0.5114471912384033,
      "learning_rate": 0.0003008112724167378,
      "loss": 7.7041,
      "step": 2800
    },
    {
      "epoch": 0.797153024911032,
      "eval_bleu": 0.1365013227417793,
      "eval_loss": 7.1796875,
      "eval_runtime": 126.3669,
      "eval_samples_per_second": 2.247,
      "eval_steps_per_second": 0.142,
      "step": 2800
    },
    {
      "epoch": 0.7974377224199288,
      "grad_norm": 0.4366322457790375,
      "learning_rate": 0.0003007401081696556,
      "loss": 7.9209,
      "step": 2801
    },
    {
      "epoch": 0.7977224199288256,
      "grad_norm": 0.6435465812683105,
      "learning_rate": 0.0003006689439225733,
      "loss": 6.6201,
      "step": 2802
    },
    {
      "epoch": 0.7980071174377225,
      "grad_norm": 0.4883662164211273,
      "learning_rate": 0.000300597779675491,
      "loss": 8.0234,
      "step": 2803
    },
    {
      "epoch": 0.7982918149466193,
      "grad_norm": 0.508457362651825,
      "learning_rate": 0.00030052661542840877,
      "loss": 7.5713,
      "step": 2804
    },
    {
      "epoch": 0.798576512455516,
      "grad_norm": 0.668455958366394,
      "learning_rate": 0.0003004554511813265,
      "loss": 7.9541,
      "step": 2805
    },
    {
      "epoch": 0.7988612099644128,
      "grad_norm": 0.5097607970237732,
      "learning_rate": 0.00030038428693424427,
      "loss": 7.5967,
      "step": 2806
    },
    {
      "epoch": 0.7991459074733096,
      "grad_norm": 0.6093512773513794,
      "learning_rate": 0.000300313122687162,
      "loss": 6.9756,
      "step": 2807
    },
    {
      "epoch": 0.7994306049822064,
      "grad_norm": 0.5423079133033752,
      "learning_rate": 0.0003002419584400797,
      "loss": 7.6514,
      "step": 2808
    },
    {
      "epoch": 0.7997153024911032,
      "grad_norm": 0.554410994052887,
      "learning_rate": 0.00030017079419299744,
      "loss": 7.8369,
      "step": 2809
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.5099748969078064,
      "learning_rate": 0.00030009962994591516,
      "loss": 7.7773,
      "step": 2810
    },
    {
      "epoch": 0.8002846975088967,
      "grad_norm": 0.6028956770896912,
      "learning_rate": 0.0003000284656988329,
      "loss": 7.5352,
      "step": 2811
    },
    {
      "epoch": 0.8005693950177936,
      "grad_norm": 0.5269107222557068,
      "learning_rate": 0.00029995730145175066,
      "loss": 7.627,
      "step": 2812
    },
    {
      "epoch": 0.8008540925266904,
      "grad_norm": 0.5983403325080872,
      "learning_rate": 0.0002998861372046684,
      "loss": 7.6533,
      "step": 2813
    },
    {
      "epoch": 0.8011387900355872,
      "grad_norm": 0.5167375206947327,
      "learning_rate": 0.00029981497295758616,
      "loss": 7.8652,
      "step": 2814
    },
    {
      "epoch": 0.801423487544484,
      "grad_norm": 0.6834097504615784,
      "learning_rate": 0.00029974380871050383,
      "loss": 7.0898,
      "step": 2815
    },
    {
      "epoch": 0.8017081850533808,
      "grad_norm": 0.3638601005077362,
      "learning_rate": 0.00029967264446342155,
      "loss": 7.9258,
      "step": 2816
    },
    {
      "epoch": 0.8019928825622776,
      "grad_norm": 0.5008352398872375,
      "learning_rate": 0.00029960148021633933,
      "loss": 7.418,
      "step": 2817
    },
    {
      "epoch": 0.8022775800711743,
      "grad_norm": 0.40961354970932007,
      "learning_rate": 0.00029953031596925705,
      "loss": 8.2607,
      "step": 2818
    },
    {
      "epoch": 0.8025622775800711,
      "grad_norm": 0.5113992094993591,
      "learning_rate": 0.00029945915172217483,
      "loss": 7.2617,
      "step": 2819
    },
    {
      "epoch": 0.802846975088968,
      "grad_norm": 0.4617052674293518,
      "learning_rate": 0.0002993879874750925,
      "loss": 7.8379,
      "step": 2820
    },
    {
      "epoch": 0.8031316725978648,
      "grad_norm": 0.4900626242160797,
      "learning_rate": 0.0002993168232280102,
      "loss": 7.5449,
      "step": 2821
    },
    {
      "epoch": 0.8034163701067616,
      "grad_norm": 0.4816441833972931,
      "learning_rate": 0.000299245658980928,
      "loss": 7.165,
      "step": 2822
    },
    {
      "epoch": 0.8037010676156584,
      "grad_norm": 0.5446805357933044,
      "learning_rate": 0.0002991744947338457,
      "loss": 7.25,
      "step": 2823
    },
    {
      "epoch": 0.8039857651245551,
      "grad_norm": 0.4386015236377716,
      "learning_rate": 0.00029910333048676344,
      "loss": 7.8467,
      "step": 2824
    },
    {
      "epoch": 0.8042704626334519,
      "grad_norm": 0.5647393465042114,
      "learning_rate": 0.0002990321662396812,
      "loss": 7.4258,
      "step": 2825
    },
    {
      "epoch": 0.8045551601423487,
      "grad_norm": 0.4713270664215088,
      "learning_rate": 0.0002989610019925989,
      "loss": 7.8057,
      "step": 2826
    },
    {
      "epoch": 0.8048398576512455,
      "grad_norm": 0.45760205388069153,
      "learning_rate": 0.00029888983774551667,
      "loss": 7.4297,
      "step": 2827
    },
    {
      "epoch": 0.8051245551601424,
      "grad_norm": 0.4991759955883026,
      "learning_rate": 0.0002988186734984344,
      "loss": 7.2578,
      "step": 2828
    },
    {
      "epoch": 0.8054092526690392,
      "grad_norm": 0.44150128960609436,
      "learning_rate": 0.0002987475092513521,
      "loss": 7.6895,
      "step": 2829
    },
    {
      "epoch": 0.805693950177936,
      "grad_norm": 0.5878073573112488,
      "learning_rate": 0.0002986763450042699,
      "loss": 7.8838,
      "step": 2830
    },
    {
      "epoch": 0.8059786476868327,
      "grad_norm": 0.4612105190753937,
      "learning_rate": 0.0002986051807571876,
      "loss": 8.1475,
      "step": 2831
    },
    {
      "epoch": 0.8062633451957295,
      "grad_norm": 0.6378931999206543,
      "learning_rate": 0.00029853401651010533,
      "loss": 6.8594,
      "step": 2832
    },
    {
      "epoch": 0.8065480427046263,
      "grad_norm": 0.5221328139305115,
      "learning_rate": 0.00029846285226302306,
      "loss": 7.7793,
      "step": 2833
    },
    {
      "epoch": 0.8068327402135231,
      "grad_norm": 0.37596049904823303,
      "learning_rate": 0.0002983916880159408,
      "loss": 7.7598,
      "step": 2834
    },
    {
      "epoch": 0.80711743772242,
      "grad_norm": 0.4419538378715515,
      "learning_rate": 0.00029832052376885856,
      "loss": 8.3125,
      "step": 2835
    },
    {
      "epoch": 0.8074021352313168,
      "grad_norm": 0.5196604132652283,
      "learning_rate": 0.0002982493595217763,
      "loss": 7.4922,
      "step": 2836
    },
    {
      "epoch": 0.8076868327402136,
      "grad_norm": 0.5409771203994751,
      "learning_rate": 0.000298178195274694,
      "loss": 7.7656,
      "step": 2837
    },
    {
      "epoch": 0.8079715302491103,
      "grad_norm": 0.5351245999336243,
      "learning_rate": 0.0002981070310276117,
      "loss": 7.4062,
      "step": 2838
    },
    {
      "epoch": 0.8082562277580071,
      "grad_norm": 0.5866053104400635,
      "learning_rate": 0.00029803586678052945,
      "loss": 6.9395,
      "step": 2839
    },
    {
      "epoch": 0.8085409252669039,
      "grad_norm": 0.5391505360603333,
      "learning_rate": 0.0002979647025334472,
      "loss": 7.7617,
      "step": 2840
    },
    {
      "epoch": 0.8088256227758007,
      "grad_norm": 0.48188886046409607,
      "learning_rate": 0.00029789353828636495,
      "loss": 7.8311,
      "step": 2841
    },
    {
      "epoch": 0.8091103202846975,
      "grad_norm": 0.6336563229560852,
      "learning_rate": 0.00029782237403928267,
      "loss": 7.2256,
      "step": 2842
    },
    {
      "epoch": 0.8093950177935944,
      "grad_norm": 0.5103773474693298,
      "learning_rate": 0.0002977512097922004,
      "loss": 7.0566,
      "step": 2843
    },
    {
      "epoch": 0.809679715302491,
      "grad_norm": 0.619203507900238,
      "learning_rate": 0.0002976800455451181,
      "loss": 7.1172,
      "step": 2844
    },
    {
      "epoch": 0.8099644128113879,
      "grad_norm": 0.5115498304367065,
      "learning_rate": 0.0002976088812980359,
      "loss": 7.626,
      "step": 2845
    },
    {
      "epoch": 0.8102491103202847,
      "grad_norm": 0.6198171973228455,
      "learning_rate": 0.0002975377170509536,
      "loss": 6.8301,
      "step": 2846
    },
    {
      "epoch": 0.8105338078291815,
      "grad_norm": 0.5998125672340393,
      "learning_rate": 0.00029746655280387134,
      "loss": 7.0693,
      "step": 2847
    },
    {
      "epoch": 0.8108185053380783,
      "grad_norm": 0.5225076079368591,
      "learning_rate": 0.00029739538855678906,
      "loss": 7.2822,
      "step": 2848
    },
    {
      "epoch": 0.8111032028469751,
      "grad_norm": 0.48630180954933167,
      "learning_rate": 0.0002973242243097068,
      "loss": 7.8379,
      "step": 2849
    },
    {
      "epoch": 0.8113879003558719,
      "grad_norm": 0.4378873407840729,
      "learning_rate": 0.00029725306006262456,
      "loss": 7.9131,
      "step": 2850
    },
    {
      "epoch": 0.8116725978647686,
      "grad_norm": 0.5027636289596558,
      "learning_rate": 0.0002971818958155423,
      "loss": 7.6924,
      "step": 2851
    },
    {
      "epoch": 0.8119572953736655,
      "grad_norm": 0.5735405087471008,
      "learning_rate": 0.00029711073156846,
      "loss": 6.9521,
      "step": 2852
    },
    {
      "epoch": 0.8122419928825623,
      "grad_norm": 0.525446891784668,
      "learning_rate": 0.0002970395673213778,
      "loss": 7.001,
      "step": 2853
    },
    {
      "epoch": 0.8125266903914591,
      "grad_norm": 0.5471851825714111,
      "learning_rate": 0.00029696840307429545,
      "loss": 6.8418,
      "step": 2854
    },
    {
      "epoch": 0.8128113879003559,
      "grad_norm": 0.5497328042984009,
      "learning_rate": 0.0002968972388272132,
      "loss": 6.3955,
      "step": 2855
    },
    {
      "epoch": 0.8130960854092527,
      "grad_norm": 0.5709758996963501,
      "learning_rate": 0.00029682607458013095,
      "loss": 7.3975,
      "step": 2856
    },
    {
      "epoch": 0.8133807829181494,
      "grad_norm": 0.5811487436294556,
      "learning_rate": 0.0002967549103330487,
      "loss": 7.2871,
      "step": 2857
    },
    {
      "epoch": 0.8136654804270462,
      "grad_norm": 0.5877189040184021,
      "learning_rate": 0.00029668374608596645,
      "loss": 7.168,
      "step": 2858
    },
    {
      "epoch": 0.813950177935943,
      "grad_norm": 0.7431100606918335,
      "learning_rate": 0.0002966125818388842,
      "loss": 6.9219,
      "step": 2859
    },
    {
      "epoch": 0.8142348754448399,
      "grad_norm": 0.5880009531974792,
      "learning_rate": 0.00029654141759180184,
      "loss": 7.2412,
      "step": 2860
    },
    {
      "epoch": 0.8145195729537367,
      "grad_norm": 0.4389316141605377,
      "learning_rate": 0.0002964702533447196,
      "loss": 8.0469,
      "step": 2861
    },
    {
      "epoch": 0.8148042704626335,
      "grad_norm": 0.5003753304481506,
      "learning_rate": 0.00029639908909763734,
      "loss": 8.0059,
      "step": 2862
    },
    {
      "epoch": 0.8150889679715303,
      "grad_norm": 0.4967675507068634,
      "learning_rate": 0.0002963279248505551,
      "loss": 7.3291,
      "step": 2863
    },
    {
      "epoch": 0.815373665480427,
      "grad_norm": 0.5397920608520508,
      "learning_rate": 0.00029625676060347284,
      "loss": 7.1641,
      "step": 2864
    },
    {
      "epoch": 0.8156583629893238,
      "grad_norm": 0.5693586468696594,
      "learning_rate": 0.0002961855963563905,
      "loss": 7.1611,
      "step": 2865
    },
    {
      "epoch": 0.8159430604982206,
      "grad_norm": 0.4460238218307495,
      "learning_rate": 0.0002961144321093083,
      "loss": 7.4746,
      "step": 2866
    },
    {
      "epoch": 0.8162277580071174,
      "grad_norm": 0.5358847975730896,
      "learning_rate": 0.000296043267862226,
      "loss": 7.208,
      "step": 2867
    },
    {
      "epoch": 0.8165124555160143,
      "grad_norm": 0.5846616625785828,
      "learning_rate": 0.0002959721036151438,
      "loss": 6.9746,
      "step": 2868
    },
    {
      "epoch": 0.8167971530249111,
      "grad_norm": 0.4618805944919586,
      "learning_rate": 0.0002959009393680615,
      "loss": 7.6895,
      "step": 2869
    },
    {
      "epoch": 0.8170818505338078,
      "grad_norm": 0.4851011633872986,
      "learning_rate": 0.00029582977512097923,
      "loss": 8.0244,
      "step": 2870
    },
    {
      "epoch": 0.8173665480427046,
      "grad_norm": 0.4739722013473511,
      "learning_rate": 0.00029575861087389695,
      "loss": 7.1934,
      "step": 2871
    },
    {
      "epoch": 0.8176512455516014,
      "grad_norm": 0.5545089840888977,
      "learning_rate": 0.0002956874466268147,
      "loss": 7.8916,
      "step": 2872
    },
    {
      "epoch": 0.8179359430604982,
      "grad_norm": 0.4177158772945404,
      "learning_rate": 0.0002956162823797324,
      "loss": 7.791,
      "step": 2873
    },
    {
      "epoch": 0.818220640569395,
      "grad_norm": 0.6685541868209839,
      "learning_rate": 0.0002955451181326502,
      "loss": 6.498,
      "step": 2874
    },
    {
      "epoch": 0.8185053380782918,
      "grad_norm": 0.5549607276916504,
      "learning_rate": 0.0002954739538855679,
      "loss": 7.29,
      "step": 2875
    },
    {
      "epoch": 0.8187900355871887,
      "grad_norm": 0.4926052689552307,
      "learning_rate": 0.0002954027896384857,
      "loss": 7.4922,
      "step": 2876
    },
    {
      "epoch": 0.8190747330960854,
      "grad_norm": 0.6647663116455078,
      "learning_rate": 0.00029533162539140335,
      "loss": 6.9795,
      "step": 2877
    },
    {
      "epoch": 0.8193594306049822,
      "grad_norm": 0.48186373710632324,
      "learning_rate": 0.00029526046114432107,
      "loss": 7.9004,
      "step": 2878
    },
    {
      "epoch": 0.819644128113879,
      "grad_norm": 0.724568247795105,
      "learning_rate": 0.00029518929689723885,
      "loss": 6.7754,
      "step": 2879
    },
    {
      "epoch": 0.8199288256227758,
      "grad_norm": 0.5359857082366943,
      "learning_rate": 0.00029511813265015657,
      "loss": 7.4238,
      "step": 2880
    },
    {
      "epoch": 0.8202135231316726,
      "grad_norm": 0.5278159379959106,
      "learning_rate": 0.00029504696840307434,
      "loss": 7.5264,
      "step": 2881
    },
    {
      "epoch": 0.8204982206405694,
      "grad_norm": 0.5313935279846191,
      "learning_rate": 0.000294975804155992,
      "loss": 7.5752,
      "step": 2882
    },
    {
      "epoch": 0.8207829181494662,
      "grad_norm": 0.5994811654090881,
      "learning_rate": 0.00029490463990890974,
      "loss": 7.0742,
      "step": 2883
    },
    {
      "epoch": 0.821067615658363,
      "grad_norm": 0.4644065201282501,
      "learning_rate": 0.0002948334756618275,
      "loss": 7.833,
      "step": 2884
    },
    {
      "epoch": 0.8213523131672598,
      "grad_norm": 0.42427536845207214,
      "learning_rate": 0.00029476231141474524,
      "loss": 8.0703,
      "step": 2885
    },
    {
      "epoch": 0.8216370106761566,
      "grad_norm": 0.4382162392139435,
      "learning_rate": 0.000294691147167663,
      "loss": 7.7217,
      "step": 2886
    },
    {
      "epoch": 0.8219217081850534,
      "grad_norm": 0.4552121162414551,
      "learning_rate": 0.00029461998292058074,
      "loss": 7.9404,
      "step": 2887
    },
    {
      "epoch": 0.8222064056939502,
      "grad_norm": 0.5048282742500305,
      "learning_rate": 0.0002945488186734984,
      "loss": 7.2148,
      "step": 2888
    },
    {
      "epoch": 0.822491103202847,
      "grad_norm": 0.5262734293937683,
      "learning_rate": 0.0002944776544264162,
      "loss": 7.4385,
      "step": 2889
    },
    {
      "epoch": 0.8227758007117437,
      "grad_norm": 0.46539247035980225,
      "learning_rate": 0.0002944064901793339,
      "loss": 7.7373,
      "step": 2890
    },
    {
      "epoch": 0.8230604982206405,
      "grad_norm": 0.6122764945030212,
      "learning_rate": 0.0002943353259322516,
      "loss": 7.4834,
      "step": 2891
    },
    {
      "epoch": 0.8233451957295373,
      "grad_norm": 0.5657654404640198,
      "learning_rate": 0.0002942641616851694,
      "loss": 7.7285,
      "step": 2892
    },
    {
      "epoch": 0.8236298932384342,
      "grad_norm": 0.5704545378684998,
      "learning_rate": 0.00029419299743808707,
      "loss": 7.0938,
      "step": 2893
    },
    {
      "epoch": 0.823914590747331,
      "grad_norm": 0.50274258852005,
      "learning_rate": 0.00029412183319100485,
      "loss": 7.4766,
      "step": 2894
    },
    {
      "epoch": 0.8241992882562278,
      "grad_norm": 0.6617230176925659,
      "learning_rate": 0.00029405066894392257,
      "loss": 7.0,
      "step": 2895
    },
    {
      "epoch": 0.8244839857651246,
      "grad_norm": 0.49099719524383545,
      "learning_rate": 0.0002939795046968403,
      "loss": 8.0361,
      "step": 2896
    },
    {
      "epoch": 0.8247686832740213,
      "grad_norm": 0.46738335490226746,
      "learning_rate": 0.00029390834044975807,
      "loss": 7.5381,
      "step": 2897
    },
    {
      "epoch": 0.8250533807829181,
      "grad_norm": 0.47487953305244446,
      "learning_rate": 0.0002938371762026758,
      "loss": 7.7578,
      "step": 2898
    },
    {
      "epoch": 0.8253380782918149,
      "grad_norm": 0.5392792820930481,
      "learning_rate": 0.0002937660119555935,
      "loss": 7.7383,
      "step": 2899
    },
    {
      "epoch": 0.8256227758007118,
      "grad_norm": 0.46173375844955444,
      "learning_rate": 0.00029369484770851124,
      "loss": 7.748,
      "step": 2900
    },
    {
      "epoch": 0.8259074733096086,
      "grad_norm": 0.6226107478141785,
      "learning_rate": 0.00029362368346142896,
      "loss": 7.4033,
      "step": 2901
    },
    {
      "epoch": 0.8261921708185054,
      "grad_norm": 0.5728002190589905,
      "learning_rate": 0.00029355251921434674,
      "loss": 7.3633,
      "step": 2902
    },
    {
      "epoch": 0.8264768683274021,
      "grad_norm": 0.651461660861969,
      "learning_rate": 0.00029348135496726446,
      "loss": 6.8008,
      "step": 2903
    },
    {
      "epoch": 0.8267615658362989,
      "grad_norm": 0.5464851260185242,
      "learning_rate": 0.00029341019072018224,
      "loss": 7.374,
      "step": 2904
    },
    {
      "epoch": 0.8270462633451957,
      "grad_norm": 0.503681480884552,
      "learning_rate": 0.0002933390264730999,
      "loss": 7.3467,
      "step": 2905
    },
    {
      "epoch": 0.8273309608540925,
      "grad_norm": 0.5701079964637756,
      "learning_rate": 0.00029326786222601763,
      "loss": 7.1816,
      "step": 2906
    },
    {
      "epoch": 0.8276156583629893,
      "grad_norm": 0.5388365387916565,
      "learning_rate": 0.0002931966979789354,
      "loss": 7.5645,
      "step": 2907
    },
    {
      "epoch": 0.8279003558718862,
      "grad_norm": 0.4993080198764801,
      "learning_rate": 0.00029312553373185313,
      "loss": 7.7676,
      "step": 2908
    },
    {
      "epoch": 0.828185053380783,
      "grad_norm": 0.47794389724731445,
      "learning_rate": 0.00029305436948477085,
      "loss": 8.083,
      "step": 2909
    },
    {
      "epoch": 0.8284697508896797,
      "grad_norm": 0.5830177068710327,
      "learning_rate": 0.0002929832052376886,
      "loss": 7.0078,
      "step": 2910
    },
    {
      "epoch": 0.8287544483985765,
      "grad_norm": 0.6201160550117493,
      "learning_rate": 0.0002929120409906063,
      "loss": 6.6514,
      "step": 2911
    },
    {
      "epoch": 0.8290391459074733,
      "grad_norm": 0.5156487822532654,
      "learning_rate": 0.0002928408767435241,
      "loss": 7.8438,
      "step": 2912
    },
    {
      "epoch": 0.8293238434163701,
      "grad_norm": 0.6153691411018372,
      "learning_rate": 0.0002927697124964418,
      "loss": 7.2324,
      "step": 2913
    },
    {
      "epoch": 0.8296085409252669,
      "grad_norm": 0.5429337024688721,
      "learning_rate": 0.0002926985482493595,
      "loss": 7.1396,
      "step": 2914
    },
    {
      "epoch": 0.8298932384341637,
      "grad_norm": 0.5016563534736633,
      "learning_rate": 0.0002926273840022773,
      "loss": 7.8926,
      "step": 2915
    },
    {
      "epoch": 0.8301779359430606,
      "grad_norm": 0.543500542640686,
      "learning_rate": 0.00029255621975519497,
      "loss": 7.2959,
      "step": 2916
    },
    {
      "epoch": 0.8304626334519573,
      "grad_norm": 0.7229723930358887,
      "learning_rate": 0.00029248505550811274,
      "loss": 6.96,
      "step": 2917
    },
    {
      "epoch": 0.8307473309608541,
      "grad_norm": 0.4501098692417145,
      "learning_rate": 0.00029241389126103047,
      "loss": 7.6436,
      "step": 2918
    },
    {
      "epoch": 0.8310320284697509,
      "grad_norm": 0.4919080436229706,
      "learning_rate": 0.0002923427270139482,
      "loss": 6.9834,
      "step": 2919
    },
    {
      "epoch": 0.8313167259786477,
      "grad_norm": 0.4828876852989197,
      "learning_rate": 0.00029227156276686597,
      "loss": 7.6953,
      "step": 2920
    },
    {
      "epoch": 0.8316014234875445,
      "grad_norm": 0.5332732796669006,
      "learning_rate": 0.0002922003985197837,
      "loss": 7.709,
      "step": 2921
    },
    {
      "epoch": 0.8318861209964413,
      "grad_norm": 0.6317005753517151,
      "learning_rate": 0.00029212923427270136,
      "loss": 7.5225,
      "step": 2922
    },
    {
      "epoch": 0.832170818505338,
      "grad_norm": 0.551774799823761,
      "learning_rate": 0.00029205807002561913,
      "loss": 7.5098,
      "step": 2923
    },
    {
      "epoch": 0.8324555160142348,
      "grad_norm": 0.6763797998428345,
      "learning_rate": 0.00029198690577853686,
      "loss": 6.9385,
      "step": 2924
    },
    {
      "epoch": 0.8327402135231317,
      "grad_norm": 0.5455992817878723,
      "learning_rate": 0.00029191574153145463,
      "loss": 7.5225,
      "step": 2925
    },
    {
      "epoch": 0.8330249110320285,
      "grad_norm": 0.4341807961463928,
      "learning_rate": 0.00029184457728437236,
      "loss": 7.791,
      "step": 2926
    },
    {
      "epoch": 0.8333096085409253,
      "grad_norm": 0.5020560026168823,
      "learning_rate": 0.00029177341303729,
      "loss": 7.6758,
      "step": 2927
    },
    {
      "epoch": 0.8335943060498221,
      "grad_norm": 0.5980111360549927,
      "learning_rate": 0.0002917022487902078,
      "loss": 6.9648,
      "step": 2928
    },
    {
      "epoch": 0.8338790035587189,
      "grad_norm": 0.6239586472511292,
      "learning_rate": 0.0002916310845431255,
      "loss": 6.6201,
      "step": 2929
    },
    {
      "epoch": 0.8341637010676156,
      "grad_norm": 0.4285376965999603,
      "learning_rate": 0.0002915599202960433,
      "loss": 7.6572,
      "step": 2930
    },
    {
      "epoch": 0.8344483985765124,
      "grad_norm": 0.518779993057251,
      "learning_rate": 0.000291488756048961,
      "loss": 6.958,
      "step": 2931
    },
    {
      "epoch": 0.8347330960854092,
      "grad_norm": 0.46416670083999634,
      "learning_rate": 0.00029141759180187875,
      "loss": 7.6172,
      "step": 2932
    },
    {
      "epoch": 0.8350177935943061,
      "grad_norm": 0.5220642685890198,
      "learning_rate": 0.00029134642755479647,
      "loss": 6.8623,
      "step": 2933
    },
    {
      "epoch": 0.8353024911032029,
      "grad_norm": 0.5910351276397705,
      "learning_rate": 0.0002912752633077142,
      "loss": 6.9287,
      "step": 2934
    },
    {
      "epoch": 0.8355871886120997,
      "grad_norm": 0.46877947449684143,
      "learning_rate": 0.00029120409906063197,
      "loss": 7.5664,
      "step": 2935
    },
    {
      "epoch": 0.8358718861209964,
      "grad_norm": 0.4998994469642639,
      "learning_rate": 0.0002911329348135497,
      "loss": 7.6641,
      "step": 2936
    },
    {
      "epoch": 0.8361565836298932,
      "grad_norm": 0.5982417464256287,
      "learning_rate": 0.0002910617705664674,
      "loss": 6.9824,
      "step": 2937
    },
    {
      "epoch": 0.83644128113879,
      "grad_norm": 0.5369412302970886,
      "learning_rate": 0.00029099060631938514,
      "loss": 7.3809,
      "step": 2938
    },
    {
      "epoch": 0.8367259786476868,
      "grad_norm": 0.44904571771621704,
      "learning_rate": 0.00029091944207230286,
      "loss": 7.9922,
      "step": 2939
    },
    {
      "epoch": 0.8370106761565836,
      "grad_norm": 0.4868450164794922,
      "learning_rate": 0.0002908482778252206,
      "loss": 7.6855,
      "step": 2940
    },
    {
      "epoch": 0.8372953736654805,
      "grad_norm": 0.5913464426994324,
      "learning_rate": 0.00029077711357813836,
      "loss": 6.8887,
      "step": 2941
    },
    {
      "epoch": 0.8375800711743773,
      "grad_norm": 0.541583240032196,
      "learning_rate": 0.0002907059493310561,
      "loss": 7.7617,
      "step": 2942
    },
    {
      "epoch": 0.837864768683274,
      "grad_norm": 0.5252390503883362,
      "learning_rate": 0.00029063478508397386,
      "loss": 7.5547,
      "step": 2943
    },
    {
      "epoch": 0.8381494661921708,
      "grad_norm": 0.5416910648345947,
      "learning_rate": 0.00029056362083689153,
      "loss": 7.248,
      "step": 2944
    },
    {
      "epoch": 0.8384341637010676,
      "grad_norm": 0.49555736780166626,
      "learning_rate": 0.00029049245658980925,
      "loss": 7.5801,
      "step": 2945
    },
    {
      "epoch": 0.8387188612099644,
      "grad_norm": 0.4896506369113922,
      "learning_rate": 0.00029042129234272703,
      "loss": 7.5068,
      "step": 2946
    },
    {
      "epoch": 0.8390035587188612,
      "grad_norm": 0.547698438167572,
      "learning_rate": 0.00029035012809564475,
      "loss": 7.5791,
      "step": 2947
    },
    {
      "epoch": 0.839288256227758,
      "grad_norm": 0.5743728280067444,
      "learning_rate": 0.00029027896384856253,
      "loss": 7.2285,
      "step": 2948
    },
    {
      "epoch": 0.8395729537366549,
      "grad_norm": 0.4714544117450714,
      "learning_rate": 0.00029020779960148025,
      "loss": 7.9639,
      "step": 2949
    },
    {
      "epoch": 0.8398576512455516,
      "grad_norm": 0.5556052327156067,
      "learning_rate": 0.0002901366353543979,
      "loss": 7.4414,
      "step": 2950
    },
    {
      "epoch": 0.8401423487544484,
      "grad_norm": 0.5194971561431885,
      "learning_rate": 0.0002900654711073157,
      "loss": 7.7949,
      "step": 2951
    },
    {
      "epoch": 0.8404270462633452,
      "grad_norm": 0.5065634846687317,
      "learning_rate": 0.0002899943068602334,
      "loss": 7.2041,
      "step": 2952
    },
    {
      "epoch": 0.840711743772242,
      "grad_norm": 0.5396409034729004,
      "learning_rate": 0.0002899231426131512,
      "loss": 7.623,
      "step": 2953
    },
    {
      "epoch": 0.8409964412811388,
      "grad_norm": 0.5944949388504028,
      "learning_rate": 0.0002898519783660689,
      "loss": 7.6123,
      "step": 2954
    },
    {
      "epoch": 0.8412811387900356,
      "grad_norm": 0.602473795413971,
      "learning_rate": 0.0002897808141189866,
      "loss": 6.8633,
      "step": 2955
    },
    {
      "epoch": 0.8415658362989323,
      "grad_norm": 0.6788619756698608,
      "learning_rate": 0.00028970964987190436,
      "loss": 6.6094,
      "step": 2956
    },
    {
      "epoch": 0.8418505338078291,
      "grad_norm": 0.5031066536903381,
      "learning_rate": 0.0002896384856248221,
      "loss": 7.6387,
      "step": 2957
    },
    {
      "epoch": 0.842135231316726,
      "grad_norm": 0.4419020414352417,
      "learning_rate": 0.0002895673213777398,
      "loss": 8.2891,
      "step": 2958
    },
    {
      "epoch": 0.8424199288256228,
      "grad_norm": 0.6626766324043274,
      "learning_rate": 0.0002894961571306576,
      "loss": 6.793,
      "step": 2959
    },
    {
      "epoch": 0.8427046263345196,
      "grad_norm": 0.49644550681114197,
      "learning_rate": 0.0002894249928835753,
      "loss": 7.8252,
      "step": 2960
    },
    {
      "epoch": 0.8429893238434164,
      "grad_norm": 0.505065381526947,
      "learning_rate": 0.00028935382863649303,
      "loss": 7.2285,
      "step": 2961
    },
    {
      "epoch": 0.8432740213523132,
      "grad_norm": 0.6081777215003967,
      "learning_rate": 0.00028928266438941076,
      "loss": 7.4795,
      "step": 2962
    },
    {
      "epoch": 0.8435587188612099,
      "grad_norm": 0.4770597815513611,
      "learning_rate": 0.0002892115001423285,
      "loss": 7.4512,
      "step": 2963
    },
    {
      "epoch": 0.8438434163701067,
      "grad_norm": 0.5196690559387207,
      "learning_rate": 0.00028914033589524626,
      "loss": 7.3516,
      "step": 2964
    },
    {
      "epoch": 0.8441281138790035,
      "grad_norm": 0.5653435587882996,
      "learning_rate": 0.000289069171648164,
      "loss": 6.708,
      "step": 2965
    },
    {
      "epoch": 0.8444128113879004,
      "grad_norm": 0.5020940899848938,
      "learning_rate": 0.0002889980074010817,
      "loss": 7.3701,
      "step": 2966
    },
    {
      "epoch": 0.8446975088967972,
      "grad_norm": 0.6254937648773193,
      "learning_rate": 0.0002889268431539994,
      "loss": 6.5576,
      "step": 2967
    },
    {
      "epoch": 0.844982206405694,
      "grad_norm": 0.5759880542755127,
      "learning_rate": 0.00028885567890691715,
      "loss": 7.0918,
      "step": 2968
    },
    {
      "epoch": 0.8452669039145907,
      "grad_norm": 0.48996731638908386,
      "learning_rate": 0.0002887845146598349,
      "loss": 7.5615,
      "step": 2969
    },
    {
      "epoch": 0.8455516014234875,
      "grad_norm": 0.5695719718933105,
      "learning_rate": 0.00028871335041275265,
      "loss": 6.6055,
      "step": 2970
    },
    {
      "epoch": 0.8458362989323843,
      "grad_norm": 0.4751853048801422,
      "learning_rate": 0.00028864218616567037,
      "loss": 7.7266,
      "step": 2971
    },
    {
      "epoch": 0.8461209964412811,
      "grad_norm": 0.4919789433479309,
      "learning_rate": 0.0002885710219185881,
      "loss": 7.667,
      "step": 2972
    },
    {
      "epoch": 0.846405693950178,
      "grad_norm": 0.4890647232532501,
      "learning_rate": 0.0002884998576715058,
      "loss": 7.8711,
      "step": 2973
    },
    {
      "epoch": 0.8466903914590748,
      "grad_norm": 0.4198705852031708,
      "learning_rate": 0.0002884286934244236,
      "loss": 7.8242,
      "step": 2974
    },
    {
      "epoch": 0.8469750889679716,
      "grad_norm": 0.4951741695404053,
      "learning_rate": 0.0002883575291773413,
      "loss": 7.0732,
      "step": 2975
    },
    {
      "epoch": 0.8472597864768683,
      "grad_norm": 0.4614298641681671,
      "learning_rate": 0.00028828636493025904,
      "loss": 8.0674,
      "step": 2976
    },
    {
      "epoch": 0.8475444839857651,
      "grad_norm": 0.5194230675697327,
      "learning_rate": 0.0002882152006831768,
      "loss": 7.3555,
      "step": 2977
    },
    {
      "epoch": 0.8478291814946619,
      "grad_norm": 0.5074778199195862,
      "learning_rate": 0.0002881440364360945,
      "loss": 7.9824,
      "step": 2978
    },
    {
      "epoch": 0.8481138790035587,
      "grad_norm": 0.6814352869987488,
      "learning_rate": 0.00028807287218901226,
      "loss": 6.7842,
      "step": 2979
    },
    {
      "epoch": 0.8483985765124555,
      "grad_norm": 0.4877645969390869,
      "learning_rate": 0.00028800170794193,
      "loss": 7.8115,
      "step": 2980
    },
    {
      "epoch": 0.8486832740213524,
      "grad_norm": 0.4711715579032898,
      "learning_rate": 0.0002879305436948477,
      "loss": 7.8496,
      "step": 2981
    },
    {
      "epoch": 0.8489679715302492,
      "grad_norm": 0.5010735988616943,
      "learning_rate": 0.0002878593794477655,
      "loss": 7.6328,
      "step": 2982
    },
    {
      "epoch": 0.8492526690391459,
      "grad_norm": 0.6003854870796204,
      "learning_rate": 0.00028778821520068315,
      "loss": 7.0557,
      "step": 2983
    },
    {
      "epoch": 0.8495373665480427,
      "grad_norm": 0.5168098211288452,
      "learning_rate": 0.00028771705095360093,
      "loss": 7.5791,
      "step": 2984
    },
    {
      "epoch": 0.8498220640569395,
      "grad_norm": 0.6423295140266418,
      "learning_rate": 0.00028764588670651865,
      "loss": 6.791,
      "step": 2985
    },
    {
      "epoch": 0.8501067615658363,
      "grad_norm": 0.6456513404846191,
      "learning_rate": 0.00028757472245943637,
      "loss": 6.835,
      "step": 2986
    },
    {
      "epoch": 0.8503914590747331,
      "grad_norm": 0.5211045742034912,
      "learning_rate": 0.00028750355821235415,
      "loss": 7.7402,
      "step": 2987
    },
    {
      "epoch": 0.8506761565836299,
      "grad_norm": 0.546527624130249,
      "learning_rate": 0.00028743239396527187,
      "loss": 7.502,
      "step": 2988
    },
    {
      "epoch": 0.8509608540925266,
      "grad_norm": 0.5783496499061584,
      "learning_rate": 0.00028736122971818954,
      "loss": 7.7305,
      "step": 2989
    },
    {
      "epoch": 0.8512455516014235,
      "grad_norm": 0.5635664463043213,
      "learning_rate": 0.0002872900654711073,
      "loss": 7.0381,
      "step": 2990
    },
    {
      "epoch": 0.8515302491103203,
      "grad_norm": 0.5308234691619873,
      "learning_rate": 0.00028721890122402504,
      "loss": 7.7188,
      "step": 2991
    },
    {
      "epoch": 0.8518149466192171,
      "grad_norm": 0.4156032204627991,
      "learning_rate": 0.0002871477369769428,
      "loss": 7.9277,
      "step": 2992
    },
    {
      "epoch": 0.8520996441281139,
      "grad_norm": 0.4775456488132477,
      "learning_rate": 0.00028707657272986054,
      "loss": 7.5479,
      "step": 2993
    },
    {
      "epoch": 0.8523843416370107,
      "grad_norm": 0.4898991286754608,
      "learning_rate": 0.00028700540848277826,
      "loss": 7.6084,
      "step": 2994
    },
    {
      "epoch": 0.8526690391459075,
      "grad_norm": 0.4812511205673218,
      "learning_rate": 0.000286934244235696,
      "loss": 7.4805,
      "step": 2995
    },
    {
      "epoch": 0.8529537366548042,
      "grad_norm": 0.5089350938796997,
      "learning_rate": 0.0002868630799886137,
      "loss": 7.3281,
      "step": 2996
    },
    {
      "epoch": 0.853238434163701,
      "grad_norm": 0.577480673789978,
      "learning_rate": 0.0002867919157415315,
      "loss": 7.6084,
      "step": 2997
    },
    {
      "epoch": 0.8535231316725979,
      "grad_norm": 0.5122290849685669,
      "learning_rate": 0.0002867207514944492,
      "loss": 7.2529,
      "step": 2998
    },
    {
      "epoch": 0.8538078291814947,
      "grad_norm": 0.5206592679023743,
      "learning_rate": 0.00028664958724736693,
      "loss": 7.29,
      "step": 2999
    },
    {
      "epoch": 0.8540925266903915,
      "grad_norm": 0.4641942083835602,
      "learning_rate": 0.00028657842300028465,
      "loss": 7.8867,
      "step": 3000
    },
    {
      "epoch": 0.8540925266903915,
      "eval_bleu": 0.14115295193778626,
      "eval_loss": 7.1875,
      "eval_runtime": 118.2679,
      "eval_samples_per_second": 2.401,
      "eval_steps_per_second": 0.152,
      "step": 3000
    },
    {
      "epoch": 0.8543772241992883,
      "grad_norm": 0.43838560581207275,
      "learning_rate": 0.0002865072587532024,
      "loss": 7.7715,
      "step": 3001
    },
    {
      "epoch": 0.854661921708185,
      "grad_norm": 0.5235442519187927,
      "learning_rate": 0.0002864360945061201,
      "loss": 7.5234,
      "step": 3002
    },
    {
      "epoch": 0.8549466192170818,
      "grad_norm": 0.5522905588150024,
      "learning_rate": 0.0002863649302590379,
      "loss": 7.6699,
      "step": 3003
    },
    {
      "epoch": 0.8552313167259786,
      "grad_norm": 0.5250788331031799,
      "learning_rate": 0.0002862937660119556,
      "loss": 7.5,
      "step": 3004
    },
    {
      "epoch": 0.8555160142348754,
      "grad_norm": 0.5278091430664062,
      "learning_rate": 0.0002862226017648734,
      "loss": 7.7725,
      "step": 3005
    },
    {
      "epoch": 0.8558007117437723,
      "grad_norm": 0.4550979435443878,
      "learning_rate": 0.00028615143751779104,
      "loss": 7.4932,
      "step": 3006
    },
    {
      "epoch": 0.8560854092526691,
      "grad_norm": 0.6251030564308167,
      "learning_rate": 0.00028608027327070877,
      "loss": 7.2559,
      "step": 3007
    },
    {
      "epoch": 0.8563701067615659,
      "grad_norm": 0.6265691518783569,
      "learning_rate": 0.00028600910902362654,
      "loss": 7.5225,
      "step": 3008
    },
    {
      "epoch": 0.8566548042704626,
      "grad_norm": 0.5920205116271973,
      "learning_rate": 0.00028593794477654427,
      "loss": 7.1904,
      "step": 3009
    },
    {
      "epoch": 0.8569395017793594,
      "grad_norm": 0.624319314956665,
      "learning_rate": 0.00028586678052946204,
      "loss": 7.6797,
      "step": 3010
    },
    {
      "epoch": 0.8572241992882562,
      "grad_norm": 0.5414064526557922,
      "learning_rate": 0.0002857956162823797,
      "loss": 7.4512,
      "step": 3011
    },
    {
      "epoch": 0.857508896797153,
      "grad_norm": 0.580666184425354,
      "learning_rate": 0.00028572445203529744,
      "loss": 7.2188,
      "step": 3012
    },
    {
      "epoch": 0.8577935943060498,
      "grad_norm": 0.5600217580795288,
      "learning_rate": 0.0002856532877882152,
      "loss": 7.5859,
      "step": 3013
    },
    {
      "epoch": 0.8580782918149467,
      "grad_norm": 0.4794575870037079,
      "learning_rate": 0.00028558212354113294,
      "loss": 7.6855,
      "step": 3014
    },
    {
      "epoch": 0.8583629893238434,
      "grad_norm": 0.5584384202957153,
      "learning_rate": 0.0002855109592940507,
      "loss": 7.4004,
      "step": 3015
    },
    {
      "epoch": 0.8586476868327402,
      "grad_norm": 0.6063984036445618,
      "learning_rate": 0.00028543979504696843,
      "loss": 7.0869,
      "step": 3016
    },
    {
      "epoch": 0.858932384341637,
      "grad_norm": 0.5789662003517151,
      "learning_rate": 0.0002853686307998861,
      "loss": 7.8184,
      "step": 3017
    },
    {
      "epoch": 0.8592170818505338,
      "grad_norm": 0.6027911901473999,
      "learning_rate": 0.0002852974665528039,
      "loss": 6.8721,
      "step": 3018
    },
    {
      "epoch": 0.8595017793594306,
      "grad_norm": 0.4849622845649719,
      "learning_rate": 0.0002852263023057216,
      "loss": 7.8027,
      "step": 3019
    },
    {
      "epoch": 0.8597864768683274,
      "grad_norm": 0.5520667433738708,
      "learning_rate": 0.0002851551380586393,
      "loss": 7.4336,
      "step": 3020
    },
    {
      "epoch": 0.8600711743772242,
      "grad_norm": 0.5987728238105774,
      "learning_rate": 0.0002850839738115571,
      "loss": 7.0488,
      "step": 3021
    },
    {
      "epoch": 0.860355871886121,
      "grad_norm": 0.5028682351112366,
      "learning_rate": 0.0002850128095644748,
      "loss": 7.1123,
      "step": 3022
    },
    {
      "epoch": 0.8606405693950178,
      "grad_norm": 0.7498019337654114,
      "learning_rate": 0.00028494164531739255,
      "loss": 6.998,
      "step": 3023
    },
    {
      "epoch": 0.8609252669039146,
      "grad_norm": 0.5931838154792786,
      "learning_rate": 0.00028487048107031027,
      "loss": 6.9883,
      "step": 3024
    },
    {
      "epoch": 0.8612099644128114,
      "grad_norm": 0.4606742560863495,
      "learning_rate": 0.000284799316823228,
      "loss": 7.6797,
      "step": 3025
    },
    {
      "epoch": 0.8614946619217082,
      "grad_norm": 0.5937454104423523,
      "learning_rate": 0.00028472815257614577,
      "loss": 7.0674,
      "step": 3026
    },
    {
      "epoch": 0.861779359430605,
      "grad_norm": 0.4724361300468445,
      "learning_rate": 0.0002846569883290635,
      "loss": 7.9658,
      "step": 3027
    },
    {
      "epoch": 0.8620640569395018,
      "grad_norm": 0.6448618173599243,
      "learning_rate": 0.0002845858240819812,
      "loss": 7.0342,
      "step": 3028
    },
    {
      "epoch": 0.8623487544483985,
      "grad_norm": 0.591201663017273,
      "learning_rate": 0.00028451465983489894,
      "loss": 7.3906,
      "step": 3029
    },
    {
      "epoch": 0.8626334519572953,
      "grad_norm": 0.5071381330490112,
      "learning_rate": 0.00028444349558781666,
      "loss": 7.2539,
      "step": 3030
    },
    {
      "epoch": 0.8629181494661922,
      "grad_norm": 0.5736438632011414,
      "learning_rate": 0.00028437233134073444,
      "loss": 7.2119,
      "step": 3031
    },
    {
      "epoch": 0.863202846975089,
      "grad_norm": 0.5574355721473694,
      "learning_rate": 0.00028430116709365216,
      "loss": 7.1855,
      "step": 3032
    },
    {
      "epoch": 0.8634875444839858,
      "grad_norm": 0.6416983604431152,
      "learning_rate": 0.00028423000284656994,
      "loss": 7.4434,
      "step": 3033
    },
    {
      "epoch": 0.8637722419928826,
      "grad_norm": 0.49479612708091736,
      "learning_rate": 0.0002841588385994876,
      "loss": 7.3301,
      "step": 3034
    },
    {
      "epoch": 0.8640569395017793,
      "grad_norm": 0.5077500939369202,
      "learning_rate": 0.00028408767435240533,
      "loss": 7.4355,
      "step": 3035
    },
    {
      "epoch": 0.8643416370106761,
      "grad_norm": 0.5138323903083801,
      "learning_rate": 0.0002840165101053231,
      "loss": 7.3799,
      "step": 3036
    },
    {
      "epoch": 0.8646263345195729,
      "grad_norm": 0.61850905418396,
      "learning_rate": 0.00028394534585824083,
      "loss": 7.0068,
      "step": 3037
    },
    {
      "epoch": 0.8649110320284698,
      "grad_norm": 0.7099434733390808,
      "learning_rate": 0.00028387418161115855,
      "loss": 6.8574,
      "step": 3038
    },
    {
      "epoch": 0.8651957295373666,
      "grad_norm": 0.6962096095085144,
      "learning_rate": 0.00028380301736407633,
      "loss": 6.6953,
      "step": 3039
    },
    {
      "epoch": 0.8654804270462634,
      "grad_norm": 0.5490751266479492,
      "learning_rate": 0.000283731853116994,
      "loss": 7.5439,
      "step": 3040
    },
    {
      "epoch": 0.8657651245551602,
      "grad_norm": 0.622568666934967,
      "learning_rate": 0.0002836606888699118,
      "loss": 7.332,
      "step": 3041
    },
    {
      "epoch": 0.8660498220640569,
      "grad_norm": 0.554176926612854,
      "learning_rate": 0.0002835895246228295,
      "loss": 7.3389,
      "step": 3042
    },
    {
      "epoch": 0.8663345195729537,
      "grad_norm": 0.5685608983039856,
      "learning_rate": 0.0002835183603757472,
      "loss": 7.2344,
      "step": 3043
    },
    {
      "epoch": 0.8666192170818505,
      "grad_norm": 0.5276616215705872,
      "learning_rate": 0.000283447196128665,
      "loss": 7.332,
      "step": 3044
    },
    {
      "epoch": 0.8669039145907473,
      "grad_norm": 0.5317633152008057,
      "learning_rate": 0.00028337603188158267,
      "loss": 7.502,
      "step": 3045
    },
    {
      "epoch": 0.8671886120996442,
      "grad_norm": 0.5578818321228027,
      "learning_rate": 0.00028330486763450044,
      "loss": 7.5742,
      "step": 3046
    },
    {
      "epoch": 0.867473309608541,
      "grad_norm": 0.47466179728507996,
      "learning_rate": 0.00028323370338741817,
      "loss": 8.0713,
      "step": 3047
    },
    {
      "epoch": 0.8677580071174377,
      "grad_norm": 0.5722448229789734,
      "learning_rate": 0.0002831625391403359,
      "loss": 7.1768,
      "step": 3048
    },
    {
      "epoch": 0.8680427046263345,
      "grad_norm": 0.5063913464546204,
      "learning_rate": 0.00028309137489325367,
      "loss": 7.2412,
      "step": 3049
    },
    {
      "epoch": 0.8683274021352313,
      "grad_norm": 0.47729870676994324,
      "learning_rate": 0.0002830202106461714,
      "loss": 8.2256,
      "step": 3050
    },
    {
      "epoch": 0.8686120996441281,
      "grad_norm": 0.7070218920707703,
      "learning_rate": 0.00028294904639908906,
      "loss": 6.6631,
      "step": 3051
    },
    {
      "epoch": 0.8688967971530249,
      "grad_norm": 0.5421034693717957,
      "learning_rate": 0.00028287788215200683,
      "loss": 7.9395,
      "step": 3052
    },
    {
      "epoch": 0.8691814946619217,
      "grad_norm": 0.39937883615493774,
      "learning_rate": 0.00028280671790492456,
      "loss": 7.6846,
      "step": 3053
    },
    {
      "epoch": 0.8694661921708186,
      "grad_norm": 0.5363669395446777,
      "learning_rate": 0.00028273555365784233,
      "loss": 7.083,
      "step": 3054
    },
    {
      "epoch": 0.8697508896797153,
      "grad_norm": 0.46762025356292725,
      "learning_rate": 0.00028266438941076006,
      "loss": 7.6582,
      "step": 3055
    },
    {
      "epoch": 0.8700355871886121,
      "grad_norm": 0.5020787715911865,
      "learning_rate": 0.0002825932251636777,
      "loss": 7.6318,
      "step": 3056
    },
    {
      "epoch": 0.8703202846975089,
      "grad_norm": 0.5191118717193604,
      "learning_rate": 0.0002825220609165955,
      "loss": 7.4375,
      "step": 3057
    },
    {
      "epoch": 0.8706049822064057,
      "grad_norm": 0.6543094515800476,
      "learning_rate": 0.0002824508966695132,
      "loss": 7.1846,
      "step": 3058
    },
    {
      "epoch": 0.8708896797153025,
      "grad_norm": 0.49503618478775024,
      "learning_rate": 0.000282379732422431,
      "loss": 7.5547,
      "step": 3059
    },
    {
      "epoch": 0.8711743772241993,
      "grad_norm": 0.5053434371948242,
      "learning_rate": 0.0002823085681753487,
      "loss": 7.9102,
      "step": 3060
    },
    {
      "epoch": 0.8714590747330961,
      "grad_norm": 0.6238411068916321,
      "learning_rate": 0.00028223740392826645,
      "loss": 7.2764,
      "step": 3061
    },
    {
      "epoch": 0.8717437722419928,
      "grad_norm": 0.5801379084587097,
      "learning_rate": 0.00028216623968118417,
      "loss": 6.9268,
      "step": 3062
    },
    {
      "epoch": 0.8720284697508897,
      "grad_norm": 0.5912047028541565,
      "learning_rate": 0.0002820950754341019,
      "loss": 7.3906,
      "step": 3063
    },
    {
      "epoch": 0.8723131672597865,
      "grad_norm": 0.4282529056072235,
      "learning_rate": 0.00028202391118701967,
      "loss": 8.0576,
      "step": 3064
    },
    {
      "epoch": 0.8725978647686833,
      "grad_norm": 0.46133551001548767,
      "learning_rate": 0.0002819527469399374,
      "loss": 7.3906,
      "step": 3065
    },
    {
      "epoch": 0.8728825622775801,
      "grad_norm": 0.5819246768951416,
      "learning_rate": 0.0002818815826928551,
      "loss": 7.2295,
      "step": 3066
    },
    {
      "epoch": 0.8731672597864769,
      "grad_norm": 0.5783908367156982,
      "learning_rate": 0.0002818104184457729,
      "loss": 7.1797,
      "step": 3067
    },
    {
      "epoch": 0.8734519572953736,
      "grad_norm": 0.4961581528186798,
      "learning_rate": 0.00028173925419869056,
      "loss": 7.3662,
      "step": 3068
    },
    {
      "epoch": 0.8737366548042704,
      "grad_norm": 0.48812779784202576,
      "learning_rate": 0.0002816680899516083,
      "loss": 7.5342,
      "step": 3069
    },
    {
      "epoch": 0.8740213523131672,
      "grad_norm": 0.6271644234657288,
      "learning_rate": 0.00028159692570452606,
      "loss": 7.2549,
      "step": 3070
    },
    {
      "epoch": 0.8743060498220641,
      "grad_norm": 0.526840329170227,
      "learning_rate": 0.0002815257614574438,
      "loss": 7.5312,
      "step": 3071
    },
    {
      "epoch": 0.8745907473309609,
      "grad_norm": 0.5215483903884888,
      "learning_rate": 0.00028145459721036156,
      "loss": 7.3271,
      "step": 3072
    },
    {
      "epoch": 0.8748754448398577,
      "grad_norm": 0.5366951823234558,
      "learning_rate": 0.00028138343296327923,
      "loss": 7.3604,
      "step": 3073
    },
    {
      "epoch": 0.8751601423487545,
      "grad_norm": 0.5800265669822693,
      "learning_rate": 0.00028131226871619695,
      "loss": 7.1348,
      "step": 3074
    },
    {
      "epoch": 0.8754448398576512,
      "grad_norm": 0.5644967555999756,
      "learning_rate": 0.00028124110446911473,
      "loss": 7.2549,
      "step": 3075
    },
    {
      "epoch": 0.875729537366548,
      "grad_norm": 0.5224553942680359,
      "learning_rate": 0.00028116994022203245,
      "loss": 7.2734,
      "step": 3076
    },
    {
      "epoch": 0.8760142348754448,
      "grad_norm": 0.5145871639251709,
      "learning_rate": 0.00028109877597495023,
      "loss": 7.4277,
      "step": 3077
    },
    {
      "epoch": 0.8762989323843416,
      "grad_norm": 0.5461599826812744,
      "learning_rate": 0.00028102761172786795,
      "loss": 7.4502,
      "step": 3078
    },
    {
      "epoch": 0.8765836298932385,
      "grad_norm": 0.5628382563591003,
      "learning_rate": 0.0002809564474807856,
      "loss": 7.3711,
      "step": 3079
    },
    {
      "epoch": 0.8768683274021353,
      "grad_norm": 0.5583856701850891,
      "learning_rate": 0.0002808852832337034,
      "loss": 7.8242,
      "step": 3080
    },
    {
      "epoch": 0.877153024911032,
      "grad_norm": 0.5027297735214233,
      "learning_rate": 0.0002808141189866211,
      "loss": 7.9463,
      "step": 3081
    },
    {
      "epoch": 0.8774377224199288,
      "grad_norm": 0.5520659685134888,
      "learning_rate": 0.0002807429547395389,
      "loss": 6.9023,
      "step": 3082
    },
    {
      "epoch": 0.8777224199288256,
      "grad_norm": 0.4830836057662964,
      "learning_rate": 0.0002806717904924566,
      "loss": 7.752,
      "step": 3083
    },
    {
      "epoch": 0.8780071174377224,
      "grad_norm": 0.4201578199863434,
      "learning_rate": 0.00028060062624537434,
      "loss": 8.0713,
      "step": 3084
    },
    {
      "epoch": 0.8782918149466192,
      "grad_norm": 0.5825881361961365,
      "learning_rate": 0.00028052946199829206,
      "loss": 7.1123,
      "step": 3085
    },
    {
      "epoch": 0.878576512455516,
      "grad_norm": 0.5017299056053162,
      "learning_rate": 0.0002804582977512098,
      "loss": 7.3301,
      "step": 3086
    },
    {
      "epoch": 0.8788612099644129,
      "grad_norm": 0.6689668297767639,
      "learning_rate": 0.0002803871335041275,
      "loss": 7.2314,
      "step": 3087
    },
    {
      "epoch": 0.8791459074733096,
      "grad_norm": 0.7329010963439941,
      "learning_rate": 0.0002803159692570453,
      "loss": 7.1904,
      "step": 3088
    },
    {
      "epoch": 0.8794306049822064,
      "grad_norm": 0.5570193529129028,
      "learning_rate": 0.000280244805009963,
      "loss": 7.165,
      "step": 3089
    },
    {
      "epoch": 0.8797153024911032,
      "grad_norm": 0.5482802391052246,
      "learning_rate": 0.00028017364076288073,
      "loss": 7.5537,
      "step": 3090
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.4912322759628296,
      "learning_rate": 0.00028010247651579845,
      "loss": 7.6025,
      "step": 3091
    },
    {
      "epoch": 0.8802846975088968,
      "grad_norm": 2.2309093475341797,
      "learning_rate": 0.0002800313122687162,
      "loss": 8.25,
      "step": 3092
    },
    {
      "epoch": 0.8805693950177936,
      "grad_norm": 0.5651352405548096,
      "learning_rate": 0.00027996014802163395,
      "loss": 7.2832,
      "step": 3093
    },
    {
      "epoch": 0.8808540925266904,
      "grad_norm": 0.4873019754886627,
      "learning_rate": 0.0002798889837745517,
      "loss": 7.5635,
      "step": 3094
    },
    {
      "epoch": 0.8811387900355871,
      "grad_norm": 0.5409858226776123,
      "learning_rate": 0.00027981781952746945,
      "loss": 7.6592,
      "step": 3095
    },
    {
      "epoch": 0.881423487544484,
      "grad_norm": 0.43399906158447266,
      "learning_rate": 0.0002797466552803871,
      "loss": 7.4092,
      "step": 3096
    },
    {
      "epoch": 0.8817081850533808,
      "grad_norm": 0.5362834930419922,
      "learning_rate": 0.00027967549103330485,
      "loss": 7.4492,
      "step": 3097
    },
    {
      "epoch": 0.8819928825622776,
      "grad_norm": 0.5556686520576477,
      "learning_rate": 0.0002796043267862226,
      "loss": 7.7168,
      "step": 3098
    },
    {
      "epoch": 0.8822775800711744,
      "grad_norm": 0.6528327465057373,
      "learning_rate": 0.00027953316253914035,
      "loss": 6.9746,
      "step": 3099
    },
    {
      "epoch": 0.8825622775800712,
      "grad_norm": 0.5681182742118835,
      "learning_rate": 0.00027946199829205807,
      "loss": 7.0273,
      "step": 3100
    },
    {
      "epoch": 0.8828469750889679,
      "grad_norm": 0.5345963835716248,
      "learning_rate": 0.0002793908340449758,
      "loss": 7.5576,
      "step": 3101
    },
    {
      "epoch": 0.8831316725978647,
      "grad_norm": 0.4878937900066376,
      "learning_rate": 0.0002793196697978935,
      "loss": 7.7656,
      "step": 3102
    },
    {
      "epoch": 0.8834163701067616,
      "grad_norm": 0.44907352328300476,
      "learning_rate": 0.0002792485055508113,
      "loss": 7.9639,
      "step": 3103
    },
    {
      "epoch": 0.8837010676156584,
      "grad_norm": 1.2519561052322388,
      "learning_rate": 0.000279177341303729,
      "loss": 7.5137,
      "step": 3104
    },
    {
      "epoch": 0.8839857651245552,
      "grad_norm": 0.4996868968009949,
      "learning_rate": 0.00027910617705664674,
      "loss": 7.2764,
      "step": 3105
    },
    {
      "epoch": 0.884270462633452,
      "grad_norm": 0.4029379189014435,
      "learning_rate": 0.0002790350128095645,
      "loss": 7.6553,
      "step": 3106
    },
    {
      "epoch": 0.8845551601423488,
      "grad_norm": 0.5408071875572205,
      "learning_rate": 0.0002789638485624822,
      "loss": 7.4199,
      "step": 3107
    },
    {
      "epoch": 0.8848398576512455,
      "grad_norm": 0.6991518139839172,
      "learning_rate": 0.00027889268431539996,
      "loss": 7.4062,
      "step": 3108
    },
    {
      "epoch": 0.8851245551601423,
      "grad_norm": 0.4633163511753082,
      "learning_rate": 0.0002788215200683177,
      "loss": 7.7266,
      "step": 3109
    },
    {
      "epoch": 0.8854092526690391,
      "grad_norm": 0.5114037394523621,
      "learning_rate": 0.0002787503558212354,
      "loss": 8.2227,
      "step": 3110
    },
    {
      "epoch": 0.885693950177936,
      "grad_norm": 0.5401403307914734,
      "learning_rate": 0.0002786791915741532,
      "loss": 7.4199,
      "step": 3111
    },
    {
      "epoch": 0.8859786476868328,
      "grad_norm": 0.47539880871772766,
      "learning_rate": 0.0002786080273270709,
      "loss": 7.4541,
      "step": 3112
    },
    {
      "epoch": 0.8862633451957296,
      "grad_norm": 0.7226040959358215,
      "learning_rate": 0.0002785368630799886,
      "loss": 6.7188,
      "step": 3113
    },
    {
      "epoch": 0.8865480427046263,
      "grad_norm": 0.4396054744720459,
      "learning_rate": 0.00027846569883290635,
      "loss": 7.7051,
      "step": 3114
    },
    {
      "epoch": 0.8868327402135231,
      "grad_norm": 0.47691434621810913,
      "learning_rate": 0.00027839453458582407,
      "loss": 7.5244,
      "step": 3115
    },
    {
      "epoch": 0.8871174377224199,
      "grad_norm": 0.3708919286727905,
      "learning_rate": 0.00027832337033874185,
      "loss": 8.2988,
      "step": 3116
    },
    {
      "epoch": 0.8874021352313167,
      "grad_norm": 0.5343900918960571,
      "learning_rate": 0.00027825220609165957,
      "loss": 7.3252,
      "step": 3117
    },
    {
      "epoch": 0.8876868327402135,
      "grad_norm": 0.5353043079376221,
      "learning_rate": 0.00027818104184457724,
      "loss": 7.6562,
      "step": 3118
    },
    {
      "epoch": 0.8879715302491104,
      "grad_norm": 0.5595890283584595,
      "learning_rate": 0.000278109877597495,
      "loss": 7.6328,
      "step": 3119
    },
    {
      "epoch": 0.8882562277580072,
      "grad_norm": 0.5575851202011108,
      "learning_rate": 0.00027803871335041274,
      "loss": 7.2695,
      "step": 3120
    },
    {
      "epoch": 0.8885409252669039,
      "grad_norm": 0.41911351680755615,
      "learning_rate": 0.0002779675491033305,
      "loss": 8.1885,
      "step": 3121
    },
    {
      "epoch": 0.8888256227758007,
      "grad_norm": 0.46832743287086487,
      "learning_rate": 0.00027789638485624824,
      "loss": 7.7695,
      "step": 3122
    },
    {
      "epoch": 0.8891103202846975,
      "grad_norm": 0.5042351484298706,
      "learning_rate": 0.00027782522060916596,
      "loss": 7.5996,
      "step": 3123
    },
    {
      "epoch": 0.8893950177935943,
      "grad_norm": 0.6332950592041016,
      "learning_rate": 0.0002777540563620837,
      "loss": 7.1592,
      "step": 3124
    },
    {
      "epoch": 0.8896797153024911,
      "grad_norm": 0.5015016794204712,
      "learning_rate": 0.0002776828921150014,
      "loss": 7.833,
      "step": 3125
    },
    {
      "epoch": 0.8899644128113879,
      "grad_norm": 0.5481613278388977,
      "learning_rate": 0.0002776117278679192,
      "loss": 7.1221,
      "step": 3126
    },
    {
      "epoch": 0.8902491103202846,
      "grad_norm": 0.5739569067955017,
      "learning_rate": 0.0002775405636208369,
      "loss": 7.4873,
      "step": 3127
    },
    {
      "epoch": 0.8905338078291815,
      "grad_norm": 0.4153132140636444,
      "learning_rate": 0.00027746939937375463,
      "loss": 8.1221,
      "step": 3128
    },
    {
      "epoch": 0.8908185053380783,
      "grad_norm": 0.5260197520256042,
      "learning_rate": 0.00027739823512667235,
      "loss": 7.4355,
      "step": 3129
    },
    {
      "epoch": 0.8911032028469751,
      "grad_norm": 0.5186213254928589,
      "learning_rate": 0.0002773270708795901,
      "loss": 7.3496,
      "step": 3130
    },
    {
      "epoch": 0.8913879003558719,
      "grad_norm": 0.5588082671165466,
      "learning_rate": 0.00027725590663250785,
      "loss": 7.165,
      "step": 3131
    },
    {
      "epoch": 0.8916725978647687,
      "grad_norm": 0.6211076378822327,
      "learning_rate": 0.0002771847423854256,
      "loss": 7.4473,
      "step": 3132
    },
    {
      "epoch": 0.8919572953736655,
      "grad_norm": 0.4754480719566345,
      "learning_rate": 0.0002771135781383433,
      "loss": 7.6514,
      "step": 3133
    },
    {
      "epoch": 0.8922419928825622,
      "grad_norm": 0.5634303092956543,
      "learning_rate": 0.0002770424138912611,
      "loss": 7.3643,
      "step": 3134
    },
    {
      "epoch": 0.892526690391459,
      "grad_norm": 0.4101797640323639,
      "learning_rate": 0.00027697124964417874,
      "loss": 8.0215,
      "step": 3135
    },
    {
      "epoch": 0.8928113879003559,
      "grad_norm": 0.5415500998497009,
      "learning_rate": 0.00027690008539709647,
      "loss": 7.8936,
      "step": 3136
    },
    {
      "epoch": 0.8930960854092527,
      "grad_norm": 0.5912952423095703,
      "learning_rate": 0.00027682892115001424,
      "loss": 7.5459,
      "step": 3137
    },
    {
      "epoch": 0.8933807829181495,
      "grad_norm": 0.5060358047485352,
      "learning_rate": 0.00027675775690293197,
      "loss": 7.7148,
      "step": 3138
    },
    {
      "epoch": 0.8936654804270463,
      "grad_norm": 0.46236804127693176,
      "learning_rate": 0.00027668659265584974,
      "loss": 7.6738,
      "step": 3139
    },
    {
      "epoch": 0.8939501779359431,
      "grad_norm": 0.5547524094581604,
      "learning_rate": 0.00027661542840876747,
      "loss": 7.4746,
      "step": 3140
    },
    {
      "epoch": 0.8942348754448398,
      "grad_norm": 0.5038734674453735,
      "learning_rate": 0.00027654426416168513,
      "loss": 7.5078,
      "step": 3141
    },
    {
      "epoch": 0.8945195729537366,
      "grad_norm": 0.5187249779701233,
      "learning_rate": 0.0002764730999146029,
      "loss": 7.3408,
      "step": 3142
    },
    {
      "epoch": 0.8948042704626334,
      "grad_norm": 0.5759621262550354,
      "learning_rate": 0.00027640193566752063,
      "loss": 7.1162,
      "step": 3143
    },
    {
      "epoch": 0.8950889679715303,
      "grad_norm": 0.46457135677337646,
      "learning_rate": 0.0002763307714204384,
      "loss": 7.5459,
      "step": 3144
    },
    {
      "epoch": 0.8953736654804271,
      "grad_norm": 0.4592977464199066,
      "learning_rate": 0.00027625960717335613,
      "loss": 7.8828,
      "step": 3145
    },
    {
      "epoch": 0.8956583629893239,
      "grad_norm": 0.496532678604126,
      "learning_rate": 0.0002761884429262738,
      "loss": 7.752,
      "step": 3146
    },
    {
      "epoch": 0.8959430604982206,
      "grad_norm": 0.6144600510597229,
      "learning_rate": 0.0002761172786791916,
      "loss": 7.0273,
      "step": 3147
    },
    {
      "epoch": 0.8962277580071174,
      "grad_norm": 0.5173940658569336,
      "learning_rate": 0.0002760461144321093,
      "loss": 7.7197,
      "step": 3148
    },
    {
      "epoch": 0.8965124555160142,
      "grad_norm": 0.5492969155311584,
      "learning_rate": 0.000275974950185027,
      "loss": 6.7393,
      "step": 3149
    },
    {
      "epoch": 0.896797153024911,
      "grad_norm": 1.070974349975586,
      "learning_rate": 0.0002759037859379448,
      "loss": 7.749,
      "step": 3150
    },
    {
      "epoch": 0.8970818505338078,
      "grad_norm": 0.48724380135536194,
      "learning_rate": 0.0002758326216908625,
      "loss": 7.5615,
      "step": 3151
    },
    {
      "epoch": 0.8973665480427047,
      "grad_norm": 0.5991201996803284,
      "learning_rate": 0.00027576145744378025,
      "loss": 7.3135,
      "step": 3152
    },
    {
      "epoch": 0.8976512455516015,
      "grad_norm": 0.6105537414550781,
      "learning_rate": 0.00027569029319669797,
      "loss": 6.8418,
      "step": 3153
    },
    {
      "epoch": 0.8979359430604982,
      "grad_norm": 0.5674680471420288,
      "learning_rate": 0.0002756191289496157,
      "loss": 7.5537,
      "step": 3154
    },
    {
      "epoch": 0.898220640569395,
      "grad_norm": 0.4547697603702545,
      "learning_rate": 0.00027554796470253347,
      "loss": 7.833,
      "step": 3155
    },
    {
      "epoch": 0.8985053380782918,
      "grad_norm": 0.5391954779624939,
      "learning_rate": 0.0002754768004554512,
      "loss": 7.4424,
      "step": 3156
    },
    {
      "epoch": 0.8987900355871886,
      "grad_norm": 0.5706303119659424,
      "learning_rate": 0.00027540563620836897,
      "loss": 7.0449,
      "step": 3157
    },
    {
      "epoch": 0.8990747330960854,
      "grad_norm": 0.689810037612915,
      "learning_rate": 0.00027533447196128664,
      "loss": 6.6914,
      "step": 3158
    },
    {
      "epoch": 0.8993594306049822,
      "grad_norm": 1.6351346969604492,
      "learning_rate": 0.00027526330771420436,
      "loss": 7.7285,
      "step": 3159
    },
    {
      "epoch": 0.899644128113879,
      "grad_norm": 0.5228148102760315,
      "learning_rate": 0.00027519214346712214,
      "loss": 7.7715,
      "step": 3160
    },
    {
      "epoch": 0.8999288256227758,
      "grad_norm": 0.5396202206611633,
      "learning_rate": 0.00027512097922003986,
      "loss": 7.1309,
      "step": 3161
    },
    {
      "epoch": 0.9002135231316726,
      "grad_norm": 0.6179732084274292,
      "learning_rate": 0.00027504981497295764,
      "loss": 7.4492,
      "step": 3162
    },
    {
      "epoch": 0.9004982206405694,
      "grad_norm": 0.6026339530944824,
      "learning_rate": 0.0002749786507258753,
      "loss": 7.1387,
      "step": 3163
    },
    {
      "epoch": 0.9007829181494662,
      "grad_norm": 0.5630602240562439,
      "learning_rate": 0.00027490748647879303,
      "loss": 7.6123,
      "step": 3164
    },
    {
      "epoch": 0.901067615658363,
      "grad_norm": 0.4620493948459625,
      "learning_rate": 0.0002748363222317108,
      "loss": 7.9023,
      "step": 3165
    },
    {
      "epoch": 0.9013523131672598,
      "grad_norm": 0.48856398463249207,
      "learning_rate": 0.00027476515798462853,
      "loss": 7.5928,
      "step": 3166
    },
    {
      "epoch": 0.9016370106761565,
      "grad_norm": 0.47969815135002136,
      "learning_rate": 0.00027469399373754625,
      "loss": 7.8896,
      "step": 3167
    },
    {
      "epoch": 0.9019217081850534,
      "grad_norm": 0.5698290467262268,
      "learning_rate": 0.00027462282949046403,
      "loss": 7.667,
      "step": 3168
    },
    {
      "epoch": 0.9022064056939502,
      "grad_norm": 0.5009581446647644,
      "learning_rate": 0.0002745516652433817,
      "loss": 7.2871,
      "step": 3169
    },
    {
      "epoch": 0.902491103202847,
      "grad_norm": 0.592872679233551,
      "learning_rate": 0.0002744805009962995,
      "loss": 7.3164,
      "step": 3170
    },
    {
      "epoch": 0.9027758007117438,
      "grad_norm": 0.45243149995803833,
      "learning_rate": 0.0002744093367492172,
      "loss": 7.7705,
      "step": 3171
    },
    {
      "epoch": 0.9030604982206406,
      "grad_norm": 0.4909220337867737,
      "learning_rate": 0.0002743381725021349,
      "loss": 7.9141,
      "step": 3172
    },
    {
      "epoch": 0.9033451957295374,
      "grad_norm": 0.7660431861877441,
      "learning_rate": 0.0002742670082550527,
      "loss": 6.4512,
      "step": 3173
    },
    {
      "epoch": 0.9036298932384341,
      "grad_norm": 0.6101886630058289,
      "learning_rate": 0.00027419584400797037,
      "loss": 6.9824,
      "step": 3174
    },
    {
      "epoch": 0.9039145907473309,
      "grad_norm": 0.49506044387817383,
      "learning_rate": 0.00027412467976088814,
      "loss": 7.7637,
      "step": 3175
    },
    {
      "epoch": 0.9041992882562278,
      "grad_norm": 0.48582154512405396,
      "learning_rate": 0.00027405351551380586,
      "loss": 7.7061,
      "step": 3176
    },
    {
      "epoch": 0.9044839857651246,
      "grad_norm": 0.5871376395225525,
      "learning_rate": 0.0002739823512667236,
      "loss": 6.8633,
      "step": 3177
    },
    {
      "epoch": 0.9047686832740214,
      "grad_norm": 0.5311984419822693,
      "learning_rate": 0.00027391118701964136,
      "loss": 7.5352,
      "step": 3178
    },
    {
      "epoch": 0.9050533807829182,
      "grad_norm": 0.5073509216308594,
      "learning_rate": 0.0002738400227725591,
      "loss": 7.4805,
      "step": 3179
    },
    {
      "epoch": 0.9053380782918149,
      "grad_norm": 0.5000579357147217,
      "learning_rate": 0.00027376885852547676,
      "loss": 7.874,
      "step": 3180
    },
    {
      "epoch": 0.9056227758007117,
      "grad_norm": 0.46852007508277893,
      "learning_rate": 0.00027369769427839453,
      "loss": 8.0303,
      "step": 3181
    },
    {
      "epoch": 0.9059074733096085,
      "grad_norm": 0.5285821557044983,
      "learning_rate": 0.00027362653003131226,
      "loss": 7.6826,
      "step": 3182
    },
    {
      "epoch": 0.9061921708185053,
      "grad_norm": 0.5116063356399536,
      "learning_rate": 0.00027355536578423003,
      "loss": 7.6152,
      "step": 3183
    },
    {
      "epoch": 0.9064768683274022,
      "grad_norm": 0.6086752414703369,
      "learning_rate": 0.00027348420153714776,
      "loss": 7.0332,
      "step": 3184
    },
    {
      "epoch": 0.906761565836299,
      "grad_norm": 0.6342504620552063,
      "learning_rate": 0.0002734130372900655,
      "loss": 7.0166,
      "step": 3185
    },
    {
      "epoch": 0.9070462633451958,
      "grad_norm": 0.39906132221221924,
      "learning_rate": 0.0002733418730429832,
      "loss": 8.0996,
      "step": 3186
    },
    {
      "epoch": 0.9073309608540925,
      "grad_norm": 0.4398525655269623,
      "learning_rate": 0.0002732707087959009,
      "loss": 7.7197,
      "step": 3187
    },
    {
      "epoch": 0.9076156583629893,
      "grad_norm": 0.5834694504737854,
      "learning_rate": 0.0002731995445488187,
      "loss": 6.9053,
      "step": 3188
    },
    {
      "epoch": 0.9079003558718861,
      "grad_norm": 0.4447949528694153,
      "learning_rate": 0.0002731283803017364,
      "loss": 7.7422,
      "step": 3189
    },
    {
      "epoch": 0.9081850533807829,
      "grad_norm": 0.4741443693637848,
      "learning_rate": 0.00027305721605465415,
      "loss": 7.8213,
      "step": 3190
    },
    {
      "epoch": 0.9084697508896797,
      "grad_norm": 0.6493892073631287,
      "learning_rate": 0.00027298605180757187,
      "loss": 6.7002,
      "step": 3191
    },
    {
      "epoch": 0.9087544483985766,
      "grad_norm": 0.5509630441665649,
      "learning_rate": 0.0002729148875604896,
      "loss": 7.2686,
      "step": 3192
    },
    {
      "epoch": 0.9090391459074733,
      "grad_norm": 0.5614623427391052,
      "learning_rate": 0.00027284372331340737,
      "loss": 7.2402,
      "step": 3193
    },
    {
      "epoch": 0.9093238434163701,
      "grad_norm": 0.5465844869613647,
      "learning_rate": 0.0002727725590663251,
      "loss": 7.8232,
      "step": 3194
    },
    {
      "epoch": 0.9096085409252669,
      "grad_norm": 0.5034477114677429,
      "learning_rate": 0.0002727013948192428,
      "loss": 7.8525,
      "step": 3195
    },
    {
      "epoch": 0.9098932384341637,
      "grad_norm": 0.588121771812439,
      "learning_rate": 0.0002726302305721606,
      "loss": 6.6455,
      "step": 3196
    },
    {
      "epoch": 0.9101779359430605,
      "grad_norm": 0.5669780969619751,
      "learning_rate": 0.00027255906632507826,
      "loss": 7.7939,
      "step": 3197
    },
    {
      "epoch": 0.9104626334519573,
      "grad_norm": 0.6072568297386169,
      "learning_rate": 0.000272487902077996,
      "loss": 7.5195,
      "step": 3198
    },
    {
      "epoch": 0.9107473309608541,
      "grad_norm": 0.5315139889717102,
      "learning_rate": 0.00027241673783091376,
      "loss": 7.6885,
      "step": 3199
    },
    {
      "epoch": 0.9110320284697508,
      "grad_norm": 0.5798125267028809,
      "learning_rate": 0.0002723455735838315,
      "loss": 7.7969,
      "step": 3200
    },
    {
      "epoch": 0.9110320284697508,
      "eval_bleu": 0.13258179575155835,
      "eval_loss": 7.19140625,
      "eval_runtime": 118.6046,
      "eval_samples_per_second": 2.395,
      "eval_steps_per_second": 0.152,
      "step": 3200
    },
    {
      "epoch": 0.9113167259786477,
      "grad_norm": 0.5376034379005432,
      "learning_rate": 0.00027227440933674926,
      "loss": 7.6738,
      "step": 3201
    },
    {
      "epoch": 0.9116014234875445,
      "grad_norm": 0.4701717495918274,
      "learning_rate": 0.000272203245089667,
      "loss": 7.4102,
      "step": 3202
    },
    {
      "epoch": 0.9118861209964413,
      "grad_norm": 0.5499773621559143,
      "learning_rate": 0.00027213208084258465,
      "loss": 7.1455,
      "step": 3203
    },
    {
      "epoch": 0.9121708185053381,
      "grad_norm": 0.671004593372345,
      "learning_rate": 0.00027206091659550243,
      "loss": 6.6357,
      "step": 3204
    },
    {
      "epoch": 0.9124555160142349,
      "grad_norm": 0.6145957708358765,
      "learning_rate": 0.00027198975234842015,
      "loss": 7.208,
      "step": 3205
    },
    {
      "epoch": 0.9127402135231317,
      "grad_norm": 0.492208868265152,
      "learning_rate": 0.0002719185881013379,
      "loss": 7.377,
      "step": 3206
    },
    {
      "epoch": 0.9130249110320284,
      "grad_norm": 0.5253836512565613,
      "learning_rate": 0.00027184742385425565,
      "loss": 7.7061,
      "step": 3207
    },
    {
      "epoch": 0.9133096085409252,
      "grad_norm": 0.6498883366584778,
      "learning_rate": 0.0002717762596071733,
      "loss": 7.1074,
      "step": 3208
    },
    {
      "epoch": 0.9135943060498221,
      "grad_norm": 0.6288406252861023,
      "learning_rate": 0.0002717050953600911,
      "loss": 7.4316,
      "step": 3209
    },
    {
      "epoch": 0.9138790035587189,
      "grad_norm": 0.5122976899147034,
      "learning_rate": 0.0002716339311130088,
      "loss": 7.2412,
      "step": 3210
    },
    {
      "epoch": 0.9141637010676157,
      "grad_norm": 0.6497461795806885,
      "learning_rate": 0.0002715627668659266,
      "loss": 7.2314,
      "step": 3211
    },
    {
      "epoch": 0.9144483985765125,
      "grad_norm": 0.49464985728263855,
      "learning_rate": 0.0002714916026188443,
      "loss": 7.4121,
      "step": 3212
    },
    {
      "epoch": 0.9147330960854092,
      "grad_norm": 0.5870091915130615,
      "learning_rate": 0.00027142043837176204,
      "loss": 7.0879,
      "step": 3213
    },
    {
      "epoch": 0.915017793594306,
      "grad_norm": 0.4872588515281677,
      "learning_rate": 0.00027134927412467976,
      "loss": 7.3135,
      "step": 3214
    },
    {
      "epoch": 0.9153024911032028,
      "grad_norm": 0.5292404294013977,
      "learning_rate": 0.0002712781098775975,
      "loss": 7.4111,
      "step": 3215
    },
    {
      "epoch": 0.9155871886120996,
      "grad_norm": 0.5179793238639832,
      "learning_rate": 0.0002712069456305152,
      "loss": 7.4512,
      "step": 3216
    },
    {
      "epoch": 0.9158718861209965,
      "grad_norm": 0.5008764863014221,
      "learning_rate": 0.000271135781383433,
      "loss": 7.3896,
      "step": 3217
    },
    {
      "epoch": 0.9161565836298933,
      "grad_norm": 0.4892398416996002,
      "learning_rate": 0.0002710646171363507,
      "loss": 7.8193,
      "step": 3218
    },
    {
      "epoch": 0.9164412811387901,
      "grad_norm": 0.6333082318305969,
      "learning_rate": 0.00027099345288926843,
      "loss": 7.4004,
      "step": 3219
    },
    {
      "epoch": 0.9167259786476868,
      "grad_norm": 0.5098070502281189,
      "learning_rate": 0.00027092228864218615,
      "loss": 7.1025,
      "step": 3220
    },
    {
      "epoch": 0.9170106761565836,
      "grad_norm": 0.5061480402946472,
      "learning_rate": 0.0002708511243951039,
      "loss": 7.8496,
      "step": 3221
    },
    {
      "epoch": 0.9172953736654804,
      "grad_norm": 0.5438953042030334,
      "learning_rate": 0.00027077996014802165,
      "loss": 8.0576,
      "step": 3222
    },
    {
      "epoch": 0.9175800711743772,
      "grad_norm": 0.6590709090232849,
      "learning_rate": 0.0002707087959009394,
      "loss": 6.8066,
      "step": 3223
    },
    {
      "epoch": 0.917864768683274,
      "grad_norm": 0.5052556991577148,
      "learning_rate": 0.00027063763165385715,
      "loss": 7.7012,
      "step": 3224
    },
    {
      "epoch": 0.9181494661921709,
      "grad_norm": 0.5070505142211914,
      "learning_rate": 0.0002705664674067748,
      "loss": 7.6777,
      "step": 3225
    },
    {
      "epoch": 0.9184341637010676,
      "grad_norm": 0.5833612084388733,
      "learning_rate": 0.00027049530315969254,
      "loss": 6.9648,
      "step": 3226
    },
    {
      "epoch": 0.9187188612099644,
      "grad_norm": 0.5067337155342102,
      "learning_rate": 0.0002704241389126103,
      "loss": 7.1943,
      "step": 3227
    },
    {
      "epoch": 0.9190035587188612,
      "grad_norm": 0.48680490255355835,
      "learning_rate": 0.00027035297466552804,
      "loss": 7.335,
      "step": 3228
    },
    {
      "epoch": 0.919288256227758,
      "grad_norm": 0.4560459554195404,
      "learning_rate": 0.0002702818104184458,
      "loss": 7.667,
      "step": 3229
    },
    {
      "epoch": 0.9195729537366548,
      "grad_norm": 0.5065756440162659,
      "learning_rate": 0.00027021064617136354,
      "loss": 7.7119,
      "step": 3230
    },
    {
      "epoch": 0.9198576512455516,
      "grad_norm": 0.6035388708114624,
      "learning_rate": 0.0002701394819242812,
      "loss": 7.1143,
      "step": 3231
    },
    {
      "epoch": 0.9201423487544484,
      "grad_norm": 0.5527863502502441,
      "learning_rate": 0.000270068317677199,
      "loss": 6.8867,
      "step": 3232
    },
    {
      "epoch": 0.9204270462633451,
      "grad_norm": 0.4766753315925598,
      "learning_rate": 0.0002699971534301167,
      "loss": 7.707,
      "step": 3233
    },
    {
      "epoch": 0.920711743772242,
      "grad_norm": 0.550078272819519,
      "learning_rate": 0.00026992598918303444,
      "loss": 7.7568,
      "step": 3234
    },
    {
      "epoch": 0.9209964412811388,
      "grad_norm": 0.6214766502380371,
      "learning_rate": 0.0002698548249359522,
      "loss": 6.7656,
      "step": 3235
    },
    {
      "epoch": 0.9212811387900356,
      "grad_norm": 0.6192547082901001,
      "learning_rate": 0.0002697836606888699,
      "loss": 7.2334,
      "step": 3236
    },
    {
      "epoch": 0.9215658362989324,
      "grad_norm": 0.5292291045188904,
      "learning_rate": 0.00026971249644178766,
      "loss": 7.5059,
      "step": 3237
    },
    {
      "epoch": 0.9218505338078292,
      "grad_norm": 0.5957803726196289,
      "learning_rate": 0.0002696413321947054,
      "loss": 7.5928,
      "step": 3238
    },
    {
      "epoch": 0.922135231316726,
      "grad_norm": 0.48048827052116394,
      "learning_rate": 0.0002695701679476231,
      "loss": 7.7178,
      "step": 3239
    },
    {
      "epoch": 0.9224199288256227,
      "grad_norm": 0.5252416133880615,
      "learning_rate": 0.0002694990037005409,
      "loss": 7.7988,
      "step": 3240
    },
    {
      "epoch": 0.9227046263345196,
      "grad_norm": 0.5854065418243408,
      "learning_rate": 0.0002694278394534586,
      "loss": 7.1553,
      "step": 3241
    },
    {
      "epoch": 0.9229893238434164,
      "grad_norm": 0.4846639037132263,
      "learning_rate": 0.0002693566752063763,
      "loss": 7.5244,
      "step": 3242
    },
    {
      "epoch": 0.9232740213523132,
      "grad_norm": 0.5670462846755981,
      "learning_rate": 0.00026928551095929405,
      "loss": 7.2451,
      "step": 3243
    },
    {
      "epoch": 0.92355871886121,
      "grad_norm": 0.5466600060462952,
      "learning_rate": 0.00026921434671221177,
      "loss": 7.3662,
      "step": 3244
    },
    {
      "epoch": 0.9238434163701068,
      "grad_norm": 0.4804023504257202,
      "learning_rate": 0.00026914318246512955,
      "loss": 7.3721,
      "step": 3245
    },
    {
      "epoch": 0.9241281138790035,
      "grad_norm": 0.5130466818809509,
      "learning_rate": 0.00026907201821804727,
      "loss": 7.6533,
      "step": 3246
    },
    {
      "epoch": 0.9244128113879003,
      "grad_norm": 0.5311350226402283,
      "learning_rate": 0.000269000853970965,
      "loss": 7.6006,
      "step": 3247
    },
    {
      "epoch": 0.9246975088967971,
      "grad_norm": 0.8233193755149841,
      "learning_rate": 0.0002689296897238827,
      "loss": 6.1475,
      "step": 3248
    },
    {
      "epoch": 0.924982206405694,
      "grad_norm": 0.5325547456741333,
      "learning_rate": 0.00026885852547680044,
      "loss": 7.4795,
      "step": 3249
    },
    {
      "epoch": 0.9252669039145908,
      "grad_norm": 0.5422199964523315,
      "learning_rate": 0.0002687873612297182,
      "loss": 7.6143,
      "step": 3250
    },
    {
      "epoch": 0.9255516014234876,
      "grad_norm": 0.6390559077262878,
      "learning_rate": 0.00026871619698263594,
      "loss": 7.2324,
      "step": 3251
    },
    {
      "epoch": 0.9258362989323844,
      "grad_norm": 0.775672435760498,
      "learning_rate": 0.00026864503273555366,
      "loss": 6.7354,
      "step": 3252
    },
    {
      "epoch": 0.9261209964412811,
      "grad_norm": 0.5697236061096191,
      "learning_rate": 0.0002685738684884714,
      "loss": 7.8916,
      "step": 3253
    },
    {
      "epoch": 0.9264056939501779,
      "grad_norm": 0.5534671545028687,
      "learning_rate": 0.0002685027042413891,
      "loss": 7.4775,
      "step": 3254
    },
    {
      "epoch": 0.9266903914590747,
      "grad_norm": 0.508283257484436,
      "learning_rate": 0.0002684315399943069,
      "loss": 7.6299,
      "step": 3255
    },
    {
      "epoch": 0.9269750889679715,
      "grad_norm": 0.4809110760688782,
      "learning_rate": 0.0002683603757472246,
      "loss": 7.7285,
      "step": 3256
    },
    {
      "epoch": 0.9272597864768684,
      "grad_norm": 0.4433010518550873,
      "learning_rate": 0.00026828921150014233,
      "loss": 7.8076,
      "step": 3257
    },
    {
      "epoch": 0.9275444839857652,
      "grad_norm": 0.5444433689117432,
      "learning_rate": 0.0002682180472530601,
      "loss": 7.2666,
      "step": 3258
    },
    {
      "epoch": 0.9278291814946619,
      "grad_norm": 0.5611394047737122,
      "learning_rate": 0.0002681468830059778,
      "loss": 8.3574,
      "step": 3259
    },
    {
      "epoch": 0.9281138790035587,
      "grad_norm": 0.5418546795845032,
      "learning_rate": 0.00026807571875889555,
      "loss": 7.5947,
      "step": 3260
    },
    {
      "epoch": 0.9283985765124555,
      "grad_norm": 0.5082615613937378,
      "learning_rate": 0.0002680045545118133,
      "loss": 7.3486,
      "step": 3261
    },
    {
      "epoch": 0.9286832740213523,
      "grad_norm": 0.4412146508693695,
      "learning_rate": 0.000267933390264731,
      "loss": 7.8359,
      "step": 3262
    },
    {
      "epoch": 0.9289679715302491,
      "grad_norm": 0.5394509434700012,
      "learning_rate": 0.0002678622260176488,
      "loss": 7.9336,
      "step": 3263
    },
    {
      "epoch": 0.9292526690391459,
      "grad_norm": 0.5103852152824402,
      "learning_rate": 0.00026779106177056644,
      "loss": 7.8789,
      "step": 3264
    },
    {
      "epoch": 0.9295373665480428,
      "grad_norm": 0.4658895432949066,
      "learning_rate": 0.00026771989752348417,
      "loss": 7.8369,
      "step": 3265
    },
    {
      "epoch": 0.9298220640569395,
      "grad_norm": 0.48396316170692444,
      "learning_rate": 0.00026764873327640194,
      "loss": 7.8037,
      "step": 3266
    },
    {
      "epoch": 0.9301067615658363,
      "grad_norm": 0.549019455909729,
      "learning_rate": 0.00026757756902931967,
      "loss": 7.6494,
      "step": 3267
    },
    {
      "epoch": 0.9303914590747331,
      "grad_norm": 0.5370675921440125,
      "learning_rate": 0.00026750640478223744,
      "loss": 7.3008,
      "step": 3268
    },
    {
      "epoch": 0.9306761565836299,
      "grad_norm": 0.45335695147514343,
      "learning_rate": 0.00026743524053515517,
      "loss": 7.9619,
      "step": 3269
    },
    {
      "epoch": 0.9309608540925267,
      "grad_norm": 0.6284888386726379,
      "learning_rate": 0.00026736407628807283,
      "loss": 7.3721,
      "step": 3270
    },
    {
      "epoch": 0.9312455516014235,
      "grad_norm": 0.5251436233520508,
      "learning_rate": 0.0002672929120409906,
      "loss": 7.4648,
      "step": 3271
    },
    {
      "epoch": 0.9315302491103202,
      "grad_norm": 0.5720113515853882,
      "learning_rate": 0.00026722174779390833,
      "loss": 7.208,
      "step": 3272
    },
    {
      "epoch": 0.931814946619217,
      "grad_norm": 0.5898692011833191,
      "learning_rate": 0.0002671505835468261,
      "loss": 7.2461,
      "step": 3273
    },
    {
      "epoch": 0.9320996441281139,
      "grad_norm": 0.5782837271690369,
      "learning_rate": 0.00026707941929974383,
      "loss": 7.3223,
      "step": 3274
    },
    {
      "epoch": 0.9323843416370107,
      "grad_norm": 0.5770065784454346,
      "learning_rate": 0.00026700825505266156,
      "loss": 6.9395,
      "step": 3275
    },
    {
      "epoch": 0.9326690391459075,
      "grad_norm": 0.5000047087669373,
      "learning_rate": 0.0002669370908055793,
      "loss": 7.3906,
      "step": 3276
    },
    {
      "epoch": 0.9329537366548043,
      "grad_norm": 0.44757047295570374,
      "learning_rate": 0.000266865926558497,
      "loss": 7.8887,
      "step": 3277
    },
    {
      "epoch": 0.9332384341637011,
      "grad_norm": 0.7160705924034119,
      "learning_rate": 0.0002667947623114148,
      "loss": 7.5977,
      "step": 3278
    },
    {
      "epoch": 0.9335231316725978,
      "grad_norm": 0.4756827652454376,
      "learning_rate": 0.0002667235980643325,
      "loss": 7.8545,
      "step": 3279
    },
    {
      "epoch": 0.9338078291814946,
      "grad_norm": 0.6191202402114868,
      "learning_rate": 0.0002666524338172502,
      "loss": 6.9199,
      "step": 3280
    },
    {
      "epoch": 0.9340925266903914,
      "grad_norm": 0.5907383561134338,
      "learning_rate": 0.00026658126957016795,
      "loss": 7.4463,
      "step": 3281
    },
    {
      "epoch": 0.9343772241992883,
      "grad_norm": 0.521700382232666,
      "learning_rate": 0.00026651010532308567,
      "loss": 7.9736,
      "step": 3282
    },
    {
      "epoch": 0.9346619217081851,
      "grad_norm": 0.3965546190738678,
      "learning_rate": 0.0002664389410760034,
      "loss": 7.9883,
      "step": 3283
    },
    {
      "epoch": 0.9349466192170819,
      "grad_norm": 0.511185884475708,
      "learning_rate": 0.00026636777682892117,
      "loss": 7.3652,
      "step": 3284
    },
    {
      "epoch": 0.9352313167259787,
      "grad_norm": 0.4863254725933075,
      "learning_rate": 0.0002662966125818389,
      "loss": 7.5605,
      "step": 3285
    },
    {
      "epoch": 0.9355160142348754,
      "grad_norm": 0.4958791434764862,
      "learning_rate": 0.00026622544833475667,
      "loss": 7.582,
      "step": 3286
    },
    {
      "epoch": 0.9358007117437722,
      "grad_norm": 0.6026458144187927,
      "learning_rate": 0.00026615428408767434,
      "loss": 7.0693,
      "step": 3287
    },
    {
      "epoch": 0.936085409252669,
      "grad_norm": 0.6023535132408142,
      "learning_rate": 0.00026608311984059206,
      "loss": 7.5732,
      "step": 3288
    },
    {
      "epoch": 0.9363701067615658,
      "grad_norm": 0.5058068037033081,
      "learning_rate": 0.00026601195559350984,
      "loss": 7.2148,
      "step": 3289
    },
    {
      "epoch": 0.9366548042704627,
      "grad_norm": 0.5182356238365173,
      "learning_rate": 0.00026594079134642756,
      "loss": 7.5859,
      "step": 3290
    },
    {
      "epoch": 0.9369395017793595,
      "grad_norm": 0.5553226470947266,
      "learning_rate": 0.00026586962709934534,
      "loss": 7.291,
      "step": 3291
    },
    {
      "epoch": 0.9372241992882562,
      "grad_norm": 0.5498570799827576,
      "learning_rate": 0.000265798462852263,
      "loss": 6.9912,
      "step": 3292
    },
    {
      "epoch": 0.937508896797153,
      "grad_norm": 0.49067750573158264,
      "learning_rate": 0.00026572729860518073,
      "loss": 7.0986,
      "step": 3293
    },
    {
      "epoch": 0.9377935943060498,
      "grad_norm": 0.4361690878868103,
      "learning_rate": 0.0002656561343580985,
      "loss": 7.9883,
      "step": 3294
    },
    {
      "epoch": 0.9380782918149466,
      "grad_norm": 0.5539669394493103,
      "learning_rate": 0.00026558497011101623,
      "loss": 7.3271,
      "step": 3295
    },
    {
      "epoch": 0.9383629893238434,
      "grad_norm": 0.5077876448631287,
      "learning_rate": 0.00026551380586393395,
      "loss": 7.7646,
      "step": 3296
    },
    {
      "epoch": 0.9386476868327402,
      "grad_norm": 0.4746239185333252,
      "learning_rate": 0.00026544264161685173,
      "loss": 7.4785,
      "step": 3297
    },
    {
      "epoch": 0.9389323843416371,
      "grad_norm": 0.5588517189025879,
      "learning_rate": 0.0002653714773697694,
      "loss": 7.3926,
      "step": 3298
    },
    {
      "epoch": 0.9392170818505338,
      "grad_norm": 0.4662167727947235,
      "learning_rate": 0.0002653003131226872,
      "loss": 7.2842,
      "step": 3299
    },
    {
      "epoch": 0.9395017793594306,
      "grad_norm": 0.5283779501914978,
      "learning_rate": 0.0002652291488756049,
      "loss": 7.5898,
      "step": 3300
    },
    {
      "epoch": 0.9397864768683274,
      "grad_norm": 0.5775339603424072,
      "learning_rate": 0.0002651579846285226,
      "loss": 7.584,
      "step": 3301
    },
    {
      "epoch": 0.9400711743772242,
      "grad_norm": 0.5250722169876099,
      "learning_rate": 0.0002650868203814404,
      "loss": 7.5527,
      "step": 3302
    },
    {
      "epoch": 0.940355871886121,
      "grad_norm": 0.5437557697296143,
      "learning_rate": 0.0002650156561343581,
      "loss": 7.293,
      "step": 3303
    },
    {
      "epoch": 0.9406405693950178,
      "grad_norm": 0.4950057864189148,
      "learning_rate": 0.00026494449188727584,
      "loss": 7.9424,
      "step": 3304
    },
    {
      "epoch": 0.9409252669039145,
      "grad_norm": 0.5347456336021423,
      "learning_rate": 0.00026487332764019356,
      "loss": 7.835,
      "step": 3305
    },
    {
      "epoch": 0.9412099644128114,
      "grad_norm": 0.5162736773490906,
      "learning_rate": 0.0002648021633931113,
      "loss": 7.8789,
      "step": 3306
    },
    {
      "epoch": 0.9414946619217082,
      "grad_norm": 0.49374714493751526,
      "learning_rate": 0.00026473099914602906,
      "loss": 7.8311,
      "step": 3307
    },
    {
      "epoch": 0.941779359430605,
      "grad_norm": 0.6290493011474609,
      "learning_rate": 0.0002646598348989468,
      "loss": 6.9551,
      "step": 3308
    },
    {
      "epoch": 0.9420640569395018,
      "grad_norm": 0.5318243503570557,
      "learning_rate": 0.0002645886706518645,
      "loss": 7.9062,
      "step": 3309
    },
    {
      "epoch": 0.9423487544483986,
      "grad_norm": 0.6360445618629456,
      "learning_rate": 0.00026451750640478223,
      "loss": 7.1191,
      "step": 3310
    },
    {
      "epoch": 0.9426334519572954,
      "grad_norm": 0.5819425582885742,
      "learning_rate": 0.00026444634215769995,
      "loss": 7.1162,
      "step": 3311
    },
    {
      "epoch": 0.9429181494661921,
      "grad_norm": 0.5409653186798096,
      "learning_rate": 0.00026437517791061773,
      "loss": 7.6133,
      "step": 3312
    },
    {
      "epoch": 0.9432028469750889,
      "grad_norm": 0.4004253149032593,
      "learning_rate": 0.00026430401366353545,
      "loss": 8.0342,
      "step": 3313
    },
    {
      "epoch": 0.9434875444839858,
      "grad_norm": 0.5227109789848328,
      "learning_rate": 0.0002642328494164532,
      "loss": 7.7695,
      "step": 3314
    },
    {
      "epoch": 0.9437722419928826,
      "grad_norm": 0.5666325688362122,
      "learning_rate": 0.0002641616851693709,
      "loss": 7.5889,
      "step": 3315
    },
    {
      "epoch": 0.9440569395017794,
      "grad_norm": 0.5243922472000122,
      "learning_rate": 0.0002640905209222886,
      "loss": 7.5146,
      "step": 3316
    },
    {
      "epoch": 0.9443416370106762,
      "grad_norm": 0.5842487215995789,
      "learning_rate": 0.0002640193566752064,
      "loss": 7.124,
      "step": 3317
    },
    {
      "epoch": 0.944626334519573,
      "grad_norm": 0.6078001260757446,
      "learning_rate": 0.0002639481924281241,
      "loss": 7.5078,
      "step": 3318
    },
    {
      "epoch": 0.9449110320284697,
      "grad_norm": 0.46149325370788574,
      "learning_rate": 0.00026387702818104185,
      "loss": 7.8438,
      "step": 3319
    },
    {
      "epoch": 0.9451957295373665,
      "grad_norm": 0.5267667174339294,
      "learning_rate": 0.0002638058639339596,
      "loss": 7.3174,
      "step": 3320
    },
    {
      "epoch": 0.9454804270462633,
      "grad_norm": 0.5622223019599915,
      "learning_rate": 0.0002637346996868773,
      "loss": 7.2441,
      "step": 3321
    },
    {
      "epoch": 0.9457651245551602,
      "grad_norm": 0.5527371764183044,
      "learning_rate": 0.00026366353543979507,
      "loss": 7.3193,
      "step": 3322
    },
    {
      "epoch": 0.946049822064057,
      "grad_norm": 0.47036388516426086,
      "learning_rate": 0.0002635923711927128,
      "loss": 7.4541,
      "step": 3323
    },
    {
      "epoch": 0.9463345195729538,
      "grad_norm": 0.5821477174758911,
      "learning_rate": 0.0002635212069456305,
      "loss": 7.1455,
      "step": 3324
    },
    {
      "epoch": 0.9466192170818505,
      "grad_norm": 0.4783112108707428,
      "learning_rate": 0.0002634500426985483,
      "loss": 7.3848,
      "step": 3325
    },
    {
      "epoch": 0.9469039145907473,
      "grad_norm": 0.5583381056785583,
      "learning_rate": 0.00026337887845146596,
      "loss": 7.4609,
      "step": 3326
    },
    {
      "epoch": 0.9471886120996441,
      "grad_norm": 0.5437659025192261,
      "learning_rate": 0.0002633077142043837,
      "loss": 7.6836,
      "step": 3327
    },
    {
      "epoch": 0.9474733096085409,
      "grad_norm": 0.4930652678012848,
      "learning_rate": 0.00026323654995730146,
      "loss": 7.5195,
      "step": 3328
    },
    {
      "epoch": 0.9477580071174377,
      "grad_norm": 0.5067018866539001,
      "learning_rate": 0.0002631653857102192,
      "loss": 7.7383,
      "step": 3329
    },
    {
      "epoch": 0.9480427046263346,
      "grad_norm": 0.5984372496604919,
      "learning_rate": 0.00026309422146313696,
      "loss": 7.0508,
      "step": 3330
    },
    {
      "epoch": 0.9483274021352314,
      "grad_norm": 0.5220186114311218,
      "learning_rate": 0.0002630230572160547,
      "loss": 7.3213,
      "step": 3331
    },
    {
      "epoch": 0.9486120996441281,
      "grad_norm": 0.5187951326370239,
      "learning_rate": 0.00026295189296897235,
      "loss": 7.4307,
      "step": 3332
    },
    {
      "epoch": 0.9488967971530249,
      "grad_norm": 0.4584461748600006,
      "learning_rate": 0.0002628807287218901,
      "loss": 7.6729,
      "step": 3333
    },
    {
      "epoch": 0.9491814946619217,
      "grad_norm": 0.57470703125,
      "learning_rate": 0.00026280956447480785,
      "loss": 7.1582,
      "step": 3334
    },
    {
      "epoch": 0.9494661921708185,
      "grad_norm": 0.5039565563201904,
      "learning_rate": 0.0002627384002277256,
      "loss": 7.4258,
      "step": 3335
    },
    {
      "epoch": 0.9497508896797153,
      "grad_norm": 0.5717530846595764,
      "learning_rate": 0.00026266723598064335,
      "loss": 7.0938,
      "step": 3336
    },
    {
      "epoch": 0.9500355871886121,
      "grad_norm": 0.5401315093040466,
      "learning_rate": 0.000262596071733561,
      "loss": 7.707,
      "step": 3337
    },
    {
      "epoch": 0.9503202846975088,
      "grad_norm": 0.5657802820205688,
      "learning_rate": 0.0002625249074864788,
      "loss": 7.3662,
      "step": 3338
    },
    {
      "epoch": 0.9506049822064057,
      "grad_norm": 0.5742127895355225,
      "learning_rate": 0.0002624537432393965,
      "loss": 7.5469,
      "step": 3339
    },
    {
      "epoch": 0.9508896797153025,
      "grad_norm": 0.6317152976989746,
      "learning_rate": 0.0002623825789923143,
      "loss": 7.207,
      "step": 3340
    },
    {
      "epoch": 0.9511743772241993,
      "grad_norm": 0.6099809408187866,
      "learning_rate": 0.000262311414745232,
      "loss": 6.7969,
      "step": 3341
    },
    {
      "epoch": 0.9514590747330961,
      "grad_norm": 0.6314372420310974,
      "learning_rate": 0.00026224025049814974,
      "loss": 6.1387,
      "step": 3342
    },
    {
      "epoch": 0.9517437722419929,
      "grad_norm": 0.5816494822502136,
      "learning_rate": 0.00026216908625106746,
      "loss": 7.2393,
      "step": 3343
    },
    {
      "epoch": 0.9520284697508897,
      "grad_norm": 0.4866476058959961,
      "learning_rate": 0.0002620979220039852,
      "loss": 7.9521,
      "step": 3344
    },
    {
      "epoch": 0.9523131672597864,
      "grad_norm": 0.6285499930381775,
      "learning_rate": 0.0002620267577569029,
      "loss": 7.1328,
      "step": 3345
    },
    {
      "epoch": 0.9525978647686832,
      "grad_norm": 0.5079755187034607,
      "learning_rate": 0.0002619555935098207,
      "loss": 7.084,
      "step": 3346
    },
    {
      "epoch": 0.9528825622775801,
      "grad_norm": 0.47182542085647583,
      "learning_rate": 0.0002618844292627384,
      "loss": 7.7314,
      "step": 3347
    },
    {
      "epoch": 0.9531672597864769,
      "grad_norm": 0.4536728262901306,
      "learning_rate": 0.0002618132650156562,
      "loss": 7.6777,
      "step": 3348
    },
    {
      "epoch": 0.9534519572953737,
      "grad_norm": 0.4631863534450531,
      "learning_rate": 0.00026174210076857385,
      "loss": 7.5625,
      "step": 3349
    },
    {
      "epoch": 0.9537366548042705,
      "grad_norm": 0.6216831207275391,
      "learning_rate": 0.0002616709365214916,
      "loss": 7.3281,
      "step": 3350
    },
    {
      "epoch": 0.9540213523131673,
      "grad_norm": 0.5534892082214355,
      "learning_rate": 0.00026159977227440935,
      "loss": 7.5986,
      "step": 3351
    },
    {
      "epoch": 0.954306049822064,
      "grad_norm": 0.5252399444580078,
      "learning_rate": 0.0002615286080273271,
      "loss": 7.4961,
      "step": 3352
    },
    {
      "epoch": 0.9545907473309608,
      "grad_norm": 0.5050674080848694,
      "learning_rate": 0.00026145744378024485,
      "loss": 7.585,
      "step": 3353
    },
    {
      "epoch": 0.9548754448398576,
      "grad_norm": 0.5906735062599182,
      "learning_rate": 0.0002613862795331625,
      "loss": 7.0557,
      "step": 3354
    },
    {
      "epoch": 0.9551601423487545,
      "grad_norm": 0.5237690806388855,
      "learning_rate": 0.00026131511528608024,
      "loss": 8.0439,
      "step": 3355
    },
    {
      "epoch": 0.9554448398576513,
      "grad_norm": 0.5109413862228394,
      "learning_rate": 0.000261243951038998,
      "loss": 7.25,
      "step": 3356
    },
    {
      "epoch": 0.9557295373665481,
      "grad_norm": 0.5231671333312988,
      "learning_rate": 0.00026117278679191574,
      "loss": 7.5557,
      "step": 3357
    },
    {
      "epoch": 0.9560142348754448,
      "grad_norm": 0.605821430683136,
      "learning_rate": 0.0002611016225448335,
      "loss": 6.8545,
      "step": 3358
    },
    {
      "epoch": 0.9562989323843416,
      "grad_norm": 0.5436892509460449,
      "learning_rate": 0.00026103045829775124,
      "loss": 7.6572,
      "step": 3359
    },
    {
      "epoch": 0.9565836298932384,
      "grad_norm": 0.5580775737762451,
      "learning_rate": 0.0002609592940506689,
      "loss": 7.8828,
      "step": 3360
    },
    {
      "epoch": 0.9568683274021352,
      "grad_norm": 0.4247756004333496,
      "learning_rate": 0.0002608881298035867,
      "loss": 7.9736,
      "step": 3361
    },
    {
      "epoch": 0.957153024911032,
      "grad_norm": 4.1650590896606445,
      "learning_rate": 0.0002608169655565044,
      "loss": 7.5039,
      "step": 3362
    },
    {
      "epoch": 0.9574377224199289,
      "grad_norm": 0.5966189503669739,
      "learning_rate": 0.00026074580130942213,
      "loss": 6.8721,
      "step": 3363
    },
    {
      "epoch": 0.9577224199288257,
      "grad_norm": 0.5184897184371948,
      "learning_rate": 0.0002606746370623399,
      "loss": 7.21,
      "step": 3364
    },
    {
      "epoch": 0.9580071174377224,
      "grad_norm": 0.4620699882507324,
      "learning_rate": 0.00026060347281525763,
      "loss": 7.7822,
      "step": 3365
    },
    {
      "epoch": 0.9582918149466192,
      "grad_norm": 0.5272870063781738,
      "learning_rate": 0.00026053230856817536,
      "loss": 7.3975,
      "step": 3366
    },
    {
      "epoch": 0.958576512455516,
      "grad_norm": 0.5648930668830872,
      "learning_rate": 0.0002604611443210931,
      "loss": 7.3916,
      "step": 3367
    },
    {
      "epoch": 0.9588612099644128,
      "grad_norm": 0.5892084240913391,
      "learning_rate": 0.0002603899800740108,
      "loss": 7.6406,
      "step": 3368
    },
    {
      "epoch": 0.9591459074733096,
      "grad_norm": 0.5818761587142944,
      "learning_rate": 0.0002603188158269286,
      "loss": 7.2061,
      "step": 3369
    },
    {
      "epoch": 0.9594306049822064,
      "grad_norm": 0.4996194541454315,
      "learning_rate": 0.0002602476515798463,
      "loss": 7.5654,
      "step": 3370
    },
    {
      "epoch": 0.9597153024911032,
      "grad_norm": 0.4272514879703522,
      "learning_rate": 0.000260176487332764,
      "loss": 8.2246,
      "step": 3371
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.5711843967437744,
      "learning_rate": 0.00026010532308568175,
      "loss": 7.5391,
      "step": 3372
    },
    {
      "epoch": 0.9602846975088968,
      "grad_norm": 0.5286685824394226,
      "learning_rate": 0.00026003415883859947,
      "loss": 7.126,
      "step": 3373
    },
    {
      "epoch": 0.9605693950177936,
      "grad_norm": 0.5573874711990356,
      "learning_rate": 0.00025996299459151725,
      "loss": 7.4805,
      "step": 3374
    },
    {
      "epoch": 0.9608540925266904,
      "grad_norm": 0.5179297924041748,
      "learning_rate": 0.00025989183034443497,
      "loss": 7.9053,
      "step": 3375
    },
    {
      "epoch": 0.9611387900355872,
      "grad_norm": 0.521755576133728,
      "learning_rate": 0.00025982066609735275,
      "loss": 7.6816,
      "step": 3376
    },
    {
      "epoch": 0.961423487544484,
      "grad_norm": 0.48866036534309387,
      "learning_rate": 0.0002597495018502704,
      "loss": 8.2041,
      "step": 3377
    },
    {
      "epoch": 0.9617081850533807,
      "grad_norm": 0.5015320777893066,
      "learning_rate": 0.00025967833760318814,
      "loss": 7.832,
      "step": 3378
    },
    {
      "epoch": 0.9619928825622776,
      "grad_norm": 0.5645074844360352,
      "learning_rate": 0.0002596071733561059,
      "loss": 7.5234,
      "step": 3379
    },
    {
      "epoch": 0.9622775800711744,
      "grad_norm": 0.5623124241828918,
      "learning_rate": 0.00025953600910902364,
      "loss": 7.1748,
      "step": 3380
    },
    {
      "epoch": 0.9625622775800712,
      "grad_norm": 0.48571348190307617,
      "learning_rate": 0.00025946484486194136,
      "loss": 7.8896,
      "step": 3381
    },
    {
      "epoch": 0.962846975088968,
      "grad_norm": 0.5530520677566528,
      "learning_rate": 0.0002593936806148591,
      "loss": 7.5303,
      "step": 3382
    },
    {
      "epoch": 0.9631316725978648,
      "grad_norm": 0.5123804211616516,
      "learning_rate": 0.0002593225163677768,
      "loss": 7.7363,
      "step": 3383
    },
    {
      "epoch": 0.9634163701067615,
      "grad_norm": 0.5643565654754639,
      "learning_rate": 0.0002592513521206946,
      "loss": 7.5557,
      "step": 3384
    },
    {
      "epoch": 0.9637010676156583,
      "grad_norm": 0.5032460689544678,
      "learning_rate": 0.0002591801878736123,
      "loss": 7.373,
      "step": 3385
    },
    {
      "epoch": 0.9639857651245551,
      "grad_norm": 0.6415847539901733,
      "learning_rate": 0.00025910902362653003,
      "loss": 6.9395,
      "step": 3386
    },
    {
      "epoch": 0.964270462633452,
      "grad_norm": 0.5873409509658813,
      "learning_rate": 0.0002590378593794478,
      "loss": 7.0332,
      "step": 3387
    },
    {
      "epoch": 0.9645551601423488,
      "grad_norm": 0.48665153980255127,
      "learning_rate": 0.0002589666951323655,
      "loss": 7.8779,
      "step": 3388
    },
    {
      "epoch": 0.9648398576512456,
      "grad_norm": 0.560894787311554,
      "learning_rate": 0.00025889553088528325,
      "loss": 7.4814,
      "step": 3389
    },
    {
      "epoch": 0.9651245551601424,
      "grad_norm": 0.5005226731300354,
      "learning_rate": 0.000258824366638201,
      "loss": 7.1123,
      "step": 3390
    },
    {
      "epoch": 0.9654092526690391,
      "grad_norm": 0.4856753945350647,
      "learning_rate": 0.0002587532023911187,
      "loss": 7.5479,
      "step": 3391
    },
    {
      "epoch": 0.9656939501779359,
      "grad_norm": 0.5255371332168579,
      "learning_rate": 0.0002586820381440365,
      "loss": 7.2832,
      "step": 3392
    },
    {
      "epoch": 0.9659786476868327,
      "grad_norm": 0.49085626006126404,
      "learning_rate": 0.0002586108738969542,
      "loss": 7.5488,
      "step": 3393
    },
    {
      "epoch": 0.9662633451957295,
      "grad_norm": 0.4883959889411926,
      "learning_rate": 0.00025853970964987187,
      "loss": 7.5957,
      "step": 3394
    },
    {
      "epoch": 0.9665480427046264,
      "grad_norm": 0.6680588722229004,
      "learning_rate": 0.00025846854540278964,
      "loss": 7.9346,
      "step": 3395
    },
    {
      "epoch": 0.9668327402135232,
      "grad_norm": 0.43854761123657227,
      "learning_rate": 0.00025839738115570736,
      "loss": 7.9434,
      "step": 3396
    },
    {
      "epoch": 0.96711743772242,
      "grad_norm": 0.529097318649292,
      "learning_rate": 0.00025832621690862514,
      "loss": 7.2012,
      "step": 3397
    },
    {
      "epoch": 0.9674021352313167,
      "grad_norm": 0.6804172992706299,
      "learning_rate": 0.00025825505266154286,
      "loss": 7.5195,
      "step": 3398
    },
    {
      "epoch": 0.9676868327402135,
      "grad_norm": 0.4768712520599365,
      "learning_rate": 0.00025818388841446053,
      "loss": 7.9785,
      "step": 3399
    },
    {
      "epoch": 0.9679715302491103,
      "grad_norm": 0.5476545691490173,
      "learning_rate": 0.0002581127241673783,
      "loss": 7.3252,
      "step": 3400
    },
    {
      "epoch": 0.9679715302491103,
      "eval_bleu": 0.14402324527886887,
      "eval_loss": 7.21875,
      "eval_runtime": 115.5815,
      "eval_samples_per_second": 2.457,
      "eval_steps_per_second": 0.156,
      "step": 3400
    },
    {
      "epoch": 0.9682562277580071,
      "grad_norm": 0.5623397827148438,
      "learning_rate": 0.00025804155992029603,
      "loss": 7.9336,
      "step": 3401
    },
    {
      "epoch": 0.9685409252669039,
      "grad_norm": 0.5140218138694763,
      "learning_rate": 0.0002579703956732138,
      "loss": 7.3115,
      "step": 3402
    },
    {
      "epoch": 0.9688256227758008,
      "grad_norm": 0.4771810472011566,
      "learning_rate": 0.00025789923142613153,
      "loss": 8.0049,
      "step": 3403
    },
    {
      "epoch": 0.9691103202846975,
      "grad_norm": 0.7258356809616089,
      "learning_rate": 0.00025782806717904926,
      "loss": 6.6221,
      "step": 3404
    },
    {
      "epoch": 0.9693950177935943,
      "grad_norm": 0.5907997488975525,
      "learning_rate": 0.000257756902931967,
      "loss": 7.2344,
      "step": 3405
    },
    {
      "epoch": 0.9696797153024911,
      "grad_norm": 0.5279775857925415,
      "learning_rate": 0.0002576857386848847,
      "loss": 8.0674,
      "step": 3406
    },
    {
      "epoch": 0.9699644128113879,
      "grad_norm": 0.55947345495224,
      "learning_rate": 0.0002576145744378025,
      "loss": 7.4102,
      "step": 3407
    },
    {
      "epoch": 0.9702491103202847,
      "grad_norm": 0.707697868347168,
      "learning_rate": 0.0002575434101907202,
      "loss": 7.1943,
      "step": 3408
    },
    {
      "epoch": 0.9705338078291815,
      "grad_norm": 0.4468507468700409,
      "learning_rate": 0.0002574722459436379,
      "loss": 8.0938,
      "step": 3409
    },
    {
      "epoch": 0.9708185053380783,
      "grad_norm": 0.4741066098213196,
      "learning_rate": 0.0002574010816965557,
      "loss": 8.0596,
      "step": 3410
    },
    {
      "epoch": 0.971103202846975,
      "grad_norm": 0.5719946622848511,
      "learning_rate": 0.00025732991744947337,
      "loss": 6.9824,
      "step": 3411
    },
    {
      "epoch": 0.9713879003558719,
      "grad_norm": 0.5917583107948303,
      "learning_rate": 0.0002572587532023911,
      "loss": 7.5371,
      "step": 3412
    },
    {
      "epoch": 0.9716725978647687,
      "grad_norm": 0.6130540370941162,
      "learning_rate": 0.00025718758895530887,
      "loss": 7.3096,
      "step": 3413
    },
    {
      "epoch": 0.9719572953736655,
      "grad_norm": 0.47187602519989014,
      "learning_rate": 0.0002571164247082266,
      "loss": 7.5947,
      "step": 3414
    },
    {
      "epoch": 0.9722419928825623,
      "grad_norm": 0.501513659954071,
      "learning_rate": 0.00025704526046114437,
      "loss": 7.7881,
      "step": 3415
    },
    {
      "epoch": 0.9725266903914591,
      "grad_norm": 0.4664040803909302,
      "learning_rate": 0.00025697409621406204,
      "loss": 7.7529,
      "step": 3416
    },
    {
      "epoch": 0.9728113879003558,
      "grad_norm": 0.5009340047836304,
      "learning_rate": 0.00025690293196697976,
      "loss": 7.3945,
      "step": 3417
    },
    {
      "epoch": 0.9730960854092526,
      "grad_norm": 0.5810549259185791,
      "learning_rate": 0.00025683176771989754,
      "loss": 7.3643,
      "step": 3418
    },
    {
      "epoch": 0.9733807829181494,
      "grad_norm": 0.653583288192749,
      "learning_rate": 0.00025676060347281526,
      "loss": 7.1211,
      "step": 3419
    },
    {
      "epoch": 0.9736654804270463,
      "grad_norm": 0.5430318713188171,
      "learning_rate": 0.00025668943922573304,
      "loss": 7.5586,
      "step": 3420
    },
    {
      "epoch": 0.9739501779359431,
      "grad_norm": 0.4909273684024811,
      "learning_rate": 0.00025661827497865076,
      "loss": 7.7021,
      "step": 3421
    },
    {
      "epoch": 0.9742348754448399,
      "grad_norm": 0.5599708557128906,
      "learning_rate": 0.00025654711073156843,
      "loss": 7.7168,
      "step": 3422
    },
    {
      "epoch": 0.9745195729537367,
      "grad_norm": 0.6214874386787415,
      "learning_rate": 0.0002564759464844862,
      "loss": 7.29,
      "step": 3423
    },
    {
      "epoch": 0.9748042704626334,
      "grad_norm": 0.5425530672073364,
      "learning_rate": 0.00025640478223740393,
      "loss": 7.8467,
      "step": 3424
    },
    {
      "epoch": 0.9750889679715302,
      "grad_norm": 0.6874010562896729,
      "learning_rate": 0.00025633361799032165,
      "loss": 6.8496,
      "step": 3425
    },
    {
      "epoch": 0.975373665480427,
      "grad_norm": 0.6252590417861938,
      "learning_rate": 0.0002562624537432394,
      "loss": 7.5605,
      "step": 3426
    },
    {
      "epoch": 0.9756583629893238,
      "grad_norm": 0.425830602645874,
      "learning_rate": 0.0002561912894961571,
      "loss": 7.8027,
      "step": 3427
    },
    {
      "epoch": 0.9759430604982207,
      "grad_norm": 0.47604426741600037,
      "learning_rate": 0.00025612012524907487,
      "loss": 7.8682,
      "step": 3428
    },
    {
      "epoch": 0.9762277580071175,
      "grad_norm": 0.6145363450050354,
      "learning_rate": 0.0002560489610019926,
      "loss": 7.2354,
      "step": 3429
    },
    {
      "epoch": 0.9765124555160143,
      "grad_norm": 0.4913404583930969,
      "learning_rate": 0.0002559777967549103,
      "loss": 7.6641,
      "step": 3430
    },
    {
      "epoch": 0.976797153024911,
      "grad_norm": 0.49762246012687683,
      "learning_rate": 0.0002559066325078281,
      "loss": 7.5244,
      "step": 3431
    },
    {
      "epoch": 0.9770818505338078,
      "grad_norm": 0.6122978925704956,
      "learning_rate": 0.0002558354682607458,
      "loss": 7.5361,
      "step": 3432
    },
    {
      "epoch": 0.9773665480427046,
      "grad_norm": 0.5063184499740601,
      "learning_rate": 0.00025576430401366354,
      "loss": 8.2832,
      "step": 3433
    },
    {
      "epoch": 0.9776512455516014,
      "grad_norm": 0.56259685754776,
      "learning_rate": 0.00025569313976658126,
      "loss": 7.4121,
      "step": 3434
    },
    {
      "epoch": 0.9779359430604982,
      "grad_norm": 0.6011261940002441,
      "learning_rate": 0.000255621975519499,
      "loss": 7.2822,
      "step": 3435
    },
    {
      "epoch": 0.9782206405693951,
      "grad_norm": 0.46456050872802734,
      "learning_rate": 0.00025555081127241676,
      "loss": 8.0469,
      "step": 3436
    },
    {
      "epoch": 0.9785053380782918,
      "grad_norm": 0.5711768269538879,
      "learning_rate": 0.0002554796470253345,
      "loss": 7.707,
      "step": 3437
    },
    {
      "epoch": 0.9787900355871886,
      "grad_norm": 0.6150893568992615,
      "learning_rate": 0.00025540848277825226,
      "loss": 6.9717,
      "step": 3438
    },
    {
      "epoch": 0.9790747330960854,
      "grad_norm": 0.5183292627334595,
      "learning_rate": 0.00025533731853116993,
      "loss": 7.8262,
      "step": 3439
    },
    {
      "epoch": 0.9793594306049822,
      "grad_norm": 0.5491702556610107,
      "learning_rate": 0.00025526615428408765,
      "loss": 7.5723,
      "step": 3440
    },
    {
      "epoch": 0.979644128113879,
      "grad_norm": 0.587271511554718,
      "learning_rate": 0.00025519499003700543,
      "loss": 7.332,
      "step": 3441
    },
    {
      "epoch": 0.9799288256227758,
      "grad_norm": 0.6144239902496338,
      "learning_rate": 0.00025512382578992315,
      "loss": 7.3652,
      "step": 3442
    },
    {
      "epoch": 0.9802135231316726,
      "grad_norm": 0.5288955569267273,
      "learning_rate": 0.0002550526615428409,
      "loss": 7.4131,
      "step": 3443
    },
    {
      "epoch": 0.9804982206405694,
      "grad_norm": 0.5186343789100647,
      "learning_rate": 0.0002549814972957586,
      "loss": 7.8076,
      "step": 3444
    },
    {
      "epoch": 0.9807829181494662,
      "grad_norm": 0.4478129744529724,
      "learning_rate": 0.0002549103330486763,
      "loss": 8.1953,
      "step": 3445
    },
    {
      "epoch": 0.981067615658363,
      "grad_norm": 0.41993552446365356,
      "learning_rate": 0.0002548391688015941,
      "loss": 8.3057,
      "step": 3446
    },
    {
      "epoch": 0.9813523131672598,
      "grad_norm": 0.5712823271751404,
      "learning_rate": 0.0002547680045545118,
      "loss": 7.5811,
      "step": 3447
    },
    {
      "epoch": 0.9816370106761566,
      "grad_norm": 0.588682234287262,
      "learning_rate": 0.00025469684030742954,
      "loss": 7.4072,
      "step": 3448
    },
    {
      "epoch": 0.9819217081850534,
      "grad_norm": 0.5510414242744446,
      "learning_rate": 0.0002546256760603473,
      "loss": 7.2637,
      "step": 3449
    },
    {
      "epoch": 0.9822064056939501,
      "grad_norm": 0.5119998455047607,
      "learning_rate": 0.000254554511813265,
      "loss": 7.4375,
      "step": 3450
    },
    {
      "epoch": 0.9824911032028469,
      "grad_norm": 0.6338230967521667,
      "learning_rate": 0.00025448334756618277,
      "loss": 7.2061,
      "step": 3451
    },
    {
      "epoch": 0.9827758007117438,
      "grad_norm": 0.49613437056541443,
      "learning_rate": 0.0002544121833191005,
      "loss": 7.8428,
      "step": 3452
    },
    {
      "epoch": 0.9830604982206406,
      "grad_norm": 0.48243021965026855,
      "learning_rate": 0.0002543410190720182,
      "loss": 7.8857,
      "step": 3453
    },
    {
      "epoch": 0.9833451957295374,
      "grad_norm": 0.4064277708530426,
      "learning_rate": 0.000254269854824936,
      "loss": 8.0117,
      "step": 3454
    },
    {
      "epoch": 0.9836298932384342,
      "grad_norm": 0.4452855587005615,
      "learning_rate": 0.00025419869057785366,
      "loss": 7.7598,
      "step": 3455
    },
    {
      "epoch": 0.983914590747331,
      "grad_norm": 0.5097564458847046,
      "learning_rate": 0.00025412752633077143,
      "loss": 7.3096,
      "step": 3456
    },
    {
      "epoch": 0.9841992882562277,
      "grad_norm": 0.5631262063980103,
      "learning_rate": 0.00025405636208368916,
      "loss": 6.876,
      "step": 3457
    },
    {
      "epoch": 0.9844839857651245,
      "grad_norm": 0.5096146464347839,
      "learning_rate": 0.0002539851978366069,
      "loss": 7.5244,
      "step": 3458
    },
    {
      "epoch": 0.9847686832740213,
      "grad_norm": 0.6540708541870117,
      "learning_rate": 0.00025391403358952466,
      "loss": 6.8379,
      "step": 3459
    },
    {
      "epoch": 0.9850533807829182,
      "grad_norm": 0.6074324250221252,
      "learning_rate": 0.0002538428693424424,
      "loss": 7.7061,
      "step": 3460
    },
    {
      "epoch": 0.985338078291815,
      "grad_norm": 0.4011852443218231,
      "learning_rate": 0.00025377170509536005,
      "loss": 8.0752,
      "step": 3461
    },
    {
      "epoch": 0.9856227758007118,
      "grad_norm": 0.5424057245254517,
      "learning_rate": 0.0002537005408482778,
      "loss": 7.4072,
      "step": 3462
    },
    {
      "epoch": 0.9859074733096086,
      "grad_norm": 0.5351366400718689,
      "learning_rate": 0.00025362937660119555,
      "loss": 7.4424,
      "step": 3463
    },
    {
      "epoch": 0.9861921708185053,
      "grad_norm": 0.5651853084564209,
      "learning_rate": 0.0002535582123541133,
      "loss": 7.5928,
      "step": 3464
    },
    {
      "epoch": 0.9864768683274021,
      "grad_norm": 0.5795883536338806,
      "learning_rate": 0.00025348704810703105,
      "loss": 7.4365,
      "step": 3465
    },
    {
      "epoch": 0.9867615658362989,
      "grad_norm": 0.4126593768596649,
      "learning_rate": 0.00025341588385994877,
      "loss": 7.6787,
      "step": 3466
    },
    {
      "epoch": 0.9870462633451957,
      "grad_norm": 0.5532855987548828,
      "learning_rate": 0.0002533447196128665,
      "loss": 7.2588,
      "step": 3467
    },
    {
      "epoch": 0.9873309608540926,
      "grad_norm": 0.5864046216011047,
      "learning_rate": 0.0002532735553657842,
      "loss": 7.0371,
      "step": 3468
    },
    {
      "epoch": 0.9876156583629894,
      "grad_norm": 0.6067349314689636,
      "learning_rate": 0.000253202391118702,
      "loss": 7.3086,
      "step": 3469
    },
    {
      "epoch": 0.9879003558718861,
      "grad_norm": 0.4911758005619049,
      "learning_rate": 0.0002531312268716197,
      "loss": 7.832,
      "step": 3470
    },
    {
      "epoch": 0.9881850533807829,
      "grad_norm": 0.5602343678474426,
      "learning_rate": 0.00025306006262453744,
      "loss": 7.4053,
      "step": 3471
    },
    {
      "epoch": 0.9884697508896797,
      "grad_norm": 0.7905860543251038,
      "learning_rate": 0.00025298889837745516,
      "loss": 6.458,
      "step": 3472
    },
    {
      "epoch": 0.9887544483985765,
      "grad_norm": 0.5236520171165466,
      "learning_rate": 0.0002529177341303729,
      "loss": 7.1016,
      "step": 3473
    },
    {
      "epoch": 0.9890391459074733,
      "grad_norm": 0.5640308260917664,
      "learning_rate": 0.0002528465698832906,
      "loss": 7.2969,
      "step": 3474
    },
    {
      "epoch": 0.9893238434163701,
      "grad_norm": 0.5552005767822266,
      "learning_rate": 0.0002527754056362084,
      "loss": 7.4229,
      "step": 3475
    },
    {
      "epoch": 0.989608540925267,
      "grad_norm": 0.5474613904953003,
      "learning_rate": 0.0002527042413891261,
      "loss": 7.0918,
      "step": 3476
    },
    {
      "epoch": 0.9898932384341637,
      "grad_norm": 0.5024515986442566,
      "learning_rate": 0.0002526330771420439,
      "loss": 8.0225,
      "step": 3477
    },
    {
      "epoch": 0.9901779359430605,
      "grad_norm": 0.4590266942977905,
      "learning_rate": 0.00025256191289496155,
      "loss": 8.167,
      "step": 3478
    },
    {
      "epoch": 0.9904626334519573,
      "grad_norm": 0.5763940811157227,
      "learning_rate": 0.0002524907486478793,
      "loss": 7.2139,
      "step": 3479
    },
    {
      "epoch": 0.9907473309608541,
      "grad_norm": 0.6008351445198059,
      "learning_rate": 0.00025241958440079705,
      "loss": 6.7363,
      "step": 3480
    },
    {
      "epoch": 0.9910320284697509,
      "grad_norm": 0.538310706615448,
      "learning_rate": 0.0002523484201537148,
      "loss": 7.5693,
      "step": 3481
    },
    {
      "epoch": 0.9913167259786477,
      "grad_norm": 0.5482929944992065,
      "learning_rate": 0.00025227725590663255,
      "loss": 7.3203,
      "step": 3482
    },
    {
      "epoch": 0.9916014234875444,
      "grad_norm": 0.5539014935493469,
      "learning_rate": 0.0002522060916595503,
      "loss": 7.2412,
      "step": 3483
    },
    {
      "epoch": 0.9918861209964412,
      "grad_norm": 0.4967955946922302,
      "learning_rate": 0.00025213492741246794,
      "loss": 7.9092,
      "step": 3484
    },
    {
      "epoch": 0.9921708185053381,
      "grad_norm": 0.525306761264801,
      "learning_rate": 0.0002520637631653857,
      "loss": 7.5791,
      "step": 3485
    },
    {
      "epoch": 0.9924555160142349,
      "grad_norm": 0.6535788178443909,
      "learning_rate": 0.00025199259891830344,
      "loss": 6.918,
      "step": 3486
    },
    {
      "epoch": 0.9927402135231317,
      "grad_norm": 0.6987755298614502,
      "learning_rate": 0.0002519214346712212,
      "loss": 7.2949,
      "step": 3487
    },
    {
      "epoch": 0.9930249110320285,
      "grad_norm": 0.5158640146255493,
      "learning_rate": 0.00025185027042413894,
      "loss": 8.0547,
      "step": 3488
    },
    {
      "epoch": 0.9933096085409253,
      "grad_norm": 0.5028722882270813,
      "learning_rate": 0.0002517791061770566,
      "loss": 7.8066,
      "step": 3489
    },
    {
      "epoch": 0.993594306049822,
      "grad_norm": 0.4740253686904907,
      "learning_rate": 0.0002517079419299744,
      "loss": 7.7764,
      "step": 3490
    },
    {
      "epoch": 0.9938790035587188,
      "grad_norm": 0.4868961274623871,
      "learning_rate": 0.0002516367776828921,
      "loss": 7.748,
      "step": 3491
    },
    {
      "epoch": 0.9941637010676156,
      "grad_norm": 0.492053359746933,
      "learning_rate": 0.00025156561343580983,
      "loss": 7.7139,
      "step": 3492
    },
    {
      "epoch": 0.9944483985765125,
      "grad_norm": 0.6299134492874146,
      "learning_rate": 0.0002514944491887276,
      "loss": 7.041,
      "step": 3493
    },
    {
      "epoch": 0.9947330960854093,
      "grad_norm": 0.599029541015625,
      "learning_rate": 0.00025142328494164533,
      "loss": 6.8984,
      "step": 3494
    },
    {
      "epoch": 0.9950177935943061,
      "grad_norm": 0.5068178772926331,
      "learning_rate": 0.00025135212069456306,
      "loss": 7.5723,
      "step": 3495
    },
    {
      "epoch": 0.9953024911032029,
      "grad_norm": 0.5154197812080383,
      "learning_rate": 0.0002512809564474808,
      "loss": 7.416,
      "step": 3496
    },
    {
      "epoch": 0.9955871886120996,
      "grad_norm": 0.5553001761436462,
      "learning_rate": 0.0002512097922003985,
      "loss": 7.1885,
      "step": 3497
    },
    {
      "epoch": 0.9958718861209964,
      "grad_norm": 0.5081984400749207,
      "learning_rate": 0.0002511386279533163,
      "loss": 7.6357,
      "step": 3498
    },
    {
      "epoch": 0.9961565836298932,
      "grad_norm": 0.4515342116355896,
      "learning_rate": 0.000251067463706234,
      "loss": 7.9951,
      "step": 3499
    },
    {
      "epoch": 0.99644128113879,
      "grad_norm": 0.43641579151153564,
      "learning_rate": 0.0002509962994591517,
      "loss": 8.1104,
      "step": 3500
    },
    {
      "epoch": 0.9967259786476869,
      "grad_norm": 0.6224738359451294,
      "learning_rate": 0.00025092513521206945,
      "loss": 7.3115,
      "step": 3501
    },
    {
      "epoch": 0.9970106761565837,
      "grad_norm": 0.45478391647338867,
      "learning_rate": 0.00025085397096498717,
      "loss": 8.0986,
      "step": 3502
    },
    {
      "epoch": 0.9972953736654804,
      "grad_norm": 0.49814069271087646,
      "learning_rate": 0.00025078280671790495,
      "loss": 7.1494,
      "step": 3503
    },
    {
      "epoch": 0.9975800711743772,
      "grad_norm": 0.5863118767738342,
      "learning_rate": 0.00025071164247082267,
      "loss": 7.584,
      "step": 3504
    },
    {
      "epoch": 0.997864768683274,
      "grad_norm": 0.49493587017059326,
      "learning_rate": 0.00025064047822374045,
      "loss": 7.625,
      "step": 3505
    },
    {
      "epoch": 0.9981494661921708,
      "grad_norm": 0.4839777648448944,
      "learning_rate": 0.0002505693139766581,
      "loss": 7.8223,
      "step": 3506
    },
    {
      "epoch": 0.9984341637010676,
      "grad_norm": 0.5564265251159668,
      "learning_rate": 0.00025049814972957584,
      "loss": 7.2266,
      "step": 3507
    },
    {
      "epoch": 0.9987188612099644,
      "grad_norm": 0.5708491802215576,
      "learning_rate": 0.0002504269854824936,
      "loss": 7.4658,
      "step": 3508
    },
    {
      "epoch": 0.9990035587188613,
      "grad_norm": 0.5286405086517334,
      "learning_rate": 0.00025035582123541134,
      "loss": 7.4121,
      "step": 3509
    },
    {
      "epoch": 0.999288256227758,
      "grad_norm": 0.5082256197929382,
      "learning_rate": 0.00025028465698832906,
      "loss": 7.7734,
      "step": 3510
    },
    {
      "epoch": 0.9995729537366548,
      "grad_norm": 0.56646728515625,
      "learning_rate": 0.00025021349274124684,
      "loss": 7.207,
      "step": 3511
    },
    {
      "epoch": 0.9998576512455516,
      "grad_norm": 0.5469280481338501,
      "learning_rate": 0.0002501423284941645,
      "loss": 7.6543,
      "step": 3512
    },
    {
      "epoch": 1.0002846975088968,
      "grad_norm": 1.0959484577178955,
      "learning_rate": 0.0002500711642470823,
      "loss": 14.6914,
      "step": 3513
    },
    {
      "epoch": 1.0005693950177936,
      "grad_norm": 0.5685948133468628,
      "learning_rate": 0.00025,
      "loss": 7.4209,
      "step": 3514
    },
    {
      "epoch": 1.0008540925266904,
      "grad_norm": 0.5061274170875549,
      "learning_rate": 0.00024992883575291773,
      "loss": 7.8008,
      "step": 3515
    },
    {
      "epoch": 1.0011387900355873,
      "grad_norm": 0.5230201482772827,
      "learning_rate": 0.00024985767150583545,
      "loss": 7.5107,
      "step": 3516
    },
    {
      "epoch": 1.001423487544484,
      "grad_norm": 0.4669858515262604,
      "learning_rate": 0.00024978650725875323,
      "loss": 7.7695,
      "step": 3517
    },
    {
      "epoch": 1.0017081850533809,
      "grad_norm": 0.48713991045951843,
      "learning_rate": 0.00024971534301167095,
      "loss": 7.2832,
      "step": 3518
    },
    {
      "epoch": 1.0019928825622775,
      "grad_norm": 0.5037928223609924,
      "learning_rate": 0.0002496441787645887,
      "loss": 7.8096,
      "step": 3519
    },
    {
      "epoch": 1.0022775800711743,
      "grad_norm": 0.724045991897583,
      "learning_rate": 0.0002495730145175064,
      "loss": 6.9961,
      "step": 3520
    },
    {
      "epoch": 1.002562277580071,
      "grad_norm": 0.5110613107681274,
      "learning_rate": 0.0002495018502704242,
      "loss": 7.4873,
      "step": 3521
    },
    {
      "epoch": 1.002846975088968,
      "grad_norm": 0.4811145067214966,
      "learning_rate": 0.0002494306860233419,
      "loss": 7.6855,
      "step": 3522
    },
    {
      "epoch": 1.0031316725978647,
      "grad_norm": 0.5713363289833069,
      "learning_rate": 0.0002493595217762596,
      "loss": 7.7285,
      "step": 3523
    },
    {
      "epoch": 1.0034163701067615,
      "grad_norm": 0.6275967359542847,
      "learning_rate": 0.00024928835752917734,
      "loss": 7.1826,
      "step": 3524
    },
    {
      "epoch": 1.0037010676156584,
      "grad_norm": 0.8799850344657898,
      "learning_rate": 0.00024921719328209506,
      "loss": 7.8008,
      "step": 3525
    },
    {
      "epoch": 1.0039857651245552,
      "grad_norm": 0.6357625722885132,
      "learning_rate": 0.00024914602903501284,
      "loss": 6.7178,
      "step": 3526
    },
    {
      "epoch": 1.004270462633452,
      "grad_norm": 0.5419339537620544,
      "learning_rate": 0.00024907486478793056,
      "loss": 7.4072,
      "step": 3527
    },
    {
      "epoch": 1.0045551601423488,
      "grad_norm": 0.7701615691184998,
      "learning_rate": 0.0002490037005408483,
      "loss": 6.708,
      "step": 3528
    },
    {
      "epoch": 1.0048398576512456,
      "grad_norm": 0.5513796210289001,
      "learning_rate": 0.000248932536293766,
      "loss": 7.4912,
      "step": 3529
    },
    {
      "epoch": 1.0051245551601424,
      "grad_norm": 0.6493164300918579,
      "learning_rate": 0.00024886137204668373,
      "loss": 6.9268,
      "step": 3530
    },
    {
      "epoch": 1.0054092526690392,
      "grad_norm": 0.33738937973976135,
      "learning_rate": 0.0002487902077996015,
      "loss": 8.3643,
      "step": 3531
    },
    {
      "epoch": 1.0056939501779358,
      "grad_norm": 0.636280357837677,
      "learning_rate": 0.00024871904355251923,
      "loss": 7.6289,
      "step": 3532
    },
    {
      "epoch": 1.0059786476868326,
      "grad_norm": 0.539933979511261,
      "learning_rate": 0.00024864787930543695,
      "loss": 7.4238,
      "step": 3533
    },
    {
      "epoch": 1.0062633451957295,
      "grad_norm": 0.5195510983467102,
      "learning_rate": 0.0002485767150583547,
      "loss": 7.6064,
      "step": 3534
    },
    {
      "epoch": 1.0065480427046263,
      "grad_norm": 0.6853159070014954,
      "learning_rate": 0.00024850555081127245,
      "loss": 6.8496,
      "step": 3535
    },
    {
      "epoch": 1.006832740213523,
      "grad_norm": 0.5342375636100769,
      "learning_rate": 0.0002484343865641902,
      "loss": 7.1133,
      "step": 3536
    },
    {
      "epoch": 1.00711743772242,
      "grad_norm": 0.5048784613609314,
      "learning_rate": 0.0002483632223171079,
      "loss": 8.0225,
      "step": 3537
    },
    {
      "epoch": 1.0074021352313167,
      "grad_norm": 0.6147692203521729,
      "learning_rate": 0.0002482920580700256,
      "loss": 7.1533,
      "step": 3538
    },
    {
      "epoch": 1.0076868327402135,
      "grad_norm": 0.5295256972312927,
      "learning_rate": 0.00024822089382294335,
      "loss": 7.2373,
      "step": 3539
    },
    {
      "epoch": 1.0079715302491103,
      "grad_norm": 0.5749496221542358,
      "learning_rate": 0.0002481497295758611,
      "loss": 7.1797,
      "step": 3540
    },
    {
      "epoch": 1.0082562277580072,
      "grad_norm": 0.48676878213882446,
      "learning_rate": 0.0002480785653287788,
      "loss": 8.1338,
      "step": 3541
    },
    {
      "epoch": 1.008540925266904,
      "grad_norm": 0.5208123922348022,
      "learning_rate": 0.00024800740108169657,
      "loss": 7.2861,
      "step": 3542
    },
    {
      "epoch": 1.0088256227758008,
      "grad_norm": 0.5697729587554932,
      "learning_rate": 0.0002479362368346143,
      "loss": 7.3193,
      "step": 3543
    },
    {
      "epoch": 1.0091103202846976,
      "grad_norm": 0.6704827547073364,
      "learning_rate": 0.000247865072587532,
      "loss": 7.0,
      "step": 3544
    },
    {
      "epoch": 1.0093950177935942,
      "grad_norm": 0.5593731999397278,
      "learning_rate": 0.0002477939083404498,
      "loss": 7.1074,
      "step": 3545
    },
    {
      "epoch": 1.009679715302491,
      "grad_norm": 0.573114275932312,
      "learning_rate": 0.0002477227440933675,
      "loss": 7.1826,
      "step": 3546
    },
    {
      "epoch": 1.0099644128113878,
      "grad_norm": 0.6244479417800903,
      "learning_rate": 0.00024765157984628524,
      "loss": 7.1855,
      "step": 3547
    },
    {
      "epoch": 1.0102491103202846,
      "grad_norm": 0.5614387392997742,
      "learning_rate": 0.00024758041559920296,
      "loss": 7.5508,
      "step": 3548
    },
    {
      "epoch": 1.0105338078291815,
      "grad_norm": 0.44726595282554626,
      "learning_rate": 0.00024750925135212074,
      "loss": 7.8818,
      "step": 3549
    },
    {
      "epoch": 1.0108185053380783,
      "grad_norm": 0.5710427165031433,
      "learning_rate": 0.0002474380871050384,
      "loss": 7.0938,
      "step": 3550
    },
    {
      "epoch": 1.011103202846975,
      "grad_norm": 0.44075438380241394,
      "learning_rate": 0.0002473669228579562,
      "loss": 8.0625,
      "step": 3551
    },
    {
      "epoch": 1.011387900355872,
      "grad_norm": 0.4880983829498291,
      "learning_rate": 0.0002472957586108739,
      "loss": 7.667,
      "step": 3552
    },
    {
      "epoch": 1.0116725978647687,
      "grad_norm": 0.533961832523346,
      "learning_rate": 0.0002472245943637916,
      "loss": 7.335,
      "step": 3553
    },
    {
      "epoch": 1.0119572953736655,
      "grad_norm": 0.4745534360408783,
      "learning_rate": 0.0002471534301167094,
      "loss": 7.3301,
      "step": 3554
    },
    {
      "epoch": 1.0122419928825623,
      "grad_norm": 0.5261629223823547,
      "learning_rate": 0.00024708226586962707,
      "loss": 7.6494,
      "step": 3555
    },
    {
      "epoch": 1.0125266903914591,
      "grad_norm": 0.46531346440315247,
      "learning_rate": 0.00024701110162254485,
      "loss": 7.5166,
      "step": 3556
    },
    {
      "epoch": 1.012811387900356,
      "grad_norm": 0.6917848587036133,
      "learning_rate": 0.00024693993737546257,
      "loss": 7.4688,
      "step": 3557
    },
    {
      "epoch": 1.0130960854092528,
      "grad_norm": 0.5564698576927185,
      "learning_rate": 0.0002468687731283803,
      "loss": 7.625,
      "step": 3558
    },
    {
      "epoch": 1.0133807829181494,
      "grad_norm": 0.49519097805023193,
      "learning_rate": 0.000246797608881298,
      "loss": 7.4199,
      "step": 3559
    },
    {
      "epoch": 1.0136654804270462,
      "grad_norm": 0.6376436352729797,
      "learning_rate": 0.0002467264446342158,
      "loss": 7.3369,
      "step": 3560
    },
    {
      "epoch": 1.013950177935943,
      "grad_norm": 0.5902950763702393,
      "learning_rate": 0.0002466552803871335,
      "loss": 7.1699,
      "step": 3561
    },
    {
      "epoch": 1.0142348754448398,
      "grad_norm": 0.4746224284172058,
      "learning_rate": 0.00024658411614005124,
      "loss": 7.9365,
      "step": 3562
    },
    {
      "epoch": 1.0145195729537366,
      "grad_norm": 0.6012412905693054,
      "learning_rate": 0.000246512951892969,
      "loss": 7.3916,
      "step": 3563
    },
    {
      "epoch": 1.0148042704626334,
      "grad_norm": 0.470093697309494,
      "learning_rate": 0.0002464417876458867,
      "loss": 7.6416,
      "step": 3564
    },
    {
      "epoch": 1.0150889679715303,
      "grad_norm": 0.4985117018222809,
      "learning_rate": 0.00024637062339880446,
      "loss": 7.167,
      "step": 3565
    },
    {
      "epoch": 1.015373665480427,
      "grad_norm": 0.6013071537017822,
      "learning_rate": 0.0002462994591517222,
      "loss": 7.127,
      "step": 3566
    },
    {
      "epoch": 1.0156583629893239,
      "grad_norm": 0.4950224757194519,
      "learning_rate": 0.0002462282949046399,
      "loss": 7.6758,
      "step": 3567
    },
    {
      "epoch": 1.0159430604982207,
      "grad_norm": 0.6142073273658752,
      "learning_rate": 0.00024615713065755763,
      "loss": 6.8438,
      "step": 3568
    },
    {
      "epoch": 1.0162277580071175,
      "grad_norm": 0.4809626340866089,
      "learning_rate": 0.00024608596641047535,
      "loss": 7.9541,
      "step": 3569
    },
    {
      "epoch": 1.0165124555160143,
      "grad_norm": 0.5955183506011963,
      "learning_rate": 0.00024601480216339313,
      "loss": 7.1943,
      "step": 3570
    },
    {
      "epoch": 1.0167971530249111,
      "grad_norm": 0.5443080067634583,
      "learning_rate": 0.00024594363791631085,
      "loss": 7.6035,
      "step": 3571
    },
    {
      "epoch": 1.0170818505338077,
      "grad_norm": 0.5559050440788269,
      "learning_rate": 0.0002458724736692286,
      "loss": 7.1836,
      "step": 3572
    },
    {
      "epoch": 1.0173665480427045,
      "grad_norm": 0.5922147035598755,
      "learning_rate": 0.0002458013094221463,
      "loss": 6.9434,
      "step": 3573
    },
    {
      "epoch": 1.0176512455516014,
      "grad_norm": 0.4755072593688965,
      "learning_rate": 0.0002457301451750641,
      "loss": 7.7285,
      "step": 3574
    },
    {
      "epoch": 1.0179359430604982,
      "grad_norm": 0.5772145390510559,
      "learning_rate": 0.0002456589809279818,
      "loss": 7.4824,
      "step": 3575
    },
    {
      "epoch": 1.018220640569395,
      "grad_norm": 0.4872186481952667,
      "learning_rate": 0.0002455878166808995,
      "loss": 8.2891,
      "step": 3576
    },
    {
      "epoch": 1.0185053380782918,
      "grad_norm": 0.5396019220352173,
      "learning_rate": 0.00024551665243381724,
      "loss": 7.3301,
      "step": 3577
    },
    {
      "epoch": 1.0187900355871886,
      "grad_norm": 0.5766385793685913,
      "learning_rate": 0.00024544548818673497,
      "loss": 7.1865,
      "step": 3578
    },
    {
      "epoch": 1.0190747330960854,
      "grad_norm": 0.49264559149742126,
      "learning_rate": 0.00024537432393965274,
      "loss": 7.5693,
      "step": 3579
    },
    {
      "epoch": 1.0193594306049822,
      "grad_norm": 0.5567781329154968,
      "learning_rate": 0.00024530315969257047,
      "loss": 7.3818,
      "step": 3580
    },
    {
      "epoch": 1.019644128113879,
      "grad_norm": 0.4682323634624481,
      "learning_rate": 0.0002452319954454882,
      "loss": 7.9639,
      "step": 3581
    },
    {
      "epoch": 1.0199288256227759,
      "grad_norm": 0.5409399271011353,
      "learning_rate": 0.0002451608311984059,
      "loss": 7.3369,
      "step": 3582
    },
    {
      "epoch": 1.0202135231316727,
      "grad_norm": 0.43192851543426514,
      "learning_rate": 0.00024508966695132363,
      "loss": 7.9043,
      "step": 3583
    },
    {
      "epoch": 1.0204982206405695,
      "grad_norm": 0.5941455364227295,
      "learning_rate": 0.0002450185027042414,
      "loss": 7.6162,
      "step": 3584
    },
    {
      "epoch": 1.020782918149466,
      "grad_norm": 0.5848554372787476,
      "learning_rate": 0.00024494733845715913,
      "loss": 7.3438,
      "step": 3585
    },
    {
      "epoch": 1.021067615658363,
      "grad_norm": 1.0311400890350342,
      "learning_rate": 0.00024487617421007686,
      "loss": 7.251,
      "step": 3586
    },
    {
      "epoch": 1.0213523131672597,
      "grad_norm": 0.4415445625782013,
      "learning_rate": 0.0002448050099629946,
      "loss": 8.0674,
      "step": 3587
    },
    {
      "epoch": 1.0216370106761565,
      "grad_norm": 0.4988853931427002,
      "learning_rate": 0.00024473384571591236,
      "loss": 7.457,
      "step": 3588
    },
    {
      "epoch": 1.0219217081850533,
      "grad_norm": 0.5850105881690979,
      "learning_rate": 0.0002446626814688301,
      "loss": 7.3867,
      "step": 3589
    },
    {
      "epoch": 1.0222064056939502,
      "grad_norm": 0.5100435018539429,
      "learning_rate": 0.0002445915172217478,
      "loss": 7.5098,
      "step": 3590
    },
    {
      "epoch": 1.022491103202847,
      "grad_norm": 0.5327336192131042,
      "learning_rate": 0.0002445203529746655,
      "loss": 7.4043,
      "step": 3591
    },
    {
      "epoch": 1.0227758007117438,
      "grad_norm": 0.5508014559745789,
      "learning_rate": 0.00024444918872758325,
      "loss": 7.2715,
      "step": 3592
    },
    {
      "epoch": 1.0230604982206406,
      "grad_norm": 0.5500774383544922,
      "learning_rate": 0.000244378024480501,
      "loss": 7.542,
      "step": 3593
    },
    {
      "epoch": 1.0233451957295374,
      "grad_norm": 0.5295575261116028,
      "learning_rate": 0.00024430686023341875,
      "loss": 6.9893,
      "step": 3594
    },
    {
      "epoch": 1.0236298932384342,
      "grad_norm": 0.47250133752822876,
      "learning_rate": 0.00024423569598633647,
      "loss": 7.6748,
      "step": 3595
    },
    {
      "epoch": 1.023914590747331,
      "grad_norm": 0.6553662419319153,
      "learning_rate": 0.0002441645317392542,
      "loss": 7.2422,
      "step": 3596
    },
    {
      "epoch": 1.0241992882562279,
      "grad_norm": 0.670922577381134,
      "learning_rate": 0.00024409336749217192,
      "loss": 7.041,
      "step": 3597
    },
    {
      "epoch": 1.0244839857651244,
      "grad_norm": 0.48628857731819153,
      "learning_rate": 0.00024402220324508967,
      "loss": 7.4648,
      "step": 3598
    },
    {
      "epoch": 1.0247686832740213,
      "grad_norm": 0.5109262466430664,
      "learning_rate": 0.00024395103899800742,
      "loss": 7.2773,
      "step": 3599
    },
    {
      "epoch": 1.025053380782918,
      "grad_norm": 0.5412747859954834,
      "learning_rate": 0.00024387987475092514,
      "loss": 7.2559,
      "step": 3600
    },
    {
      "epoch": 1.025053380782918,
      "eval_bleu": 0.14168105215306434,
      "eval_loss": 7.203125,
      "eval_runtime": 121.1365,
      "eval_samples_per_second": 2.344,
      "eval_steps_per_second": 0.149,
      "step": 3600
    }
  ],
  "logging_steps": 1,
  "max_steps": 7026,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 200,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 7841803445207040.0,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
