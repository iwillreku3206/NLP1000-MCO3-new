{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a62792e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5183230b593466db8c69a29285420c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "103694d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, M2M100ForConditionalGeneration, AutoModelForCausalLM, BitsAndBytesConfig, AutoModelForSeq2SeqLM, NllbTokenizerFast\n",
    "from tokenization_small100 import SMALL100Tokenizer\n",
    "from peft import LoraModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "485fa950",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=\"bfloat16\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f64b07b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "FAIRSEQ_LANGUAGE_CODES = ['ace_Arab', 'ace_Latn', 'acm_Arab', 'acq_Arab', 'aeb_Arab', 'afr_Latn', 'ajp_Arab', 'aka_Latn', 'amh_Ethi', 'apc_Arab', 'arb_Arab', 'ars_Arab', 'ary_Arab', 'arz_Arab', 'asm_Beng', 'ast_Latn', 'awa_Deva', 'ayr_Latn', 'azb_Arab', 'azj_Latn', 'bak_Cyrl', 'bam_Latn', 'ban_Latn', 'bel_Cyrl', 'bem_Latn', 'ben_Beng', 'bho_Deva', 'bjn_Arab', 'bjn_Latn', 'bod_Tibt', 'bos_Latn', 'bug_Latn', 'bul_Cyrl', 'cat_Latn', 'ceb_Latn', 'ces_Latn', 'cjk_Latn', 'ckb_Arab', 'crh_Latn', 'cym_Latn', 'dan_Latn', 'deu_Latn', 'dik_Latn', 'dyu_Latn', 'dzo_Tibt', 'ell_Grek', 'eng_Latn', 'epo_Latn', 'est_Latn', 'eus_Latn', 'ewe_Latn', 'fao_Latn', 'pes_Arab', 'fij_Latn', 'fin_Latn', 'fon_Latn', 'fra_Latn', 'fur_Latn', 'fuv_Latn', 'gla_Latn', 'gle_Latn', 'glg_Latn', 'grn_Latn', 'guj_Gujr', 'hat_Latn', 'hau_Latn', 'heb_Hebr', 'hin_Deva', 'hne_Deva', 'hrv_Latn', 'hun_Latn', 'hye_Armn', 'ibo_Latn', 'ilo_Latn', 'ind_Latn', 'isl_Latn', 'ita_Latn', 'jav_Latn', 'jpn_Jpan', 'kab_Latn', 'kac_Latn', 'kam_Latn', 'kan_Knda', 'kas_Arab', 'kas_Deva', 'kat_Geor', 'knc_Arab', 'knc_Latn', 'kaz_Cyrl', 'kbp_Latn', 'kea_Latn', 'khm_Khmr', 'kik_Latn', 'kin_Latn', 'kir_Cyrl', 'kmb_Latn', 'kon_Latn', 'kor_Hang', 'kmr_Latn', 'lao_Laoo', 'lvs_Latn', 'lij_Latn', 'lim_Latn', 'lin_Latn', 'lit_Latn', 'lmo_Latn', 'ltg_Latn', 'ltz_Latn', 'lua_Latn', 'lug_Latn', 'luo_Latn', 'lus_Latn', 'mag_Deva', 'mai_Deva', 'mal_Mlym', 'mar_Deva', 'min_Latn', 'mkd_Cyrl', 'plt_Latn', 'mlt_Latn', 'mni_Beng', 'khk_Cyrl', 'mos_Latn', 'mri_Latn', 'zsm_Latn', 'mya_Mymr', 'nld_Latn', 'nno_Latn', 'nob_Latn', 'npi_Deva', 'nso_Latn', 'nus_Latn', 'nya_Latn', 'oci_Latn', 'gaz_Latn', 'ory_Orya', 'pag_Latn', 'pan_Guru', 'pap_Latn', 'pol_Latn', 'por_Latn', 'prs_Arab', 'pbt_Arab', 'quy_Latn', 'ron_Latn', 'run_Latn', 'rus_Cyrl', 'sag_Latn', 'san_Deva', 'sat_Beng', 'scn_Latn', 'shn_Mymr', 'sin_Sinh', 'slk_Latn', 'slv_Latn', 'smo_Latn', 'sna_Latn', 'snd_Arab', 'som_Latn', 'sot_Latn', 'spa_Latn', 'als_Latn', 'srd_Latn', 'srp_Cyrl', 'ssw_Latn', 'sun_Latn', 'swe_Latn', 'swh_Latn', 'szl_Latn', 'tam_Taml', 'tat_Cyrl', 'tel_Telu', 'tgk_Cyrl', 'tgl_Latn', 'tha_Thai', 'tir_Ethi', 'taq_Latn', 'taq_Tfng', 'tpi_Latn', 'tsn_Latn', 'tso_Latn', 'tuk_Latn', 'tum_Latn', 'tur_Latn', 'twi_Latn', 'tzm_Tfng', 'uig_Arab', 'ukr_Cyrl', 'umb_Latn', 'urd_Arab', 'uzn_Latn', 'vec_Latn', 'vie_Latn', 'war_Latn', 'wol_Latn', 'xho_Latn', 'ydd_Hebr', 'yor_Latn', 'yue_Hant', 'zho_Hans', 'zho_Hant', 'zul_Latn']  # fmt: skip\n",
    "\n",
    "tokenizer = NllbTokenizerFast.from_pretrained(\"facebook/nllb-200-distilled-600M\",\n",
    "                                                src_lang=\"eng_Latn\",\n",
    "                                                tgt_lang=\"bicol\",)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-600M\", quantization_config=bnb_config)\n",
    "\n",
    "tokenizer.add_special_tokens({\"additional_special_tokens\": ['bicol']}, replace_additional_special_tokens=False)\n",
    "\n",
    "#tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b7a89765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,769,472 || all params: 616,843,264 || trainable%: 0.2869\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\"],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_2_SEQ_LM\",\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b80da49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer(\"How is you day today?\", return_tensors=\"pt\").to(model.device)\n",
    "translated_tokens = model.generate(\n",
    "    **tokens, forced_bos_token_id=tokenizer.convert_tokens_to_ids(\"tgl_Latn\"), max_length=30,\n",
    ")\n",
    "text = tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c8464a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Kumusta ka sa araw ngayon?']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "46495d35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05c9012213d54580b795c434410fb038",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/28100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15d47e2bc15b40e095e04a2d8789a84e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/284 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datasets\n",
    "\n",
    "parallel_corpora = pd.read_csv(\"english-to-bicol-corpora.csv\")\n",
    "\n",
    "def preprocess(batch):\n",
    "    model_inputs = tokenizer(\n",
    "        batch[\"language1_text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "        \n",
    "    )\n",
    "\n",
    "    labels = tokenizer(\n",
    "        batch[\"language2_text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "    )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "corpora = parallel_corpora.sample(frac=1, random_state=42)\n",
    "train_df = corpora.sample(frac=0.99, random_state=42)\n",
    "eval_df = corpora.drop(train_df.index).reset_index(drop=True)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "\n",
    "train_dataset = datasets.Dataset.from_pandas(train_df)\n",
    "eval_dataset = datasets.Dataset.from_pandas(eval_df)\n",
    "\n",
    "train_dataset_processed = train_dataset.map(preprocess, batched=True, remove_columns=['language1_text', 'language2_text'])\n",
    "eval_dataset_processed = eval_dataset.map(preprocess, batched=True, remove_columns=['language1_text', 'language2_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "07057887",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"trained-nllb-en-to-bicol\",\n",
    "    learning_rate=5e-4,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,\n",
    "    load_best_model_at_end=True,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=1,\n",
    "    logging_first_step=True,\n",
    "    report_to=\"none\",\n",
    "    predict_with_generate=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7d33539b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rek\\AppData\\Local\\Temp\\ipykernel_26412\\4143228653.py:67: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "from peft import LoraConfig\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "gl_eval_pred = None\n",
    "\n",
    "#def compute_metrics(eval_pred):\n",
    "#    logits, labels = eval_pred\n",
    "#    # If logits are None (some eval configurations), return empty dict\n",
    "#    if logits is None:\n",
    "#        return {}\n",
    "#    # Convert logits to predicted token ids. Logits may be (batch, seq, vocab)\n",
    "#    preds = np.argmax(logits, axis=-1)\n",
    "#    labels = np.array(labels)\n",
    "#    labels_str = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "#    preds = np.array(preds)\n",
    "#    # Mask out label padding (we use -100 for padding labels). Only keep positions where label != -100\n",
    "#    mask = labels != -100\n",
    "#    if mask.sum() == 0:\n",
    "#        return {}\n",
    "#    preds_flat = preds[mask]\n",
    "#    labels_flat = labels[mask]\n",
    "#\n",
    "#    pred_str = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "#    bleu = bleu.compute(predictions=pred_str, references=[[l] for l in labels_str])\n",
    "#    return {\n",
    "#        \"accuracy\": metric.compute(predictions=preds_flat.astype(np.int32), references=labels_flat.astype(np.int32)),\n",
    "#        \"bleu\": bleu[\"bleu\"],\n",
    "#    }\n",
    "\n",
    "gl_decoded_preds = None\n",
    "gl_decoded_labels = None\n",
    "gl_eval_preds = None\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    global gl_eval_preds\n",
    "    gl_eval_preds = eval_preds\n",
    "    preds, labels = eval_preds\n",
    "\n",
    "    labels = np.where(labels == -100, tokenizer.pad_token_id, labels)\n",
    "\n",
    "    if isinstance(preds, torch.Tensor):\n",
    "        preds = preds.tolist()\n",
    "    if isinstance(labels, torch.Tensor):\n",
    "        labels = labels.tolist()\n",
    "    # Convert token IDs to text\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    global gl_decoded_preds, gl_decoded_labels\n",
    "    gl_decoded_preds = decoded_preds\n",
    "    gl_decoded_labels = decoded_labels\n",
    "\n",
    "    # sacrebleu expects list of predictions, list of list of references\n",
    "    result = bleu.compute(predictions=decoded_preds,\n",
    "                            references=[[l] for l in decoded_labels])\n",
    "    gl_eval_preds = result\n",
    "    return {\"bleu\": result[\"bleu\"]}\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset_processed,\n",
    "    eval_dataset=eval_dataset_processed,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "401d2637",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7025' max='7026' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7025/7026 2:43:11 < 00:01, 0.64 it/s, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>7.585900</td>\n",
       "      <td>7.042969</td>\n",
       "      <td>0.101965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>7.150400</td>\n",
       "      <td>7.050781</td>\n",
       "      <td>0.100378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>7.145500</td>\n",
       "      <td>7.070312</td>\n",
       "      <td>0.108067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>7.180700</td>\n",
       "      <td>7.085938</td>\n",
       "      <td>0.109990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>6.995100</td>\n",
       "      <td>7.097656</td>\n",
       "      <td>0.108344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>7.554700</td>\n",
       "      <td>7.113281</td>\n",
       "      <td>0.122409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>7.091800</td>\n",
       "      <td>7.132812</td>\n",
       "      <td>0.132876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>7.690400</td>\n",
       "      <td>7.140625</td>\n",
       "      <td>0.130888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>7.651400</td>\n",
       "      <td>7.156250</td>\n",
       "      <td>0.131196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>7.704100</td>\n",
       "      <td>7.179688</td>\n",
       "      <td>0.136501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>7.886700</td>\n",
       "      <td>7.187500</td>\n",
       "      <td>0.141153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>7.796900</td>\n",
       "      <td>7.191406</td>\n",
       "      <td>0.132582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>7.325200</td>\n",
       "      <td>7.218750</td>\n",
       "      <td>0.144023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>7.255900</td>\n",
       "      <td>7.203125</td>\n",
       "      <td>0.141681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>7.509800</td>\n",
       "      <td>7.210938</td>\n",
       "      <td>0.146029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>7.298800</td>\n",
       "      <td>7.230469</td>\n",
       "      <td>0.150024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>7.782200</td>\n",
       "      <td>7.242188</td>\n",
       "      <td>0.146459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>8.037100</td>\n",
       "      <td>7.257812</td>\n",
       "      <td>0.155108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>7.615200</td>\n",
       "      <td>7.273438</td>\n",
       "      <td>0.147909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>7.363300</td>\n",
       "      <td>7.289062</td>\n",
       "      <td>0.145019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>7.936500</td>\n",
       "      <td>7.296875</td>\n",
       "      <td>0.154166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>7.542000</td>\n",
       "      <td>7.296875</td>\n",
       "      <td>0.157178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>7.578100</td>\n",
       "      <td>7.304688</td>\n",
       "      <td>0.160467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>7.565400</td>\n",
       "      <td>7.308594</td>\n",
       "      <td>0.155795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>7.869100</td>\n",
       "      <td>7.316406</td>\n",
       "      <td>0.158415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>7.879900</td>\n",
       "      <td>7.320312</td>\n",
       "      <td>0.160151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>7.194300</td>\n",
       "      <td>7.324219</td>\n",
       "      <td>0.161544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>8.251000</td>\n",
       "      <td>7.328125</td>\n",
       "      <td>0.159099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>7.373000</td>\n",
       "      <td>7.332031</td>\n",
       "      <td>0.160380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>7.725600</td>\n",
       "      <td>7.335938</td>\n",
       "      <td>0.160064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>7.019500</td>\n",
       "      <td>7.335938</td>\n",
       "      <td>0.160160</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7025, training_loss=6.627189098643238, metrics={'train_runtime': 9792.7594, 'train_samples_per_second': 5.739, 'train_steps_per_second': 0.717, 'total_flos': 1.5300283072512e+16, 'train_loss': 6.627189098643238, 'epoch': 2.0})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "276365fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"nllb-en-to-bicol-seq2seq-model\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b5b4d6f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[', asa si?']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer(\"Where are you?\", return_tensors=\"pt\").to(model.device)\n",
    "translated_tokens = trainer.model.generate(\n",
    "    **tokens, forced_bos_token_id=tokenizer.convert_tokens_to_ids(\"bicol\"), max_length=30,\n",
    ")\n",
    "text = tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dd45eae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_tokens = trainer.model.generate(**tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f9a07948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[     2, 256057,  32045,    287,    133, 169975,    385,      2]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
